{"repo": "facebook/react", "issue_id": 145259386, "issue_number": 6394, "timestamp": "2016-04-01T19:01:31Z", "text": "Homepage URL for package.json", "context": "<issue_start><issue_comment>Title: Update package.json\nusername_0: Homepage URL for package.json\n<issue_comment>username_1: That's a private package that will never be published so I don't think there's any value in having this. The real package that gets published to npm does have this set (https://github.com/facebook/react/blob/master/packages/react/package.json#L8). Thanks though!"}
{"repo": "facebook/react", "issue_id": 145259386, "issue_number": 6394, "timestamp": "2016-04-01 19:06:27+00:00", "text": "That's a private package that will never be published so I don't think there's any value in having this. The real package that gets published to npm does have this set (https://github.com/facebook/react/blob/master/packages/react/package.json#L8). Thanks though!", "context": "<issue_start><issue_comment>Title: Update package.json\nusername_0: Homepage URL for package.json\n<issue_comment>username_1: That's a private package that will never be published so I don't think there's any value in having this. The real package that gets published to npm does have this set (https://github.com/facebook/react/blob/master/packages/react/package.json#L8). Thanks though!"}
{"repo": "facebook/react", "issue_id": 186222382, "issue_number": 8160, "timestamp": "2016-10-31T07:40:15Z", "text": "We should test $$typeof field against REACT_ELEMENT_TYPE to check if something is a React Element.", "context": "<issue_start><issue_comment>Title: Fix docs on ReactElement\nusername_0: We should test $$typeof field against REACT_ELEMENT_TYPE to check if something is a React Element.\n<issue_comment>username_1: Thanks for the PR but I believe the original is clearer. REACT_ELEMENT_TYPE [is](https://github.com/username_0/react/blob/bedbc5f066b7a45520ab78ecb806ab557485535a/src/shared/utils/ReactElementSymbol.js#L18) Symbol.for('react.element'). Cheers!\n<issue_comment>username_0: REACT_ELEMENT_TYPE is a plain number if there is no native Symbol nor polyfill.\n<issue_comment>username_1: Right, but this is a fallback case and doesn't seem as important to document here to me.\r\nMaybe could add \"or a magic number as a fallback\".\n<issue_comment>username_0: Got it. Then I'll just ignore it. Thanks."}
{"repo": "facebook/react", "issue_id": 186222382, "issue_number": 8160, "timestamp": "2016-10-31 11:40:58+00:00", "text": "Thanks for the PR but I believe the original is clearer. REACT_ELEMENT_TYPE [is](https://github.com/username_0/react/blob/bedbc5f066b7a45520ab78ecb806ab557485535a/src/shared/utils/ReactElementSymbol.js#L18) Symbol.for('react.element'). Cheers!", "context": "<issue_start><issue_comment>Title: Fix docs on ReactElement\nusername_0: We should test $$typeof field against REACT_ELEMENT_TYPE to check if something is a React Element.\n<issue_comment>username_1: Thanks for the PR but I believe the original is clearer. REACT_ELEMENT_TYPE [is](https://github.com/username_0/react/blob/bedbc5f066b7a45520ab78ecb806ab557485535a/src/shared/utils/ReactElementSymbol.js#L18) Symbol.for('react.element'). Cheers!\n<issue_comment>username_0: REACT_ELEMENT_TYPE is a plain number if there is no native Symbol nor polyfill.\n<issue_comment>username_1: Right, but this is a fallback case and doesn't seem as important to document here to me.\r\nMaybe could add \"or a magic number as a fallback\".\n<issue_comment>username_0: Got it. Then I'll just ignore it. Thanks."}
{"repo": "facebook/react", "issue_id": 186222382, "issue_number": 8160, "timestamp": "2016-10-31 11:49:50+00:00", "text": "REACT_ELEMENT_TYPE is a plain number if there is no native Symbol nor polyfill.", "context": "<issue_start><issue_comment>Title: Fix docs on ReactElement\nusername_0: We should test $$typeof field against REACT_ELEMENT_TYPE to check if something is a React Element.\n<issue_comment>username_1: Thanks for the PR but I believe the original is clearer. REACT_ELEMENT_TYPE [is](https://github.com/username_0/react/blob/bedbc5f066b7a45520ab78ecb806ab557485535a/src/shared/utils/ReactElementSymbol.js#L18) Symbol.for('react.element'). Cheers!\n<issue_comment>username_0: REACT_ELEMENT_TYPE is a plain number if there is no native Symbol nor polyfill.\n<issue_comment>username_1: Right, but this is a fallback case and doesn't seem as important to document here to me.\r\nMaybe could add \"or a magic number as a fallback\".\n<issue_comment>username_0: Got it. Then I'll just ignore it. Thanks."}
{"repo": "facebook/react", "issue_id": 186222382, "issue_number": 8160, "timestamp": "2016-10-31 11:52:19+00:00", "text": "Right, but this is a fallback case and doesn't seem as important to document here to me.\r\nMaybe could add \"or a magic number as a fallback\".", "context": "<issue_start><issue_comment>Title: Fix docs on ReactElement\nusername_0: We should test $$typeof field against REACT_ELEMENT_TYPE to check if something is a React Element.\n<issue_comment>username_1: Thanks for the PR but I believe the original is clearer. REACT_ELEMENT_TYPE [is](https://github.com/username_0/react/blob/bedbc5f066b7a45520ab78ecb806ab557485535a/src/shared/utils/ReactElementSymbol.js#L18) Symbol.for('react.element'). Cheers!\n<issue_comment>username_0: REACT_ELEMENT_TYPE is a plain number if there is no native Symbol nor polyfill.\n<issue_comment>username_1: Right, but this is a fallback case and doesn't seem as important to document here to me.\r\nMaybe could add \"or a magic number as a fallback\".\n<issue_comment>username_0: Got it. Then I'll just ignore it. Thanks."}
{"repo": "facebook/react", "issue_id": 186222382, "issue_number": 8160, "timestamp": "2016-10-31 12:02:41+00:00", "text": "Got it. Then I'll just ignore it. Thanks.", "context": "<issue_start><issue_comment>Title: Fix docs on ReactElement\nusername_0: We should test $$typeof field against REACT_ELEMENT_TYPE to check if something is a React Element.\n<issue_comment>username_1: Thanks for the PR but I believe the original is clearer. REACT_ELEMENT_TYPE [is](https://github.com/username_0/react/blob/bedbc5f066b7a45520ab78ecb806ab557485535a/src/shared/utils/ReactElementSymbol.js#L18) Symbol.for('react.element'). Cheers!\n<issue_comment>username_0: REACT_ELEMENT_TYPE is a plain number if there is no native Symbol nor polyfill.\n<issue_comment>username_1: Right, but this is a fallback case and doesn't seem as important to document here to me.\r\nMaybe could add \"or a magic number as a fallback\".\n<issue_comment>username_0: Got it. Then I'll just ignore it. Thanks."}
{"repo": "pytorch/pytorch", "issue_id": 239293180, "issue_number": 1934, "timestamp": "2017-06-28T20:52:50Z", "text": "Fix #1922.", "context": "<issue_start><issue_comment>Title: fix torch.is_tensor not recognizing HalfTensor\nusername_0: Fix #1922.\n<issue_comment>username_1: This needs some additional changes in the tests\n<issue_comment>username_0: @username_1 Hmm now I am bit curious about the reasons for the failure of the test cases. It says that ne and fill are not implemented for the HalfTensor. From from the source code I think we are actually generating those methods since there isn't any restriction in the cwrap definitions?\n<issue_comment>username_2: @username_0 CPU HalfTensor has no methods implemented except copy and serialization stuff. So almost all tests have to be filtered out for CPU HalfTensor\n<issue_comment>username_3: what we do in TH is basically define an \"is_tensor\" and a \"has_tensor_math\"; then you write the tests based on which of those you are actually testing.\n<issue_comment>username_0: @username_2 so I took a look at `test_torch.py`, it looks like the only reference to tensor_classes is in test_print. For other tests usually they are self-contained about which types they are testing against.\n<issue_comment>username_4: @pytorchbot add to whitelist\n<issue_comment>username_0: After fixing this issue. I wonder why it would be a good idea to have a CPU HalfTensor at all. Since HalfTensor is a subtype of Tensor, we would expect it to support the same interface as Tensor. However, the current implementation of HalfTensor does not allow such behaviour.\n<issue_comment>username_2: on the CPU, HalfTensor is just there for serialization purposes. For example saving a GPU HalfTensor and being able to load the weights back on a CPU-only machine.\r\n\r\nOther than that it has no purpose, as Half is not a native type on CPU and all operations will be very very very slow."}
{"repo": "pytorch/pytorch", "issue_id": 239293180, "issue_number": 1934, "timestamp": "2017-06-28 23:46:38+00:00", "text": "This needs some additional changes in the tests", "context": "<issue_start><issue_comment>Title: fix torch.is_tensor not recognizing HalfTensor\nusername_0: Fix #1922.\n<issue_comment>username_1: This needs some additional changes in the tests\n<issue_comment>username_0: @username_1 Hmm now I am bit curious about the reasons for the failure of the test cases. It says that ne and fill are not implemented for the HalfTensor. From from the source code I think we are actually generating those methods since there isn't any restriction in the cwrap definitions?\n<issue_comment>username_2: @username_0 CPU HalfTensor has no methods implemented except copy and serialization stuff. So almost all tests have to be filtered out for CPU HalfTensor\n<issue_comment>username_3: what we do in TH is basically define an \"is_tensor\" and a \"has_tensor_math\"; then you write the tests based on which of those you are actually testing.\n<issue_comment>username_0: @username_2 so I took a look at `test_torch.py`, it looks like the only reference to tensor_classes is in test_print. For other tests usually they are self-contained about which types they are testing against.\n<issue_comment>username_4: @pytorchbot add to whitelist\n<issue_comment>username_0: After fixing this issue. I wonder why it would be a good idea to have a CPU HalfTensor at all. Since HalfTensor is a subtype of Tensor, we would expect it to support the same interface as Tensor. However, the current implementation of HalfTensor does not allow such behaviour.\n<issue_comment>username_2: on the CPU, HalfTensor is just there for serialization purposes. For example saving a GPU HalfTensor and being able to load the weights back on a CPU-only machine.\r\n\r\nOther than that it has no purpose, as Half is not a native type on CPU and all operations will be very very very slow."}
{"repo": "pytorch/pytorch", "issue_id": 239293180, "issue_number": 1934, "timestamp": "2017-06-29 19:30:25+00:00", "text": "@username_1 Hmm now I am bit curious about the reasons for the failure of the test cases. It says that ne and fill are not implemented for the HalfTensor. From from the source code I think we are actually generating those methods since there isn't any restriction in the cwrap definitions?", "context": "<issue_start><issue_comment>Title: fix torch.is_tensor not recognizing HalfTensor\nusername_0: Fix #1922.\n<issue_comment>username_1: This needs some additional changes in the tests\n<issue_comment>username_0: @username_1 Hmm now I am bit curious about the reasons for the failure of the test cases. It says that ne and fill are not implemented for the HalfTensor. From from the source code I think we are actually generating those methods since there isn't any restriction in the cwrap definitions?\n<issue_comment>username_2: @username_0 CPU HalfTensor has no methods implemented except copy and serialization stuff. So almost all tests have to be filtered out for CPU HalfTensor\n<issue_comment>username_3: what we do in TH is basically define an \"is_tensor\" and a \"has_tensor_math\"; then you write the tests based on which of those you are actually testing.\n<issue_comment>username_0: @username_2 so I took a look at `test_torch.py`, it looks like the only reference to tensor_classes is in test_print. For other tests usually they are self-contained about which types they are testing against.\n<issue_comment>username_4: @pytorchbot add to whitelist\n<issue_comment>username_0: After fixing this issue. I wonder why it would be a good idea to have a CPU HalfTensor at all. Since HalfTensor is a subtype of Tensor, we would expect it to support the same interface as Tensor. However, the current implementation of HalfTensor does not allow such behaviour.\n<issue_comment>username_2: on the CPU, HalfTensor is just there for serialization purposes. For example saving a GPU HalfTensor and being able to load the weights back on a CPU-only machine.\r\n\r\nOther than that it has no purpose, as Half is not a native type on CPU and all operations will be very very very slow."}
{"repo": "pytorch/pytorch", "issue_id": 239293180, "issue_number": 1934, "timestamp": "2017-06-29 21:07:08+00:00", "text": "@username_0 CPU HalfTensor has no methods implemented except copy and serialization stuff. So almost all tests have to be filtered out for CPU HalfTensor", "context": "<issue_start><issue_comment>Title: fix torch.is_tensor not recognizing HalfTensor\nusername_0: Fix #1922.\n<issue_comment>username_1: This needs some additional changes in the tests\n<issue_comment>username_0: @username_1 Hmm now I am bit curious about the reasons for the failure of the test cases. It says that ne and fill are not implemented for the HalfTensor. From from the source code I think we are actually generating those methods since there isn't any restriction in the cwrap definitions?\n<issue_comment>username_2: @username_0 CPU HalfTensor has no methods implemented except copy and serialization stuff. So almost all tests have to be filtered out for CPU HalfTensor\n<issue_comment>username_3: what we do in TH is basically define an \"is_tensor\" and a \"has_tensor_math\"; then you write the tests based on which of those you are actually testing.\n<issue_comment>username_0: @username_2 so I took a look at `test_torch.py`, it looks like the only reference to tensor_classes is in test_print. For other tests usually they are self-contained about which types they are testing against.\n<issue_comment>username_4: @pytorchbot add to whitelist\n<issue_comment>username_0: After fixing this issue. I wonder why it would be a good idea to have a CPU HalfTensor at all. Since HalfTensor is a subtype of Tensor, we would expect it to support the same interface as Tensor. However, the current implementation of HalfTensor does not allow such behaviour.\n<issue_comment>username_2: on the CPU, HalfTensor is just there for serialization purposes. For example saving a GPU HalfTensor and being able to load the weights back on a CPU-only machine.\r\n\r\nOther than that it has no purpose, as Half is not a native type on CPU and all operations will be very very very slow."}
{"repo": "pytorch/pytorch", "issue_id": 239293180, "issue_number": 1934, "timestamp": "2017-06-29 21:11:07+00:00", "text": "what we do in TH is basically define an \"is_tensor\" and a \"has_tensor_math\"; then you write the tests based on which of those you are actually testing.", "context": "<issue_start><issue_comment>Title: fix torch.is_tensor not recognizing HalfTensor\nusername_0: Fix #1922.\n<issue_comment>username_1: This needs some additional changes in the tests\n<issue_comment>username_0: @username_1 Hmm now I am bit curious about the reasons for the failure of the test cases. It says that ne and fill are not implemented for the HalfTensor. From from the source code I think we are actually generating those methods since there isn't any restriction in the cwrap definitions?\n<issue_comment>username_2: @username_0 CPU HalfTensor has no methods implemented except copy and serialization stuff. So almost all tests have to be filtered out for CPU HalfTensor\n<issue_comment>username_3: what we do in TH is basically define an \"is_tensor\" and a \"has_tensor_math\"; then you write the tests based on which of those you are actually testing.\n<issue_comment>username_0: @username_2 so I took a look at `test_torch.py`, it looks like the only reference to tensor_classes is in test_print. For other tests usually they are self-contained about which types they are testing against.\n<issue_comment>username_4: @pytorchbot add to whitelist\n<issue_comment>username_0: After fixing this issue. I wonder why it would be a good idea to have a CPU HalfTensor at all. Since HalfTensor is a subtype of Tensor, we would expect it to support the same interface as Tensor. However, the current implementation of HalfTensor does not allow such behaviour.\n<issue_comment>username_2: on the CPU, HalfTensor is just there for serialization purposes. For example saving a GPU HalfTensor and being able to load the weights back on a CPU-only machine.\r\n\r\nOther than that it has no purpose, as Half is not a native type on CPU and all operations will be very very very slow."}
{"repo": "pytorch/pytorch", "issue_id": 239293180, "issue_number": 1934, "timestamp": "2017-06-29 21:18:37+00:00", "text": "@username_2 so I took a look at `test_torch.py`, it looks like the only reference to tensor_classes is in test_print. For other tests usually they are self-contained about which types they are testing against.", "context": "<issue_start><issue_comment>Title: fix torch.is_tensor not recognizing HalfTensor\nusername_0: Fix #1922.\n<issue_comment>username_1: This needs some additional changes in the tests\n<issue_comment>username_0: @username_1 Hmm now I am bit curious about the reasons for the failure of the test cases. It says that ne and fill are not implemented for the HalfTensor. From from the source code I think we are actually generating those methods since there isn't any restriction in the cwrap definitions?\n<issue_comment>username_2: @username_0 CPU HalfTensor has no methods implemented except copy and serialization stuff. So almost all tests have to be filtered out for CPU HalfTensor\n<issue_comment>username_3: what we do in TH is basically define an \"is_tensor\" and a \"has_tensor_math\"; then you write the tests based on which of those you are actually testing.\n<issue_comment>username_0: @username_2 so I took a look at `test_torch.py`, it looks like the only reference to tensor_classes is in test_print. For other tests usually they are self-contained about which types they are testing against.\n<issue_comment>username_4: @pytorchbot add to whitelist\n<issue_comment>username_0: After fixing this issue. I wonder why it would be a good idea to have a CPU HalfTensor at all. Since HalfTensor is a subtype of Tensor, we would expect it to support the same interface as Tensor. However, the current implementation of HalfTensor does not allow such behaviour.\n<issue_comment>username_2: on the CPU, HalfTensor is just there for serialization purposes. For example saving a GPU HalfTensor and being able to load the weights back on a CPU-only machine.\r\n\r\nOther than that it has no purpose, as Half is not a native type on CPU and all operations will be very very very slow."}
{"repo": "pytorch/pytorch", "issue_id": 239293180, "issue_number": 1934, "timestamp": "2017-06-30 20:33:07+00:00", "text": "@pytorchbot add to whitelist", "context": "<issue_start><issue_comment>Title: fix torch.is_tensor not recognizing HalfTensor\nusername_0: Fix #1922.\n<issue_comment>username_1: This needs some additional changes in the tests\n<issue_comment>username_0: @username_1 Hmm now I am bit curious about the reasons for the failure of the test cases. It says that ne and fill are not implemented for the HalfTensor. From from the source code I think we are actually generating those methods since there isn't any restriction in the cwrap definitions?\n<issue_comment>username_2: @username_0 CPU HalfTensor has no methods implemented except copy and serialization stuff. So almost all tests have to be filtered out for CPU HalfTensor\n<issue_comment>username_3: what we do in TH is basically define an \"is_tensor\" and a \"has_tensor_math\"; then you write the tests based on which of those you are actually testing.\n<issue_comment>username_0: @username_2 so I took a look at `test_torch.py`, it looks like the only reference to tensor_classes is in test_print. For other tests usually they are self-contained about which types they are testing against.\n<issue_comment>username_4: @pytorchbot add to whitelist\n<issue_comment>username_0: After fixing this issue. I wonder why it would be a good idea to have a CPU HalfTensor at all. Since HalfTensor is a subtype of Tensor, we would expect it to support the same interface as Tensor. However, the current implementation of HalfTensor does not allow such behaviour.\n<issue_comment>username_2: on the CPU, HalfTensor is just there for serialization purposes. For example saving a GPU HalfTensor and being able to load the weights back on a CPU-only machine.\r\n\r\nOther than that it has no purpose, as Half is not a native type on CPU and all operations will be very very very slow."}
{"repo": "pytorch/pytorch", "issue_id": 239293180, "issue_number": 1934, "timestamp": "2017-07-02 11:06:09+00:00", "text": "After fixing this issue. I wonder why it would be a good idea to have a CPU HalfTensor at all. Since HalfTensor is a subtype of Tensor, we would expect it to support the same interface as Tensor. However, the current implementation of HalfTensor does not allow such behaviour.", "context": "<issue_start><issue_comment>Title: fix torch.is_tensor not recognizing HalfTensor\nusername_0: Fix #1922.\n<issue_comment>username_1: This needs some additional changes in the tests\n<issue_comment>username_0: @username_1 Hmm now I am bit curious about the reasons for the failure of the test cases. It says that ne and fill are not implemented for the HalfTensor. From from the source code I think we are actually generating those methods since there isn't any restriction in the cwrap definitions?\n<issue_comment>username_2: @username_0 CPU HalfTensor has no methods implemented except copy and serialization stuff. So almost all tests have to be filtered out for CPU HalfTensor\n<issue_comment>username_3: what we do in TH is basically define an \"is_tensor\" and a \"has_tensor_math\"; then you write the tests based on which of those you are actually testing.\n<issue_comment>username_0: @username_2 so I took a look at `test_torch.py`, it looks like the only reference to tensor_classes is in test_print. For other tests usually they are self-contained about which types they are testing against.\n<issue_comment>username_4: @pytorchbot add to whitelist\n<issue_comment>username_0: After fixing this issue. I wonder why it would be a good idea to have a CPU HalfTensor at all. Since HalfTensor is a subtype of Tensor, we would expect it to support the same interface as Tensor. However, the current implementation of HalfTensor does not allow such behaviour.\n<issue_comment>username_2: on the CPU, HalfTensor is just there for serialization purposes. For example saving a GPU HalfTensor and being able to load the weights back on a CPU-only machine.\r\n\r\nOther than that it has no purpose, as Half is not a native type on CPU and all operations will be very very very slow."}
{"repo": "pytorch/pytorch", "issue_id": 239293180, "issue_number": 1934, "timestamp": "2017-07-02 14:12:52+00:00", "text": "on the CPU, HalfTensor is just there for serialization purposes. For example saving a GPU HalfTensor and being able to load the weights back on a CPU-only machine.\r\n\r\nOther than that it has no purpose, as Half is not a native type on CPU and all operations will be very very very slow.", "context": "<issue_start><issue_comment>Title: fix torch.is_tensor not recognizing HalfTensor\nusername_0: Fix #1922.\n<issue_comment>username_1: This needs some additional changes in the tests\n<issue_comment>username_0: @username_1 Hmm now I am bit curious about the reasons for the failure of the test cases. It says that ne and fill are not implemented for the HalfTensor. From from the source code I think we are actually generating those methods since there isn't any restriction in the cwrap definitions?\n<issue_comment>username_2: @username_0 CPU HalfTensor has no methods implemented except copy and serialization stuff. So almost all tests have to be filtered out for CPU HalfTensor\n<issue_comment>username_3: what we do in TH is basically define an \"is_tensor\" and a \"has_tensor_math\"; then you write the tests based on which of those you are actually testing.\n<issue_comment>username_0: @username_2 so I took a look at `test_torch.py`, it looks like the only reference to tensor_classes is in test_print. For other tests usually they are self-contained about which types they are testing against.\n<issue_comment>username_4: @pytorchbot add to whitelist\n<issue_comment>username_0: After fixing this issue. I wonder why it would be a good idea to have a CPU HalfTensor at all. Since HalfTensor is a subtype of Tensor, we would expect it to support the same interface as Tensor. However, the current implementation of HalfTensor does not allow such behaviour.\n<issue_comment>username_2: on the CPU, HalfTensor is just there for serialization purposes. For example saving a GPU HalfTensor and being able to load the weights back on a CPU-only machine.\r\n\r\nOther than that it has no purpose, as Half is not a native type on CPU and all operations will be very very very slow."}
{"repo": "facebook/react", "issue_id": 248247023, "issue_number": 10395, "timestamp": "2017-08-06 13:24:21+00:00", "text": "In fact I would like to integrate Facebook authentication in my React app, and save user credentials in the database, along with additional data that will be used to authenticate with an API.\r\n\r\nI have no idea about how to handle authentication and authorization in a Facebook app. I went through some few books, but none of them touched that aspect. \r\n\r\nCan you point me to a book, a tutorial, any link ? whatever you think can point me to the right direction.\r\n\r\nThanks in advance.", "context": "<issue_start><issue_comment>Title: (Non-issue) Need advise for adding authentication in a react app\nusername_0: In fact I would like to integrate Facebook authentication in my React app, and save user credentials in the database, along with additional data that will be used to authenticate with an API.\r\n\r\nI have no idea about how to handle authentication and authorization in a Facebook app. I went through some few books, but none of them touched that aspect. \r\n\r\nCan you point me to a book, a tutorial, any link ? whatever you think can point me to the right direction.\r\n\r\nThanks in advance.\n<issue_comment>username_1: @username_0  The best technique out there right now is to use Jason Web Token(JWT).\r\nI am sure you can find decent amount of information on that online\n<issue_comment>username_0: @username_1, thanks for your reply. I am fine with the API authentication, I opted for HAWK in HapiJS. So I can include Hawk credentials in my request header, and fetch data from my API.\r\n\r\nThis is what I want to achieve:\r\n\r\n1. User try to access a route in React Router that required Authentication (No idea how to set it)\r\n2. User is redirected to a Login component that allow Facebook login\r\n3. Upon successful login authentication, Facebook credentials are saved in a database, along with hawk credentials, and stored in a cookie\r\n4. User can now authenticate with the API using credentials in his cookie and fetch data.\r\n\r\nFrom what I know about JWT and HAWK, they are only concerned with step 4. My problem is steps 1 to 3.\r\n\r\nIs that possible ? Am I getting something wrong ?\r\n\r\nThanks<issue_closed>"}
{"repo": "facebook/react", "issue_id": 248247023, "issue_number": 10395, "timestamp": "2017-08-06 14:33:28+00:00", "text": "@username_0  The best technique out there right now is to use Jason Web Token(JWT).\r\nI am sure you can find decent amount of information on that online", "context": "<issue_start><issue_comment>Title: (Non-issue) Need advise for adding authentication in a react app\nusername_0: In fact I would like to integrate Facebook authentication in my React app, and save user credentials in the database, along with additional data that will be used to authenticate with an API.\r\n\r\nI have no idea about how to handle authentication and authorization in a Facebook app. I went through some few books, but none of them touched that aspect. \r\n\r\nCan you point me to a book, a tutorial, any link ? whatever you think can point me to the right direction.\r\n\r\nThanks in advance.\n<issue_comment>username_1: @username_0  The best technique out there right now is to use Jason Web Token(JWT).\r\nI am sure you can find decent amount of information on that online\n<issue_comment>username_0: @username_1, thanks for your reply. I am fine with the API authentication, I opted for HAWK in HapiJS. So I can include Hawk credentials in my request header, and fetch data from my API.\r\n\r\nThis is what I want to achieve:\r\n\r\n1. User try to access a route in React Router that required Authentication (No idea how to set it)\r\n2. User is redirected to a Login component that allow Facebook login\r\n3. Upon successful login authentication, Facebook credentials are saved in a database, along with hawk credentials, and stored in a cookie\r\n4. User can now authenticate with the API using credentials in his cookie and fetch data.\r\n\r\nFrom what I know about JWT and HAWK, they are only concerned with step 4. My problem is steps 1 to 3.\r\n\r\nIs that possible ? Am I getting something wrong ?\r\n\r\nThanks<issue_closed>"}
{"repo": "facebook/react", "issue_id": 248247023, "issue_number": 10395, "timestamp": "2017-08-06 14:58:01+00:00", "text": "@username_1, thanks for your reply. I am fine with the API authentication, I opted for HAWK in HapiJS. So I can include Hawk credentials in my request header, and fetch data from my API.\r\n\r\nThis is what I want to achieve:\r\n\r\n1. User try to access a route in React Router that required Authentication (No idea how to set it)\r\n2. User is redirected to a Login component that allow Facebook login\r\n3. Upon successful login authentication, Facebook credentials are saved in a database, along with hawk credentials, and stored in a cookie\r\n4. User can now authenticate with the API using credentials in his cookie and fetch data.\r\n\r\nFrom what I know about JWT and HAWK, they are only concerned with step 4. My problem is steps 1 to 3.\r\n\r\nIs that possible ? Am I getting something wrong ?\r\n\r\nThanks", "context": "<issue_start><issue_comment>Title: (Non-issue) Need advise for adding authentication in a react app\nusername_0: In fact I would like to integrate Facebook authentication in my React app, and save user credentials in the database, along with additional data that will be used to authenticate with an API.\r\n\r\nI have no idea about how to handle authentication and authorization in a Facebook app. I went through some few books, but none of them touched that aspect. \r\n\r\nCan you point me to a book, a tutorial, any link ? whatever you think can point me to the right direction.\r\n\r\nThanks in advance.\n<issue_comment>username_1: @username_0  The best technique out there right now is to use Jason Web Token(JWT).\r\nI am sure you can find decent amount of information on that online\n<issue_comment>username_0: @username_1, thanks for your reply. I am fine with the API authentication, I opted for HAWK in HapiJS. So I can include Hawk credentials in my request header, and fetch data from my API.\r\n\r\nThis is what I want to achieve:\r\n\r\n1. User try to access a route in React Router that required Authentication (No idea how to set it)\r\n2. User is redirected to a Login component that allow Facebook login\r\n3. Upon successful login authentication, Facebook credentials are saved in a database, along with hawk credentials, and stored in a cookie\r\n4. User can now authenticate with the API using credentials in his cookie and fetch data.\r\n\r\nFrom what I know about JWT and HAWK, they are only concerned with step 4. My problem is steps 1 to 3.\r\n\r\nIs that possible ? Am I getting something wrong ?\r\n\r\nThanks<issue_closed>"}
{"repo": "facebook/react", "issue_id": 248247023, "issue_number": 10395, "timestamp": "2017-08-06 16:23:37+00:00", "text": "", "context": "<issue_start><issue_comment>Title: (Non-issue) Need advise for adding authentication in a react app\nusername_0: In fact I would like to integrate Facebook authentication in my React app, and save user credentials in the database, along with additional data that will be used to authenticate with an API.\r\n\r\nI have no idea about how to handle authentication and authorization in a Facebook app. I went through some few books, but none of them touched that aspect. \r\n\r\nCan you point me to a book, a tutorial, any link ? whatever you think can point me to the right direction.\r\n\r\nThanks in advance.\n<issue_comment>username_1: @username_0  The best technique out there right now is to use Jason Web Token(JWT).\r\nI am sure you can find decent amount of information on that online\n<issue_comment>username_0: @username_1, thanks for your reply. I am fine with the API authentication, I opted for HAWK in HapiJS. So I can include Hawk credentials in my request header, and fetch data from my API.\r\n\r\nThis is what I want to achieve:\r\n\r\n1. User try to access a route in React Router that required Authentication (No idea how to set it)\r\n2. User is redirected to a Login component that allow Facebook login\r\n3. Upon successful login authentication, Facebook credentials are saved in a database, along with hawk credentials, and stored in a cookie\r\n4. User can now authenticate with the API using credentials in his cookie and fetch data.\r\n\r\nFrom what I know about JWT and HAWK, they are only concerned with step 4. My problem is steps 1 to 3.\r\n\r\nIs that possible ? Am I getting something wrong ?\r\n\r\nThanks<issue_closed>"}
{"repo": "pytorch/pytorch", "issue_id": 320115344, "issue_number": 7270, "timestamp": "2018-05-03T23:40:42Z", "text": "With `torch.cuda.memory_allocated` available, we can test if CUDA methods has memory leaks. This PR adds a wrapper around each CUDA tests that checks if the CUDA memory usage before and after stays constant.", "context": "<issue_start><issue_comment>Title: Add memory leak check in CUDA tests\nusername_0: With `torch.cuda.memory_allocated` available, we can test if CUDA methods has memory leaks. This PR adds a wrapper around each CUDA tests that checks if the CUDA memory usage before and after stays constant.\n<issue_comment>username_1: this is kinda cool!\n<issue_comment>username_2: Excellent. Have you purposely introduced a memory leak and verified that this test catches it?\n<issue_comment>username_0: @username_2 I added a test for the new method in new commit.\r\n\r\ncc @username_3 for changes to `run_test.py`.\n<issue_comment>username_3: Cool, just make sure the tests actually run in the CI, even when green\n<issue_comment>username_4: Does it have any effect on the perf of the tests?\n<issue_comment>username_0: In local tests and CI test, this new mechanism detected `MaxUnpool3d` (`test_nn.TestNN.test_MaxUnpool3d_net_cuda`) and `MultiLabelMarginCriterion` leaking memory (`test_legacy_nn.TestNN.test_MultiLabelMarginCriterion_1d_cuda`).\n<issue_comment>username_0: @username_4 I don't have concrete numbers on full test suite. But for `test_cuda` on devgpu, the total time went from `20s` to `58s`. The overall test time increase on CUDA tests is likely less than 3 fold because `test_cuda` tests are small and thus the wrapper overhead area more obvious.\r\n\r\nStill, I think there are good reasons to get this in. The above detected leaks are good examples.\r\n\r\nOnce I fix those leaks, I will run the full suite and report the total increased time here.\n<issue_comment>username_2: Not for this PR, but we can do something similar for CPU leaks too; all we need to do is distinguish between tensor allocations (unlikely to stay live across tests) and metadata allocations (we won't track leaks for those.)\r\n\r\nAlthough, when @gchanan has his way all the manual refcounting in TH will go away and it should become a lot harder to make this kind of mistake.\n<issue_comment>username_0: @pytorchbot retest this please\n<issue_comment>username_2: Maybe, to avoid having to fix all of the leaking kernels before we merge this, we should just blacklist all of the currently failing tests and then subsequently fix them.\n<issue_comment>username_0: This PR also fixed the `test_cuda.py` running `TestTorch` problem, so the total test time increase on CUDA 9, CUDNN7, and py3 CI is 27s.\n<issue_comment>username_0: @username_4 @username_1 Require a review! \r\n\r\npr/caffe2-py2-gcc5-ubuntu16.04-test fail is unrelated and is being fixed.\n<issue_comment>username_2: @pytorchbot retest this please\n<issue_comment>username_0: @pytorchbot retest this please\n<issue_comment>username_0: @pytorchbot retest this please\n<issue_comment>username_0: @pytorchbot retest this please\n<issue_comment>username_2: @pytorchbot retest this please\n<issue_comment>username_0: @pytorchbot retest this please"}
{"repo": "pytorch/pytorch", "issue_id": 320115344, "issue_number": 7270, "timestamp": "2018-05-03 23:42:47+00:00", "text": "this is kinda cool!", "context": "<issue_start><issue_comment>Title: Add memory leak check in CUDA tests\nusername_0: With `torch.cuda.memory_allocated` available, we can test if CUDA methods has memory leaks. This PR adds a wrapper around each CUDA tests that checks if the CUDA memory usage before and after stays constant.\n<issue_comment>username_1: this is kinda cool!\n<issue_comment>username_2: Excellent. Have you purposely introduced a memory leak and verified that this test catches it?\n<issue_comment>username_0: @username_2 I added a test for the new method in new commit.\r\n\r\ncc @username_3 for changes to `run_test.py`.\n<issue_comment>username_3: Cool, just make sure the tests actually run in the CI, even when green\n<issue_comment>username_4: Does it have any effect on the perf of the tests?\n<issue_comment>username_0: In local tests and CI test, this new mechanism detected `MaxUnpool3d` (`test_nn.TestNN.test_MaxUnpool3d_net_cuda`) and `MultiLabelMarginCriterion` leaking memory (`test_legacy_nn.TestNN.test_MultiLabelMarginCriterion_1d_cuda`).\n<issue_comment>username_0: @username_4 I don't have concrete numbers on full test suite. But for `test_cuda` on devgpu, the total time went from `20s` to `58s`. The overall test time increase on CUDA tests is likely less than 3 fold because `test_cuda` tests are small and thus the wrapper overhead area more obvious.\r\n\r\nStill, I think there are good reasons to get this in. The above detected leaks are good examples.\r\n\r\nOnce I fix those leaks, I will run the full suite and report the total increased time here.\n<issue_comment>username_2: Not for this PR, but we can do something similar for CPU leaks too; all we need to do is distinguish between tensor allocations (unlikely to stay live across tests) and metadata allocations (we won't track leaks for those.)\r\n\r\nAlthough, when @gchanan has his way all the manual refcounting in TH will go away and it should become a lot harder to make this kind of mistake.\n<issue_comment>username_0: @pytorchbot retest this please\n<issue_comment>username_2: Maybe, to avoid having to fix all of the leaking kernels before we merge this, we should just blacklist all of the currently failing tests and then subsequently fix them.\n<issue_comment>username_0: This PR also fixed the `test_cuda.py` running `TestTorch` problem, so the total test time increase on CUDA 9, CUDNN7, and py3 CI is 27s.\n<issue_comment>username_0: @username_4 @username_1 Require a review! \r\n\r\npr/caffe2-py2-gcc5-ubuntu16.04-test fail is unrelated and is being fixed.\n<issue_comment>username_2: @pytorchbot retest this please\n<issue_comment>username_0: @pytorchbot retest this please\n<issue_comment>username_0: @pytorchbot retest this please\n<issue_comment>username_0: @pytorchbot retest this please\n<issue_comment>username_2: @pytorchbot retest this please\n<issue_comment>username_0: @pytorchbot retest this please"}
{"repo": "pytorch/pytorch", "issue_id": 320115344, "issue_number": 7270, "timestamp": "2018-05-04 00:58:34+00:00", "text": "Excellent. Have you purposely introduced a memory leak and verified that this test catches it?", "context": "<issue_start><issue_comment>Title: Add memory leak check in CUDA tests\nusername_0: With `torch.cuda.memory_allocated` available, we can test if CUDA methods has memory leaks. This PR adds a wrapper around each CUDA tests that checks if the CUDA memory usage before and after stays constant.\n<issue_comment>username_1: this is kinda cool!\n<issue_comment>username_2: Excellent. Have you purposely introduced a memory leak and verified that this test catches it?\n<issue_comment>username_0: @username_2 I added a test for the new method in new commit.\r\n\r\ncc @username_3 for changes to `run_test.py`.\n<issue_comment>username_3: Cool, just make sure the tests actually run in the CI, even when green\n<issue_comment>username_4: Does it have any effect on the perf of the tests?\n<issue_comment>username_0: In local tests and CI test, this new mechanism detected `MaxUnpool3d` (`test_nn.TestNN.test_MaxUnpool3d_net_cuda`) and `MultiLabelMarginCriterion` leaking memory (`test_legacy_nn.TestNN.test_MultiLabelMarginCriterion_1d_cuda`).\n<issue_comment>username_0: @username_4 I don't have concrete numbers on full test suite. But for `test_cuda` on devgpu, the total time went from `20s` to `58s`. The overall test time increase on CUDA tests is likely less than 3 fold because `test_cuda` tests are small and thus the wrapper overhead area more obvious.\r\n\r\nStill, I think there are good reasons to get this in. The above detected leaks are good examples.\r\n\r\nOnce I fix those leaks, I will run the full suite and report the total increased time here.\n<issue_comment>username_2: Not for this PR, but we can do something similar for CPU leaks too; all we need to do is distinguish between tensor allocations (unlikely to stay live across tests) and metadata allocations (we won't track leaks for those.)\r\n\r\nAlthough, when @gchanan has his way all the manual refcounting in TH will go away and it should become a lot harder to make this kind of mistake.\n<issue_comment>username_0: @pytorchbot retest this please\n<issue_comment>username_2: Maybe, to avoid having to fix all of the leaking kernels before we merge this, we should just blacklist all of the currently failing tests and then subsequently fix them.\n<issue_comment>username_0: This PR also fixed the `test_cuda.py` running `TestTorch` problem, so the total test time increase on CUDA 9, CUDNN7, and py3 CI is 27s.\n<issue_comment>username_0: @username_4 @username_1 Require a review! \r\n\r\npr/caffe2-py2-gcc5-ubuntu16.04-test fail is unrelated and is being fixed.\n<issue_comment>username_2: @pytorchbot retest this please\n<issue_comment>username_0: @pytorchbot retest this please\n<issue_comment>username_0: @pytorchbot retest this please\n<issue_comment>username_0: @pytorchbot retest this please\n<issue_comment>username_2: @pytorchbot retest this please\n<issue_comment>username_0: @pytorchbot retest this please"}
{"repo": "pytorch/pytorch", "issue_id": 320115344, "issue_number": 7270, "timestamp": "2018-05-04 01:17:40+00:00", "text": "@username_2 I added a test for the new method in new commit.\r\n\r\ncc @username_3 for changes to `run_test.py`.", "context": "<issue_start><issue_comment>Title: Add memory leak check in CUDA tests\nusername_0: With `torch.cuda.memory_allocated` available, we can test if CUDA methods has memory leaks. This PR adds a wrapper around each CUDA tests that checks if the CUDA memory usage before and after stays constant.\n<issue_comment>username_1: this is kinda cool!\n<issue_comment>username_2: Excellent. Have you purposely introduced a memory leak and verified that this test catches it?\n<issue_comment>username_0: @username_2 I added a test for the new method in new commit.\r\n\r\ncc @username_3 for changes to `run_test.py`.\n<issue_comment>username_3: Cool, just make sure the tests actually run in the CI, even when green\n<issue_comment>username_4: Does it have any effect on the perf of the tests?\n<issue_comment>username_0: In local tests and CI test, this new mechanism detected `MaxUnpool3d` (`test_nn.TestNN.test_MaxUnpool3d_net_cuda`) and `MultiLabelMarginCriterion` leaking memory (`test_legacy_nn.TestNN.test_MultiLabelMarginCriterion_1d_cuda`).\n<issue_comment>username_0: @username_4 I don't have concrete numbers on full test suite. But for `test_cuda` on devgpu, the total time went from `20s` to `58s`. The overall test time increase on CUDA tests is likely less than 3 fold because `test_cuda` tests are small and thus the wrapper overhead area more obvious.\r\n\r\nStill, I think there are good reasons to get this in. The above detected leaks are good examples.\r\n\r\nOnce I fix those leaks, I will run the full suite and report the total increased time here.\n<issue_comment>username_2: Not for this PR, but we can do something similar for CPU leaks too; all we need to do is distinguish between tensor allocations (unlikely to stay live across tests) and metadata allocations (we won't track leaks for those.)\r\n\r\nAlthough, when @gchanan has his way all the manual refcounting in TH will go away and it should become a lot harder to make this kind of mistake.\n<issue_comment>username_0: @pytorchbot retest this please\n<issue_comment>username_2: Maybe, to avoid having to fix all of the leaking kernels before we merge this, we should just blacklist all of the currently failing tests and then subsequently fix them.\n<issue_comment>username_0: This PR also fixed the `test_cuda.py` running `TestTorch` problem, so the total test time increase on CUDA 9, CUDNN7, and py3 CI is 27s.\n<issue_comment>username_0: @username_4 @username_1 Require a review! \r\n\r\npr/caffe2-py2-gcc5-ubuntu16.04-test fail is unrelated and is being fixed.\n<issue_comment>username_2: @pytorchbot retest this please\n<issue_comment>username_0: @pytorchbot retest this please\n<issue_comment>username_0: @pytorchbot retest this please\n<issue_comment>username_0: @pytorchbot retest this please\n<issue_comment>username_2: @pytorchbot retest this please\n<issue_comment>username_0: @pytorchbot retest this please"}
{"repo": "pytorch/pytorch", "issue_id": 320115344, "issue_number": 7270, "timestamp": "2018-05-04 01:36:04+00:00", "text": "Cool, just make sure the tests actually run in the CI, even when green", "context": "<issue_start><issue_comment>Title: Add memory leak check in CUDA tests\nusername_0: With `torch.cuda.memory_allocated` available, we can test if CUDA methods has memory leaks. This PR adds a wrapper around each CUDA tests that checks if the CUDA memory usage before and after stays constant.\n<issue_comment>username_1: this is kinda cool!\n<issue_comment>username_2: Excellent. Have you purposely introduced a memory leak and verified that this test catches it?\n<issue_comment>username_0: @username_2 I added a test for the new method in new commit.\r\n\r\ncc @username_3 for changes to `run_test.py`.\n<issue_comment>username_3: Cool, just make sure the tests actually run in the CI, even when green\n<issue_comment>username_4: Does it have any effect on the perf of the tests?\n<issue_comment>username_0: In local tests and CI test, this new mechanism detected `MaxUnpool3d` (`test_nn.TestNN.test_MaxUnpool3d_net_cuda`) and `MultiLabelMarginCriterion` leaking memory (`test_legacy_nn.TestNN.test_MultiLabelMarginCriterion_1d_cuda`).\n<issue_comment>username_0: @username_4 I don't have concrete numbers on full test suite. But for `test_cuda` on devgpu, the total time went from `20s` to `58s`. The overall test time increase on CUDA tests is likely less than 3 fold because `test_cuda` tests are small and thus the wrapper overhead area more obvious.\r\n\r\nStill, I think there are good reasons to get this in. The above detected leaks are good examples.\r\n\r\nOnce I fix those leaks, I will run the full suite and report the total increased time here.\n<issue_comment>username_2: Not for this PR, but we can do something similar for CPU leaks too; all we need to do is distinguish between tensor allocations (unlikely to stay live across tests) and metadata allocations (we won't track leaks for those.)\r\n\r\nAlthough, when @gchanan has his way all the manual refcounting in TH will go away and it should become a lot harder to make this kind of mistake.\n<issue_comment>username_0: @pytorchbot retest this please\n<issue_comment>username_2: Maybe, to avoid having to fix all of the leaking kernels before we merge this, we should just blacklist all of the currently failing tests and then subsequently fix them.\n<issue_comment>username_0: This PR also fixed the `test_cuda.py` running `TestTorch` problem, so the total test time increase on CUDA 9, CUDNN7, and py3 CI is 27s.\n<issue_comment>username_0: @username_4 @username_1 Require a review! \r\n\r\npr/caffe2-py2-gcc5-ubuntu16.04-test fail is unrelated and is being fixed.\n<issue_comment>username_2: @pytorchbot retest this please\n<issue_comment>username_0: @pytorchbot retest this please\n<issue_comment>username_0: @pytorchbot retest this please\n<issue_comment>username_0: @pytorchbot retest this please\n<issue_comment>username_2: @pytorchbot retest this please\n<issue_comment>username_0: @pytorchbot retest this please"}
{"repo": "pytorch/pytorch", "issue_id": 320115344, "issue_number": 7270, "timestamp": "2018-05-04 08:42:57+00:00", "text": "Does it have any effect on the perf of the tests?", "context": "<issue_start><issue_comment>Title: Add memory leak check in CUDA tests\nusername_0: With `torch.cuda.memory_allocated` available, we can test if CUDA methods has memory leaks. This PR adds a wrapper around each CUDA tests that checks if the CUDA memory usage before and after stays constant.\n<issue_comment>username_1: this is kinda cool!\n<issue_comment>username_2: Excellent. Have you purposely introduced a memory leak and verified that this test catches it?\n<issue_comment>username_0: @username_2 I added a test for the new method in new commit.\r\n\r\ncc @username_3 for changes to `run_test.py`.\n<issue_comment>username_3: Cool, just make sure the tests actually run in the CI, even when green\n<issue_comment>username_4: Does it have any effect on the perf of the tests?\n<issue_comment>username_0: In local tests and CI test, this new mechanism detected `MaxUnpool3d` (`test_nn.TestNN.test_MaxUnpool3d_net_cuda`) and `MultiLabelMarginCriterion` leaking memory (`test_legacy_nn.TestNN.test_MultiLabelMarginCriterion_1d_cuda`).\n<issue_comment>username_0: @username_4 I don't have concrete numbers on full test suite. But for `test_cuda` on devgpu, the total time went from `20s` to `58s`. The overall test time increase on CUDA tests is likely less than 3 fold because `test_cuda` tests are small and thus the wrapper overhead area more obvious.\r\n\r\nStill, I think there are good reasons to get this in. The above detected leaks are good examples.\r\n\r\nOnce I fix those leaks, I will run the full suite and report the total increased time here.\n<issue_comment>username_2: Not for this PR, but we can do something similar for CPU leaks too; all we need to do is distinguish between tensor allocations (unlikely to stay live across tests) and metadata allocations (we won't track leaks for those.)\r\n\r\nAlthough, when @gchanan has his way all the manual refcounting in TH will go away and it should become a lot harder to make this kind of mistake.\n<issue_comment>username_0: @pytorchbot retest this please\n<issue_comment>username_2: Maybe, to avoid having to fix all of the leaking kernels before we merge this, we should just blacklist all of the currently failing tests and then subsequently fix them.\n<issue_comment>username_0: This PR also fixed the `test_cuda.py` running `TestTorch` problem, so the total test time increase on CUDA 9, CUDNN7, and py3 CI is 27s.\n<issue_comment>username_0: @username_4 @username_1 Require a review! \r\n\r\npr/caffe2-py2-gcc5-ubuntu16.04-test fail is unrelated and is being fixed.\n<issue_comment>username_2: @pytorchbot retest this please\n<issue_comment>username_0: @pytorchbot retest this please\n<issue_comment>username_0: @pytorchbot retest this please\n<issue_comment>username_0: @pytorchbot retest this please\n<issue_comment>username_2: @pytorchbot retest this please\n<issue_comment>username_0: @pytorchbot retest this please"}
{"repo": "pytorch/pytorch", "issue_id": 320115344, "issue_number": 7270, "timestamp": "2018-05-04 20:12:15+00:00", "text": "In local tests and CI test, this new mechanism detected `MaxUnpool3d` (`test_nn.TestNN.test_MaxUnpool3d_net_cuda`) and `MultiLabelMarginCriterion` leaking memory (`test_legacy_nn.TestNN.test_MultiLabelMarginCriterion_1d_cuda`).", "context": "<issue_start><issue_comment>Title: Add memory leak check in CUDA tests\nusername_0: With `torch.cuda.memory_allocated` available, we can test if CUDA methods has memory leaks. This PR adds a wrapper around each CUDA tests that checks if the CUDA memory usage before and after stays constant.\n<issue_comment>username_1: this is kinda cool!\n<issue_comment>username_2: Excellent. Have you purposely introduced a memory leak and verified that this test catches it?\n<issue_comment>username_0: @username_2 I added a test for the new method in new commit.\r\n\r\ncc @username_3 for changes to `run_test.py`.\n<issue_comment>username_3: Cool, just make sure the tests actually run in the CI, even when green\n<issue_comment>username_4: Does it have any effect on the perf of the tests?\n<issue_comment>username_0: In local tests and CI test, this new mechanism detected `MaxUnpool3d` (`test_nn.TestNN.test_MaxUnpool3d_net_cuda`) and `MultiLabelMarginCriterion` leaking memory (`test_legacy_nn.TestNN.test_MultiLabelMarginCriterion_1d_cuda`).\n<issue_comment>username_0: @username_4 I don't have concrete numbers on full test suite. But for `test_cuda` on devgpu, the total time went from `20s` to `58s`. The overall test time increase on CUDA tests is likely less than 3 fold because `test_cuda` tests are small and thus the wrapper overhead area more obvious.\r\n\r\nStill, I think there are good reasons to get this in. The above detected leaks are good examples.\r\n\r\nOnce I fix those leaks, I will run the full suite and report the total increased time here.\n<issue_comment>username_2: Not for this PR, but we can do something similar for CPU leaks too; all we need to do is distinguish between tensor allocations (unlikely to stay live across tests) and metadata allocations (we won't track leaks for those.)\r\n\r\nAlthough, when @gchanan has his way all the manual refcounting in TH will go away and it should become a lot harder to make this kind of mistake.\n<issue_comment>username_0: @pytorchbot retest this please\n<issue_comment>username_2: Maybe, to avoid having to fix all of the leaking kernels before we merge this, we should just blacklist all of the currently failing tests and then subsequently fix them.\n<issue_comment>username_0: This PR also fixed the `test_cuda.py` running `TestTorch` problem, so the total test time increase on CUDA 9, CUDNN7, and py3 CI is 27s.\n<issue_comment>username_0: @username_4 @username_1 Require a review! \r\n\r\npr/caffe2-py2-gcc5-ubuntu16.04-test fail is unrelated and is being fixed.\n<issue_comment>username_2: @pytorchbot retest this please\n<issue_comment>username_0: @pytorchbot retest this please\n<issue_comment>username_0: @pytorchbot retest this please\n<issue_comment>username_0: @pytorchbot retest this please\n<issue_comment>username_2: @pytorchbot retest this please\n<issue_comment>username_0: @pytorchbot retest this please"}
{"repo": "pytorch/pytorch", "issue_id": 320115344, "issue_number": 7270, "timestamp": "2018-05-04 20:12:54+00:00", "text": "@username_4 I don't have concrete numbers on full test suite. But for `test_cuda` on devgpu, the total time went from `20s` to `58s`. The overall test time increase on CUDA tests is likely less than 3 fold because `test_cuda` tests are small and thus the wrapper overhead area more obvious.\r\n\r\nStill, I think there are good reasons to get this in. The above detected leaks are good examples.\r\n\r\nOnce I fix those leaks, I will run the full suite and report the total increased time here.", "context": "<issue_start><issue_comment>Title: Add memory leak check in CUDA tests\nusername_0: With `torch.cuda.memory_allocated` available, we can test if CUDA methods has memory leaks. This PR adds a wrapper around each CUDA tests that checks if the CUDA memory usage before and after stays constant.\n<issue_comment>username_1: this is kinda cool!\n<issue_comment>username_2: Excellent. Have you purposely introduced a memory leak and verified that this test catches it?\n<issue_comment>username_0: @username_2 I added a test for the new method in new commit.\r\n\r\ncc @username_3 for changes to `run_test.py`.\n<issue_comment>username_3: Cool, just make sure the tests actually run in the CI, even when green\n<issue_comment>username_4: Does it have any effect on the perf of the tests?\n<issue_comment>username_0: In local tests and CI test, this new mechanism detected `MaxUnpool3d` (`test_nn.TestNN.test_MaxUnpool3d_net_cuda`) and `MultiLabelMarginCriterion` leaking memory (`test_legacy_nn.TestNN.test_MultiLabelMarginCriterion_1d_cuda`).\n<issue_comment>username_0: @username_4 I don't have concrete numbers on full test suite. But for `test_cuda` on devgpu, the total time went from `20s` to `58s`. The overall test time increase on CUDA tests is likely less than 3 fold because `test_cuda` tests are small and thus the wrapper overhead area more obvious.\r\n\r\nStill, I think there are good reasons to get this in. The above detected leaks are good examples.\r\n\r\nOnce I fix those leaks, I will run the full suite and report the total increased time here.\n<issue_comment>username_2: Not for this PR, but we can do something similar for CPU leaks too; all we need to do is distinguish between tensor allocations (unlikely to stay live across tests) and metadata allocations (we won't track leaks for those.)\r\n\r\nAlthough, when @gchanan has his way all the manual refcounting in TH will go away and it should become a lot harder to make this kind of mistake.\n<issue_comment>username_0: @pytorchbot retest this please\n<issue_comment>username_2: Maybe, to avoid having to fix all of the leaking kernels before we merge this, we should just blacklist all of the currently failing tests and then subsequently fix them.\n<issue_comment>username_0: This PR also fixed the `test_cuda.py` running `TestTorch` problem, so the total test time increase on CUDA 9, CUDNN7, and py3 CI is 27s.\n<issue_comment>username_0: @username_4 @username_1 Require a review! \r\n\r\npr/caffe2-py2-gcc5-ubuntu16.04-test fail is unrelated and is being fixed.\n<issue_comment>username_2: @pytorchbot retest this please\n<issue_comment>username_0: @pytorchbot retest this please\n<issue_comment>username_0: @pytorchbot retest this please\n<issue_comment>username_0: @pytorchbot retest this please\n<issue_comment>username_2: @pytorchbot retest this please\n<issue_comment>username_0: @pytorchbot retest this please"}
{"repo": "pytorch/pytorch", "issue_id": 320115344, "issue_number": 7270, "timestamp": "2018-05-04 20:55:10+00:00", "text": "Not for this PR, but we can do something similar for CPU leaks too; all we need to do is distinguish between tensor allocations (unlikely to stay live across tests) and metadata allocations (we won't track leaks for those.)\r\n\r\nAlthough, when @gchanan has his way all the manual refcounting in TH will go away and it should become a lot harder to make this kind of mistake.", "context": "<issue_start><issue_comment>Title: Add memory leak check in CUDA tests\nusername_0: With `torch.cuda.memory_allocated` available, we can test if CUDA methods has memory leaks. This PR adds a wrapper around each CUDA tests that checks if the CUDA memory usage before and after stays constant.\n<issue_comment>username_1: this is kinda cool!\n<issue_comment>username_2: Excellent. Have you purposely introduced a memory leak and verified that this test catches it?\n<issue_comment>username_0: @username_2 I added a test for the new method in new commit.\r\n\r\ncc @username_3 for changes to `run_test.py`.\n<issue_comment>username_3: Cool, just make sure the tests actually run in the CI, even when green\n<issue_comment>username_4: Does it have any effect on the perf of the tests?\n<issue_comment>username_0: In local tests and CI test, this new mechanism detected `MaxUnpool3d` (`test_nn.TestNN.test_MaxUnpool3d_net_cuda`) and `MultiLabelMarginCriterion` leaking memory (`test_legacy_nn.TestNN.test_MultiLabelMarginCriterion_1d_cuda`).\n<issue_comment>username_0: @username_4 I don't have concrete numbers on full test suite. But for `test_cuda` on devgpu, the total time went from `20s` to `58s`. The overall test time increase on CUDA tests is likely less than 3 fold because `test_cuda` tests are small and thus the wrapper overhead area more obvious.\r\n\r\nStill, I think there are good reasons to get this in. The above detected leaks are good examples.\r\n\r\nOnce I fix those leaks, I will run the full suite and report the total increased time here.\n<issue_comment>username_2: Not for this PR, but we can do something similar for CPU leaks too; all we need to do is distinguish between tensor allocations (unlikely to stay live across tests) and metadata allocations (we won't track leaks for those.)\r\n\r\nAlthough, when @gchanan has his way all the manual refcounting in TH will go away and it should become a lot harder to make this kind of mistake.\n<issue_comment>username_0: @pytorchbot retest this please\n<issue_comment>username_2: Maybe, to avoid having to fix all of the leaking kernels before we merge this, we should just blacklist all of the currently failing tests and then subsequently fix them.\n<issue_comment>username_0: This PR also fixed the `test_cuda.py` running `TestTorch` problem, so the total test time increase on CUDA 9, CUDNN7, and py3 CI is 27s.\n<issue_comment>username_0: @username_4 @username_1 Require a review! \r\n\r\npr/caffe2-py2-gcc5-ubuntu16.04-test fail is unrelated and is being fixed.\n<issue_comment>username_2: @pytorchbot retest this please\n<issue_comment>username_0: @pytorchbot retest this please\n<issue_comment>username_0: @pytorchbot retest this please\n<issue_comment>username_0: @pytorchbot retest this please\n<issue_comment>username_2: @pytorchbot retest this please\n<issue_comment>username_0: @pytorchbot retest this please"}
{"repo": "pytorch/pytorch", "issue_id": 320115344, "issue_number": 7270, "timestamp": "2018-05-04 21:00:39+00:00", "text": "@pytorchbot retest this please", "context": "<issue_start><issue_comment>Title: Add memory leak check in CUDA tests\nusername_0: With `torch.cuda.memory_allocated` available, we can test if CUDA methods has memory leaks. This PR adds a wrapper around each CUDA tests that checks if the CUDA memory usage before and after stays constant.\n<issue_comment>username_1: this is kinda cool!\n<issue_comment>username_2: Excellent. Have you purposely introduced a memory leak and verified that this test catches it?\n<issue_comment>username_0: @username_2 I added a test for the new method in new commit.\r\n\r\ncc @username_3 for changes to `run_test.py`.\n<issue_comment>username_3: Cool, just make sure the tests actually run in the CI, even when green\n<issue_comment>username_4: Does it have any effect on the perf of the tests?\n<issue_comment>username_0: In local tests and CI test, this new mechanism detected `MaxUnpool3d` (`test_nn.TestNN.test_MaxUnpool3d_net_cuda`) and `MultiLabelMarginCriterion` leaking memory (`test_legacy_nn.TestNN.test_MultiLabelMarginCriterion_1d_cuda`).\n<issue_comment>username_0: @username_4 I don't have concrete numbers on full test suite. But for `test_cuda` on devgpu, the total time went from `20s` to `58s`. The overall test time increase on CUDA tests is likely less than 3 fold because `test_cuda` tests are small and thus the wrapper overhead area more obvious.\r\n\r\nStill, I think there are good reasons to get this in. The above detected leaks are good examples.\r\n\r\nOnce I fix those leaks, I will run the full suite and report the total increased time here.\n<issue_comment>username_2: Not for this PR, but we can do something similar for CPU leaks too; all we need to do is distinguish between tensor allocations (unlikely to stay live across tests) and metadata allocations (we won't track leaks for those.)\r\n\r\nAlthough, when @gchanan has his way all the manual refcounting in TH will go away and it should become a lot harder to make this kind of mistake.\n<issue_comment>username_0: @pytorchbot retest this please\n<issue_comment>username_2: Maybe, to avoid having to fix all of the leaking kernels before we merge this, we should just blacklist all of the currently failing tests and then subsequently fix them.\n<issue_comment>username_0: This PR also fixed the `test_cuda.py` running `TestTorch` problem, so the total test time increase on CUDA 9, CUDNN7, and py3 CI is 27s.\n<issue_comment>username_0: @username_4 @username_1 Require a review! \r\n\r\npr/caffe2-py2-gcc5-ubuntu16.04-test fail is unrelated and is being fixed.\n<issue_comment>username_2: @pytorchbot retest this please\n<issue_comment>username_0: @pytorchbot retest this please\n<issue_comment>username_0: @pytorchbot retest this please\n<issue_comment>username_0: @pytorchbot retest this please\n<issue_comment>username_2: @pytorchbot retest this please\n<issue_comment>username_0: @pytorchbot retest this please"}
{"repo": "pytorch/pytorch", "issue_id": 320115344, "issue_number": 7270, "timestamp": "2018-05-13 03:35:57+00:00", "text": "Maybe, to avoid having to fix all of the leaking kernels before we merge this, we should just blacklist all of the currently failing tests and then subsequently fix them.", "context": "<issue_start><issue_comment>Title: Add memory leak check in CUDA tests\nusername_0: With `torch.cuda.memory_allocated` available, we can test if CUDA methods has memory leaks. This PR adds a wrapper around each CUDA tests that checks if the CUDA memory usage before and after stays constant.\n<issue_comment>username_1: this is kinda cool!\n<issue_comment>username_2: Excellent. Have you purposely introduced a memory leak and verified that this test catches it?\n<issue_comment>username_0: @username_2 I added a test for the new method in new commit.\r\n\r\ncc @username_3 for changes to `run_test.py`.\n<issue_comment>username_3: Cool, just make sure the tests actually run in the CI, even when green\n<issue_comment>username_4: Does it have any effect on the perf of the tests?\n<issue_comment>username_0: In local tests and CI test, this new mechanism detected `MaxUnpool3d` (`test_nn.TestNN.test_MaxUnpool3d_net_cuda`) and `MultiLabelMarginCriterion` leaking memory (`test_legacy_nn.TestNN.test_MultiLabelMarginCriterion_1d_cuda`).\n<issue_comment>username_0: @username_4 I don't have concrete numbers on full test suite. But for `test_cuda` on devgpu, the total time went from `20s` to `58s`. The overall test time increase on CUDA tests is likely less than 3 fold because `test_cuda` tests are small and thus the wrapper overhead area more obvious.\r\n\r\nStill, I think there are good reasons to get this in. The above detected leaks are good examples.\r\n\r\nOnce I fix those leaks, I will run the full suite and report the total increased time here.\n<issue_comment>username_2: Not for this PR, but we can do something similar for CPU leaks too; all we need to do is distinguish between tensor allocations (unlikely to stay live across tests) and metadata allocations (we won't track leaks for those.)\r\n\r\nAlthough, when @gchanan has his way all the manual refcounting in TH will go away and it should become a lot harder to make this kind of mistake.\n<issue_comment>username_0: @pytorchbot retest this please\n<issue_comment>username_2: Maybe, to avoid having to fix all of the leaking kernels before we merge this, we should just blacklist all of the currently failing tests and then subsequently fix them.\n<issue_comment>username_0: This PR also fixed the `test_cuda.py` running `TestTorch` problem, so the total test time increase on CUDA 9, CUDNN7, and py3 CI is 27s.\n<issue_comment>username_0: @username_4 @username_1 Require a review! \r\n\r\npr/caffe2-py2-gcc5-ubuntu16.04-test fail is unrelated and is being fixed.\n<issue_comment>username_2: @pytorchbot retest this please\n<issue_comment>username_0: @pytorchbot retest this please\n<issue_comment>username_0: @pytorchbot retest this please\n<issue_comment>username_0: @pytorchbot retest this please\n<issue_comment>username_2: @pytorchbot retest this please\n<issue_comment>username_0: @pytorchbot retest this please"}
{"repo": "pytorch/pytorch", "issue_id": 320115344, "issue_number": 7270, "timestamp": "2018-05-23 21:53:22+00:00", "text": "This PR also fixed the `test_cuda.py` running `TestTorch` problem, so the total test time increase on CUDA 9, CUDNN7, and py3 CI is 27s.", "context": "<issue_start><issue_comment>Title: Add memory leak check in CUDA tests\nusername_0: With `torch.cuda.memory_allocated` available, we can test if CUDA methods has memory leaks. This PR adds a wrapper around each CUDA tests that checks if the CUDA memory usage before and after stays constant.\n<issue_comment>username_1: this is kinda cool!\n<issue_comment>username_2: Excellent. Have you purposely introduced a memory leak and verified that this test catches it?\n<issue_comment>username_0: @username_2 I added a test for the new method in new commit.\r\n\r\ncc @username_3 for changes to `run_test.py`.\n<issue_comment>username_3: Cool, just make sure the tests actually run in the CI, even when green\n<issue_comment>username_4: Does it have any effect on the perf of the tests?\n<issue_comment>username_0: In local tests and CI test, this new mechanism detected `MaxUnpool3d` (`test_nn.TestNN.test_MaxUnpool3d_net_cuda`) and `MultiLabelMarginCriterion` leaking memory (`test_legacy_nn.TestNN.test_MultiLabelMarginCriterion_1d_cuda`).\n<issue_comment>username_0: @username_4 I don't have concrete numbers on full test suite. But for `test_cuda` on devgpu, the total time went from `20s` to `58s`. The overall test time increase on CUDA tests is likely less than 3 fold because `test_cuda` tests are small and thus the wrapper overhead area more obvious.\r\n\r\nStill, I think there are good reasons to get this in. The above detected leaks are good examples.\r\n\r\nOnce I fix those leaks, I will run the full suite and report the total increased time here.\n<issue_comment>username_2: Not for this PR, but we can do something similar for CPU leaks too; all we need to do is distinguish between tensor allocations (unlikely to stay live across tests) and metadata allocations (we won't track leaks for those.)\r\n\r\nAlthough, when @gchanan has his way all the manual refcounting in TH will go away and it should become a lot harder to make this kind of mistake.\n<issue_comment>username_0: @pytorchbot retest this please\n<issue_comment>username_2: Maybe, to avoid having to fix all of the leaking kernels before we merge this, we should just blacklist all of the currently failing tests and then subsequently fix them.\n<issue_comment>username_0: This PR also fixed the `test_cuda.py` running `TestTorch` problem, so the total test time increase on CUDA 9, CUDNN7, and py3 CI is 27s.\n<issue_comment>username_0: @username_4 @username_1 Require a review! \r\n\r\npr/caffe2-py2-gcc5-ubuntu16.04-test fail is unrelated and is being fixed.\n<issue_comment>username_2: @pytorchbot retest this please\n<issue_comment>username_0: @pytorchbot retest this please\n<issue_comment>username_0: @pytorchbot retest this please\n<issue_comment>username_0: @pytorchbot retest this please\n<issue_comment>username_2: @pytorchbot retest this please\n<issue_comment>username_0: @pytorchbot retest this please"}
{"repo": "pytorch/pytorch", "issue_id": 320115344, "issue_number": 7270, "timestamp": "2018-05-24 18:25:54+00:00", "text": "@username_4 @username_1 Require a review! \r\n\r\npr/caffe2-py2-gcc5-ubuntu16.04-test fail is unrelated and is being fixed.", "context": "<issue_start><issue_comment>Title: Add memory leak check in CUDA tests\nusername_0: With `torch.cuda.memory_allocated` available, we can test if CUDA methods has memory leaks. This PR adds a wrapper around each CUDA tests that checks if the CUDA memory usage before and after stays constant.\n<issue_comment>username_1: this is kinda cool!\n<issue_comment>username_2: Excellent. Have you purposely introduced a memory leak and verified that this test catches it?\n<issue_comment>username_0: @username_2 I added a test for the new method in new commit.\r\n\r\ncc @username_3 for changes to `run_test.py`.\n<issue_comment>username_3: Cool, just make sure the tests actually run in the CI, even when green\n<issue_comment>username_4: Does it have any effect on the perf of the tests?\n<issue_comment>username_0: In local tests and CI test, this new mechanism detected `MaxUnpool3d` (`test_nn.TestNN.test_MaxUnpool3d_net_cuda`) and `MultiLabelMarginCriterion` leaking memory (`test_legacy_nn.TestNN.test_MultiLabelMarginCriterion_1d_cuda`).\n<issue_comment>username_0: @username_4 I don't have concrete numbers on full test suite. But for `test_cuda` on devgpu, the total time went from `20s` to `58s`. The overall test time increase on CUDA tests is likely less than 3 fold because `test_cuda` tests are small and thus the wrapper overhead area more obvious.\r\n\r\nStill, I think there are good reasons to get this in. The above detected leaks are good examples.\r\n\r\nOnce I fix those leaks, I will run the full suite and report the total increased time here.\n<issue_comment>username_2: Not for this PR, but we can do something similar for CPU leaks too; all we need to do is distinguish between tensor allocations (unlikely to stay live across tests) and metadata allocations (we won't track leaks for those.)\r\n\r\nAlthough, when @gchanan has his way all the manual refcounting in TH will go away and it should become a lot harder to make this kind of mistake.\n<issue_comment>username_0: @pytorchbot retest this please\n<issue_comment>username_2: Maybe, to avoid having to fix all of the leaking kernels before we merge this, we should just blacklist all of the currently failing tests and then subsequently fix them.\n<issue_comment>username_0: This PR also fixed the `test_cuda.py` running `TestTorch` problem, so the total test time increase on CUDA 9, CUDNN7, and py3 CI is 27s.\n<issue_comment>username_0: @username_4 @username_1 Require a review! \r\n\r\npr/caffe2-py2-gcc5-ubuntu16.04-test fail is unrelated and is being fixed.\n<issue_comment>username_2: @pytorchbot retest this please\n<issue_comment>username_0: @pytorchbot retest this please\n<issue_comment>username_0: @pytorchbot retest this please\n<issue_comment>username_0: @pytorchbot retest this please\n<issue_comment>username_2: @pytorchbot retest this please\n<issue_comment>username_0: @pytorchbot retest this please"}
{"repo": "pytorch/pytorch", "issue_id": 320115344, "issue_number": 7270, "timestamp": "2018-05-29 21:09:21+00:00", "text": "@pytorchbot retest this please", "context": "<issue_start><issue_comment>Title: Add memory leak check in CUDA tests\nusername_0: With `torch.cuda.memory_allocated` available, we can test if CUDA methods has memory leaks. This PR adds a wrapper around each CUDA tests that checks if the CUDA memory usage before and after stays constant.\n<issue_comment>username_1: this is kinda cool!\n<issue_comment>username_2: Excellent. Have you purposely introduced a memory leak and verified that this test catches it?\n<issue_comment>username_0: @username_2 I added a test for the new method in new commit.\r\n\r\ncc @username_3 for changes to `run_test.py`.\n<issue_comment>username_3: Cool, just make sure the tests actually run in the CI, even when green\n<issue_comment>username_4: Does it have any effect on the perf of the tests?\n<issue_comment>username_0: In local tests and CI test, this new mechanism detected `MaxUnpool3d` (`test_nn.TestNN.test_MaxUnpool3d_net_cuda`) and `MultiLabelMarginCriterion` leaking memory (`test_legacy_nn.TestNN.test_MultiLabelMarginCriterion_1d_cuda`).\n<issue_comment>username_0: @username_4 I don't have concrete numbers on full test suite. But for `test_cuda` on devgpu, the total time went from `20s` to `58s`. The overall test time increase on CUDA tests is likely less than 3 fold because `test_cuda` tests are small and thus the wrapper overhead area more obvious.\r\n\r\nStill, I think there are good reasons to get this in. The above detected leaks are good examples.\r\n\r\nOnce I fix those leaks, I will run the full suite and report the total increased time here.\n<issue_comment>username_2: Not for this PR, but we can do something similar for CPU leaks too; all we need to do is distinguish between tensor allocations (unlikely to stay live across tests) and metadata allocations (we won't track leaks for those.)\r\n\r\nAlthough, when @gchanan has his way all the manual refcounting in TH will go away and it should become a lot harder to make this kind of mistake.\n<issue_comment>username_0: @pytorchbot retest this please\n<issue_comment>username_2: Maybe, to avoid having to fix all of the leaking kernels before we merge this, we should just blacklist all of the currently failing tests and then subsequently fix them.\n<issue_comment>username_0: This PR also fixed the `test_cuda.py` running `TestTorch` problem, so the total test time increase on CUDA 9, CUDNN7, and py3 CI is 27s.\n<issue_comment>username_0: @username_4 @username_1 Require a review! \r\n\r\npr/caffe2-py2-gcc5-ubuntu16.04-test fail is unrelated and is being fixed.\n<issue_comment>username_2: @pytorchbot retest this please\n<issue_comment>username_0: @pytorchbot retest this please\n<issue_comment>username_0: @pytorchbot retest this please\n<issue_comment>username_0: @pytorchbot retest this please\n<issue_comment>username_2: @pytorchbot retest this please\n<issue_comment>username_0: @pytorchbot retest this please"}
{"repo": "pytorch/pytorch", "issue_id": 320115344, "issue_number": 7270, "timestamp": "2018-05-29 21:33:54+00:00", "text": "@pytorchbot retest this please", "context": "<issue_start><issue_comment>Title: Add memory leak check in CUDA tests\nusername_0: With `torch.cuda.memory_allocated` available, we can test if CUDA methods has memory leaks. This PR adds a wrapper around each CUDA tests that checks if the CUDA memory usage before and after stays constant.\n<issue_comment>username_1: this is kinda cool!\n<issue_comment>username_2: Excellent. Have you purposely introduced a memory leak and verified that this test catches it?\n<issue_comment>username_0: @username_2 I added a test for the new method in new commit.\r\n\r\ncc @username_3 for changes to `run_test.py`.\n<issue_comment>username_3: Cool, just make sure the tests actually run in the CI, even when green\n<issue_comment>username_4: Does it have any effect on the perf of the tests?\n<issue_comment>username_0: In local tests and CI test, this new mechanism detected `MaxUnpool3d` (`test_nn.TestNN.test_MaxUnpool3d_net_cuda`) and `MultiLabelMarginCriterion` leaking memory (`test_legacy_nn.TestNN.test_MultiLabelMarginCriterion_1d_cuda`).\n<issue_comment>username_0: @username_4 I don't have concrete numbers on full test suite. But for `test_cuda` on devgpu, the total time went from `20s` to `58s`. The overall test time increase on CUDA tests is likely less than 3 fold because `test_cuda` tests are small and thus the wrapper overhead area more obvious.\r\n\r\nStill, I think there are good reasons to get this in. The above detected leaks are good examples.\r\n\r\nOnce I fix those leaks, I will run the full suite and report the total increased time here.\n<issue_comment>username_2: Not for this PR, but we can do something similar for CPU leaks too; all we need to do is distinguish between tensor allocations (unlikely to stay live across tests) and metadata allocations (we won't track leaks for those.)\r\n\r\nAlthough, when @gchanan has his way all the manual refcounting in TH will go away and it should become a lot harder to make this kind of mistake.\n<issue_comment>username_0: @pytorchbot retest this please\n<issue_comment>username_2: Maybe, to avoid having to fix all of the leaking kernels before we merge this, we should just blacklist all of the currently failing tests and then subsequently fix them.\n<issue_comment>username_0: This PR also fixed the `test_cuda.py` running `TestTorch` problem, so the total test time increase on CUDA 9, CUDNN7, and py3 CI is 27s.\n<issue_comment>username_0: @username_4 @username_1 Require a review! \r\n\r\npr/caffe2-py2-gcc5-ubuntu16.04-test fail is unrelated and is being fixed.\n<issue_comment>username_2: @pytorchbot retest this please\n<issue_comment>username_0: @pytorchbot retest this please\n<issue_comment>username_0: @pytorchbot retest this please\n<issue_comment>username_0: @pytorchbot retest this please\n<issue_comment>username_2: @pytorchbot retest this please\n<issue_comment>username_0: @pytorchbot retest this please"}
{"repo": "pytorch/pytorch", "issue_id": 320115344, "issue_number": 7270, "timestamp": "2018-05-29 22:43:21+00:00", "text": "@pytorchbot retest this please", "context": "<issue_start><issue_comment>Title: Add memory leak check in CUDA tests\nusername_0: With `torch.cuda.memory_allocated` available, we can test if CUDA methods has memory leaks. This PR adds a wrapper around each CUDA tests that checks if the CUDA memory usage before and after stays constant.\n<issue_comment>username_1: this is kinda cool!\n<issue_comment>username_2: Excellent. Have you purposely introduced a memory leak and verified that this test catches it?\n<issue_comment>username_0: @username_2 I added a test for the new method in new commit.\r\n\r\ncc @username_3 for changes to `run_test.py`.\n<issue_comment>username_3: Cool, just make sure the tests actually run in the CI, even when green\n<issue_comment>username_4: Does it have any effect on the perf of the tests?\n<issue_comment>username_0: In local tests and CI test, this new mechanism detected `MaxUnpool3d` (`test_nn.TestNN.test_MaxUnpool3d_net_cuda`) and `MultiLabelMarginCriterion` leaking memory (`test_legacy_nn.TestNN.test_MultiLabelMarginCriterion_1d_cuda`).\n<issue_comment>username_0: @username_4 I don't have concrete numbers on full test suite. But for `test_cuda` on devgpu, the total time went from `20s` to `58s`. The overall test time increase on CUDA tests is likely less than 3 fold because `test_cuda` tests are small and thus the wrapper overhead area more obvious.\r\n\r\nStill, I think there are good reasons to get this in. The above detected leaks are good examples.\r\n\r\nOnce I fix those leaks, I will run the full suite and report the total increased time here.\n<issue_comment>username_2: Not for this PR, but we can do something similar for CPU leaks too; all we need to do is distinguish between tensor allocations (unlikely to stay live across tests) and metadata allocations (we won't track leaks for those.)\r\n\r\nAlthough, when @gchanan has his way all the manual refcounting in TH will go away and it should become a lot harder to make this kind of mistake.\n<issue_comment>username_0: @pytorchbot retest this please\n<issue_comment>username_2: Maybe, to avoid having to fix all of the leaking kernels before we merge this, we should just blacklist all of the currently failing tests and then subsequently fix them.\n<issue_comment>username_0: This PR also fixed the `test_cuda.py` running `TestTorch` problem, so the total test time increase on CUDA 9, CUDNN7, and py3 CI is 27s.\n<issue_comment>username_0: @username_4 @username_1 Require a review! \r\n\r\npr/caffe2-py2-gcc5-ubuntu16.04-test fail is unrelated and is being fixed.\n<issue_comment>username_2: @pytorchbot retest this please\n<issue_comment>username_0: @pytorchbot retest this please\n<issue_comment>username_0: @pytorchbot retest this please\n<issue_comment>username_0: @pytorchbot retest this please\n<issue_comment>username_2: @pytorchbot retest this please\n<issue_comment>username_0: @pytorchbot retest this please"}
{"repo": "pytorch/pytorch", "issue_id": 320115344, "issue_number": 7270, "timestamp": "2018-05-30 06:30:27+00:00", "text": "@pytorchbot retest this please", "context": "<issue_start><issue_comment>Title: Add memory leak check in CUDA tests\nusername_0: With `torch.cuda.memory_allocated` available, we can test if CUDA methods has memory leaks. This PR adds a wrapper around each CUDA tests that checks if the CUDA memory usage before and after stays constant.\n<issue_comment>username_1: this is kinda cool!\n<issue_comment>username_2: Excellent. Have you purposely introduced a memory leak and verified that this test catches it?\n<issue_comment>username_0: @username_2 I added a test for the new method in new commit.\r\n\r\ncc @username_3 for changes to `run_test.py`.\n<issue_comment>username_3: Cool, just make sure the tests actually run in the CI, even when green\n<issue_comment>username_4: Does it have any effect on the perf of the tests?\n<issue_comment>username_0: In local tests and CI test, this new mechanism detected `MaxUnpool3d` (`test_nn.TestNN.test_MaxUnpool3d_net_cuda`) and `MultiLabelMarginCriterion` leaking memory (`test_legacy_nn.TestNN.test_MultiLabelMarginCriterion_1d_cuda`).\n<issue_comment>username_0: @username_4 I don't have concrete numbers on full test suite. But for `test_cuda` on devgpu, the total time went from `20s` to `58s`. The overall test time increase on CUDA tests is likely less than 3 fold because `test_cuda` tests are small and thus the wrapper overhead area more obvious.\r\n\r\nStill, I think there are good reasons to get this in. The above detected leaks are good examples.\r\n\r\nOnce I fix those leaks, I will run the full suite and report the total increased time here.\n<issue_comment>username_2: Not for this PR, but we can do something similar for CPU leaks too; all we need to do is distinguish between tensor allocations (unlikely to stay live across tests) and metadata allocations (we won't track leaks for those.)\r\n\r\nAlthough, when @gchanan has his way all the manual refcounting in TH will go away and it should become a lot harder to make this kind of mistake.\n<issue_comment>username_0: @pytorchbot retest this please\n<issue_comment>username_2: Maybe, to avoid having to fix all of the leaking kernels before we merge this, we should just blacklist all of the currently failing tests and then subsequently fix them.\n<issue_comment>username_0: This PR also fixed the `test_cuda.py` running `TestTorch` problem, so the total test time increase on CUDA 9, CUDNN7, and py3 CI is 27s.\n<issue_comment>username_0: @username_4 @username_1 Require a review! \r\n\r\npr/caffe2-py2-gcc5-ubuntu16.04-test fail is unrelated and is being fixed.\n<issue_comment>username_2: @pytorchbot retest this please\n<issue_comment>username_0: @pytorchbot retest this please\n<issue_comment>username_0: @pytorchbot retest this please\n<issue_comment>username_0: @pytorchbot retest this please\n<issue_comment>username_2: @pytorchbot retest this please\n<issue_comment>username_0: @pytorchbot retest this please"}
{"repo": "pytorch/pytorch", "issue_id": 320115344, "issue_number": 7270, "timestamp": "2018-05-30 11:00:55+00:00", "text": "@pytorchbot retest this please", "context": "<issue_start><issue_comment>Title: Add memory leak check in CUDA tests\nusername_0: With `torch.cuda.memory_allocated` available, we can test if CUDA methods has memory leaks. This PR adds a wrapper around each CUDA tests that checks if the CUDA memory usage before and after stays constant.\n<issue_comment>username_1: this is kinda cool!\n<issue_comment>username_2: Excellent. Have you purposely introduced a memory leak and verified that this test catches it?\n<issue_comment>username_0: @username_2 I added a test for the new method in new commit.\r\n\r\ncc @username_3 for changes to `run_test.py`.\n<issue_comment>username_3: Cool, just make sure the tests actually run in the CI, even when green\n<issue_comment>username_4: Does it have any effect on the perf of the tests?\n<issue_comment>username_0: In local tests and CI test, this new mechanism detected `MaxUnpool3d` (`test_nn.TestNN.test_MaxUnpool3d_net_cuda`) and `MultiLabelMarginCriterion` leaking memory (`test_legacy_nn.TestNN.test_MultiLabelMarginCriterion_1d_cuda`).\n<issue_comment>username_0: @username_4 I don't have concrete numbers on full test suite. But for `test_cuda` on devgpu, the total time went from `20s` to `58s`. The overall test time increase on CUDA tests is likely less than 3 fold because `test_cuda` tests are small and thus the wrapper overhead area more obvious.\r\n\r\nStill, I think there are good reasons to get this in. The above detected leaks are good examples.\r\n\r\nOnce I fix those leaks, I will run the full suite and report the total increased time here.\n<issue_comment>username_2: Not for this PR, but we can do something similar for CPU leaks too; all we need to do is distinguish between tensor allocations (unlikely to stay live across tests) and metadata allocations (we won't track leaks for those.)\r\n\r\nAlthough, when @gchanan has his way all the manual refcounting in TH will go away and it should become a lot harder to make this kind of mistake.\n<issue_comment>username_0: @pytorchbot retest this please\n<issue_comment>username_2: Maybe, to avoid having to fix all of the leaking kernels before we merge this, we should just blacklist all of the currently failing tests and then subsequently fix them.\n<issue_comment>username_0: This PR also fixed the `test_cuda.py` running `TestTorch` problem, so the total test time increase on CUDA 9, CUDNN7, and py3 CI is 27s.\n<issue_comment>username_0: @username_4 @username_1 Require a review! \r\n\r\npr/caffe2-py2-gcc5-ubuntu16.04-test fail is unrelated and is being fixed.\n<issue_comment>username_2: @pytorchbot retest this please\n<issue_comment>username_0: @pytorchbot retest this please\n<issue_comment>username_0: @pytorchbot retest this please\n<issue_comment>username_0: @pytorchbot retest this please\n<issue_comment>username_2: @pytorchbot retest this please\n<issue_comment>username_0: @pytorchbot retest this please"}
{"repo": "pytorch/pytorch", "issue_id": 320115344, "issue_number": 7270, "timestamp": "2018-05-30 21:27:37+00:00", "text": "@pytorchbot retest this please", "context": "<issue_start><issue_comment>Title: Add memory leak check in CUDA tests\nusername_0: With `torch.cuda.memory_allocated` available, we can test if CUDA methods has memory leaks. This PR adds a wrapper around each CUDA tests that checks if the CUDA memory usage before and after stays constant.\n<issue_comment>username_1: this is kinda cool!\n<issue_comment>username_2: Excellent. Have you purposely introduced a memory leak and verified that this test catches it?\n<issue_comment>username_0: @username_2 I added a test for the new method in new commit.\r\n\r\ncc @username_3 for changes to `run_test.py`.\n<issue_comment>username_3: Cool, just make sure the tests actually run in the CI, even when green\n<issue_comment>username_4: Does it have any effect on the perf of the tests?\n<issue_comment>username_0: In local tests and CI test, this new mechanism detected `MaxUnpool3d` (`test_nn.TestNN.test_MaxUnpool3d_net_cuda`) and `MultiLabelMarginCriterion` leaking memory (`test_legacy_nn.TestNN.test_MultiLabelMarginCriterion_1d_cuda`).\n<issue_comment>username_0: @username_4 I don't have concrete numbers on full test suite. But for `test_cuda` on devgpu, the total time went from `20s` to `58s`. The overall test time increase on CUDA tests is likely less than 3 fold because `test_cuda` tests are small and thus the wrapper overhead area more obvious.\r\n\r\nStill, I think there are good reasons to get this in. The above detected leaks are good examples.\r\n\r\nOnce I fix those leaks, I will run the full suite and report the total increased time here.\n<issue_comment>username_2: Not for this PR, but we can do something similar for CPU leaks too; all we need to do is distinguish between tensor allocations (unlikely to stay live across tests) and metadata allocations (we won't track leaks for those.)\r\n\r\nAlthough, when @gchanan has his way all the manual refcounting in TH will go away and it should become a lot harder to make this kind of mistake.\n<issue_comment>username_0: @pytorchbot retest this please\n<issue_comment>username_2: Maybe, to avoid having to fix all of the leaking kernels before we merge this, we should just blacklist all of the currently failing tests and then subsequently fix them.\n<issue_comment>username_0: This PR also fixed the `test_cuda.py` running `TestTorch` problem, so the total test time increase on CUDA 9, CUDNN7, and py3 CI is 27s.\n<issue_comment>username_0: @username_4 @username_1 Require a review! \r\n\r\npr/caffe2-py2-gcc5-ubuntu16.04-test fail is unrelated and is being fixed.\n<issue_comment>username_2: @pytorchbot retest this please\n<issue_comment>username_0: @pytorchbot retest this please\n<issue_comment>username_0: @pytorchbot retest this please\n<issue_comment>username_0: @pytorchbot retest this please\n<issue_comment>username_2: @pytorchbot retest this please\n<issue_comment>username_0: @pytorchbot retest this please"}
{"repo": "pytorch/pytorch", "issue_id": 459270796, "issue_number": 22076, "timestamp": "2019-06-21T16:09:17Z", "text": "See https://pybind11.readthedocs.io/en/stable/faq.html#someclass-declared-with-greater-visibility-than-the-type-of-its-field-someclass-member-wattributes\r\n\r\nLet's see whether it passes CI", "context": "<issue_start><issue_comment>Title: Add hidden visibility to _C.so\nusername_0: See https://pybind11.readthedocs.io/en/stable/faq.html#someclass-declared-with-greater-visibility-than-the-type-of-its-field-someclass-member-wattributes\r\n\r\nLet's see whether it passes CI\n<issue_comment>username_1: wouldn't C++ extensions fail because of this? they require the symbols coming out of libtorch_python and stuff to be global visibility in the current process, or else they fail to load at runtime (but the C++ extensions will compile fine)\n<issue_comment>username_2: UBSAN failure looks real:\r\n\r\n```\r\nJun 21 16:45:39 /var/lib/jenkins/workspace/build/aten/src/ATen/Functions.h:2779:12: runtime error: call to function at::TypeDefault::randn(c10::ArrayRef<long>, c10::TensorOptions const&) through pointer to incorrect function type 'at::Tensor (*)(c10::ArrayRef<long>, const c10::TensorOptions &)'\r\nJun 21 16:45:39 (/opt/conda/lib/python3.6/site-packages/torch/lib/libtorch.so+0xa6b7860): note: at::TypeDefault::randn(c10::ArrayRef<long>, c10::TensorOptions const&) defined here\r\nJun 21 16:45:40     #0 0x7f25f2696882 in torch::randn(c10::ArrayRef<long>, c10::TensorOptions const&) (/opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_python.so+0x11aa882)\r\nJun 21 16:45:40     #1 0x7f25f2694ccc in torch::autograd::dispatch_randn(c10::ArrayRef<long>, c10::TensorOptions const&) (/opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_python.so+0x11a8ccc)\r\nJun 21 16:45:40     #2 0x7f25f258d08d in torch::autograd::THPVariable_randn(_object*, _object*, _object*) (/opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_python.so+0x10a108d)\r\nJun 21 16:45:40     #3 0x558025dc7743 in _PyCFunction_FastCallDict /tmp/build/80754af9/python_1546130271559/work/Objects/methodobject.c:231\r\nJun 21 16:45:40     #4 0x558025e4e42b in call_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4851\r\nJun 21 16:45:40     #5 0x558025e73389 in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3335\r\nJun 21 16:45:40     #6 0x558025e47bfd in _PyEval_EvalCodeWithName /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4166\r\nJun 21 16:45:40     #7 0x558025e48770 in fast_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4992\r\nJun 21 16:45:40     #8 0x558025e4e504 in call_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4872\r\nJun 21 16:45:40     #9 0x558025e73389 in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3335\r\nJun 21 16:45:40     #10 0x558025e49288 in _PyEval_EvalCodeWithName /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4166\r\nJun 21 16:45:40     #11 0x558025e49288 in PyEval_EvalCodeEx /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4187\r\nJun 21 16:45:40     #12 0x558025e4a01b in PyEval_EvalCode /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:731\r\nJun 21 16:45:40     #13 0x558025e70c8a in builtin_exec_impl.isra.11 /tmp/build/80754af9/python_1546130271559/work/Python/bltinmodule.c:983\r\nJun 21 16:45:40     #14 0x558025e70c8a in builtin_exec /tmp/build/80754af9/python_1546130271559/work/Python/clinic/bltinmodule.c.h:283\r\nJun 21 16:45:40     #15 0x558025dca710 in PyCFunction_Call /tmp/build/80754af9/python_1546130271559/work/Objects/methodobject.c:114\r\nJun 21 16:45:40     #16 0x558025e784ac in do_call_core /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:5116\r\nJun 21 16:45:40     #17 0x558025e784ac in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3404\r\nJun 21 16:45:40     #18 0x558025e478e3 in _PyEval_EvalCodeWithName /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4166\r\nJun 21 16:45:40     #19 0x558025e48770 in fast_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4992\r\nJun 21 16:45:40     #20 0x558025e4e504 in call_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4872\r\nJun 21 16:45:40     #21 0x558025e73389 in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3335\r\nJun 21 16:45:40     #22 0x558025e4853a in _PyFunction_FastCall /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4933\r\nJun 21 16:45:40     #23 0x558025e4853a in fast_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4968\r\nJun 21 16:45:40     #24 0x558025e4e504 in call_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4872\r\nJun 21 16:45:40     #25 0x558025e73389 in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3335\r\nJun 21 16:45:40     #26 0x558025e4853a in _PyFunction_FastCall /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4933\r\nJun 21 16:45:40     #27 0x558025e4853a in fast_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4968\r\nJun 21 16:45:40     #28 0x558025e4e504 in call_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4872\r\nJun 21 16:45:40     #29 0x558025e73389 in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3335\r\nJun 21 16:45:40     #30 0x558025e4853a in _PyFunction_FastCall /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4933\r\nJun 21 16:45:40     #31 0x558025e4853a in fast_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4968\r\nJun 21 16:45:40     #32 0x558025e4e504 in call_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4872\r\nJun 21 16:45:40     #33 0x558025e73389 in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3335\r\nJun 21 16:45:40     #34 0x558025e48baa in _PyFunction_FastCall /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4933\r\nJun 21 16:45:40     #35 0x558025e48baa in _PyFunction_FastCallDict /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:5035\r\nJun 21 16:45:40     #36 0x558025dc7b0e in _PyObject_FastCallDict /tmp/build/80754af9/python_1546130271559/work/Objects/abstract.c:2310\r\nJun 21 16:45:40     #37 0x558025e0980f in _PyObject_CallMethodIdObjArgs /tmp/build/80754af9/python_1546130271559/work/Objects/abstract.c:2796\r\nJun 21 16:45:40     #38 0x558025dbeb0f in PyImport_ImportModuleLevelObject /tmp/build/80754af9/python_1546130271559/work/Python/import.c:1578\r\nJun 21 16:45:40     #39 0x558025e75a8a in import_name /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:5245\r\nJun 21 16:45:40     #40 0x558025e75a8a in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:2899\r\nJun 21 16:45:40     #41 0x558025e49288 in _PyEval_EvalCodeWithName /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4166\r\nJun 21 16:45:40     #42 0x558025e49288 in PyEval_EvalCodeEx /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4187\r\nJun 21 16:45:40     #43 0x558025e4a01b in PyEval_EvalCode /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:731\r\nJun 21 16:45:40     #44 0x558025ecc3c3 in run_mod /tmp/build/80754af9/python_1546130271559/work/Python/pythonrun.c:1025\r\nJun 21 16:45:40     #45 0x558025ecc7c0 in PyRun_FileExFlags /tmp/build/80754af9/python_1546130271559/work/Python/pythonrun.c:978\r\nJun 21 16:45:40     #46 0x558025ecc9c2 in PyRun_SimpleFileExFlags /tmp/build/80754af9/python_1546130271559/work/Python/pythonrun.c:419\r\nJun 21 16:45:40     #47 0x558025ed04b2 in run_file /tmp/build/80754af9/python_1546130271559/work/Modules/main.c:340\r\nJun 21 16:45:40     #48 0x558025ed04b2 in Py_Main /tmp/build/80754af9/python_1546130271559/work/Modules/main.c:811\r\nJun 21 16:45:40     #49 0x558025d9902d in main /tmp/build/80754af9/python_1546130271559/work/Programs/python.c:69\r\nJun 21 16:45:40     #50 0x7f26053c082f in __libc_start_main /build/glibc-LK5gWL/glibc-2.23/csu/../csu/libc-start.c:291\r\nJun 21 16:45:40     #51 0x558025e79e0d in _start /home/rdonnelly/mc/conda-bld/compilers_linux-64_1534865402226/work/.build/src/glibc-2.12.2/csu/../sysdeps/x86_64/elf/start.S:103\r\nJun 21 16:45:40 \r\nJun 21 16:45:40 SUMMARY: UndefinedBehaviorSanitizer: undefined-behavior /var/lib/jenkins/workspace/build/aten/src/ATen/Functions.h:2779:12 in \r\nJun 21 16:45:40 Traceback (most recent call last):\r\n```\n<issue_comment>username_2: I guess, ideally, we'd expose symbols that our ours, but not expose anything from pybind11. I don't know how easy that is to do. (Maybe this is a reason not to use pybind11, but that ship has kind of sailed...)\n<issue_comment>username_3: If everything works on MSVC with its default option -- that is, no exposure unless explicitly asked to -- could replacing all occurrences of `__declspec(dllexport)` with something also compatible with `gcc` work? Like something similar to [this](https://stackoverflow.com/a/8258786/1150462)?\n<issue_comment>username_0: Yes, I realized that it needs adjusting annotations.\r\n\r\nAs @username_3 pointed out, we should have already annotated all the right things to make Windows work. But we use windows-only macro in THP_API/THP_CLASS (https://github.com/pytorch/pytorch/blob/master/torch/csrc/THP_export.h). We should do the same thing as for other places - https://github.com/pytorch/pytorch/blob/master/c10/macros/Export.h\r\n\r\nI'll try to get back to it at some point."}
{"repo": "pytorch/pytorch", "issue_id": 459270796, "issue_number": 22076, "timestamp": "2019-06-21 16:23:35+00:00", "text": "wouldn't C++ extensions fail because of this? they require the symbols coming out of libtorch_python and stuff to be global visibility in the current process, or else they fail to load at runtime (but the C++ extensions will compile fine)", "context": "<issue_start><issue_comment>Title: Add hidden visibility to _C.so\nusername_0: See https://pybind11.readthedocs.io/en/stable/faq.html#someclass-declared-with-greater-visibility-than-the-type-of-its-field-someclass-member-wattributes\r\n\r\nLet's see whether it passes CI\n<issue_comment>username_1: wouldn't C++ extensions fail because of this? they require the symbols coming out of libtorch_python and stuff to be global visibility in the current process, or else they fail to load at runtime (but the C++ extensions will compile fine)\n<issue_comment>username_2: UBSAN failure looks real:\r\n\r\n```\r\nJun 21 16:45:39 /var/lib/jenkins/workspace/build/aten/src/ATen/Functions.h:2779:12: runtime error: call to function at::TypeDefault::randn(c10::ArrayRef<long>, c10::TensorOptions const&) through pointer to incorrect function type 'at::Tensor (*)(c10::ArrayRef<long>, const c10::TensorOptions &)'\r\nJun 21 16:45:39 (/opt/conda/lib/python3.6/site-packages/torch/lib/libtorch.so+0xa6b7860): note: at::TypeDefault::randn(c10::ArrayRef<long>, c10::TensorOptions const&) defined here\r\nJun 21 16:45:40     #0 0x7f25f2696882 in torch::randn(c10::ArrayRef<long>, c10::TensorOptions const&) (/opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_python.so+0x11aa882)\r\nJun 21 16:45:40     #1 0x7f25f2694ccc in torch::autograd::dispatch_randn(c10::ArrayRef<long>, c10::TensorOptions const&) (/opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_python.so+0x11a8ccc)\r\nJun 21 16:45:40     #2 0x7f25f258d08d in torch::autograd::THPVariable_randn(_object*, _object*, _object*) (/opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_python.so+0x10a108d)\r\nJun 21 16:45:40     #3 0x558025dc7743 in _PyCFunction_FastCallDict /tmp/build/80754af9/python_1546130271559/work/Objects/methodobject.c:231\r\nJun 21 16:45:40     #4 0x558025e4e42b in call_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4851\r\nJun 21 16:45:40     #5 0x558025e73389 in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3335\r\nJun 21 16:45:40     #6 0x558025e47bfd in _PyEval_EvalCodeWithName /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4166\r\nJun 21 16:45:40     #7 0x558025e48770 in fast_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4992\r\nJun 21 16:45:40     #8 0x558025e4e504 in call_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4872\r\nJun 21 16:45:40     #9 0x558025e73389 in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3335\r\nJun 21 16:45:40     #10 0x558025e49288 in _PyEval_EvalCodeWithName /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4166\r\nJun 21 16:45:40     #11 0x558025e49288 in PyEval_EvalCodeEx /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4187\r\nJun 21 16:45:40     #12 0x558025e4a01b in PyEval_EvalCode /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:731\r\nJun 21 16:45:40     #13 0x558025e70c8a in builtin_exec_impl.isra.11 /tmp/build/80754af9/python_1546130271559/work/Python/bltinmodule.c:983\r\nJun 21 16:45:40     #14 0x558025e70c8a in builtin_exec /tmp/build/80754af9/python_1546130271559/work/Python/clinic/bltinmodule.c.h:283\r\nJun 21 16:45:40     #15 0x558025dca710 in PyCFunction_Call /tmp/build/80754af9/python_1546130271559/work/Objects/methodobject.c:114\r\nJun 21 16:45:40     #16 0x558025e784ac in do_call_core /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:5116\r\nJun 21 16:45:40     #17 0x558025e784ac in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3404\r\nJun 21 16:45:40     #18 0x558025e478e3 in _PyEval_EvalCodeWithName /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4166\r\nJun 21 16:45:40     #19 0x558025e48770 in fast_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4992\r\nJun 21 16:45:40     #20 0x558025e4e504 in call_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4872\r\nJun 21 16:45:40     #21 0x558025e73389 in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3335\r\nJun 21 16:45:40     #22 0x558025e4853a in _PyFunction_FastCall /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4933\r\nJun 21 16:45:40     #23 0x558025e4853a in fast_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4968\r\nJun 21 16:45:40     #24 0x558025e4e504 in call_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4872\r\nJun 21 16:45:40     #25 0x558025e73389 in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3335\r\nJun 21 16:45:40     #26 0x558025e4853a in _PyFunction_FastCall /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4933\r\nJun 21 16:45:40     #27 0x558025e4853a in fast_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4968\r\nJun 21 16:45:40     #28 0x558025e4e504 in call_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4872\r\nJun 21 16:45:40     #29 0x558025e73389 in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3335\r\nJun 21 16:45:40     #30 0x558025e4853a in _PyFunction_FastCall /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4933\r\nJun 21 16:45:40     #31 0x558025e4853a in fast_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4968\r\nJun 21 16:45:40     #32 0x558025e4e504 in call_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4872\r\nJun 21 16:45:40     #33 0x558025e73389 in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3335\r\nJun 21 16:45:40     #34 0x558025e48baa in _PyFunction_FastCall /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4933\r\nJun 21 16:45:40     #35 0x558025e48baa in _PyFunction_FastCallDict /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:5035\r\nJun 21 16:45:40     #36 0x558025dc7b0e in _PyObject_FastCallDict /tmp/build/80754af9/python_1546130271559/work/Objects/abstract.c:2310\r\nJun 21 16:45:40     #37 0x558025e0980f in _PyObject_CallMethodIdObjArgs /tmp/build/80754af9/python_1546130271559/work/Objects/abstract.c:2796\r\nJun 21 16:45:40     #38 0x558025dbeb0f in PyImport_ImportModuleLevelObject /tmp/build/80754af9/python_1546130271559/work/Python/import.c:1578\r\nJun 21 16:45:40     #39 0x558025e75a8a in import_name /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:5245\r\nJun 21 16:45:40     #40 0x558025e75a8a in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:2899\r\nJun 21 16:45:40     #41 0x558025e49288 in _PyEval_EvalCodeWithName /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4166\r\nJun 21 16:45:40     #42 0x558025e49288 in PyEval_EvalCodeEx /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4187\r\nJun 21 16:45:40     #43 0x558025e4a01b in PyEval_EvalCode /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:731\r\nJun 21 16:45:40     #44 0x558025ecc3c3 in run_mod /tmp/build/80754af9/python_1546130271559/work/Python/pythonrun.c:1025\r\nJun 21 16:45:40     #45 0x558025ecc7c0 in PyRun_FileExFlags /tmp/build/80754af9/python_1546130271559/work/Python/pythonrun.c:978\r\nJun 21 16:45:40     #46 0x558025ecc9c2 in PyRun_SimpleFileExFlags /tmp/build/80754af9/python_1546130271559/work/Python/pythonrun.c:419\r\nJun 21 16:45:40     #47 0x558025ed04b2 in run_file /tmp/build/80754af9/python_1546130271559/work/Modules/main.c:340\r\nJun 21 16:45:40     #48 0x558025ed04b2 in Py_Main /tmp/build/80754af9/python_1546130271559/work/Modules/main.c:811\r\nJun 21 16:45:40     #49 0x558025d9902d in main /tmp/build/80754af9/python_1546130271559/work/Programs/python.c:69\r\nJun 21 16:45:40     #50 0x7f26053c082f in __libc_start_main /build/glibc-LK5gWL/glibc-2.23/csu/../csu/libc-start.c:291\r\nJun 21 16:45:40     #51 0x558025e79e0d in _start /home/rdonnelly/mc/conda-bld/compilers_linux-64_1534865402226/work/.build/src/glibc-2.12.2/csu/../sysdeps/x86_64/elf/start.S:103\r\nJun 21 16:45:40 \r\nJun 21 16:45:40 SUMMARY: UndefinedBehaviorSanitizer: undefined-behavior /var/lib/jenkins/workspace/build/aten/src/ATen/Functions.h:2779:12 in \r\nJun 21 16:45:40 Traceback (most recent call last):\r\n```\n<issue_comment>username_2: I guess, ideally, we'd expose symbols that our ours, but not expose anything from pybind11. I don't know how easy that is to do. (Maybe this is a reason not to use pybind11, but that ship has kind of sailed...)\n<issue_comment>username_3: If everything works on MSVC with its default option -- that is, no exposure unless explicitly asked to -- could replacing all occurrences of `__declspec(dllexport)` with something also compatible with `gcc` work? Like something similar to [this](https://stackoverflow.com/a/8258786/1150462)?\n<issue_comment>username_0: Yes, I realized that it needs adjusting annotations.\r\n\r\nAs @username_3 pointed out, we should have already annotated all the right things to make Windows work. But we use windows-only macro in THP_API/THP_CLASS (https://github.com/pytorch/pytorch/blob/master/torch/csrc/THP_export.h). We should do the same thing as for other places - https://github.com/pytorch/pytorch/blob/master/c10/macros/Export.h\r\n\r\nI'll try to get back to it at some point."}
{"repo": "pytorch/pytorch", "issue_id": 459270796, "issue_number": 22076, "timestamp": "2019-06-21 17:11:26+00:00", "text": "UBSAN failure looks real:\r\n\r\n```\r\nJun 21 16:45:39 /var/lib/jenkins/workspace/build/aten/src/ATen/Functions.h:2779:12: runtime error: call to function at::TypeDefault::randn(c10::ArrayRef<long>, c10::TensorOptions const&) through pointer to incorrect function type 'at::Tensor (*)(c10::ArrayRef<long>, const c10::TensorOptions &)'\r\nJun 21 16:45:39 (/opt/conda/lib/python3.6/site-packages/torch/lib/libtorch.so+0xa6b7860): note: at::TypeDefault::randn(c10::ArrayRef<long>, c10::TensorOptions const&) defined here\r\nJun 21 16:45:40     #0 0x7f25f2696882 in torch::randn(c10::ArrayRef<long>, c10::TensorOptions const&) (/opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_python.so+0x11aa882)\r\nJun 21 16:45:40     #1 0x7f25f2694ccc in torch::autograd::dispatch_randn(c10::ArrayRef<long>, c10::TensorOptions const&) (/opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_python.so+0x11a8ccc)\r\nJun 21 16:45:40     #2 0x7f25f258d08d in torch::autograd::THPVariable_randn(_object*, _object*, _object*) (/opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_python.so+0x10a108d)\r\nJun 21 16:45:40     #3 0x558025dc7743 in _PyCFunction_FastCallDict /tmp/build/80754af9/python_1546130271559/work/Objects/methodobject.c:231\r\nJun 21 16:45:40     #4 0x558025e4e42b in call_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4851\r\nJun 21 16:45:40     #5 0x558025e73389 in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3335\r\nJun 21 16:45:40     #6 0x558025e47bfd in _PyEval_EvalCodeWithName /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4166\r\nJun 21 16:45:40     #7 0x558025e48770 in fast_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4992\r\nJun 21 16:45:40     #8 0x558025e4e504 in call_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4872\r\nJun 21 16:45:40     #9 0x558025e73389 in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3335\r\nJun 21 16:45:40     #10 0x558025e49288 in _PyEval_EvalCodeWithName /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4166\r\nJun 21 16:45:40     #11 0x558025e49288 in PyEval_EvalCodeEx /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4187\r\nJun 21 16:45:40     #12 0x558025e4a01b in PyEval_EvalCode /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:731\r\nJun 21 16:45:40     #13 0x558025e70c8a in builtin_exec_impl.isra.11 /tmp/build/80754af9/python_1546130271559/work/Python/bltinmodule.c:983\r\nJun 21 16:45:40     #14 0x558025e70c8a in builtin_exec /tmp/build/80754af9/python_1546130271559/work/Python/clinic/bltinmodule.c.h:283\r\nJun 21 16:45:40     #15 0x558025dca710 in PyCFunction_Call /tmp/build/80754af9/python_1546130271559/work/Objects/methodobject.c:114\r\nJun 21 16:45:40     #16 0x558025e784ac in do_call_core /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:5116\r\nJun 21 16:45:40     #17 0x558025e784ac in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3404\r\nJun 21 16:45:40     #18 0x558025e478e3 in _PyEval_EvalCodeWithName /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4166\r\nJun 21 16:45:40     #19 0x558025e48770 in fast_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4992\r\nJun 21 16:45:40     #20 0x558025e4e504 in call_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4872\r\nJun 21 16:45:40     #21 0x558025e73389 in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3335\r\nJun 21 16:45:40     #22 0x558025e4853a in _PyFunction_FastCall /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4933\r\nJun 21 16:45:40     #23 0x558025e4853a in fast_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4968\r\nJun 21 16:45:40     #24 0x558025e4e504 in call_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4872\r\nJun 21 16:45:40     #25 0x558025e73389 in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3335\r\nJun 21 16:45:40     #26 0x558025e4853a in _PyFunction_FastCall /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4933\r\nJun 21 16:45:40     #27 0x558025e4853a in fast_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4968\r\nJun 21 16:45:40     #28 0x558025e4e504 in call_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4872\r\nJun 21 16:45:40     #29 0x558025e73389 in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3335\r\nJun 21 16:45:40     #30 0x558025e4853a in _PyFunction_FastCall /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4933\r\nJun 21 16:45:40     #31 0x558025e4853a in fast_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4968\r\nJun 21 16:45:40     #32 0x558025e4e504 in call_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4872\r\nJun 21 16:45:40     #33 0x558025e73389 in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3335\r\nJun 21 16:45:40     #34 0x558025e48baa in _PyFunction_FastCall /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4933\r\nJun 21 16:45:40     #35 0x558025e48baa in _PyFunction_FastCallDict /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:5035\r\nJun 21 16:45:40     #36 0x558025dc7b0e in _PyObject_FastCallDict /tmp/build/80754af9/python_1546130271559/work/Objects/abstract.c:2310\r\nJun 21 16:45:40     #37 0x558025e0980f in _PyObject_CallMethodIdObjArgs /tmp/build/80754af9/python_1546130271559/work/Objects/abstract.c:2796\r\nJun 21 16:45:40     #38 0x558025dbeb0f in PyImport_ImportModuleLevelObject /tmp/build/80754af9/python_1546130271559/work/Python/import.c:1578\r\nJun 21 16:45:40     #39 0x558025e75a8a in import_name /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:5245\r\nJun 21 16:45:40     #40 0x558025e75a8a in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:2899\r\nJun 21 16:45:40     #41 0x558025e49288 in _PyEval_EvalCodeWithName /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4166\r\nJun 21 16:45:40     #42 0x558025e49288 in PyEval_EvalCodeEx /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4187\r\nJun 21 16:45:40     #43 0x558025e4a01b in PyEval_EvalCode /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:731\r\nJun 21 16:45:40     #44 0x558025ecc3c3 in run_mod /tmp/build/80754af9/python_1546130271559/work/Python/pythonrun.c:1025\r\nJun 21 16:45:40     #45 0x558025ecc7c0 in PyRun_FileExFlags /tmp/build/80754af9/python_1546130271559/work/Python/pythonrun.c:978\r\nJun 21 16:45:40     #46 0x558025ecc9c2 in PyRun_SimpleFileExFlags /tmp/build/80754af9/python_1546130271559/work/Python/pythonrun.c:419\r\nJun 21 16:45:40     #47 0x558025ed04b2 in run_file /tmp/build/80754af9/python_1546130271559/work/Modules/main.c:340\r\nJun 21 16:45:40     #48 0x558025ed04b2 in Py_Main /tmp/build/80754af9/python_1546130271559/work/Modules/main.c:811\r\nJun 21 16:45:40     #49 0x558025d9902d in main /tmp/build/80754af9/python_1546130271559/work/Programs/python.c:69\r\nJun 21 16:45:40     #50 0x7f26053c082f in __libc_start_main /build/glibc-LK5gWL/glibc-2.23/csu/../csu/libc-start.c:291\r\nJun 21 16:45:40     #51 0x558025e79e0d in _start /home/rdonnelly/mc/conda-bld/compilers_linux-64_1534865402226/work/.build/src/glibc-2.12.2/csu/../sysdeps/x86_64/elf/start.S:103\r\nJun 21 16:45:40 \r\nJun 21 16:45:40 SUMMARY: UndefinedBehaviorSanitizer: undefined-behavior /var/lib/jenkins/workspace/build/aten/src/ATen/Functions.h:2779:12 in \r\nJun 21 16:45:40 Traceback (most recent call last):\r\n```", "context": "<issue_start><issue_comment>Title: Add hidden visibility to _C.so\nusername_0: See https://pybind11.readthedocs.io/en/stable/faq.html#someclass-declared-with-greater-visibility-than-the-type-of-its-field-someclass-member-wattributes\r\n\r\nLet's see whether it passes CI\n<issue_comment>username_1: wouldn't C++ extensions fail because of this? they require the symbols coming out of libtorch_python and stuff to be global visibility in the current process, or else they fail to load at runtime (but the C++ extensions will compile fine)\n<issue_comment>username_2: UBSAN failure looks real:\r\n\r\n```\r\nJun 21 16:45:39 /var/lib/jenkins/workspace/build/aten/src/ATen/Functions.h:2779:12: runtime error: call to function at::TypeDefault::randn(c10::ArrayRef<long>, c10::TensorOptions const&) through pointer to incorrect function type 'at::Tensor (*)(c10::ArrayRef<long>, const c10::TensorOptions &)'\r\nJun 21 16:45:39 (/opt/conda/lib/python3.6/site-packages/torch/lib/libtorch.so+0xa6b7860): note: at::TypeDefault::randn(c10::ArrayRef<long>, c10::TensorOptions const&) defined here\r\nJun 21 16:45:40     #0 0x7f25f2696882 in torch::randn(c10::ArrayRef<long>, c10::TensorOptions const&) (/opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_python.so+0x11aa882)\r\nJun 21 16:45:40     #1 0x7f25f2694ccc in torch::autograd::dispatch_randn(c10::ArrayRef<long>, c10::TensorOptions const&) (/opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_python.so+0x11a8ccc)\r\nJun 21 16:45:40     #2 0x7f25f258d08d in torch::autograd::THPVariable_randn(_object*, _object*, _object*) (/opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_python.so+0x10a108d)\r\nJun 21 16:45:40     #3 0x558025dc7743 in _PyCFunction_FastCallDict /tmp/build/80754af9/python_1546130271559/work/Objects/methodobject.c:231\r\nJun 21 16:45:40     #4 0x558025e4e42b in call_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4851\r\nJun 21 16:45:40     #5 0x558025e73389 in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3335\r\nJun 21 16:45:40     #6 0x558025e47bfd in _PyEval_EvalCodeWithName /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4166\r\nJun 21 16:45:40     #7 0x558025e48770 in fast_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4992\r\nJun 21 16:45:40     #8 0x558025e4e504 in call_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4872\r\nJun 21 16:45:40     #9 0x558025e73389 in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3335\r\nJun 21 16:45:40     #10 0x558025e49288 in _PyEval_EvalCodeWithName /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4166\r\nJun 21 16:45:40     #11 0x558025e49288 in PyEval_EvalCodeEx /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4187\r\nJun 21 16:45:40     #12 0x558025e4a01b in PyEval_EvalCode /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:731\r\nJun 21 16:45:40     #13 0x558025e70c8a in builtin_exec_impl.isra.11 /tmp/build/80754af9/python_1546130271559/work/Python/bltinmodule.c:983\r\nJun 21 16:45:40     #14 0x558025e70c8a in builtin_exec /tmp/build/80754af9/python_1546130271559/work/Python/clinic/bltinmodule.c.h:283\r\nJun 21 16:45:40     #15 0x558025dca710 in PyCFunction_Call /tmp/build/80754af9/python_1546130271559/work/Objects/methodobject.c:114\r\nJun 21 16:45:40     #16 0x558025e784ac in do_call_core /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:5116\r\nJun 21 16:45:40     #17 0x558025e784ac in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3404\r\nJun 21 16:45:40     #18 0x558025e478e3 in _PyEval_EvalCodeWithName /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4166\r\nJun 21 16:45:40     #19 0x558025e48770 in fast_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4992\r\nJun 21 16:45:40     #20 0x558025e4e504 in call_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4872\r\nJun 21 16:45:40     #21 0x558025e73389 in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3335\r\nJun 21 16:45:40     #22 0x558025e4853a in _PyFunction_FastCall /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4933\r\nJun 21 16:45:40     #23 0x558025e4853a in fast_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4968\r\nJun 21 16:45:40     #24 0x558025e4e504 in call_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4872\r\nJun 21 16:45:40     #25 0x558025e73389 in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3335\r\nJun 21 16:45:40     #26 0x558025e4853a in _PyFunction_FastCall /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4933\r\nJun 21 16:45:40     #27 0x558025e4853a in fast_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4968\r\nJun 21 16:45:40     #28 0x558025e4e504 in call_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4872\r\nJun 21 16:45:40     #29 0x558025e73389 in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3335\r\nJun 21 16:45:40     #30 0x558025e4853a in _PyFunction_FastCall /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4933\r\nJun 21 16:45:40     #31 0x558025e4853a in fast_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4968\r\nJun 21 16:45:40     #32 0x558025e4e504 in call_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4872\r\nJun 21 16:45:40     #33 0x558025e73389 in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3335\r\nJun 21 16:45:40     #34 0x558025e48baa in _PyFunction_FastCall /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4933\r\nJun 21 16:45:40     #35 0x558025e48baa in _PyFunction_FastCallDict /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:5035\r\nJun 21 16:45:40     #36 0x558025dc7b0e in _PyObject_FastCallDict /tmp/build/80754af9/python_1546130271559/work/Objects/abstract.c:2310\r\nJun 21 16:45:40     #37 0x558025e0980f in _PyObject_CallMethodIdObjArgs /tmp/build/80754af9/python_1546130271559/work/Objects/abstract.c:2796\r\nJun 21 16:45:40     #38 0x558025dbeb0f in PyImport_ImportModuleLevelObject /tmp/build/80754af9/python_1546130271559/work/Python/import.c:1578\r\nJun 21 16:45:40     #39 0x558025e75a8a in import_name /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:5245\r\nJun 21 16:45:40     #40 0x558025e75a8a in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:2899\r\nJun 21 16:45:40     #41 0x558025e49288 in _PyEval_EvalCodeWithName /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4166\r\nJun 21 16:45:40     #42 0x558025e49288 in PyEval_EvalCodeEx /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4187\r\nJun 21 16:45:40     #43 0x558025e4a01b in PyEval_EvalCode /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:731\r\nJun 21 16:45:40     #44 0x558025ecc3c3 in run_mod /tmp/build/80754af9/python_1546130271559/work/Python/pythonrun.c:1025\r\nJun 21 16:45:40     #45 0x558025ecc7c0 in PyRun_FileExFlags /tmp/build/80754af9/python_1546130271559/work/Python/pythonrun.c:978\r\nJun 21 16:45:40     #46 0x558025ecc9c2 in PyRun_SimpleFileExFlags /tmp/build/80754af9/python_1546130271559/work/Python/pythonrun.c:419\r\nJun 21 16:45:40     #47 0x558025ed04b2 in run_file /tmp/build/80754af9/python_1546130271559/work/Modules/main.c:340\r\nJun 21 16:45:40     #48 0x558025ed04b2 in Py_Main /tmp/build/80754af9/python_1546130271559/work/Modules/main.c:811\r\nJun 21 16:45:40     #49 0x558025d9902d in main /tmp/build/80754af9/python_1546130271559/work/Programs/python.c:69\r\nJun 21 16:45:40     #50 0x7f26053c082f in __libc_start_main /build/glibc-LK5gWL/glibc-2.23/csu/../csu/libc-start.c:291\r\nJun 21 16:45:40     #51 0x558025e79e0d in _start /home/rdonnelly/mc/conda-bld/compilers_linux-64_1534865402226/work/.build/src/glibc-2.12.2/csu/../sysdeps/x86_64/elf/start.S:103\r\nJun 21 16:45:40 \r\nJun 21 16:45:40 SUMMARY: UndefinedBehaviorSanitizer: undefined-behavior /var/lib/jenkins/workspace/build/aten/src/ATen/Functions.h:2779:12 in \r\nJun 21 16:45:40 Traceback (most recent call last):\r\n```\n<issue_comment>username_2: I guess, ideally, we'd expose symbols that our ours, but not expose anything from pybind11. I don't know how easy that is to do. (Maybe this is a reason not to use pybind11, but that ship has kind of sailed...)\n<issue_comment>username_3: If everything works on MSVC with its default option -- that is, no exposure unless explicitly asked to -- could replacing all occurrences of `__declspec(dllexport)` with something also compatible with `gcc` work? Like something similar to [this](https://stackoverflow.com/a/8258786/1150462)?\n<issue_comment>username_0: Yes, I realized that it needs adjusting annotations.\r\n\r\nAs @username_3 pointed out, we should have already annotated all the right things to make Windows work. But we use windows-only macro in THP_API/THP_CLASS (https://github.com/pytorch/pytorch/blob/master/torch/csrc/THP_export.h). We should do the same thing as for other places - https://github.com/pytorch/pytorch/blob/master/c10/macros/Export.h\r\n\r\nI'll try to get back to it at some point."}
{"repo": "pytorch/pytorch", "issue_id": 459270796, "issue_number": 22076, "timestamp": "2019-06-21 17:12:05+00:00", "text": "I guess, ideally, we'd expose symbols that our ours, but not expose anything from pybind11. I don't know how easy that is to do. (Maybe this is a reason not to use pybind11, but that ship has kind of sailed...)", "context": "<issue_start><issue_comment>Title: Add hidden visibility to _C.so\nusername_0: See https://pybind11.readthedocs.io/en/stable/faq.html#someclass-declared-with-greater-visibility-than-the-type-of-its-field-someclass-member-wattributes\r\n\r\nLet's see whether it passes CI\n<issue_comment>username_1: wouldn't C++ extensions fail because of this? they require the symbols coming out of libtorch_python and stuff to be global visibility in the current process, or else they fail to load at runtime (but the C++ extensions will compile fine)\n<issue_comment>username_2: UBSAN failure looks real:\r\n\r\n```\r\nJun 21 16:45:39 /var/lib/jenkins/workspace/build/aten/src/ATen/Functions.h:2779:12: runtime error: call to function at::TypeDefault::randn(c10::ArrayRef<long>, c10::TensorOptions const&) through pointer to incorrect function type 'at::Tensor (*)(c10::ArrayRef<long>, const c10::TensorOptions &)'\r\nJun 21 16:45:39 (/opt/conda/lib/python3.6/site-packages/torch/lib/libtorch.so+0xa6b7860): note: at::TypeDefault::randn(c10::ArrayRef<long>, c10::TensorOptions const&) defined here\r\nJun 21 16:45:40     #0 0x7f25f2696882 in torch::randn(c10::ArrayRef<long>, c10::TensorOptions const&) (/opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_python.so+0x11aa882)\r\nJun 21 16:45:40     #1 0x7f25f2694ccc in torch::autograd::dispatch_randn(c10::ArrayRef<long>, c10::TensorOptions const&) (/opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_python.so+0x11a8ccc)\r\nJun 21 16:45:40     #2 0x7f25f258d08d in torch::autograd::THPVariable_randn(_object*, _object*, _object*) (/opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_python.so+0x10a108d)\r\nJun 21 16:45:40     #3 0x558025dc7743 in _PyCFunction_FastCallDict /tmp/build/80754af9/python_1546130271559/work/Objects/methodobject.c:231\r\nJun 21 16:45:40     #4 0x558025e4e42b in call_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4851\r\nJun 21 16:45:40     #5 0x558025e73389 in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3335\r\nJun 21 16:45:40     #6 0x558025e47bfd in _PyEval_EvalCodeWithName /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4166\r\nJun 21 16:45:40     #7 0x558025e48770 in fast_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4992\r\nJun 21 16:45:40     #8 0x558025e4e504 in call_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4872\r\nJun 21 16:45:40     #9 0x558025e73389 in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3335\r\nJun 21 16:45:40     #10 0x558025e49288 in _PyEval_EvalCodeWithName /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4166\r\nJun 21 16:45:40     #11 0x558025e49288 in PyEval_EvalCodeEx /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4187\r\nJun 21 16:45:40     #12 0x558025e4a01b in PyEval_EvalCode /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:731\r\nJun 21 16:45:40     #13 0x558025e70c8a in builtin_exec_impl.isra.11 /tmp/build/80754af9/python_1546130271559/work/Python/bltinmodule.c:983\r\nJun 21 16:45:40     #14 0x558025e70c8a in builtin_exec /tmp/build/80754af9/python_1546130271559/work/Python/clinic/bltinmodule.c.h:283\r\nJun 21 16:45:40     #15 0x558025dca710 in PyCFunction_Call /tmp/build/80754af9/python_1546130271559/work/Objects/methodobject.c:114\r\nJun 21 16:45:40     #16 0x558025e784ac in do_call_core /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:5116\r\nJun 21 16:45:40     #17 0x558025e784ac in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3404\r\nJun 21 16:45:40     #18 0x558025e478e3 in _PyEval_EvalCodeWithName /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4166\r\nJun 21 16:45:40     #19 0x558025e48770 in fast_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4992\r\nJun 21 16:45:40     #20 0x558025e4e504 in call_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4872\r\nJun 21 16:45:40     #21 0x558025e73389 in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3335\r\nJun 21 16:45:40     #22 0x558025e4853a in _PyFunction_FastCall /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4933\r\nJun 21 16:45:40     #23 0x558025e4853a in fast_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4968\r\nJun 21 16:45:40     #24 0x558025e4e504 in call_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4872\r\nJun 21 16:45:40     #25 0x558025e73389 in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3335\r\nJun 21 16:45:40     #26 0x558025e4853a in _PyFunction_FastCall /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4933\r\nJun 21 16:45:40     #27 0x558025e4853a in fast_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4968\r\nJun 21 16:45:40     #28 0x558025e4e504 in call_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4872\r\nJun 21 16:45:40     #29 0x558025e73389 in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3335\r\nJun 21 16:45:40     #30 0x558025e4853a in _PyFunction_FastCall /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4933\r\nJun 21 16:45:40     #31 0x558025e4853a in fast_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4968\r\nJun 21 16:45:40     #32 0x558025e4e504 in call_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4872\r\nJun 21 16:45:40     #33 0x558025e73389 in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3335\r\nJun 21 16:45:40     #34 0x558025e48baa in _PyFunction_FastCall /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4933\r\nJun 21 16:45:40     #35 0x558025e48baa in _PyFunction_FastCallDict /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:5035\r\nJun 21 16:45:40     #36 0x558025dc7b0e in _PyObject_FastCallDict /tmp/build/80754af9/python_1546130271559/work/Objects/abstract.c:2310\r\nJun 21 16:45:40     #37 0x558025e0980f in _PyObject_CallMethodIdObjArgs /tmp/build/80754af9/python_1546130271559/work/Objects/abstract.c:2796\r\nJun 21 16:45:40     #38 0x558025dbeb0f in PyImport_ImportModuleLevelObject /tmp/build/80754af9/python_1546130271559/work/Python/import.c:1578\r\nJun 21 16:45:40     #39 0x558025e75a8a in import_name /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:5245\r\nJun 21 16:45:40     #40 0x558025e75a8a in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:2899\r\nJun 21 16:45:40     #41 0x558025e49288 in _PyEval_EvalCodeWithName /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4166\r\nJun 21 16:45:40     #42 0x558025e49288 in PyEval_EvalCodeEx /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4187\r\nJun 21 16:45:40     #43 0x558025e4a01b in PyEval_EvalCode /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:731\r\nJun 21 16:45:40     #44 0x558025ecc3c3 in run_mod /tmp/build/80754af9/python_1546130271559/work/Python/pythonrun.c:1025\r\nJun 21 16:45:40     #45 0x558025ecc7c0 in PyRun_FileExFlags /tmp/build/80754af9/python_1546130271559/work/Python/pythonrun.c:978\r\nJun 21 16:45:40     #46 0x558025ecc9c2 in PyRun_SimpleFileExFlags /tmp/build/80754af9/python_1546130271559/work/Python/pythonrun.c:419\r\nJun 21 16:45:40     #47 0x558025ed04b2 in run_file /tmp/build/80754af9/python_1546130271559/work/Modules/main.c:340\r\nJun 21 16:45:40     #48 0x558025ed04b2 in Py_Main /tmp/build/80754af9/python_1546130271559/work/Modules/main.c:811\r\nJun 21 16:45:40     #49 0x558025d9902d in main /tmp/build/80754af9/python_1546130271559/work/Programs/python.c:69\r\nJun 21 16:45:40     #50 0x7f26053c082f in __libc_start_main /build/glibc-LK5gWL/glibc-2.23/csu/../csu/libc-start.c:291\r\nJun 21 16:45:40     #51 0x558025e79e0d in _start /home/rdonnelly/mc/conda-bld/compilers_linux-64_1534865402226/work/.build/src/glibc-2.12.2/csu/../sysdeps/x86_64/elf/start.S:103\r\nJun 21 16:45:40 \r\nJun 21 16:45:40 SUMMARY: UndefinedBehaviorSanitizer: undefined-behavior /var/lib/jenkins/workspace/build/aten/src/ATen/Functions.h:2779:12 in \r\nJun 21 16:45:40 Traceback (most recent call last):\r\n```\n<issue_comment>username_2: I guess, ideally, we'd expose symbols that our ours, but not expose anything from pybind11. I don't know how easy that is to do. (Maybe this is a reason not to use pybind11, but that ship has kind of sailed...)\n<issue_comment>username_3: If everything works on MSVC with its default option -- that is, no exposure unless explicitly asked to -- could replacing all occurrences of `__declspec(dllexport)` with something also compatible with `gcc` work? Like something similar to [this](https://stackoverflow.com/a/8258786/1150462)?\n<issue_comment>username_0: Yes, I realized that it needs adjusting annotations.\r\n\r\nAs @username_3 pointed out, we should have already annotated all the right things to make Windows work. But we use windows-only macro in THP_API/THP_CLASS (https://github.com/pytorch/pytorch/blob/master/torch/csrc/THP_export.h). We should do the same thing as for other places - https://github.com/pytorch/pytorch/blob/master/c10/macros/Export.h\r\n\r\nI'll try to get back to it at some point."}
{"repo": "pytorch/pytorch", "issue_id": 459270796, "issue_number": 22076, "timestamp": "2019-06-21 18:42:43+00:00", "text": "If everything works on MSVC with its default option -- that is, no exposure unless explicitly asked to -- could replacing all occurrences of `__declspec(dllexport)` with something also compatible with `gcc` work? Like something similar to [this](https://stackoverflow.com/a/8258786/1150462)?", "context": "<issue_start><issue_comment>Title: Add hidden visibility to _C.so\nusername_0: See https://pybind11.readthedocs.io/en/stable/faq.html#someclass-declared-with-greater-visibility-than-the-type-of-its-field-someclass-member-wattributes\r\n\r\nLet's see whether it passes CI\n<issue_comment>username_1: wouldn't C++ extensions fail because of this? they require the symbols coming out of libtorch_python and stuff to be global visibility in the current process, or else they fail to load at runtime (but the C++ extensions will compile fine)\n<issue_comment>username_2: UBSAN failure looks real:\r\n\r\n```\r\nJun 21 16:45:39 /var/lib/jenkins/workspace/build/aten/src/ATen/Functions.h:2779:12: runtime error: call to function at::TypeDefault::randn(c10::ArrayRef<long>, c10::TensorOptions const&) through pointer to incorrect function type 'at::Tensor (*)(c10::ArrayRef<long>, const c10::TensorOptions &)'\r\nJun 21 16:45:39 (/opt/conda/lib/python3.6/site-packages/torch/lib/libtorch.so+0xa6b7860): note: at::TypeDefault::randn(c10::ArrayRef<long>, c10::TensorOptions const&) defined here\r\nJun 21 16:45:40     #0 0x7f25f2696882 in torch::randn(c10::ArrayRef<long>, c10::TensorOptions const&) (/opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_python.so+0x11aa882)\r\nJun 21 16:45:40     #1 0x7f25f2694ccc in torch::autograd::dispatch_randn(c10::ArrayRef<long>, c10::TensorOptions const&) (/opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_python.so+0x11a8ccc)\r\nJun 21 16:45:40     #2 0x7f25f258d08d in torch::autograd::THPVariable_randn(_object*, _object*, _object*) (/opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_python.so+0x10a108d)\r\nJun 21 16:45:40     #3 0x558025dc7743 in _PyCFunction_FastCallDict /tmp/build/80754af9/python_1546130271559/work/Objects/methodobject.c:231\r\nJun 21 16:45:40     #4 0x558025e4e42b in call_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4851\r\nJun 21 16:45:40     #5 0x558025e73389 in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3335\r\nJun 21 16:45:40     #6 0x558025e47bfd in _PyEval_EvalCodeWithName /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4166\r\nJun 21 16:45:40     #7 0x558025e48770 in fast_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4992\r\nJun 21 16:45:40     #8 0x558025e4e504 in call_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4872\r\nJun 21 16:45:40     #9 0x558025e73389 in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3335\r\nJun 21 16:45:40     #10 0x558025e49288 in _PyEval_EvalCodeWithName /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4166\r\nJun 21 16:45:40     #11 0x558025e49288 in PyEval_EvalCodeEx /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4187\r\nJun 21 16:45:40     #12 0x558025e4a01b in PyEval_EvalCode /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:731\r\nJun 21 16:45:40     #13 0x558025e70c8a in builtin_exec_impl.isra.11 /tmp/build/80754af9/python_1546130271559/work/Python/bltinmodule.c:983\r\nJun 21 16:45:40     #14 0x558025e70c8a in builtin_exec /tmp/build/80754af9/python_1546130271559/work/Python/clinic/bltinmodule.c.h:283\r\nJun 21 16:45:40     #15 0x558025dca710 in PyCFunction_Call /tmp/build/80754af9/python_1546130271559/work/Objects/methodobject.c:114\r\nJun 21 16:45:40     #16 0x558025e784ac in do_call_core /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:5116\r\nJun 21 16:45:40     #17 0x558025e784ac in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3404\r\nJun 21 16:45:40     #18 0x558025e478e3 in _PyEval_EvalCodeWithName /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4166\r\nJun 21 16:45:40     #19 0x558025e48770 in fast_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4992\r\nJun 21 16:45:40     #20 0x558025e4e504 in call_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4872\r\nJun 21 16:45:40     #21 0x558025e73389 in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3335\r\nJun 21 16:45:40     #22 0x558025e4853a in _PyFunction_FastCall /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4933\r\nJun 21 16:45:40     #23 0x558025e4853a in fast_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4968\r\nJun 21 16:45:40     #24 0x558025e4e504 in call_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4872\r\nJun 21 16:45:40     #25 0x558025e73389 in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3335\r\nJun 21 16:45:40     #26 0x558025e4853a in _PyFunction_FastCall /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4933\r\nJun 21 16:45:40     #27 0x558025e4853a in fast_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4968\r\nJun 21 16:45:40     #28 0x558025e4e504 in call_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4872\r\nJun 21 16:45:40     #29 0x558025e73389 in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3335\r\nJun 21 16:45:40     #30 0x558025e4853a in _PyFunction_FastCall /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4933\r\nJun 21 16:45:40     #31 0x558025e4853a in fast_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4968\r\nJun 21 16:45:40     #32 0x558025e4e504 in call_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4872\r\nJun 21 16:45:40     #33 0x558025e73389 in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3335\r\nJun 21 16:45:40     #34 0x558025e48baa in _PyFunction_FastCall /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4933\r\nJun 21 16:45:40     #35 0x558025e48baa in _PyFunction_FastCallDict /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:5035\r\nJun 21 16:45:40     #36 0x558025dc7b0e in _PyObject_FastCallDict /tmp/build/80754af9/python_1546130271559/work/Objects/abstract.c:2310\r\nJun 21 16:45:40     #37 0x558025e0980f in _PyObject_CallMethodIdObjArgs /tmp/build/80754af9/python_1546130271559/work/Objects/abstract.c:2796\r\nJun 21 16:45:40     #38 0x558025dbeb0f in PyImport_ImportModuleLevelObject /tmp/build/80754af9/python_1546130271559/work/Python/import.c:1578\r\nJun 21 16:45:40     #39 0x558025e75a8a in import_name /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:5245\r\nJun 21 16:45:40     #40 0x558025e75a8a in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:2899\r\nJun 21 16:45:40     #41 0x558025e49288 in _PyEval_EvalCodeWithName /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4166\r\nJun 21 16:45:40     #42 0x558025e49288 in PyEval_EvalCodeEx /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4187\r\nJun 21 16:45:40     #43 0x558025e4a01b in PyEval_EvalCode /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:731\r\nJun 21 16:45:40     #44 0x558025ecc3c3 in run_mod /tmp/build/80754af9/python_1546130271559/work/Python/pythonrun.c:1025\r\nJun 21 16:45:40     #45 0x558025ecc7c0 in PyRun_FileExFlags /tmp/build/80754af9/python_1546130271559/work/Python/pythonrun.c:978\r\nJun 21 16:45:40     #46 0x558025ecc9c2 in PyRun_SimpleFileExFlags /tmp/build/80754af9/python_1546130271559/work/Python/pythonrun.c:419\r\nJun 21 16:45:40     #47 0x558025ed04b2 in run_file /tmp/build/80754af9/python_1546130271559/work/Modules/main.c:340\r\nJun 21 16:45:40     #48 0x558025ed04b2 in Py_Main /tmp/build/80754af9/python_1546130271559/work/Modules/main.c:811\r\nJun 21 16:45:40     #49 0x558025d9902d in main /tmp/build/80754af9/python_1546130271559/work/Programs/python.c:69\r\nJun 21 16:45:40     #50 0x7f26053c082f in __libc_start_main /build/glibc-LK5gWL/glibc-2.23/csu/../csu/libc-start.c:291\r\nJun 21 16:45:40     #51 0x558025e79e0d in _start /home/rdonnelly/mc/conda-bld/compilers_linux-64_1534865402226/work/.build/src/glibc-2.12.2/csu/../sysdeps/x86_64/elf/start.S:103\r\nJun 21 16:45:40 \r\nJun 21 16:45:40 SUMMARY: UndefinedBehaviorSanitizer: undefined-behavior /var/lib/jenkins/workspace/build/aten/src/ATen/Functions.h:2779:12 in \r\nJun 21 16:45:40 Traceback (most recent call last):\r\n```\n<issue_comment>username_2: I guess, ideally, we'd expose symbols that our ours, but not expose anything from pybind11. I don't know how easy that is to do. (Maybe this is a reason not to use pybind11, but that ship has kind of sailed...)\n<issue_comment>username_3: If everything works on MSVC with its default option -- that is, no exposure unless explicitly asked to -- could replacing all occurrences of `__declspec(dllexport)` with something also compatible with `gcc` work? Like something similar to [this](https://stackoverflow.com/a/8258786/1150462)?\n<issue_comment>username_0: Yes, I realized that it needs adjusting annotations.\r\n\r\nAs @username_3 pointed out, we should have already annotated all the right things to make Windows work. But we use windows-only macro in THP_API/THP_CLASS (https://github.com/pytorch/pytorch/blob/master/torch/csrc/THP_export.h). We should do the same thing as for other places - https://github.com/pytorch/pytorch/blob/master/c10/macros/Export.h\r\n\r\nI'll try to get back to it at some point."}
{"repo": "pytorch/pytorch", "issue_id": 459270796, "issue_number": 22076, "timestamp": "2019-06-24 07:01:35+00:00", "text": "Yes, I realized that it needs adjusting annotations.\r\n\r\nAs @username_3 pointed out, we should have already annotated all the right things to make Windows work. But we use windows-only macro in THP_API/THP_CLASS (https://github.com/pytorch/pytorch/blob/master/torch/csrc/THP_export.h). We should do the same thing as for other places - https://github.com/pytorch/pytorch/blob/master/c10/macros/Export.h\r\n\r\nI'll try to get back to it at some point.", "context": "<issue_start><issue_comment>Title: Add hidden visibility to _C.so\nusername_0: See https://pybind11.readthedocs.io/en/stable/faq.html#someclass-declared-with-greater-visibility-than-the-type-of-its-field-someclass-member-wattributes\r\n\r\nLet's see whether it passes CI\n<issue_comment>username_1: wouldn't C++ extensions fail because of this? they require the symbols coming out of libtorch_python and stuff to be global visibility in the current process, or else they fail to load at runtime (but the C++ extensions will compile fine)\n<issue_comment>username_2: UBSAN failure looks real:\r\n\r\n```\r\nJun 21 16:45:39 /var/lib/jenkins/workspace/build/aten/src/ATen/Functions.h:2779:12: runtime error: call to function at::TypeDefault::randn(c10::ArrayRef<long>, c10::TensorOptions const&) through pointer to incorrect function type 'at::Tensor (*)(c10::ArrayRef<long>, const c10::TensorOptions &)'\r\nJun 21 16:45:39 (/opt/conda/lib/python3.6/site-packages/torch/lib/libtorch.so+0xa6b7860): note: at::TypeDefault::randn(c10::ArrayRef<long>, c10::TensorOptions const&) defined here\r\nJun 21 16:45:40     #0 0x7f25f2696882 in torch::randn(c10::ArrayRef<long>, c10::TensorOptions const&) (/opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_python.so+0x11aa882)\r\nJun 21 16:45:40     #1 0x7f25f2694ccc in torch::autograd::dispatch_randn(c10::ArrayRef<long>, c10::TensorOptions const&) (/opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_python.so+0x11a8ccc)\r\nJun 21 16:45:40     #2 0x7f25f258d08d in torch::autograd::THPVariable_randn(_object*, _object*, _object*) (/opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_python.so+0x10a108d)\r\nJun 21 16:45:40     #3 0x558025dc7743 in _PyCFunction_FastCallDict /tmp/build/80754af9/python_1546130271559/work/Objects/methodobject.c:231\r\nJun 21 16:45:40     #4 0x558025e4e42b in call_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4851\r\nJun 21 16:45:40     #5 0x558025e73389 in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3335\r\nJun 21 16:45:40     #6 0x558025e47bfd in _PyEval_EvalCodeWithName /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4166\r\nJun 21 16:45:40     #7 0x558025e48770 in fast_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4992\r\nJun 21 16:45:40     #8 0x558025e4e504 in call_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4872\r\nJun 21 16:45:40     #9 0x558025e73389 in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3335\r\nJun 21 16:45:40     #10 0x558025e49288 in _PyEval_EvalCodeWithName /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4166\r\nJun 21 16:45:40     #11 0x558025e49288 in PyEval_EvalCodeEx /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4187\r\nJun 21 16:45:40     #12 0x558025e4a01b in PyEval_EvalCode /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:731\r\nJun 21 16:45:40     #13 0x558025e70c8a in builtin_exec_impl.isra.11 /tmp/build/80754af9/python_1546130271559/work/Python/bltinmodule.c:983\r\nJun 21 16:45:40     #14 0x558025e70c8a in builtin_exec /tmp/build/80754af9/python_1546130271559/work/Python/clinic/bltinmodule.c.h:283\r\nJun 21 16:45:40     #15 0x558025dca710 in PyCFunction_Call /tmp/build/80754af9/python_1546130271559/work/Objects/methodobject.c:114\r\nJun 21 16:45:40     #16 0x558025e784ac in do_call_core /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:5116\r\nJun 21 16:45:40     #17 0x558025e784ac in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3404\r\nJun 21 16:45:40     #18 0x558025e478e3 in _PyEval_EvalCodeWithName /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4166\r\nJun 21 16:45:40     #19 0x558025e48770 in fast_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4992\r\nJun 21 16:45:40     #20 0x558025e4e504 in call_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4872\r\nJun 21 16:45:40     #21 0x558025e73389 in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3335\r\nJun 21 16:45:40     #22 0x558025e4853a in _PyFunction_FastCall /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4933\r\nJun 21 16:45:40     #23 0x558025e4853a in fast_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4968\r\nJun 21 16:45:40     #24 0x558025e4e504 in call_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4872\r\nJun 21 16:45:40     #25 0x558025e73389 in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3335\r\nJun 21 16:45:40     #26 0x558025e4853a in _PyFunction_FastCall /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4933\r\nJun 21 16:45:40     #27 0x558025e4853a in fast_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4968\r\nJun 21 16:45:40     #28 0x558025e4e504 in call_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4872\r\nJun 21 16:45:40     #29 0x558025e73389 in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3335\r\nJun 21 16:45:40     #30 0x558025e4853a in _PyFunction_FastCall /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4933\r\nJun 21 16:45:40     #31 0x558025e4853a in fast_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4968\r\nJun 21 16:45:40     #32 0x558025e4e504 in call_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4872\r\nJun 21 16:45:40     #33 0x558025e73389 in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3335\r\nJun 21 16:45:40     #34 0x558025e48baa in _PyFunction_FastCall /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4933\r\nJun 21 16:45:40     #35 0x558025e48baa in _PyFunction_FastCallDict /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:5035\r\nJun 21 16:45:40     #36 0x558025dc7b0e in _PyObject_FastCallDict /tmp/build/80754af9/python_1546130271559/work/Objects/abstract.c:2310\r\nJun 21 16:45:40     #37 0x558025e0980f in _PyObject_CallMethodIdObjArgs /tmp/build/80754af9/python_1546130271559/work/Objects/abstract.c:2796\r\nJun 21 16:45:40     #38 0x558025dbeb0f in PyImport_ImportModuleLevelObject /tmp/build/80754af9/python_1546130271559/work/Python/import.c:1578\r\nJun 21 16:45:40     #39 0x558025e75a8a in import_name /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:5245\r\nJun 21 16:45:40     #40 0x558025e75a8a in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:2899\r\nJun 21 16:45:40     #41 0x558025e49288 in _PyEval_EvalCodeWithName /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4166\r\nJun 21 16:45:40     #42 0x558025e49288 in PyEval_EvalCodeEx /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4187\r\nJun 21 16:45:40     #43 0x558025e4a01b in PyEval_EvalCode /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:731\r\nJun 21 16:45:40     #44 0x558025ecc3c3 in run_mod /tmp/build/80754af9/python_1546130271559/work/Python/pythonrun.c:1025\r\nJun 21 16:45:40     #45 0x558025ecc7c0 in PyRun_FileExFlags /tmp/build/80754af9/python_1546130271559/work/Python/pythonrun.c:978\r\nJun 21 16:45:40     #46 0x558025ecc9c2 in PyRun_SimpleFileExFlags /tmp/build/80754af9/python_1546130271559/work/Python/pythonrun.c:419\r\nJun 21 16:45:40     #47 0x558025ed04b2 in run_file /tmp/build/80754af9/python_1546130271559/work/Modules/main.c:340\r\nJun 21 16:45:40     #48 0x558025ed04b2 in Py_Main /tmp/build/80754af9/python_1546130271559/work/Modules/main.c:811\r\nJun 21 16:45:40     #49 0x558025d9902d in main /tmp/build/80754af9/python_1546130271559/work/Programs/python.c:69\r\nJun 21 16:45:40     #50 0x7f26053c082f in __libc_start_main /build/glibc-LK5gWL/glibc-2.23/csu/../csu/libc-start.c:291\r\nJun 21 16:45:40     #51 0x558025e79e0d in _start /home/rdonnelly/mc/conda-bld/compilers_linux-64_1534865402226/work/.build/src/glibc-2.12.2/csu/../sysdeps/x86_64/elf/start.S:103\r\nJun 21 16:45:40 \r\nJun 21 16:45:40 SUMMARY: UndefinedBehaviorSanitizer: undefined-behavior /var/lib/jenkins/workspace/build/aten/src/ATen/Functions.h:2779:12 in \r\nJun 21 16:45:40 Traceback (most recent call last):\r\n```\n<issue_comment>username_2: I guess, ideally, we'd expose symbols that our ours, but not expose anything from pybind11. I don't know how easy that is to do. (Maybe this is a reason not to use pybind11, but that ship has kind of sailed...)\n<issue_comment>username_3: If everything works on MSVC with its default option -- that is, no exposure unless explicitly asked to -- could replacing all occurrences of `__declspec(dllexport)` with something also compatible with `gcc` work? Like something similar to [this](https://stackoverflow.com/a/8258786/1150462)?\n<issue_comment>username_0: Yes, I realized that it needs adjusting annotations.\r\n\r\nAs @username_3 pointed out, we should have already annotated all the right things to make Windows work. But we use windows-only macro in THP_API/THP_CLASS (https://github.com/pytorch/pytorch/blob/master/torch/csrc/THP_export.h). We should do the same thing as for other places - https://github.com/pytorch/pytorch/blob/master/c10/macros/Export.h\r\n\r\nI'll try to get back to it at some point."}
{"repo": "pytorch/pytorch", "issue_id": 511249220, "issue_number": 28500, "timestamp": "2019-10-23 11:06:14+00:00", "text": "## \ud83d\udc1b Bug\r\n\r\nAttempting to create a sparse tensor from a tensor of indices and values placed on another GPU than `cuda:0` (for instance `cuda:1`) fails with error `device of indices (0) must match device of values (1)`.\r\n\r\nPlacing the values on `cuda:0` while leaving the indices on `cuda:1` works and returns a sparse tensor on `cuda:0`.\r\n\r\nThe following demo should show the bug:\r\n```\r\nIn [1]: import torch                                                            \r\n\r\nIn [2]: ind = torch.arange(10).unsqueeze(0).to('cuda:1')                        \r\n\r\nIn [3]: sp_ind = torch.cat((ind, ind), 0)                                       \r\n\r\nIn [4]: sp_val = torch.rand(10).to('cuda:1')                                    \r\n\r\nIn [5]: sp_shape = (10, 10)                                                     \r\n\r\nIn [6]: x = torch.sparse_coo_tensor(sp_ind, sp_val, sp_shape)                   \r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-6-776f23c64083> in <module>\r\n----> 1 x = torch.sparse_coo_tensor(sp_ind, sp_val, sp_shape)\r\n\r\nRuntimeError: device of indices (0) must match device of values (1)\r\n\r\nIn [7]: # Now this                                                              \r\n\r\nIn [8]: x = torch.sparse_coo_tensor(sp_ind, sp_val.cuda(0), sp_shape)           \r\n\r\nIn [9]: x                                                                       \r\nOut[9]: \r\ntensor(indices=tensor([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\r\n                       [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]]),\r\n       values=tensor([0.5324, 0.1738, 0.4186, 0.2776, 0.4665, 0.9525, 0.6555,\r\n                      0.2805, 0.9244, 0.4847]),\r\n       device='cuda:0', size=(10, 10), nnz=10, layout=torch.sparse_coo)\r\n\r\nIn [10]: sp_ind                                                                 \r\nOut[10]: \r\ntensor([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\r\n        [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]], device='cuda:1')\r\n\r\nIn [11]: sp_val                                                                 \r\nOut[11]: \r\ntensor([0.5324, 0.1738, 0.4186, 0.2776, 0.4665, 0.9525, 0.6555, 0.2805, 0.9244,\r\n        0.4847], device='cuda:1')\r\n\r\n```\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n**See code snipped above.**\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n## Expected behavior\r\n\r\nProvided both indices and values are placed on the same device, `torch.sparse_coo_tensor` should return a sparse tensor on that device.\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n## Environment\r\n\r\nPlease copy and paste the output from our\r\n[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)\r\n(or fill out the checklist below manually).\r\n\r\nYou can get the script and run it with:\r\n```\r\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\r\n# For security purposes, please check the contents of collect_env.py before running it.\r\npython collect_env.py\r\n```\r\n\r\n - PyTorch Version (e.g., 1.0): 1.3.0\r\n - OS (e.g., Linux): Ubuntu 18.04 LTS\r\n - How you installed PyTorch (`conda`, `pip`, source): conda\r\n - Build command you used (if compiling from source): N/A\r\n - Python version: 3.7.4\r\n - CUDA/cuDNN version: 10.2/7.5\r\n - GPU models and configuration: 2x2080 Ti with Nvlink\r\n - Any other relevant information:\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context about the problem here. -->", "context": "<issue_start><issue_comment>Title: Sparse tensor creation ignores indices placement\nusername_0: ## \ud83d\udc1b Bug\r\n\r\nAttempting to create a sparse tensor from a tensor of indices and values placed on another GPU than `cuda:0` (for instance `cuda:1`) fails with error `device of indices (0) must match device of values (1)`.\r\n\r\nPlacing the values on `cuda:0` while leaving the indices on `cuda:1` works and returns a sparse tensor on `cuda:0`.\r\n\r\nThe following demo should show the bug:\r\n```\r\nIn [1]: import torch                                                            \r\n\r\nIn [2]: ind = torch.arange(10).unsqueeze(0).to('cuda:1')                        \r\n\r\nIn [3]: sp_ind = torch.cat((ind, ind), 0)                                       \r\n\r\nIn [4]: sp_val = torch.rand(10).to('cuda:1')                                    \r\n\r\nIn [5]: sp_shape = (10, 10)                                                     \r\n\r\nIn [6]: x = torch.sparse_coo_tensor(sp_ind, sp_val, sp_shape)                   \r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-6-776f23c64083> in <module>\r\n----> 1 x = torch.sparse_coo_tensor(sp_ind, sp_val, sp_shape)\r\n\r\nRuntimeError: device of indices (0) must match device of values (1)\r\n\r\nIn [7]: # Now this                                                              \r\n\r\nIn [8]: x = torch.sparse_coo_tensor(sp_ind, sp_val.cuda(0), sp_shape)           \r\n\r\nIn [9]: x                                                                       \r\nOut[9]: \r\ntensor(indices=tensor([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\r\n                       [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]]),\r\n       values=tensor([0.5324, 0.1738, 0.4186, 0.2776, 0.4665, 0.9525, 0.6555,\r\n                      0.2805, 0.9244, 0.4847]),\r\n       device='cuda:0', size=(10, 10), nnz=10, layout=torch.sparse_coo)\r\n\r\nIn [10]: sp_ind                                                                 \r\nOut[10]: \r\ntensor([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\r\n        [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]], device='cuda:1')\r\n\r\nIn [11]: sp_val                                                                 \r\nOut[11]: \r\ntensor([0.5324, 0.1738, 0.4186, 0.2776, 0.4665, 0.9525, 0.6555, 0.2805, 0.9244,\r\n        0.4847], device='cuda:1')\r\n\r\n```\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n**See code snipped above.**\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n## Expected behavior\r\n\r\nProvided both indices and values are placed on the same device, `torch.sparse_coo_tensor` should return a sparse tensor on that device.\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n## Environment\r\n\r\nPlease copy and paste the output from our\r\n[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)\r\n(or fill out the checklist below manually).\r\n\r\nYou can get the script and run it with:\r\n```\r\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\r\n# For security purposes, please check the contents of collect_env.py before running it.\r\npython collect_env.py\r\n```\r\n\r\n - PyTorch Version (e.g., 1.0): 1.3.0\r\n - OS (e.g., Linux): Ubuntu 18.04 LTS\r\n - How you installed PyTorch (`conda`, `pip`, source): conda\r\n - Build command you used (if compiling from source): N/A\r\n - Python version: 3.7.4\r\n - CUDA/cuDNN version: 10.2/7.5\r\n - GPU models and configuration: 2x2080 Ti with Nvlink\r\n - Any other relevant information:\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\n<issue_comment>username_1: Oh, I meet the same thing!\n<issue_comment>username_2: I find `torch.sparse.FloatTensor' to be a good walk around.\n<issue_comment>username_3: @username_4 , can we close this issue via https://github.com/pytorch/pytorch/pull/41984 ?\n<issue_comment>username_4: Yes, we can close. It's solved already\n<issue_comment>username_5: Closing issue, since last comment indicates it as resolved.<issue_closed>"}
{"repo": "pytorch/pytorch", "issue_id": 511249220, "issue_number": 28500, "timestamp": "2020-02-26 14:12:21+00:00", "text": "Oh, I meet the same thing!", "context": "<issue_start><issue_comment>Title: Sparse tensor creation ignores indices placement\nusername_0: ## \ud83d\udc1b Bug\r\n\r\nAttempting to create a sparse tensor from a tensor of indices and values placed on another GPU than `cuda:0` (for instance `cuda:1`) fails with error `device of indices (0) must match device of values (1)`.\r\n\r\nPlacing the values on `cuda:0` while leaving the indices on `cuda:1` works and returns a sparse tensor on `cuda:0`.\r\n\r\nThe following demo should show the bug:\r\n```\r\nIn [1]: import torch                                                            \r\n\r\nIn [2]: ind = torch.arange(10).unsqueeze(0).to('cuda:1')                        \r\n\r\nIn [3]: sp_ind = torch.cat((ind, ind), 0)                                       \r\n\r\nIn [4]: sp_val = torch.rand(10).to('cuda:1')                                    \r\n\r\nIn [5]: sp_shape = (10, 10)                                                     \r\n\r\nIn [6]: x = torch.sparse_coo_tensor(sp_ind, sp_val, sp_shape)                   \r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-6-776f23c64083> in <module>\r\n----> 1 x = torch.sparse_coo_tensor(sp_ind, sp_val, sp_shape)\r\n\r\nRuntimeError: device of indices (0) must match device of values (1)\r\n\r\nIn [7]: # Now this                                                              \r\n\r\nIn [8]: x = torch.sparse_coo_tensor(sp_ind, sp_val.cuda(0), sp_shape)           \r\n\r\nIn [9]: x                                                                       \r\nOut[9]: \r\ntensor(indices=tensor([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\r\n                       [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]]),\r\n       values=tensor([0.5324, 0.1738, 0.4186, 0.2776, 0.4665, 0.9525, 0.6555,\r\n                      0.2805, 0.9244, 0.4847]),\r\n       device='cuda:0', size=(10, 10), nnz=10, layout=torch.sparse_coo)\r\n\r\nIn [10]: sp_ind                                                                 \r\nOut[10]: \r\ntensor([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\r\n        [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]], device='cuda:1')\r\n\r\nIn [11]: sp_val                                                                 \r\nOut[11]: \r\ntensor([0.5324, 0.1738, 0.4186, 0.2776, 0.4665, 0.9525, 0.6555, 0.2805, 0.9244,\r\n        0.4847], device='cuda:1')\r\n\r\n```\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n**See code snipped above.**\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n## Expected behavior\r\n\r\nProvided both indices and values are placed on the same device, `torch.sparse_coo_tensor` should return a sparse tensor on that device.\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n## Environment\r\n\r\nPlease copy and paste the output from our\r\n[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)\r\n(or fill out the checklist below manually).\r\n\r\nYou can get the script and run it with:\r\n```\r\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\r\n# For security purposes, please check the contents of collect_env.py before running it.\r\npython collect_env.py\r\n```\r\n\r\n - PyTorch Version (e.g., 1.0): 1.3.0\r\n - OS (e.g., Linux): Ubuntu 18.04 LTS\r\n - How you installed PyTorch (`conda`, `pip`, source): conda\r\n - Build command you used (if compiling from source): N/A\r\n - Python version: 3.7.4\r\n - CUDA/cuDNN version: 10.2/7.5\r\n - GPU models and configuration: 2x2080 Ti with Nvlink\r\n - Any other relevant information:\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\n<issue_comment>username_1: Oh, I meet the same thing!\n<issue_comment>username_2: I find `torch.sparse.FloatTensor' to be a good walk around.\n<issue_comment>username_3: @username_4 , can we close this issue via https://github.com/pytorch/pytorch/pull/41984 ?\n<issue_comment>username_4: Yes, we can close. It's solved already\n<issue_comment>username_5: Closing issue, since last comment indicates it as resolved.<issue_closed>"}
{"repo": "pytorch/pytorch", "issue_id": 511249220, "issue_number": 28500, "timestamp": "2020-05-22 16:57:05+00:00", "text": "I find `torch.sparse.FloatTensor' to be a good walk around.", "context": "<issue_start><issue_comment>Title: Sparse tensor creation ignores indices placement\nusername_0: ## \ud83d\udc1b Bug\r\n\r\nAttempting to create a sparse tensor from a tensor of indices and values placed on another GPU than `cuda:0` (for instance `cuda:1`) fails with error `device of indices (0) must match device of values (1)`.\r\n\r\nPlacing the values on `cuda:0` while leaving the indices on `cuda:1` works and returns a sparse tensor on `cuda:0`.\r\n\r\nThe following demo should show the bug:\r\n```\r\nIn [1]: import torch                                                            \r\n\r\nIn [2]: ind = torch.arange(10).unsqueeze(0).to('cuda:1')                        \r\n\r\nIn [3]: sp_ind = torch.cat((ind, ind), 0)                                       \r\n\r\nIn [4]: sp_val = torch.rand(10).to('cuda:1')                                    \r\n\r\nIn [5]: sp_shape = (10, 10)                                                     \r\n\r\nIn [6]: x = torch.sparse_coo_tensor(sp_ind, sp_val, sp_shape)                   \r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-6-776f23c64083> in <module>\r\n----> 1 x = torch.sparse_coo_tensor(sp_ind, sp_val, sp_shape)\r\n\r\nRuntimeError: device of indices (0) must match device of values (1)\r\n\r\nIn [7]: # Now this                                                              \r\n\r\nIn [8]: x = torch.sparse_coo_tensor(sp_ind, sp_val.cuda(0), sp_shape)           \r\n\r\nIn [9]: x                                                                       \r\nOut[9]: \r\ntensor(indices=tensor([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\r\n                       [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]]),\r\n       values=tensor([0.5324, 0.1738, 0.4186, 0.2776, 0.4665, 0.9525, 0.6555,\r\n                      0.2805, 0.9244, 0.4847]),\r\n       device='cuda:0', size=(10, 10), nnz=10, layout=torch.sparse_coo)\r\n\r\nIn [10]: sp_ind                                                                 \r\nOut[10]: \r\ntensor([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\r\n        [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]], device='cuda:1')\r\n\r\nIn [11]: sp_val                                                                 \r\nOut[11]: \r\ntensor([0.5324, 0.1738, 0.4186, 0.2776, 0.4665, 0.9525, 0.6555, 0.2805, 0.9244,\r\n        0.4847], device='cuda:1')\r\n\r\n```\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n**See code snipped above.**\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n## Expected behavior\r\n\r\nProvided both indices and values are placed on the same device, `torch.sparse_coo_tensor` should return a sparse tensor on that device.\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n## Environment\r\n\r\nPlease copy and paste the output from our\r\n[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)\r\n(or fill out the checklist below manually).\r\n\r\nYou can get the script and run it with:\r\n```\r\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\r\n# For security purposes, please check the contents of collect_env.py before running it.\r\npython collect_env.py\r\n```\r\n\r\n - PyTorch Version (e.g., 1.0): 1.3.0\r\n - OS (e.g., Linux): Ubuntu 18.04 LTS\r\n - How you installed PyTorch (`conda`, `pip`, source): conda\r\n - Build command you used (if compiling from source): N/A\r\n - Python version: 3.7.4\r\n - CUDA/cuDNN version: 10.2/7.5\r\n - GPU models and configuration: 2x2080 Ti with Nvlink\r\n - Any other relevant information:\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\n<issue_comment>username_1: Oh, I meet the same thing!\n<issue_comment>username_2: I find `torch.sparse.FloatTensor' to be a good walk around.\n<issue_comment>username_3: @username_4 , can we close this issue via https://github.com/pytorch/pytorch/pull/41984 ?\n<issue_comment>username_4: Yes, we can close. It's solved already\n<issue_comment>username_5: Closing issue, since last comment indicates it as resolved.<issue_closed>"}
{"repo": "pytorch/pytorch", "issue_id": 511249220, "issue_number": 28500, "timestamp": "2020-09-12 17:22:35+00:00", "text": "@username_4 , can we close this issue via https://github.com/pytorch/pytorch/pull/41984 ?", "context": "<issue_start><issue_comment>Title: Sparse tensor creation ignores indices placement\nusername_0: ## \ud83d\udc1b Bug\r\n\r\nAttempting to create a sparse tensor from a tensor of indices and values placed on another GPU than `cuda:0` (for instance `cuda:1`) fails with error `device of indices (0) must match device of values (1)`.\r\n\r\nPlacing the values on `cuda:0` while leaving the indices on `cuda:1` works and returns a sparse tensor on `cuda:0`.\r\n\r\nThe following demo should show the bug:\r\n```\r\nIn [1]: import torch                                                            \r\n\r\nIn [2]: ind = torch.arange(10).unsqueeze(0).to('cuda:1')                        \r\n\r\nIn [3]: sp_ind = torch.cat((ind, ind), 0)                                       \r\n\r\nIn [4]: sp_val = torch.rand(10).to('cuda:1')                                    \r\n\r\nIn [5]: sp_shape = (10, 10)                                                     \r\n\r\nIn [6]: x = torch.sparse_coo_tensor(sp_ind, sp_val, sp_shape)                   \r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-6-776f23c64083> in <module>\r\n----> 1 x = torch.sparse_coo_tensor(sp_ind, sp_val, sp_shape)\r\n\r\nRuntimeError: device of indices (0) must match device of values (1)\r\n\r\nIn [7]: # Now this                                                              \r\n\r\nIn [8]: x = torch.sparse_coo_tensor(sp_ind, sp_val.cuda(0), sp_shape)           \r\n\r\nIn [9]: x                                                                       \r\nOut[9]: \r\ntensor(indices=tensor([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\r\n                       [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]]),\r\n       values=tensor([0.5324, 0.1738, 0.4186, 0.2776, 0.4665, 0.9525, 0.6555,\r\n                      0.2805, 0.9244, 0.4847]),\r\n       device='cuda:0', size=(10, 10), nnz=10, layout=torch.sparse_coo)\r\n\r\nIn [10]: sp_ind                                                                 \r\nOut[10]: \r\ntensor([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\r\n        [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]], device='cuda:1')\r\n\r\nIn [11]: sp_val                                                                 \r\nOut[11]: \r\ntensor([0.5324, 0.1738, 0.4186, 0.2776, 0.4665, 0.9525, 0.6555, 0.2805, 0.9244,\r\n        0.4847], device='cuda:1')\r\n\r\n```\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n**See code snipped above.**\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n## Expected behavior\r\n\r\nProvided both indices and values are placed on the same device, `torch.sparse_coo_tensor` should return a sparse tensor on that device.\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n## Environment\r\n\r\nPlease copy and paste the output from our\r\n[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)\r\n(or fill out the checklist below manually).\r\n\r\nYou can get the script and run it with:\r\n```\r\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\r\n# For security purposes, please check the contents of collect_env.py before running it.\r\npython collect_env.py\r\n```\r\n\r\n - PyTorch Version (e.g., 1.0): 1.3.0\r\n - OS (e.g., Linux): Ubuntu 18.04 LTS\r\n - How you installed PyTorch (`conda`, `pip`, source): conda\r\n - Build command you used (if compiling from source): N/A\r\n - Python version: 3.7.4\r\n - CUDA/cuDNN version: 10.2/7.5\r\n - GPU models and configuration: 2x2080 Ti with Nvlink\r\n - Any other relevant information:\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\n<issue_comment>username_1: Oh, I meet the same thing!\n<issue_comment>username_2: I find `torch.sparse.FloatTensor' to be a good walk around.\n<issue_comment>username_3: @username_4 , can we close this issue via https://github.com/pytorch/pytorch/pull/41984 ?\n<issue_comment>username_4: Yes, we can close. It's solved already\n<issue_comment>username_5: Closing issue, since last comment indicates it as resolved.<issue_closed>"}
{"repo": "pytorch/pytorch", "issue_id": 511249220, "issue_number": 28500, "timestamp": "2020-09-12 17:33:30+00:00", "text": "Yes, we can close. It's solved already", "context": "<issue_start><issue_comment>Title: Sparse tensor creation ignores indices placement\nusername_0: ## \ud83d\udc1b Bug\r\n\r\nAttempting to create a sparse tensor from a tensor of indices and values placed on another GPU than `cuda:0` (for instance `cuda:1`) fails with error `device of indices (0) must match device of values (1)`.\r\n\r\nPlacing the values on `cuda:0` while leaving the indices on `cuda:1` works and returns a sparse tensor on `cuda:0`.\r\n\r\nThe following demo should show the bug:\r\n```\r\nIn [1]: import torch                                                            \r\n\r\nIn [2]: ind = torch.arange(10).unsqueeze(0).to('cuda:1')                        \r\n\r\nIn [3]: sp_ind = torch.cat((ind, ind), 0)                                       \r\n\r\nIn [4]: sp_val = torch.rand(10).to('cuda:1')                                    \r\n\r\nIn [5]: sp_shape = (10, 10)                                                     \r\n\r\nIn [6]: x = torch.sparse_coo_tensor(sp_ind, sp_val, sp_shape)                   \r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-6-776f23c64083> in <module>\r\n----> 1 x = torch.sparse_coo_tensor(sp_ind, sp_val, sp_shape)\r\n\r\nRuntimeError: device of indices (0) must match device of values (1)\r\n\r\nIn [7]: # Now this                                                              \r\n\r\nIn [8]: x = torch.sparse_coo_tensor(sp_ind, sp_val.cuda(0), sp_shape)           \r\n\r\nIn [9]: x                                                                       \r\nOut[9]: \r\ntensor(indices=tensor([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\r\n                       [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]]),\r\n       values=tensor([0.5324, 0.1738, 0.4186, 0.2776, 0.4665, 0.9525, 0.6555,\r\n                      0.2805, 0.9244, 0.4847]),\r\n       device='cuda:0', size=(10, 10), nnz=10, layout=torch.sparse_coo)\r\n\r\nIn [10]: sp_ind                                                                 \r\nOut[10]: \r\ntensor([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\r\n        [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]], device='cuda:1')\r\n\r\nIn [11]: sp_val                                                                 \r\nOut[11]: \r\ntensor([0.5324, 0.1738, 0.4186, 0.2776, 0.4665, 0.9525, 0.6555, 0.2805, 0.9244,\r\n        0.4847], device='cuda:1')\r\n\r\n```\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n**See code snipped above.**\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n## Expected behavior\r\n\r\nProvided both indices and values are placed on the same device, `torch.sparse_coo_tensor` should return a sparse tensor on that device.\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n## Environment\r\n\r\nPlease copy and paste the output from our\r\n[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)\r\n(or fill out the checklist below manually).\r\n\r\nYou can get the script and run it with:\r\n```\r\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\r\n# For security purposes, please check the contents of collect_env.py before running it.\r\npython collect_env.py\r\n```\r\n\r\n - PyTorch Version (e.g., 1.0): 1.3.0\r\n - OS (e.g., Linux): Ubuntu 18.04 LTS\r\n - How you installed PyTorch (`conda`, `pip`, source): conda\r\n - Build command you used (if compiling from source): N/A\r\n - Python version: 3.7.4\r\n - CUDA/cuDNN version: 10.2/7.5\r\n - GPU models and configuration: 2x2080 Ti with Nvlink\r\n - Any other relevant information:\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\n<issue_comment>username_1: Oh, I meet the same thing!\n<issue_comment>username_2: I find `torch.sparse.FloatTensor' to be a good walk around.\n<issue_comment>username_3: @username_4 , can we close this issue via https://github.com/pytorch/pytorch/pull/41984 ?\n<issue_comment>username_4: Yes, we can close. It's solved already\n<issue_comment>username_5: Closing issue, since last comment indicates it as resolved.<issue_closed>"}
{"repo": "pytorch/pytorch", "issue_id": 511249220, "issue_number": 28500, "timestamp": "2021-06-03 02:10:43+00:00", "text": "Closing issue, since last comment indicates it as resolved.", "context": "<issue_start><issue_comment>Title: Sparse tensor creation ignores indices placement\nusername_0: ## \ud83d\udc1b Bug\r\n\r\nAttempting to create a sparse tensor from a tensor of indices and values placed on another GPU than `cuda:0` (for instance `cuda:1`) fails with error `device of indices (0) must match device of values (1)`.\r\n\r\nPlacing the values on `cuda:0` while leaving the indices on `cuda:1` works and returns a sparse tensor on `cuda:0`.\r\n\r\nThe following demo should show the bug:\r\n```\r\nIn [1]: import torch                                                            \r\n\r\nIn [2]: ind = torch.arange(10).unsqueeze(0).to('cuda:1')                        \r\n\r\nIn [3]: sp_ind = torch.cat((ind, ind), 0)                                       \r\n\r\nIn [4]: sp_val = torch.rand(10).to('cuda:1')                                    \r\n\r\nIn [5]: sp_shape = (10, 10)                                                     \r\n\r\nIn [6]: x = torch.sparse_coo_tensor(sp_ind, sp_val, sp_shape)                   \r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-6-776f23c64083> in <module>\r\n----> 1 x = torch.sparse_coo_tensor(sp_ind, sp_val, sp_shape)\r\n\r\nRuntimeError: device of indices (0) must match device of values (1)\r\n\r\nIn [7]: # Now this                                                              \r\n\r\nIn [8]: x = torch.sparse_coo_tensor(sp_ind, sp_val.cuda(0), sp_shape)           \r\n\r\nIn [9]: x                                                                       \r\nOut[9]: \r\ntensor(indices=tensor([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\r\n                       [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]]),\r\n       values=tensor([0.5324, 0.1738, 0.4186, 0.2776, 0.4665, 0.9525, 0.6555,\r\n                      0.2805, 0.9244, 0.4847]),\r\n       device='cuda:0', size=(10, 10), nnz=10, layout=torch.sparse_coo)\r\n\r\nIn [10]: sp_ind                                                                 \r\nOut[10]: \r\ntensor([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\r\n        [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]], device='cuda:1')\r\n\r\nIn [11]: sp_val                                                                 \r\nOut[11]: \r\ntensor([0.5324, 0.1738, 0.4186, 0.2776, 0.4665, 0.9525, 0.6555, 0.2805, 0.9244,\r\n        0.4847], device='cuda:1')\r\n\r\n```\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n**See code snipped above.**\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n## Expected behavior\r\n\r\nProvided both indices and values are placed on the same device, `torch.sparse_coo_tensor` should return a sparse tensor on that device.\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n## Environment\r\n\r\nPlease copy and paste the output from our\r\n[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)\r\n(or fill out the checklist below manually).\r\n\r\nYou can get the script and run it with:\r\n```\r\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\r\n# For security purposes, please check the contents of collect_env.py before running it.\r\npython collect_env.py\r\n```\r\n\r\n - PyTorch Version (e.g., 1.0): 1.3.0\r\n - OS (e.g., Linux): Ubuntu 18.04 LTS\r\n - How you installed PyTorch (`conda`, `pip`, source): conda\r\n - Build command you used (if compiling from source): N/A\r\n - Python version: 3.7.4\r\n - CUDA/cuDNN version: 10.2/7.5\r\n - GPU models and configuration: 2x2080 Ti with Nvlink\r\n - Any other relevant information:\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\n<issue_comment>username_1: Oh, I meet the same thing!\n<issue_comment>username_2: I find `torch.sparse.FloatTensor' to be a good walk around.\n<issue_comment>username_3: @username_4 , can we close this issue via https://github.com/pytorch/pytorch/pull/41984 ?\n<issue_comment>username_4: Yes, we can close. It's solved already\n<issue_comment>username_5: Closing issue, since last comment indicates it as resolved.<issue_closed>"}
{"repo": "pytorch/pytorch", "issue_id": 511249220, "issue_number": 28500, "timestamp": "2021-06-03 02:10:44+00:00", "text": "", "context": "<issue_start><issue_comment>Title: Sparse tensor creation ignores indices placement\nusername_0: ## \ud83d\udc1b Bug\r\n\r\nAttempting to create a sparse tensor from a tensor of indices and values placed on another GPU than `cuda:0` (for instance `cuda:1`) fails with error `device of indices (0) must match device of values (1)`.\r\n\r\nPlacing the values on `cuda:0` while leaving the indices on `cuda:1` works and returns a sparse tensor on `cuda:0`.\r\n\r\nThe following demo should show the bug:\r\n```\r\nIn [1]: import torch                                                            \r\n\r\nIn [2]: ind = torch.arange(10).unsqueeze(0).to('cuda:1')                        \r\n\r\nIn [3]: sp_ind = torch.cat((ind, ind), 0)                                       \r\n\r\nIn [4]: sp_val = torch.rand(10).to('cuda:1')                                    \r\n\r\nIn [5]: sp_shape = (10, 10)                                                     \r\n\r\nIn [6]: x = torch.sparse_coo_tensor(sp_ind, sp_val, sp_shape)                   \r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-6-776f23c64083> in <module>\r\n----> 1 x = torch.sparse_coo_tensor(sp_ind, sp_val, sp_shape)\r\n\r\nRuntimeError: device of indices (0) must match device of values (1)\r\n\r\nIn [7]: # Now this                                                              \r\n\r\nIn [8]: x = torch.sparse_coo_tensor(sp_ind, sp_val.cuda(0), sp_shape)           \r\n\r\nIn [9]: x                                                                       \r\nOut[9]: \r\ntensor(indices=tensor([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\r\n                       [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]]),\r\n       values=tensor([0.5324, 0.1738, 0.4186, 0.2776, 0.4665, 0.9525, 0.6555,\r\n                      0.2805, 0.9244, 0.4847]),\r\n       device='cuda:0', size=(10, 10), nnz=10, layout=torch.sparse_coo)\r\n\r\nIn [10]: sp_ind                                                                 \r\nOut[10]: \r\ntensor([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\r\n        [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]], device='cuda:1')\r\n\r\nIn [11]: sp_val                                                                 \r\nOut[11]: \r\ntensor([0.5324, 0.1738, 0.4186, 0.2776, 0.4665, 0.9525, 0.6555, 0.2805, 0.9244,\r\n        0.4847], device='cuda:1')\r\n\r\n```\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n**See code snipped above.**\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n## Expected behavior\r\n\r\nProvided both indices and values are placed on the same device, `torch.sparse_coo_tensor` should return a sparse tensor on that device.\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n## Environment\r\n\r\nPlease copy and paste the output from our\r\n[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)\r\n(or fill out the checklist below manually).\r\n\r\nYou can get the script and run it with:\r\n```\r\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\r\n# For security purposes, please check the contents of collect_env.py before running it.\r\npython collect_env.py\r\n```\r\n\r\n - PyTorch Version (e.g., 1.0): 1.3.0\r\n - OS (e.g., Linux): Ubuntu 18.04 LTS\r\n - How you installed PyTorch (`conda`, `pip`, source): conda\r\n - Build command you used (if compiling from source): N/A\r\n - Python version: 3.7.4\r\n - CUDA/cuDNN version: 10.2/7.5\r\n - GPU models and configuration: 2x2080 Ti with Nvlink\r\n - Any other relevant information:\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\n<issue_comment>username_1: Oh, I meet the same thing!\n<issue_comment>username_2: I find `torch.sparse.FloatTensor' to be a good walk around.\n<issue_comment>username_3: @username_4 , can we close this issue via https://github.com/pytorch/pytorch/pull/41984 ?\n<issue_comment>username_4: Yes, we can close. It's solved already\n<issue_comment>username_5: Closing issue, since last comment indicates it as resolved.<issue_closed>"}
{"repo": "pytorch/pytorch", "issue_id": 599121342, "issue_number": 36510, "timestamp": "2020-04-13 20:37:03+00:00", "text": "cc @iseeyuan \r\n\r\nThey were added in https://github.com/pytorch/pytorch/pull/33294 and the PR says \"The \"_\" prefix registration will be removed when the operators are all migrated to mobile.\" This should be reflected in the code somewhere.", "context": "<issue_start><issue_comment>Title: Meaning of _quantized namespace isn't documented\nusername_0: cc @iseeyuan \r\n\r\nThey were added in https://github.com/pytorch/pytorch/pull/33294 and the PR says \"The \"_\" prefix registration will be removed when the operators are all migrated to mobile.\" This should be reflected in the code somewhere.\n<issue_comment>username_0: I guess I'll fix this when I reorg the operator registrations\n<issue_comment>username_1: Hey @username_0  assigning this to you based on your comment. We can take this up if you can't get to it. Thanks!"}
{"repo": "pytorch/pytorch", "issue_id": 599121342, "issue_number": 36510, "timestamp": "2020-04-13 21:16:08+00:00", "text": "I guess I'll fix this when I reorg the operator registrations", "context": "<issue_start><issue_comment>Title: Meaning of _quantized namespace isn't documented\nusername_0: cc @iseeyuan \r\n\r\nThey were added in https://github.com/pytorch/pytorch/pull/33294 and the PR says \"The \"_\" prefix registration will be removed when the operators are all migrated to mobile.\" This should be reflected in the code somewhere.\n<issue_comment>username_0: I guess I'll fix this when I reorg the operator registrations\n<issue_comment>username_1: Hey @username_0  assigning this to you based on your comment. We can take this up if you can't get to it. Thanks!"}
{"repo": "pytorch/pytorch", "issue_id": 599121342, "issue_number": 36510, "timestamp": "2020-04-15 00:12:46+00:00", "text": "Hey @username_0  assigning this to you based on your comment. We can take this up if you can't get to it. Thanks!", "context": "<issue_start><issue_comment>Title: Meaning of _quantized namespace isn't documented\nusername_0: cc @iseeyuan \r\n\r\nThey were added in https://github.com/pytorch/pytorch/pull/33294 and the PR says \"The \"_\" prefix registration will be removed when the operators are all migrated to mobile.\" This should be reflected in the code somewhere.\n<issue_comment>username_0: I guess I'll fix this when I reorg the operator registrations\n<issue_comment>username_1: Hey @username_0  assigning this to you based on your comment. We can take this up if you can't get to it. Thanks!"}
{"repo": "facebook/react", "issue_id": 650765838, "issue_number": 19253, "timestamp": "2020-07-03T21:02:19Z", "text": "use `isElement` from `react-is` inside `isValidElement`", "context": "<issue_start><issue_comment>Title: removed duplicate code\nusername_0: use `isElement` from `react-is` inside `isValidElement`\n<issue_comment>username_1: Thanks, but we don't want a dependency on this package from `react`, and we also don't want to accidentally inline some code from it during bundling. The duplication is intentional."}
{"repo": "facebook/react", "issue_id": 650765838, "issue_number": 19253, "timestamp": "2020-07-08 15:58:14+00:00", "text": "Thanks, but we don't want a dependency on this package from `react`, and we also don't want to accidentally inline some code from it during bundling. The duplication is intentional.", "context": "<issue_start><issue_comment>Title: removed duplicate code\nusername_0: use `isElement` from `react-is` inside `isValidElement`\n<issue_comment>username_1: Thanks, but we don't want a dependency on this package from `react`, and we also don't want to accidentally inline some code from it during bundling. The duplication is intentional."}
{"repo": "facebook/react", "issue_id": 664173077, "issue_number": 19438, "timestamp": "2020-07-23 03:11:56+00:00", "text": "In chrome and safari, the ```console.log``` was triggered in different order. [Example](https://codesandbox.io/s/amazing-payne-4lmxo) will show different result if opened in different browsers.\r\n\r\nReact version: 16.13.1\r\n\r\n## Steps To Reproduce\r\n\r\n1. Run following code in chrome and safari.\r\n2. Check console log\r\n\r\n```jsx\r\nimport React, { useState, useEffect } from \"react\";\r\n\r\nconst Test = props => {\r\n  useEffect(() => {\r\n    console.log(\"useEffect in child\");\r\n  }, []);\r\n\r\n  return \"Test useEffect\";\r\n};\r\n\r\nconst App = () => {\r\n  setTimeout(() => {\r\n    console.log(\"timeout in render\");\r\n  }, 0);\r\n\r\n  return <Test />;\r\n};\r\n\r\nexport default App\r\n```\r\n\r\nLink to code example:\r\n\r\n[Example](https://codesandbox.io/s/amazing-payne-4lmxo)\r\n\r\n## The current behavior\r\n\r\nThe message is not the same order;\r\n\r\n```jsx\r\n/**\r\n * chrome result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n\r\n/**\r\n * safari result\r\n *\r\n * timeout in render\r\n * useEffect in child\r\n */\r\n```\r\n\r\n## The expected behavior\r\n\r\nThe message should be same;\r\n\r\n```jsx\r\n/**\r\n * chrome result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n\r\n/**\r\n * safari result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n```", "context": "<issue_start><issue_comment>Title: Bug: The trigger timing of \"useEffect\" is different in chrome and safari.\nusername_0: In chrome and safari, the ```console.log``` was triggered in different order. [Example](https://codesandbox.io/s/amazing-payne-4lmxo) will show different result if opened in different browsers.\r\n\r\nReact version: 16.13.1\r\n\r\n## Steps To Reproduce\r\n\r\n1. Run following code in chrome and safari.\r\n2. Check console log\r\n\r\n```jsx\r\nimport React, { useState, useEffect } from \"react\";\r\n\r\nconst Test = props => {\r\n  useEffect(() => {\r\n    console.log(\"useEffect in child\");\r\n  }, []);\r\n\r\n  return \"Test useEffect\";\r\n};\r\n\r\nconst App = () => {\r\n  setTimeout(() => {\r\n    console.log(\"timeout in render\");\r\n  }, 0);\r\n\r\n  return <Test />;\r\n};\r\n\r\nexport default App\r\n```\r\n\r\nLink to code example:\r\n\r\n[Example](https://codesandbox.io/s/amazing-payne-4lmxo)\r\n\r\n## The current behavior\r\n\r\nThe message is not the same order;\r\n\r\n```jsx\r\n/**\r\n * chrome result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n\r\n/**\r\n * safari result\r\n *\r\n * timeout in render\r\n * useEffect in child\r\n */\r\n```\r\n\r\n## The expected behavior\r\n\r\nThe message should be same;\r\n\r\n```jsx\r\n/**\r\n * chrome result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n\r\n/**\r\n * safari result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n```<issue_closed>\n<issue_comment>username_0: In chrome and safari, the ```console.log``` was triggered in different order. [Example](https://codesandbox.io/s/amazing-payne-4lmxo) will show different result if opened in different browsers.\r\n\r\nReact version: 16.13.1\r\n\r\n## Steps To Reproduce\r\n\r\n1. Run following code in chrome and safari.\r\n2. Check console log\r\n\r\n```jsx\r\nimport React, { useState, useEffect } from \"react\";\r\n\r\nconst Test = props => {\r\n  useEffect(() => {\r\n    console.log(\"useEffect in child\");\r\n  }, []);\r\n\r\n  return \"Test useEffect\";\r\n};\r\n\r\nconst App = () => {\r\n  setTimeout(() => {\r\n    console.log(\"timeout in render\");\r\n  }, 0);\r\n\r\n  return <Test />;\r\n};\r\n\r\nexport default App\r\n```\r\n\r\nLink to code example:\r\n\r\n[Example](https://codesandbox.io/s/amazing-payne-4lmxo)\r\n\r\n## The current behavior\r\n\r\nThe message is not the same order;\r\n\r\n```jsx\r\n/**\r\n * chrome result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n\r\n/**\r\n * safari result\r\n *\r\n * timeout in render\r\n * useEffect in child\r\n */\r\n```\r\n\r\n## The expected behavior\r\n\r\nThe message should be same;\r\n\r\n```jsx\r\n/**\r\n * chrome result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n\r\n/**\r\n * safari result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n```\n<issue_comment>username_1: You do the side effect in the render method:\r\n```typescript\r\nconst App = () => {\r\n  setTimeout(() => {\r\n    console.log(\"timeout in render\");\r\n  }, 0);\r\n\r\n  return <Test />;\r\n};\r\n```\r\nwhich is not allowed. React doesn't guarantee this kind of orders. you should put `setTimeout` inside an useEffect.\n<issue_comment>username_0: Yes, i agree this is not allowed. But different browsers should have same result.I think react should achieve lifecycle with more stable browser interfaces.\n<issue_comment>username_2: Many reasons why this is not a bug:\r\n\r\n - you are not allowed to make side effects in render. If you do that, the outcome is essentially \"undefined behaviour\", that is, nothing is guaranteed\r\n\r\n- React provides no guarantees about timing of `useEffect` with respect to render anyway.\r\n\r\nSo while this is an inconsistency and it could be fixed, there is not good reason to do so.\n<issue_comment>username_3: It is not supported to call `setTimeout` during rendering. React provides absolutely no guarantees about the order or the number of component render calls \u2014 so you can't rely on subtle timing aspects of them.\r\n\r\nIf I wrap it in `useEffect` I don't observe the difference.<issue_closed>"}
{"repo": "facebook/react", "issue_id": 664173077, "issue_number": 19438, "timestamp": "2020-07-23 03:12:10+00:00", "text": "", "context": "<issue_start><issue_comment>Title: Bug: The trigger timing of \"useEffect\" is different in chrome and safari.\nusername_0: In chrome and safari, the ```console.log``` was triggered in different order. [Example](https://codesandbox.io/s/amazing-payne-4lmxo) will show different result if opened in different browsers.\r\n\r\nReact version: 16.13.1\r\n\r\n## Steps To Reproduce\r\n\r\n1. Run following code in chrome and safari.\r\n2. Check console log\r\n\r\n```jsx\r\nimport React, { useState, useEffect } from \"react\";\r\n\r\nconst Test = props => {\r\n  useEffect(() => {\r\n    console.log(\"useEffect in child\");\r\n  }, []);\r\n\r\n  return \"Test useEffect\";\r\n};\r\n\r\nconst App = () => {\r\n  setTimeout(() => {\r\n    console.log(\"timeout in render\");\r\n  }, 0);\r\n\r\n  return <Test />;\r\n};\r\n\r\nexport default App\r\n```\r\n\r\nLink to code example:\r\n\r\n[Example](https://codesandbox.io/s/amazing-payne-4lmxo)\r\n\r\n## The current behavior\r\n\r\nThe message is not the same order;\r\n\r\n```jsx\r\n/**\r\n * chrome result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n\r\n/**\r\n * safari result\r\n *\r\n * timeout in render\r\n * useEffect in child\r\n */\r\n```\r\n\r\n## The expected behavior\r\n\r\nThe message should be same;\r\n\r\n```jsx\r\n/**\r\n * chrome result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n\r\n/**\r\n * safari result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n```<issue_closed>\n<issue_comment>username_0: In chrome and safari, the ```console.log``` was triggered in different order. [Example](https://codesandbox.io/s/amazing-payne-4lmxo) will show different result if opened in different browsers.\r\n\r\nReact version: 16.13.1\r\n\r\n## Steps To Reproduce\r\n\r\n1. Run following code in chrome and safari.\r\n2. Check console log\r\n\r\n```jsx\r\nimport React, { useState, useEffect } from \"react\";\r\n\r\nconst Test = props => {\r\n  useEffect(() => {\r\n    console.log(\"useEffect in child\");\r\n  }, []);\r\n\r\n  return \"Test useEffect\";\r\n};\r\n\r\nconst App = () => {\r\n  setTimeout(() => {\r\n    console.log(\"timeout in render\");\r\n  }, 0);\r\n\r\n  return <Test />;\r\n};\r\n\r\nexport default App\r\n```\r\n\r\nLink to code example:\r\n\r\n[Example](https://codesandbox.io/s/amazing-payne-4lmxo)\r\n\r\n## The current behavior\r\n\r\nThe message is not the same order;\r\n\r\n```jsx\r\n/**\r\n * chrome result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n\r\n/**\r\n * safari result\r\n *\r\n * timeout in render\r\n * useEffect in child\r\n */\r\n```\r\n\r\n## The expected behavior\r\n\r\nThe message should be same;\r\n\r\n```jsx\r\n/**\r\n * chrome result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n\r\n/**\r\n * safari result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n```\n<issue_comment>username_1: You do the side effect in the render method:\r\n```typescript\r\nconst App = () => {\r\n  setTimeout(() => {\r\n    console.log(\"timeout in render\");\r\n  }, 0);\r\n\r\n  return <Test />;\r\n};\r\n```\r\nwhich is not allowed. React doesn't guarantee this kind of orders. you should put `setTimeout` inside an useEffect.\n<issue_comment>username_0: Yes, i agree this is not allowed. But different browsers should have same result.I think react should achieve lifecycle with more stable browser interfaces.\n<issue_comment>username_2: Many reasons why this is not a bug:\r\n\r\n - you are not allowed to make side effects in render. If you do that, the outcome is essentially \"undefined behaviour\", that is, nothing is guaranteed\r\n\r\n- React provides no guarantees about timing of `useEffect` with respect to render anyway.\r\n\r\nSo while this is an inconsistency and it could be fixed, there is not good reason to do so.\n<issue_comment>username_3: It is not supported to call `setTimeout` during rendering. React provides absolutely no guarantees about the order or the number of component render calls \u2014 so you can't rely on subtle timing aspects of them.\r\n\r\nIf I wrap it in `useEffect` I don't observe the difference.<issue_closed>"}
{"repo": "facebook/react", "issue_id": 664173077, "issue_number": 19438, "timestamp": "2020-07-23 03:12:22+00:00", "text": "In chrome and safari, the ```console.log``` was triggered in different order. [Example](https://codesandbox.io/s/amazing-payne-4lmxo) will show different result if opened in different browsers.\r\n\r\nReact version: 16.13.1\r\n\r\n## Steps To Reproduce\r\n\r\n1. Run following code in chrome and safari.\r\n2. Check console log\r\n\r\n```jsx\r\nimport React, { useState, useEffect } from \"react\";\r\n\r\nconst Test = props => {\r\n  useEffect(() => {\r\n    console.log(\"useEffect in child\");\r\n  }, []);\r\n\r\n  return \"Test useEffect\";\r\n};\r\n\r\nconst App = () => {\r\n  setTimeout(() => {\r\n    console.log(\"timeout in render\");\r\n  }, 0);\r\n\r\n  return <Test />;\r\n};\r\n\r\nexport default App\r\n```\r\n\r\nLink to code example:\r\n\r\n[Example](https://codesandbox.io/s/amazing-payne-4lmxo)\r\n\r\n## The current behavior\r\n\r\nThe message is not the same order;\r\n\r\n```jsx\r\n/**\r\n * chrome result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n\r\n/**\r\n * safari result\r\n *\r\n * timeout in render\r\n * useEffect in child\r\n */\r\n```\r\n\r\n## The expected behavior\r\n\r\nThe message should be same;\r\n\r\n```jsx\r\n/**\r\n * chrome result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n\r\n/**\r\n * safari result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n```", "context": "<issue_start><issue_comment>Title: Bug: The trigger timing of \"useEffect\" is different in chrome and safari.\nusername_0: In chrome and safari, the ```console.log``` was triggered in different order. [Example](https://codesandbox.io/s/amazing-payne-4lmxo) will show different result if opened in different browsers.\r\n\r\nReact version: 16.13.1\r\n\r\n## Steps To Reproduce\r\n\r\n1. Run following code in chrome and safari.\r\n2. Check console log\r\n\r\n```jsx\r\nimport React, { useState, useEffect } from \"react\";\r\n\r\nconst Test = props => {\r\n  useEffect(() => {\r\n    console.log(\"useEffect in child\");\r\n  }, []);\r\n\r\n  return \"Test useEffect\";\r\n};\r\n\r\nconst App = () => {\r\n  setTimeout(() => {\r\n    console.log(\"timeout in render\");\r\n  }, 0);\r\n\r\n  return <Test />;\r\n};\r\n\r\nexport default App\r\n```\r\n\r\nLink to code example:\r\n\r\n[Example](https://codesandbox.io/s/amazing-payne-4lmxo)\r\n\r\n## The current behavior\r\n\r\nThe message is not the same order;\r\n\r\n```jsx\r\n/**\r\n * chrome result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n\r\n/**\r\n * safari result\r\n *\r\n * timeout in render\r\n * useEffect in child\r\n */\r\n```\r\n\r\n## The expected behavior\r\n\r\nThe message should be same;\r\n\r\n```jsx\r\n/**\r\n * chrome result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n\r\n/**\r\n * safari result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n```<issue_closed>\n<issue_comment>username_0: In chrome and safari, the ```console.log``` was triggered in different order. [Example](https://codesandbox.io/s/amazing-payne-4lmxo) will show different result if opened in different browsers.\r\n\r\nReact version: 16.13.1\r\n\r\n## Steps To Reproduce\r\n\r\n1. Run following code in chrome and safari.\r\n2. Check console log\r\n\r\n```jsx\r\nimport React, { useState, useEffect } from \"react\";\r\n\r\nconst Test = props => {\r\n  useEffect(() => {\r\n    console.log(\"useEffect in child\");\r\n  }, []);\r\n\r\n  return \"Test useEffect\";\r\n};\r\n\r\nconst App = () => {\r\n  setTimeout(() => {\r\n    console.log(\"timeout in render\");\r\n  }, 0);\r\n\r\n  return <Test />;\r\n};\r\n\r\nexport default App\r\n```\r\n\r\nLink to code example:\r\n\r\n[Example](https://codesandbox.io/s/amazing-payne-4lmxo)\r\n\r\n## The current behavior\r\n\r\nThe message is not the same order;\r\n\r\n```jsx\r\n/**\r\n * chrome result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n\r\n/**\r\n * safari result\r\n *\r\n * timeout in render\r\n * useEffect in child\r\n */\r\n```\r\n\r\n## The expected behavior\r\n\r\nThe message should be same;\r\n\r\n```jsx\r\n/**\r\n * chrome result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n\r\n/**\r\n * safari result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n```\n<issue_comment>username_1: You do the side effect in the render method:\r\n```typescript\r\nconst App = () => {\r\n  setTimeout(() => {\r\n    console.log(\"timeout in render\");\r\n  }, 0);\r\n\r\n  return <Test />;\r\n};\r\n```\r\nwhich is not allowed. React doesn't guarantee this kind of orders. you should put `setTimeout` inside an useEffect.\n<issue_comment>username_0: Yes, i agree this is not allowed. But different browsers should have same result.I think react should achieve lifecycle with more stable browser interfaces.\n<issue_comment>username_2: Many reasons why this is not a bug:\r\n\r\n - you are not allowed to make side effects in render. If you do that, the outcome is essentially \"undefined behaviour\", that is, nothing is guaranteed\r\n\r\n- React provides no guarantees about timing of `useEffect` with respect to render anyway.\r\n\r\nSo while this is an inconsistency and it could be fixed, there is not good reason to do so.\n<issue_comment>username_3: It is not supported to call `setTimeout` during rendering. React provides absolutely no guarantees about the order or the number of component render calls \u2014 so you can't rely on subtle timing aspects of them.\r\n\r\nIf I wrap it in `useEffect` I don't observe the difference.<issue_closed>"}
{"repo": "facebook/react", "issue_id": 664173077, "issue_number": 19438, "timestamp": "2020-07-23 07:44:58+00:00", "text": "You do the side effect in the render method:\r\n```typescript\r\nconst App = () => {\r\n  setTimeout(() => {\r\n    console.log(\"timeout in render\");\r\n  }, 0);\r\n\r\n  return <Test />;\r\n};\r\n```\r\nwhich is not allowed. React doesn't guarantee this kind of orders. you should put `setTimeout` inside an useEffect.", "context": "<issue_start><issue_comment>Title: Bug: The trigger timing of \"useEffect\" is different in chrome and safari.\nusername_0: In chrome and safari, the ```console.log``` was triggered in different order. [Example](https://codesandbox.io/s/amazing-payne-4lmxo) will show different result if opened in different browsers.\r\n\r\nReact version: 16.13.1\r\n\r\n## Steps To Reproduce\r\n\r\n1. Run following code in chrome and safari.\r\n2. Check console log\r\n\r\n```jsx\r\nimport React, { useState, useEffect } from \"react\";\r\n\r\nconst Test = props => {\r\n  useEffect(() => {\r\n    console.log(\"useEffect in child\");\r\n  }, []);\r\n\r\n  return \"Test useEffect\";\r\n};\r\n\r\nconst App = () => {\r\n  setTimeout(() => {\r\n    console.log(\"timeout in render\");\r\n  }, 0);\r\n\r\n  return <Test />;\r\n};\r\n\r\nexport default App\r\n```\r\n\r\nLink to code example:\r\n\r\n[Example](https://codesandbox.io/s/amazing-payne-4lmxo)\r\n\r\n## The current behavior\r\n\r\nThe message is not the same order;\r\n\r\n```jsx\r\n/**\r\n * chrome result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n\r\n/**\r\n * safari result\r\n *\r\n * timeout in render\r\n * useEffect in child\r\n */\r\n```\r\n\r\n## The expected behavior\r\n\r\nThe message should be same;\r\n\r\n```jsx\r\n/**\r\n * chrome result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n\r\n/**\r\n * safari result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n```<issue_closed>\n<issue_comment>username_0: In chrome and safari, the ```console.log``` was triggered in different order. [Example](https://codesandbox.io/s/amazing-payne-4lmxo) will show different result if opened in different browsers.\r\n\r\nReact version: 16.13.1\r\n\r\n## Steps To Reproduce\r\n\r\n1. Run following code in chrome and safari.\r\n2. Check console log\r\n\r\n```jsx\r\nimport React, { useState, useEffect } from \"react\";\r\n\r\nconst Test = props => {\r\n  useEffect(() => {\r\n    console.log(\"useEffect in child\");\r\n  }, []);\r\n\r\n  return \"Test useEffect\";\r\n};\r\n\r\nconst App = () => {\r\n  setTimeout(() => {\r\n    console.log(\"timeout in render\");\r\n  }, 0);\r\n\r\n  return <Test />;\r\n};\r\n\r\nexport default App\r\n```\r\n\r\nLink to code example:\r\n\r\n[Example](https://codesandbox.io/s/amazing-payne-4lmxo)\r\n\r\n## The current behavior\r\n\r\nThe message is not the same order;\r\n\r\n```jsx\r\n/**\r\n * chrome result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n\r\n/**\r\n * safari result\r\n *\r\n * timeout in render\r\n * useEffect in child\r\n */\r\n```\r\n\r\n## The expected behavior\r\n\r\nThe message should be same;\r\n\r\n```jsx\r\n/**\r\n * chrome result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n\r\n/**\r\n * safari result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n```\n<issue_comment>username_1: You do the side effect in the render method:\r\n```typescript\r\nconst App = () => {\r\n  setTimeout(() => {\r\n    console.log(\"timeout in render\");\r\n  }, 0);\r\n\r\n  return <Test />;\r\n};\r\n```\r\nwhich is not allowed. React doesn't guarantee this kind of orders. you should put `setTimeout` inside an useEffect.\n<issue_comment>username_0: Yes, i agree this is not allowed. But different browsers should have same result.I think react should achieve lifecycle with more stable browser interfaces.\n<issue_comment>username_2: Many reasons why this is not a bug:\r\n\r\n - you are not allowed to make side effects in render. If you do that, the outcome is essentially \"undefined behaviour\", that is, nothing is guaranteed\r\n\r\n- React provides no guarantees about timing of `useEffect` with respect to render anyway.\r\n\r\nSo while this is an inconsistency and it could be fixed, there is not good reason to do so.\n<issue_comment>username_3: It is not supported to call `setTimeout` during rendering. React provides absolutely no guarantees about the order or the number of component render calls \u2014 so you can't rely on subtle timing aspects of them.\r\n\r\nIf I wrap it in `useEffect` I don't observe the difference.<issue_closed>"}
{"repo": "facebook/react", "issue_id": 664173077, "issue_number": 19438, "timestamp": "2020-07-23 08:04:52+00:00", "text": "Yes, i agree this is not allowed. But different browsers should have same result.I think react should achieve lifecycle with more stable browser interfaces.", "context": "<issue_start><issue_comment>Title: Bug: The trigger timing of \"useEffect\" is different in chrome and safari.\nusername_0: In chrome and safari, the ```console.log``` was triggered in different order. [Example](https://codesandbox.io/s/amazing-payne-4lmxo) will show different result if opened in different browsers.\r\n\r\nReact version: 16.13.1\r\n\r\n## Steps To Reproduce\r\n\r\n1. Run following code in chrome and safari.\r\n2. Check console log\r\n\r\n```jsx\r\nimport React, { useState, useEffect } from \"react\";\r\n\r\nconst Test = props => {\r\n  useEffect(() => {\r\n    console.log(\"useEffect in child\");\r\n  }, []);\r\n\r\n  return \"Test useEffect\";\r\n};\r\n\r\nconst App = () => {\r\n  setTimeout(() => {\r\n    console.log(\"timeout in render\");\r\n  }, 0);\r\n\r\n  return <Test />;\r\n};\r\n\r\nexport default App\r\n```\r\n\r\nLink to code example:\r\n\r\n[Example](https://codesandbox.io/s/amazing-payne-4lmxo)\r\n\r\n## The current behavior\r\n\r\nThe message is not the same order;\r\n\r\n```jsx\r\n/**\r\n * chrome result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n\r\n/**\r\n * safari result\r\n *\r\n * timeout in render\r\n * useEffect in child\r\n */\r\n```\r\n\r\n## The expected behavior\r\n\r\nThe message should be same;\r\n\r\n```jsx\r\n/**\r\n * chrome result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n\r\n/**\r\n * safari result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n```<issue_closed>\n<issue_comment>username_0: In chrome and safari, the ```console.log``` was triggered in different order. [Example](https://codesandbox.io/s/amazing-payne-4lmxo) will show different result if opened in different browsers.\r\n\r\nReact version: 16.13.1\r\n\r\n## Steps To Reproduce\r\n\r\n1. Run following code in chrome and safari.\r\n2. Check console log\r\n\r\n```jsx\r\nimport React, { useState, useEffect } from \"react\";\r\n\r\nconst Test = props => {\r\n  useEffect(() => {\r\n    console.log(\"useEffect in child\");\r\n  }, []);\r\n\r\n  return \"Test useEffect\";\r\n};\r\n\r\nconst App = () => {\r\n  setTimeout(() => {\r\n    console.log(\"timeout in render\");\r\n  }, 0);\r\n\r\n  return <Test />;\r\n};\r\n\r\nexport default App\r\n```\r\n\r\nLink to code example:\r\n\r\n[Example](https://codesandbox.io/s/amazing-payne-4lmxo)\r\n\r\n## The current behavior\r\n\r\nThe message is not the same order;\r\n\r\n```jsx\r\n/**\r\n * chrome result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n\r\n/**\r\n * safari result\r\n *\r\n * timeout in render\r\n * useEffect in child\r\n */\r\n```\r\n\r\n## The expected behavior\r\n\r\nThe message should be same;\r\n\r\n```jsx\r\n/**\r\n * chrome result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n\r\n/**\r\n * safari result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n```\n<issue_comment>username_1: You do the side effect in the render method:\r\n```typescript\r\nconst App = () => {\r\n  setTimeout(() => {\r\n    console.log(\"timeout in render\");\r\n  }, 0);\r\n\r\n  return <Test />;\r\n};\r\n```\r\nwhich is not allowed. React doesn't guarantee this kind of orders. you should put `setTimeout` inside an useEffect.\n<issue_comment>username_0: Yes, i agree this is not allowed. But different browsers should have same result.I think react should achieve lifecycle with more stable browser interfaces.\n<issue_comment>username_2: Many reasons why this is not a bug:\r\n\r\n - you are not allowed to make side effects in render. If you do that, the outcome is essentially \"undefined behaviour\", that is, nothing is guaranteed\r\n\r\n- React provides no guarantees about timing of `useEffect` with respect to render anyway.\r\n\r\nSo while this is an inconsistency and it could be fixed, there is not good reason to do so.\n<issue_comment>username_3: It is not supported to call `setTimeout` during rendering. React provides absolutely no guarantees about the order or the number of component render calls \u2014 so you can't rely on subtle timing aspects of them.\r\n\r\nIf I wrap it in `useEffect` I don't observe the difference.<issue_closed>"}
{"repo": "facebook/react", "issue_id": 664173077, "issue_number": 19438, "timestamp": "2020-07-23 08:28:48+00:00", "text": "Many reasons why this is not a bug:\r\n\r\n - you are not allowed to make side effects in render. If you do that, the outcome is essentially \"undefined behaviour\", that is, nothing is guaranteed\r\n\r\n- React provides no guarantees about timing of `useEffect` with respect to render anyway.\r\n\r\nSo while this is an inconsistency and it could be fixed, there is not good reason to do so.", "context": "<issue_start><issue_comment>Title: Bug: The trigger timing of \"useEffect\" is different in chrome and safari.\nusername_0: In chrome and safari, the ```console.log``` was triggered in different order. [Example](https://codesandbox.io/s/amazing-payne-4lmxo) will show different result if opened in different browsers.\r\n\r\nReact version: 16.13.1\r\n\r\n## Steps To Reproduce\r\n\r\n1. Run following code in chrome and safari.\r\n2. Check console log\r\n\r\n```jsx\r\nimport React, { useState, useEffect } from \"react\";\r\n\r\nconst Test = props => {\r\n  useEffect(() => {\r\n    console.log(\"useEffect in child\");\r\n  }, []);\r\n\r\n  return \"Test useEffect\";\r\n};\r\n\r\nconst App = () => {\r\n  setTimeout(() => {\r\n    console.log(\"timeout in render\");\r\n  }, 0);\r\n\r\n  return <Test />;\r\n};\r\n\r\nexport default App\r\n```\r\n\r\nLink to code example:\r\n\r\n[Example](https://codesandbox.io/s/amazing-payne-4lmxo)\r\n\r\n## The current behavior\r\n\r\nThe message is not the same order;\r\n\r\n```jsx\r\n/**\r\n * chrome result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n\r\n/**\r\n * safari result\r\n *\r\n * timeout in render\r\n * useEffect in child\r\n */\r\n```\r\n\r\n## The expected behavior\r\n\r\nThe message should be same;\r\n\r\n```jsx\r\n/**\r\n * chrome result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n\r\n/**\r\n * safari result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n```<issue_closed>\n<issue_comment>username_0: In chrome and safari, the ```console.log``` was triggered in different order. [Example](https://codesandbox.io/s/amazing-payne-4lmxo) will show different result if opened in different browsers.\r\n\r\nReact version: 16.13.1\r\n\r\n## Steps To Reproduce\r\n\r\n1. Run following code in chrome and safari.\r\n2. Check console log\r\n\r\n```jsx\r\nimport React, { useState, useEffect } from \"react\";\r\n\r\nconst Test = props => {\r\n  useEffect(() => {\r\n    console.log(\"useEffect in child\");\r\n  }, []);\r\n\r\n  return \"Test useEffect\";\r\n};\r\n\r\nconst App = () => {\r\n  setTimeout(() => {\r\n    console.log(\"timeout in render\");\r\n  }, 0);\r\n\r\n  return <Test />;\r\n};\r\n\r\nexport default App\r\n```\r\n\r\nLink to code example:\r\n\r\n[Example](https://codesandbox.io/s/amazing-payne-4lmxo)\r\n\r\n## The current behavior\r\n\r\nThe message is not the same order;\r\n\r\n```jsx\r\n/**\r\n * chrome result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n\r\n/**\r\n * safari result\r\n *\r\n * timeout in render\r\n * useEffect in child\r\n */\r\n```\r\n\r\n## The expected behavior\r\n\r\nThe message should be same;\r\n\r\n```jsx\r\n/**\r\n * chrome result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n\r\n/**\r\n * safari result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n```\n<issue_comment>username_1: You do the side effect in the render method:\r\n```typescript\r\nconst App = () => {\r\n  setTimeout(() => {\r\n    console.log(\"timeout in render\");\r\n  }, 0);\r\n\r\n  return <Test />;\r\n};\r\n```\r\nwhich is not allowed. React doesn't guarantee this kind of orders. you should put `setTimeout` inside an useEffect.\n<issue_comment>username_0: Yes, i agree this is not allowed. But different browsers should have same result.I think react should achieve lifecycle with more stable browser interfaces.\n<issue_comment>username_2: Many reasons why this is not a bug:\r\n\r\n - you are not allowed to make side effects in render. If you do that, the outcome is essentially \"undefined behaviour\", that is, nothing is guaranteed\r\n\r\n- React provides no guarantees about timing of `useEffect` with respect to render anyway.\r\n\r\nSo while this is an inconsistency and it could be fixed, there is not good reason to do so.\n<issue_comment>username_3: It is not supported to call `setTimeout` during rendering. React provides absolutely no guarantees about the order or the number of component render calls \u2014 so you can't rely on subtle timing aspects of them.\r\n\r\nIf I wrap it in `useEffect` I don't observe the difference.<issue_closed>"}
{"repo": "facebook/react", "issue_id": 664173077, "issue_number": 19438, "timestamp": "2020-07-23 09:52:43+00:00", "text": "It is not supported to call `setTimeout` during rendering. React provides absolutely no guarantees about the order or the number of component render calls \u2014 so you can't rely on subtle timing aspects of them.\r\n\r\nIf I wrap it in `useEffect` I don't observe the difference.", "context": "<issue_start><issue_comment>Title: Bug: The trigger timing of \"useEffect\" is different in chrome and safari.\nusername_0: In chrome and safari, the ```console.log``` was triggered in different order. [Example](https://codesandbox.io/s/amazing-payne-4lmxo) will show different result if opened in different browsers.\r\n\r\nReact version: 16.13.1\r\n\r\n## Steps To Reproduce\r\n\r\n1. Run following code in chrome and safari.\r\n2. Check console log\r\n\r\n```jsx\r\nimport React, { useState, useEffect } from \"react\";\r\n\r\nconst Test = props => {\r\n  useEffect(() => {\r\n    console.log(\"useEffect in child\");\r\n  }, []);\r\n\r\n  return \"Test useEffect\";\r\n};\r\n\r\nconst App = () => {\r\n  setTimeout(() => {\r\n    console.log(\"timeout in render\");\r\n  }, 0);\r\n\r\n  return <Test />;\r\n};\r\n\r\nexport default App\r\n```\r\n\r\nLink to code example:\r\n\r\n[Example](https://codesandbox.io/s/amazing-payne-4lmxo)\r\n\r\n## The current behavior\r\n\r\nThe message is not the same order;\r\n\r\n```jsx\r\n/**\r\n * chrome result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n\r\n/**\r\n * safari result\r\n *\r\n * timeout in render\r\n * useEffect in child\r\n */\r\n```\r\n\r\n## The expected behavior\r\n\r\nThe message should be same;\r\n\r\n```jsx\r\n/**\r\n * chrome result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n\r\n/**\r\n * safari result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n```<issue_closed>\n<issue_comment>username_0: In chrome and safari, the ```console.log``` was triggered in different order. [Example](https://codesandbox.io/s/amazing-payne-4lmxo) will show different result if opened in different browsers.\r\n\r\nReact version: 16.13.1\r\n\r\n## Steps To Reproduce\r\n\r\n1. Run following code in chrome and safari.\r\n2. Check console log\r\n\r\n```jsx\r\nimport React, { useState, useEffect } from \"react\";\r\n\r\nconst Test = props => {\r\n  useEffect(() => {\r\n    console.log(\"useEffect in child\");\r\n  }, []);\r\n\r\n  return \"Test useEffect\";\r\n};\r\n\r\nconst App = () => {\r\n  setTimeout(() => {\r\n    console.log(\"timeout in render\");\r\n  }, 0);\r\n\r\n  return <Test />;\r\n};\r\n\r\nexport default App\r\n```\r\n\r\nLink to code example:\r\n\r\n[Example](https://codesandbox.io/s/amazing-payne-4lmxo)\r\n\r\n## The current behavior\r\n\r\nThe message is not the same order;\r\n\r\n```jsx\r\n/**\r\n * chrome result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n\r\n/**\r\n * safari result\r\n *\r\n * timeout in render\r\n * useEffect in child\r\n */\r\n```\r\n\r\n## The expected behavior\r\n\r\nThe message should be same;\r\n\r\n```jsx\r\n/**\r\n * chrome result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n\r\n/**\r\n * safari result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n```\n<issue_comment>username_1: You do the side effect in the render method:\r\n```typescript\r\nconst App = () => {\r\n  setTimeout(() => {\r\n    console.log(\"timeout in render\");\r\n  }, 0);\r\n\r\n  return <Test />;\r\n};\r\n```\r\nwhich is not allowed. React doesn't guarantee this kind of orders. you should put `setTimeout` inside an useEffect.\n<issue_comment>username_0: Yes, i agree this is not allowed. But different browsers should have same result.I think react should achieve lifecycle with more stable browser interfaces.\n<issue_comment>username_2: Many reasons why this is not a bug:\r\n\r\n - you are not allowed to make side effects in render. If you do that, the outcome is essentially \"undefined behaviour\", that is, nothing is guaranteed\r\n\r\n- React provides no guarantees about timing of `useEffect` with respect to render anyway.\r\n\r\nSo while this is an inconsistency and it could be fixed, there is not good reason to do so.\n<issue_comment>username_3: It is not supported to call `setTimeout` during rendering. React provides absolutely no guarantees about the order or the number of component render calls \u2014 so you can't rely on subtle timing aspects of them.\r\n\r\nIf I wrap it in `useEffect` I don't observe the difference.<issue_closed>"}
{"repo": "facebook/react", "issue_id": 664173077, "issue_number": 19438, "timestamp": "2020-07-23 09:52:43+00:00", "text": "", "context": "<issue_start><issue_comment>Title: Bug: The trigger timing of \"useEffect\" is different in chrome and safari.\nusername_0: In chrome and safari, the ```console.log``` was triggered in different order. [Example](https://codesandbox.io/s/amazing-payne-4lmxo) will show different result if opened in different browsers.\r\n\r\nReact version: 16.13.1\r\n\r\n## Steps To Reproduce\r\n\r\n1. Run following code in chrome and safari.\r\n2. Check console log\r\n\r\n```jsx\r\nimport React, { useState, useEffect } from \"react\";\r\n\r\nconst Test = props => {\r\n  useEffect(() => {\r\n    console.log(\"useEffect in child\");\r\n  }, []);\r\n\r\n  return \"Test useEffect\";\r\n};\r\n\r\nconst App = () => {\r\n  setTimeout(() => {\r\n    console.log(\"timeout in render\");\r\n  }, 0);\r\n\r\n  return <Test />;\r\n};\r\n\r\nexport default App\r\n```\r\n\r\nLink to code example:\r\n\r\n[Example](https://codesandbox.io/s/amazing-payne-4lmxo)\r\n\r\n## The current behavior\r\n\r\nThe message is not the same order;\r\n\r\n```jsx\r\n/**\r\n * chrome result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n\r\n/**\r\n * safari result\r\n *\r\n * timeout in render\r\n * useEffect in child\r\n */\r\n```\r\n\r\n## The expected behavior\r\n\r\nThe message should be same;\r\n\r\n```jsx\r\n/**\r\n * chrome result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n\r\n/**\r\n * safari result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n```<issue_closed>\n<issue_comment>username_0: In chrome and safari, the ```console.log``` was triggered in different order. [Example](https://codesandbox.io/s/amazing-payne-4lmxo) will show different result if opened in different browsers.\r\n\r\nReact version: 16.13.1\r\n\r\n## Steps To Reproduce\r\n\r\n1. Run following code in chrome and safari.\r\n2. Check console log\r\n\r\n```jsx\r\nimport React, { useState, useEffect } from \"react\";\r\n\r\nconst Test = props => {\r\n  useEffect(() => {\r\n    console.log(\"useEffect in child\");\r\n  }, []);\r\n\r\n  return \"Test useEffect\";\r\n};\r\n\r\nconst App = () => {\r\n  setTimeout(() => {\r\n    console.log(\"timeout in render\");\r\n  }, 0);\r\n\r\n  return <Test />;\r\n};\r\n\r\nexport default App\r\n```\r\n\r\nLink to code example:\r\n\r\n[Example](https://codesandbox.io/s/amazing-payne-4lmxo)\r\n\r\n## The current behavior\r\n\r\nThe message is not the same order;\r\n\r\n```jsx\r\n/**\r\n * chrome result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n\r\n/**\r\n * safari result\r\n *\r\n * timeout in render\r\n * useEffect in child\r\n */\r\n```\r\n\r\n## The expected behavior\r\n\r\nThe message should be same;\r\n\r\n```jsx\r\n/**\r\n * chrome result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n\r\n/**\r\n * safari result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n```\n<issue_comment>username_1: You do the side effect in the render method:\r\n```typescript\r\nconst App = () => {\r\n  setTimeout(() => {\r\n    console.log(\"timeout in render\");\r\n  }, 0);\r\n\r\n  return <Test />;\r\n};\r\n```\r\nwhich is not allowed. React doesn't guarantee this kind of orders. you should put `setTimeout` inside an useEffect.\n<issue_comment>username_0: Yes, i agree this is not allowed. But different browsers should have same result.I think react should achieve lifecycle with more stable browser interfaces.\n<issue_comment>username_2: Many reasons why this is not a bug:\r\n\r\n - you are not allowed to make side effects in render. If you do that, the outcome is essentially \"undefined behaviour\", that is, nothing is guaranteed\r\n\r\n- React provides no guarantees about timing of `useEffect` with respect to render anyway.\r\n\r\nSo while this is an inconsistency and it could be fixed, there is not good reason to do so.\n<issue_comment>username_3: It is not supported to call `setTimeout` during rendering. React provides absolutely no guarantees about the order or the number of component render calls \u2014 so you can't rely on subtle timing aspects of them.\r\n\r\nIf I wrap it in `useEffect` I don't observe the difference.<issue_closed>"}
{"repo": "pytorch/pytorch", "issue_id": 760718102, "issue_number": 49119, "timestamp": "2020-12-09T22:24:39Z", "text": "Stack from [ghstack](https://github.com/username_1/ghstack):\n* #49120 Add batched grad testing to gradcheck, turn it on in test_autograd\n* **#49119 Update accumulate_grad to support vmap**\n* #49118 Move inplace_is_vmap_compatible to BatchedTensorImpl.h\n\nI don't know how the accumulate_grad code gets hit, so I went through\nall places in accumulate_grad that are definitely impossible to vmap\nthrough and changed them.\n\nTo support this:\n- I added vmap support for Tensor::strides(). It returns the strides\nthat correspond to the public dimensions of the tensor (not the ones\nbeing vmapped over).\n- Changed an instance of empty_strided to new_empty_strided.\n- Replaced an in-place operation in accumulate_grad.h\n\nTest Plan:\n- added a test for calling strides() inside of vmap\n- added a test that exercise some of the accumulate grad code path. I\ndon't know if it exercises all of the code changes in this PR.\nSuggestions for more test cases are very welcome.", "context": "<issue_start><issue_comment>Title: Update accumulate_grad to support vmap\nusername_0: Stack from [ghstack](https://github.com/username_1/ghstack):\n* #49120 Add batched grad testing to gradcheck, turn it on in test_autograd\n* **#49119 Update accumulate_grad to support vmap**\n* #49118 Move inplace_is_vmap_compatible to BatchedTensorImpl.h\n\nI don't know how the accumulate_grad code gets hit, so I went through\nall places in accumulate_grad that are definitely impossible to vmap\nthrough and changed them.\n\nTo support this:\n- I added vmap support for Tensor::strides(). It returns the strides\nthat correspond to the public dimensions of the tensor (not the ones\nbeing vmapped over).\n- Changed an instance of empty_strided to new_empty_strided.\n- Replaced an in-place operation in accumulate_grad.h\n\nTest Plan:\n- added a test for calling strides() inside of vmap\n- added a test that exercise some of the accumulate grad code path. I\ndon't know if it exercises all of the code changes in this PR.\nSuggestions for more test cases are very welcome.\n<issue_comment>username_1: So, now that vmap supports strides, there was something I wasn't too clear about. Suppose that I have a contiguous tensor with strides (8, 4, 2, 1) and let's say I vmap with batch dim = 2 (e.g., not the first dimension). Is the BatchedTensor contiguous? (I don't think it should be!) I couldn't tell from this PR what would happen in this case.\n<issue_comment>username_0: It's not contiguous. The reason behind this is asking \"is a BatchedTensor contiguous\" is equivalent to asking \"are the per-examples of the BatchedTensor contiguous\". The per-examples in your example would have strides (8, 4, 1) which make them not contiguous"}
{"repo": "pytorch/pytorch", "issue_id": 760718102, "issue_number": 49119, "timestamp": "2020-12-14 16:36:36+00:00", "text": "So, now that vmap supports strides, there was something I wasn't too clear about. Suppose that I have a contiguous tensor with strides (8, 4, 2, 1) and let's say I vmap with batch dim = 2 (e.g., not the first dimension). Is the BatchedTensor contiguous? (I don't think it should be!) I couldn't tell from this PR what would happen in this case.", "context": "<issue_start><issue_comment>Title: Update accumulate_grad to support vmap\nusername_0: Stack from [ghstack](https://github.com/username_1/ghstack):\n* #49120 Add batched grad testing to gradcheck, turn it on in test_autograd\n* **#49119 Update accumulate_grad to support vmap**\n* #49118 Move inplace_is_vmap_compatible to BatchedTensorImpl.h\n\nI don't know how the accumulate_grad code gets hit, so I went through\nall places in accumulate_grad that are definitely impossible to vmap\nthrough and changed them.\n\nTo support this:\n- I added vmap support for Tensor::strides(). It returns the strides\nthat correspond to the public dimensions of the tensor (not the ones\nbeing vmapped over).\n- Changed an instance of empty_strided to new_empty_strided.\n- Replaced an in-place operation in accumulate_grad.h\n\nTest Plan:\n- added a test for calling strides() inside of vmap\n- added a test that exercise some of the accumulate grad code path. I\ndon't know if it exercises all of the code changes in this PR.\nSuggestions for more test cases are very welcome.\n<issue_comment>username_1: So, now that vmap supports strides, there was something I wasn't too clear about. Suppose that I have a contiguous tensor with strides (8, 4, 2, 1) and let's say I vmap with batch dim = 2 (e.g., not the first dimension). Is the BatchedTensor contiguous? (I don't think it should be!) I couldn't tell from this PR what would happen in this case.\n<issue_comment>username_0: It's not contiguous. The reason behind this is asking \"is a BatchedTensor contiguous\" is equivalent to asking \"are the per-examples of the BatchedTensor contiguous\". The per-examples in your example would have strides (8, 4, 1) which make them not contiguous"}
{"repo": "pytorch/pytorch", "issue_id": 760718102, "issue_number": 49119, "timestamp": "2020-12-14 16:50:36+00:00", "text": "It's not contiguous. The reason behind this is asking \"is a BatchedTensor contiguous\" is equivalent to asking \"are the per-examples of the BatchedTensor contiguous\". The per-examples in your example would have strides (8, 4, 1) which make them not contiguous", "context": "<issue_start><issue_comment>Title: Update accumulate_grad to support vmap\nusername_0: Stack from [ghstack](https://github.com/username_1/ghstack):\n* #49120 Add batched grad testing to gradcheck, turn it on in test_autograd\n* **#49119 Update accumulate_grad to support vmap**\n* #49118 Move inplace_is_vmap_compatible to BatchedTensorImpl.h\n\nI don't know how the accumulate_grad code gets hit, so I went through\nall places in accumulate_grad that are definitely impossible to vmap\nthrough and changed them.\n\nTo support this:\n- I added vmap support for Tensor::strides(). It returns the strides\nthat correspond to the public dimensions of the tensor (not the ones\nbeing vmapped over).\n- Changed an instance of empty_strided to new_empty_strided.\n- Replaced an in-place operation in accumulate_grad.h\n\nTest Plan:\n- added a test for calling strides() inside of vmap\n- added a test that exercise some of the accumulate grad code path. I\ndon't know if it exercises all of the code changes in this PR.\nSuggestions for more test cases are very welcome.\n<issue_comment>username_1: So, now that vmap supports strides, there was something I wasn't too clear about. Suppose that I have a contiguous tensor with strides (8, 4, 2, 1) and let's say I vmap with batch dim = 2 (e.g., not the first dimension). Is the BatchedTensor contiguous? (I don't think it should be!) I couldn't tell from this PR what would happen in this case.\n<issue_comment>username_0: It's not contiguous. The reason behind this is asking \"is a BatchedTensor contiguous\" is equivalent to asking \"are the per-examples of the BatchedTensor contiguous\". The per-examples in your example would have strides (8, 4, 1) which make them not contiguous"}
{"repo": "facebook/react", "issue_id": 65878895, "issue_number": 3570, "timestamp": "2015-04-02 08:34:39+00:00", "text": "I didn't realise this was in the addons bundle, would be good to get it added to the docs.\r\n\r\nhttp://facebook.github.io/react/docs/addons.html\r\nhttps://github.com/facebook/react/blob/master/src/browser/ReactWithAddons.js#L40\r\n\r\nWould be good to also include an example of using https://github.com/facebook/react/blob/master/src/addons/ReactRAFBatchingStrategy.js or something similar.", "context": "<issue_start><issue_comment>Title: Document React.addons.batchedUpdates\nusername_0: I didn't realise this was in the addons bundle, would be good to get it added to the docs.\r\n\r\nhttp://facebook.github.io/react/docs/addons.html\r\nhttps://github.com/facebook/react/blob/master/src/browser/ReactWithAddons.js#L40\r\n\r\nWould be good to also include an example of using https://github.com/facebook/react/blob/master/src/addons/ReactRAFBatchingStrategy.js or something similar.\n<issue_comment>username_1: rAF batching isn't supported and won't go in the docs, but batchedUpdates should.\n<issue_comment>username_2: There are also known issues with rAF batching (mostly around controlled components).\n<issue_comment>username_3: Should we deprecate and eventually remove the RAF batching?\n<issue_comment>username_1: It's never been supported in any fashion.\n<issue_comment>username_1: (i.e., it's already deprecated?)\n<issue_comment>username_4: shouldn't this file be deleted in that case?\r\nhttps://github.com/facebook/react/blob/master/src/addons/ReactRAFBatchingStrategy.js\n<issue_comment>username_1: That's not exposed as a public API in any of our builds, but sure, we can delete it \u2013 PR incoming.\n<issue_comment>username_5: Killed in https://github.com/facebook/react/pull/6016<issue_closed>\n<issue_comment>username_1: We should document ReactDOM.unstable_batchedUpdates.\n<issue_comment>username_1: I didn't realise this was in the addons bundle, would be good to get it added to the docs.\r\n\r\nhttp://facebook.github.io/react/docs/addons.html\r\nhttps://github.com/facebook/react/blob/master/src/browser/ReactWithAddons.js#L40\r\n\r\nWould be good to also include an example of using https://github.com/facebook/react/blob/master/src/addons/ReactRAFBatchingStrategy.js or something similar.\n<issue_comment>username_5: @username_1 Since when did we start documenting `unstable_` features?  Haven't we always left them intentionally undocumented?  We also have unstable_renderSubtree and unstable_handleError and others, but I thought the decision was to not document them until they become stable.  We document them as part of the release process.\n<issue_comment>username_1: Sure.<issue_closed>"}
{"repo": "facebook/react", "issue_id": 65878895, "issue_number": 3570, "timestamp": "2015-04-02 08:51:45+00:00", "text": "rAF batching isn't supported and won't go in the docs, but batchedUpdates should.", "context": "<issue_start><issue_comment>Title: Document React.addons.batchedUpdates\nusername_0: I didn't realise this was in the addons bundle, would be good to get it added to the docs.\r\n\r\nhttp://facebook.github.io/react/docs/addons.html\r\nhttps://github.com/facebook/react/blob/master/src/browser/ReactWithAddons.js#L40\r\n\r\nWould be good to also include an example of using https://github.com/facebook/react/blob/master/src/addons/ReactRAFBatchingStrategy.js or something similar.\n<issue_comment>username_1: rAF batching isn't supported and won't go in the docs, but batchedUpdates should.\n<issue_comment>username_2: There are also known issues with rAF batching (mostly around controlled components).\n<issue_comment>username_3: Should we deprecate and eventually remove the RAF batching?\n<issue_comment>username_1: It's never been supported in any fashion.\n<issue_comment>username_1: (i.e., it's already deprecated?)\n<issue_comment>username_4: shouldn't this file be deleted in that case?\r\nhttps://github.com/facebook/react/blob/master/src/addons/ReactRAFBatchingStrategy.js\n<issue_comment>username_1: That's not exposed as a public API in any of our builds, but sure, we can delete it \u2013 PR incoming.\n<issue_comment>username_5: Killed in https://github.com/facebook/react/pull/6016<issue_closed>\n<issue_comment>username_1: We should document ReactDOM.unstable_batchedUpdates.\n<issue_comment>username_1: I didn't realise this was in the addons bundle, would be good to get it added to the docs.\r\n\r\nhttp://facebook.github.io/react/docs/addons.html\r\nhttps://github.com/facebook/react/blob/master/src/browser/ReactWithAddons.js#L40\r\n\r\nWould be good to also include an example of using https://github.com/facebook/react/blob/master/src/addons/ReactRAFBatchingStrategy.js or something similar.\n<issue_comment>username_5: @username_1 Since when did we start documenting `unstable_` features?  Haven't we always left them intentionally undocumented?  We also have unstable_renderSubtree and unstable_handleError and others, but I thought the decision was to not document them until they become stable.  We document them as part of the release process.\n<issue_comment>username_1: Sure.<issue_closed>"}
{"repo": "facebook/react", "issue_id": 65878895, "issue_number": 3570, "timestamp": "2015-04-02 19:36:24+00:00", "text": "There are also known issues with rAF batching (mostly around controlled components).", "context": "<issue_start><issue_comment>Title: Document React.addons.batchedUpdates\nusername_0: I didn't realise this was in the addons bundle, would be good to get it added to the docs.\r\n\r\nhttp://facebook.github.io/react/docs/addons.html\r\nhttps://github.com/facebook/react/blob/master/src/browser/ReactWithAddons.js#L40\r\n\r\nWould be good to also include an example of using https://github.com/facebook/react/blob/master/src/addons/ReactRAFBatchingStrategy.js or something similar.\n<issue_comment>username_1: rAF batching isn't supported and won't go in the docs, but batchedUpdates should.\n<issue_comment>username_2: There are also known issues with rAF batching (mostly around controlled components).\n<issue_comment>username_3: Should we deprecate and eventually remove the RAF batching?\n<issue_comment>username_1: It's never been supported in any fashion.\n<issue_comment>username_1: (i.e., it's already deprecated?)\n<issue_comment>username_4: shouldn't this file be deleted in that case?\r\nhttps://github.com/facebook/react/blob/master/src/addons/ReactRAFBatchingStrategy.js\n<issue_comment>username_1: That's not exposed as a public API in any of our builds, but sure, we can delete it \u2013 PR incoming.\n<issue_comment>username_5: Killed in https://github.com/facebook/react/pull/6016<issue_closed>\n<issue_comment>username_1: We should document ReactDOM.unstable_batchedUpdates.\n<issue_comment>username_1: I didn't realise this was in the addons bundle, would be good to get it added to the docs.\r\n\r\nhttp://facebook.github.io/react/docs/addons.html\r\nhttps://github.com/facebook/react/blob/master/src/browser/ReactWithAddons.js#L40\r\n\r\nWould be good to also include an example of using https://github.com/facebook/react/blob/master/src/addons/ReactRAFBatchingStrategy.js or something similar.\n<issue_comment>username_5: @username_1 Since when did we start documenting `unstable_` features?  Haven't we always left them intentionally undocumented?  We also have unstable_renderSubtree and unstable_handleError and others, but I thought the decision was to not document them until they become stable.  We document them as part of the release process.\n<issue_comment>username_1: Sure.<issue_closed>"}
{"repo": "facebook/react", "issue_id": 65878895, "issue_number": 3570, "timestamp": "2015-04-08 23:48:13+00:00", "text": "Should we deprecate and eventually remove the RAF batching?", "context": "<issue_start><issue_comment>Title: Document React.addons.batchedUpdates\nusername_0: I didn't realise this was in the addons bundle, would be good to get it added to the docs.\r\n\r\nhttp://facebook.github.io/react/docs/addons.html\r\nhttps://github.com/facebook/react/blob/master/src/browser/ReactWithAddons.js#L40\r\n\r\nWould be good to also include an example of using https://github.com/facebook/react/blob/master/src/addons/ReactRAFBatchingStrategy.js or something similar.\n<issue_comment>username_1: rAF batching isn't supported and won't go in the docs, but batchedUpdates should.\n<issue_comment>username_2: There are also known issues with rAF batching (mostly around controlled components).\n<issue_comment>username_3: Should we deprecate and eventually remove the RAF batching?\n<issue_comment>username_1: It's never been supported in any fashion.\n<issue_comment>username_1: (i.e., it's already deprecated?)\n<issue_comment>username_4: shouldn't this file be deleted in that case?\r\nhttps://github.com/facebook/react/blob/master/src/addons/ReactRAFBatchingStrategy.js\n<issue_comment>username_1: That's not exposed as a public API in any of our builds, but sure, we can delete it \u2013 PR incoming.\n<issue_comment>username_5: Killed in https://github.com/facebook/react/pull/6016<issue_closed>\n<issue_comment>username_1: We should document ReactDOM.unstable_batchedUpdates.\n<issue_comment>username_1: I didn't realise this was in the addons bundle, would be good to get it added to the docs.\r\n\r\nhttp://facebook.github.io/react/docs/addons.html\r\nhttps://github.com/facebook/react/blob/master/src/browser/ReactWithAddons.js#L40\r\n\r\nWould be good to also include an example of using https://github.com/facebook/react/blob/master/src/addons/ReactRAFBatchingStrategy.js or something similar.\n<issue_comment>username_5: @username_1 Since when did we start documenting `unstable_` features?  Haven't we always left them intentionally undocumented?  We also have unstable_renderSubtree and unstable_handleError and others, but I thought the decision was to not document them until they become stable.  We document them as part of the release process.\n<issue_comment>username_1: Sure.<issue_closed>"}
{"repo": "facebook/react", "issue_id": 65878895, "issue_number": 3570, "timestamp": "2015-04-08 23:51:50+00:00", "text": "It's never been supported in any fashion.", "context": "<issue_start><issue_comment>Title: Document React.addons.batchedUpdates\nusername_0: I didn't realise this was in the addons bundle, would be good to get it added to the docs.\r\n\r\nhttp://facebook.github.io/react/docs/addons.html\r\nhttps://github.com/facebook/react/blob/master/src/browser/ReactWithAddons.js#L40\r\n\r\nWould be good to also include an example of using https://github.com/facebook/react/blob/master/src/addons/ReactRAFBatchingStrategy.js or something similar.\n<issue_comment>username_1: rAF batching isn't supported and won't go in the docs, but batchedUpdates should.\n<issue_comment>username_2: There are also known issues with rAF batching (mostly around controlled components).\n<issue_comment>username_3: Should we deprecate and eventually remove the RAF batching?\n<issue_comment>username_1: It's never been supported in any fashion.\n<issue_comment>username_1: (i.e., it's already deprecated?)\n<issue_comment>username_4: shouldn't this file be deleted in that case?\r\nhttps://github.com/facebook/react/blob/master/src/addons/ReactRAFBatchingStrategy.js\n<issue_comment>username_1: That's not exposed as a public API in any of our builds, but sure, we can delete it \u2013 PR incoming.\n<issue_comment>username_5: Killed in https://github.com/facebook/react/pull/6016<issue_closed>\n<issue_comment>username_1: We should document ReactDOM.unstable_batchedUpdates.\n<issue_comment>username_1: I didn't realise this was in the addons bundle, would be good to get it added to the docs.\r\n\r\nhttp://facebook.github.io/react/docs/addons.html\r\nhttps://github.com/facebook/react/blob/master/src/browser/ReactWithAddons.js#L40\r\n\r\nWould be good to also include an example of using https://github.com/facebook/react/blob/master/src/addons/ReactRAFBatchingStrategy.js or something similar.\n<issue_comment>username_5: @username_1 Since when did we start documenting `unstable_` features?  Haven't we always left them intentionally undocumented?  We also have unstable_renderSubtree and unstable_handleError and others, but I thought the decision was to not document them until they become stable.  We document them as part of the release process.\n<issue_comment>username_1: Sure.<issue_closed>"}
{"repo": "facebook/react", "issue_id": 65878895, "issue_number": 3570, "timestamp": "2015-04-08 23:52:09+00:00", "text": "(i.e., it's already deprecated?)", "context": "<issue_start><issue_comment>Title: Document React.addons.batchedUpdates\nusername_0: I didn't realise this was in the addons bundle, would be good to get it added to the docs.\r\n\r\nhttp://facebook.github.io/react/docs/addons.html\r\nhttps://github.com/facebook/react/blob/master/src/browser/ReactWithAddons.js#L40\r\n\r\nWould be good to also include an example of using https://github.com/facebook/react/blob/master/src/addons/ReactRAFBatchingStrategy.js or something similar.\n<issue_comment>username_1: rAF batching isn't supported and won't go in the docs, but batchedUpdates should.\n<issue_comment>username_2: There are also known issues with rAF batching (mostly around controlled components).\n<issue_comment>username_3: Should we deprecate and eventually remove the RAF batching?\n<issue_comment>username_1: It's never been supported in any fashion.\n<issue_comment>username_1: (i.e., it's already deprecated?)\n<issue_comment>username_4: shouldn't this file be deleted in that case?\r\nhttps://github.com/facebook/react/blob/master/src/addons/ReactRAFBatchingStrategy.js\n<issue_comment>username_1: That's not exposed as a public API in any of our builds, but sure, we can delete it \u2013 PR incoming.\n<issue_comment>username_5: Killed in https://github.com/facebook/react/pull/6016<issue_closed>\n<issue_comment>username_1: We should document ReactDOM.unstable_batchedUpdates.\n<issue_comment>username_1: I didn't realise this was in the addons bundle, would be good to get it added to the docs.\r\n\r\nhttp://facebook.github.io/react/docs/addons.html\r\nhttps://github.com/facebook/react/blob/master/src/browser/ReactWithAddons.js#L40\r\n\r\nWould be good to also include an example of using https://github.com/facebook/react/blob/master/src/addons/ReactRAFBatchingStrategy.js or something similar.\n<issue_comment>username_5: @username_1 Since when did we start documenting `unstable_` features?  Haven't we always left them intentionally undocumented?  We also have unstable_renderSubtree and unstable_handleError and others, but I thought the decision was to not document them until they become stable.  We document them as part of the release process.\n<issue_comment>username_1: Sure.<issue_closed>"}
{"repo": "facebook/react", "issue_id": 65878895, "issue_number": 3570, "timestamp": "2015-04-20 19:04:22+00:00", "text": "shouldn't this file be deleted in that case?\r\nhttps://github.com/facebook/react/blob/master/src/addons/ReactRAFBatchingStrategy.js", "context": "<issue_start><issue_comment>Title: Document React.addons.batchedUpdates\nusername_0: I didn't realise this was in the addons bundle, would be good to get it added to the docs.\r\n\r\nhttp://facebook.github.io/react/docs/addons.html\r\nhttps://github.com/facebook/react/blob/master/src/browser/ReactWithAddons.js#L40\r\n\r\nWould be good to also include an example of using https://github.com/facebook/react/blob/master/src/addons/ReactRAFBatchingStrategy.js or something similar.\n<issue_comment>username_1: rAF batching isn't supported and won't go in the docs, but batchedUpdates should.\n<issue_comment>username_2: There are also known issues with rAF batching (mostly around controlled components).\n<issue_comment>username_3: Should we deprecate and eventually remove the RAF batching?\n<issue_comment>username_1: It's never been supported in any fashion.\n<issue_comment>username_1: (i.e., it's already deprecated?)\n<issue_comment>username_4: shouldn't this file be deleted in that case?\r\nhttps://github.com/facebook/react/blob/master/src/addons/ReactRAFBatchingStrategy.js\n<issue_comment>username_1: That's not exposed as a public API in any of our builds, but sure, we can delete it \u2013 PR incoming.\n<issue_comment>username_5: Killed in https://github.com/facebook/react/pull/6016<issue_closed>\n<issue_comment>username_1: We should document ReactDOM.unstable_batchedUpdates.\n<issue_comment>username_1: I didn't realise this was in the addons bundle, would be good to get it added to the docs.\r\n\r\nhttp://facebook.github.io/react/docs/addons.html\r\nhttps://github.com/facebook/react/blob/master/src/browser/ReactWithAddons.js#L40\r\n\r\nWould be good to also include an example of using https://github.com/facebook/react/blob/master/src/addons/ReactRAFBatchingStrategy.js or something similar.\n<issue_comment>username_5: @username_1 Since when did we start documenting `unstable_` features?  Haven't we always left them intentionally undocumented?  We also have unstable_renderSubtree and unstable_handleError and others, but I thought the decision was to not document them until they become stable.  We document them as part of the release process.\n<issue_comment>username_1: Sure.<issue_closed>"}
{"repo": "facebook/react", "issue_id": 65878895, "issue_number": 3570, "timestamp": "2015-04-20 19:08:42+00:00", "text": "That's not exposed as a public API in any of our builds, but sure, we can delete it \u2013 PR incoming.", "context": "<issue_start><issue_comment>Title: Document React.addons.batchedUpdates\nusername_0: I didn't realise this was in the addons bundle, would be good to get it added to the docs.\r\n\r\nhttp://facebook.github.io/react/docs/addons.html\r\nhttps://github.com/facebook/react/blob/master/src/browser/ReactWithAddons.js#L40\r\n\r\nWould be good to also include an example of using https://github.com/facebook/react/blob/master/src/addons/ReactRAFBatchingStrategy.js or something similar.\n<issue_comment>username_1: rAF batching isn't supported and won't go in the docs, but batchedUpdates should.\n<issue_comment>username_2: There are also known issues with rAF batching (mostly around controlled components).\n<issue_comment>username_3: Should we deprecate and eventually remove the RAF batching?\n<issue_comment>username_1: It's never been supported in any fashion.\n<issue_comment>username_1: (i.e., it's already deprecated?)\n<issue_comment>username_4: shouldn't this file be deleted in that case?\r\nhttps://github.com/facebook/react/blob/master/src/addons/ReactRAFBatchingStrategy.js\n<issue_comment>username_1: That's not exposed as a public API in any of our builds, but sure, we can delete it \u2013 PR incoming.\n<issue_comment>username_5: Killed in https://github.com/facebook/react/pull/6016<issue_closed>\n<issue_comment>username_1: We should document ReactDOM.unstable_batchedUpdates.\n<issue_comment>username_1: I didn't realise this was in the addons bundle, would be good to get it added to the docs.\r\n\r\nhttp://facebook.github.io/react/docs/addons.html\r\nhttps://github.com/facebook/react/blob/master/src/browser/ReactWithAddons.js#L40\r\n\r\nWould be good to also include an example of using https://github.com/facebook/react/blob/master/src/addons/ReactRAFBatchingStrategy.js or something similar.\n<issue_comment>username_5: @username_1 Since when did we start documenting `unstable_` features?  Haven't we always left them intentionally undocumented?  We also have unstable_renderSubtree and unstable_handleError and others, but I thought the decision was to not document them until they become stable.  We document them as part of the release process.\n<issue_comment>username_1: Sure.<issue_closed>"}
{"repo": "facebook/react", "issue_id": 65878895, "issue_number": 3570, "timestamp": "2016-02-10 23:32:04+00:00", "text": "Killed in https://github.com/facebook/react/pull/6016", "context": "<issue_start><issue_comment>Title: Document React.addons.batchedUpdates\nusername_0: I didn't realise this was in the addons bundle, would be good to get it added to the docs.\r\n\r\nhttp://facebook.github.io/react/docs/addons.html\r\nhttps://github.com/facebook/react/blob/master/src/browser/ReactWithAddons.js#L40\r\n\r\nWould be good to also include an example of using https://github.com/facebook/react/blob/master/src/addons/ReactRAFBatchingStrategy.js or something similar.\n<issue_comment>username_1: rAF batching isn't supported and won't go in the docs, but batchedUpdates should.\n<issue_comment>username_2: There are also known issues with rAF batching (mostly around controlled components).\n<issue_comment>username_3: Should we deprecate and eventually remove the RAF batching?\n<issue_comment>username_1: It's never been supported in any fashion.\n<issue_comment>username_1: (i.e., it's already deprecated?)\n<issue_comment>username_4: shouldn't this file be deleted in that case?\r\nhttps://github.com/facebook/react/blob/master/src/addons/ReactRAFBatchingStrategy.js\n<issue_comment>username_1: That's not exposed as a public API in any of our builds, but sure, we can delete it \u2013 PR incoming.\n<issue_comment>username_5: Killed in https://github.com/facebook/react/pull/6016<issue_closed>\n<issue_comment>username_1: We should document ReactDOM.unstable_batchedUpdates.\n<issue_comment>username_1: I didn't realise this was in the addons bundle, would be good to get it added to the docs.\r\n\r\nhttp://facebook.github.io/react/docs/addons.html\r\nhttps://github.com/facebook/react/blob/master/src/browser/ReactWithAddons.js#L40\r\n\r\nWould be good to also include an example of using https://github.com/facebook/react/blob/master/src/addons/ReactRAFBatchingStrategy.js or something similar.\n<issue_comment>username_5: @username_1 Since when did we start documenting `unstable_` features?  Haven't we always left them intentionally undocumented?  We also have unstable_renderSubtree and unstable_handleError and others, but I thought the decision was to not document them until they become stable.  We document them as part of the release process.\n<issue_comment>username_1: Sure.<issue_closed>"}
{"repo": "facebook/react", "issue_id": 65878895, "issue_number": 3570, "timestamp": "2016-02-10 23:32:05+00:00", "text": "", "context": "<issue_start><issue_comment>Title: Document React.addons.batchedUpdates\nusername_0: I didn't realise this was in the addons bundle, would be good to get it added to the docs.\r\n\r\nhttp://facebook.github.io/react/docs/addons.html\r\nhttps://github.com/facebook/react/blob/master/src/browser/ReactWithAddons.js#L40\r\n\r\nWould be good to also include an example of using https://github.com/facebook/react/blob/master/src/addons/ReactRAFBatchingStrategy.js or something similar.\n<issue_comment>username_1: rAF batching isn't supported and won't go in the docs, but batchedUpdates should.\n<issue_comment>username_2: There are also known issues with rAF batching (mostly around controlled components).\n<issue_comment>username_3: Should we deprecate and eventually remove the RAF batching?\n<issue_comment>username_1: It's never been supported in any fashion.\n<issue_comment>username_1: (i.e., it's already deprecated?)\n<issue_comment>username_4: shouldn't this file be deleted in that case?\r\nhttps://github.com/facebook/react/blob/master/src/addons/ReactRAFBatchingStrategy.js\n<issue_comment>username_1: That's not exposed as a public API in any of our builds, but sure, we can delete it \u2013 PR incoming.\n<issue_comment>username_5: Killed in https://github.com/facebook/react/pull/6016<issue_closed>\n<issue_comment>username_1: We should document ReactDOM.unstable_batchedUpdates.\n<issue_comment>username_1: I didn't realise this was in the addons bundle, would be good to get it added to the docs.\r\n\r\nhttp://facebook.github.io/react/docs/addons.html\r\nhttps://github.com/facebook/react/blob/master/src/browser/ReactWithAddons.js#L40\r\n\r\nWould be good to also include an example of using https://github.com/facebook/react/blob/master/src/addons/ReactRAFBatchingStrategy.js or something similar.\n<issue_comment>username_5: @username_1 Since when did we start documenting `unstable_` features?  Haven't we always left them intentionally undocumented?  We also have unstable_renderSubtree and unstable_handleError and others, but I thought the decision was to not document them until they become stable.  We document them as part of the release process.\n<issue_comment>username_1: Sure.<issue_closed>"}
{"repo": "facebook/react", "issue_id": 65878895, "issue_number": 3570, "timestamp": "2016-02-10 23:41:08+00:00", "text": "We should document ReactDOM.unstable_batchedUpdates.", "context": "<issue_start><issue_comment>Title: Document React.addons.batchedUpdates\nusername_0: I didn't realise this was in the addons bundle, would be good to get it added to the docs.\r\n\r\nhttp://facebook.github.io/react/docs/addons.html\r\nhttps://github.com/facebook/react/blob/master/src/browser/ReactWithAddons.js#L40\r\n\r\nWould be good to also include an example of using https://github.com/facebook/react/blob/master/src/addons/ReactRAFBatchingStrategy.js or something similar.\n<issue_comment>username_1: rAF batching isn't supported and won't go in the docs, but batchedUpdates should.\n<issue_comment>username_2: There are also known issues with rAF batching (mostly around controlled components).\n<issue_comment>username_3: Should we deprecate and eventually remove the RAF batching?\n<issue_comment>username_1: It's never been supported in any fashion.\n<issue_comment>username_1: (i.e., it's already deprecated?)\n<issue_comment>username_4: shouldn't this file be deleted in that case?\r\nhttps://github.com/facebook/react/blob/master/src/addons/ReactRAFBatchingStrategy.js\n<issue_comment>username_1: That's not exposed as a public API in any of our builds, but sure, we can delete it \u2013 PR incoming.\n<issue_comment>username_5: Killed in https://github.com/facebook/react/pull/6016<issue_closed>\n<issue_comment>username_1: We should document ReactDOM.unstable_batchedUpdates.\n<issue_comment>username_1: I didn't realise this was in the addons bundle, would be good to get it added to the docs.\r\n\r\nhttp://facebook.github.io/react/docs/addons.html\r\nhttps://github.com/facebook/react/blob/master/src/browser/ReactWithAddons.js#L40\r\n\r\nWould be good to also include an example of using https://github.com/facebook/react/blob/master/src/addons/ReactRAFBatchingStrategy.js or something similar.\n<issue_comment>username_5: @username_1 Since when did we start documenting `unstable_` features?  Haven't we always left them intentionally undocumented?  We also have unstable_renderSubtree and unstable_handleError and others, but I thought the decision was to not document them until they become stable.  We document them as part of the release process.\n<issue_comment>username_1: Sure.<issue_closed>"}
{"repo": "facebook/react", "issue_id": 65878895, "issue_number": 3570, "timestamp": "2016-02-10 23:41:09+00:00", "text": "I didn't realise this was in the addons bundle, would be good to get it added to the docs.\r\n\r\nhttp://facebook.github.io/react/docs/addons.html\r\nhttps://github.com/facebook/react/blob/master/src/browser/ReactWithAddons.js#L40\r\n\r\nWould be good to also include an example of using https://github.com/facebook/react/blob/master/src/addons/ReactRAFBatchingStrategy.js or something similar.", "context": "<issue_start><issue_comment>Title: Document React.addons.batchedUpdates\nusername_0: I didn't realise this was in the addons bundle, would be good to get it added to the docs.\r\n\r\nhttp://facebook.github.io/react/docs/addons.html\r\nhttps://github.com/facebook/react/blob/master/src/browser/ReactWithAddons.js#L40\r\n\r\nWould be good to also include an example of using https://github.com/facebook/react/blob/master/src/addons/ReactRAFBatchingStrategy.js or something similar.\n<issue_comment>username_1: rAF batching isn't supported and won't go in the docs, but batchedUpdates should.\n<issue_comment>username_2: There are also known issues with rAF batching (mostly around controlled components).\n<issue_comment>username_3: Should we deprecate and eventually remove the RAF batching?\n<issue_comment>username_1: It's never been supported in any fashion.\n<issue_comment>username_1: (i.e., it's already deprecated?)\n<issue_comment>username_4: shouldn't this file be deleted in that case?\r\nhttps://github.com/facebook/react/blob/master/src/addons/ReactRAFBatchingStrategy.js\n<issue_comment>username_1: That's not exposed as a public API in any of our builds, but sure, we can delete it \u2013 PR incoming.\n<issue_comment>username_5: Killed in https://github.com/facebook/react/pull/6016<issue_closed>\n<issue_comment>username_1: We should document ReactDOM.unstable_batchedUpdates.\n<issue_comment>username_1: I didn't realise this was in the addons bundle, would be good to get it added to the docs.\r\n\r\nhttp://facebook.github.io/react/docs/addons.html\r\nhttps://github.com/facebook/react/blob/master/src/browser/ReactWithAddons.js#L40\r\n\r\nWould be good to also include an example of using https://github.com/facebook/react/blob/master/src/addons/ReactRAFBatchingStrategy.js or something similar.\n<issue_comment>username_5: @username_1 Since when did we start documenting `unstable_` features?  Haven't we always left them intentionally undocumented?  We also have unstable_renderSubtree and unstable_handleError and others, but I thought the decision was to not document them until they become stable.  We document them as part of the release process.\n<issue_comment>username_1: Sure.<issue_closed>"}
{"repo": "facebook/react", "issue_id": 65878895, "issue_number": 3570, "timestamp": "2016-02-11 00:16:08+00:00", "text": "@username_1 Since when did we start documenting `unstable_` features?  Haven't we always left them intentionally undocumented?  We also have unstable_renderSubtree and unstable_handleError and others, but I thought the decision was to not document them until they become stable.  We document them as part of the release process.", "context": "<issue_start><issue_comment>Title: Document React.addons.batchedUpdates\nusername_0: I didn't realise this was in the addons bundle, would be good to get it added to the docs.\r\n\r\nhttp://facebook.github.io/react/docs/addons.html\r\nhttps://github.com/facebook/react/blob/master/src/browser/ReactWithAddons.js#L40\r\n\r\nWould be good to also include an example of using https://github.com/facebook/react/blob/master/src/addons/ReactRAFBatchingStrategy.js or something similar.\n<issue_comment>username_1: rAF batching isn't supported and won't go in the docs, but batchedUpdates should.\n<issue_comment>username_2: There are also known issues with rAF batching (mostly around controlled components).\n<issue_comment>username_3: Should we deprecate and eventually remove the RAF batching?\n<issue_comment>username_1: It's never been supported in any fashion.\n<issue_comment>username_1: (i.e., it's already deprecated?)\n<issue_comment>username_4: shouldn't this file be deleted in that case?\r\nhttps://github.com/facebook/react/blob/master/src/addons/ReactRAFBatchingStrategy.js\n<issue_comment>username_1: That's not exposed as a public API in any of our builds, but sure, we can delete it \u2013 PR incoming.\n<issue_comment>username_5: Killed in https://github.com/facebook/react/pull/6016<issue_closed>\n<issue_comment>username_1: We should document ReactDOM.unstable_batchedUpdates.\n<issue_comment>username_1: I didn't realise this was in the addons bundle, would be good to get it added to the docs.\r\n\r\nhttp://facebook.github.io/react/docs/addons.html\r\nhttps://github.com/facebook/react/blob/master/src/browser/ReactWithAddons.js#L40\r\n\r\nWould be good to also include an example of using https://github.com/facebook/react/blob/master/src/addons/ReactRAFBatchingStrategy.js or something similar.\n<issue_comment>username_5: @username_1 Since when did we start documenting `unstable_` features?  Haven't we always left them intentionally undocumented?  We also have unstable_renderSubtree and unstable_handleError and others, but I thought the decision was to not document them until they become stable.  We document them as part of the release process.\n<issue_comment>username_1: Sure.<issue_closed>"}
{"repo": "facebook/react", "issue_id": 65878895, "issue_number": 3570, "timestamp": "2016-02-11 00:39:21+00:00", "text": "Sure.", "context": "<issue_start><issue_comment>Title: Document React.addons.batchedUpdates\nusername_0: I didn't realise this was in the addons bundle, would be good to get it added to the docs.\r\n\r\nhttp://facebook.github.io/react/docs/addons.html\r\nhttps://github.com/facebook/react/blob/master/src/browser/ReactWithAddons.js#L40\r\n\r\nWould be good to also include an example of using https://github.com/facebook/react/blob/master/src/addons/ReactRAFBatchingStrategy.js or something similar.\n<issue_comment>username_1: rAF batching isn't supported and won't go in the docs, but batchedUpdates should.\n<issue_comment>username_2: There are also known issues with rAF batching (mostly around controlled components).\n<issue_comment>username_3: Should we deprecate and eventually remove the RAF batching?\n<issue_comment>username_1: It's never been supported in any fashion.\n<issue_comment>username_1: (i.e., it's already deprecated?)\n<issue_comment>username_4: shouldn't this file be deleted in that case?\r\nhttps://github.com/facebook/react/blob/master/src/addons/ReactRAFBatchingStrategy.js\n<issue_comment>username_1: That's not exposed as a public API in any of our builds, but sure, we can delete it \u2013 PR incoming.\n<issue_comment>username_5: Killed in https://github.com/facebook/react/pull/6016<issue_closed>\n<issue_comment>username_1: We should document ReactDOM.unstable_batchedUpdates.\n<issue_comment>username_1: I didn't realise this was in the addons bundle, would be good to get it added to the docs.\r\n\r\nhttp://facebook.github.io/react/docs/addons.html\r\nhttps://github.com/facebook/react/blob/master/src/browser/ReactWithAddons.js#L40\r\n\r\nWould be good to also include an example of using https://github.com/facebook/react/blob/master/src/addons/ReactRAFBatchingStrategy.js or something similar.\n<issue_comment>username_5: @username_1 Since when did we start documenting `unstable_` features?  Haven't we always left them intentionally undocumented?  We also have unstable_renderSubtree and unstable_handleError and others, but I thought the decision was to not document them until they become stable.  We document them as part of the release process.\n<issue_comment>username_1: Sure.<issue_closed>"}
{"repo": "facebook/react", "issue_id": 65878895, "issue_number": 3570, "timestamp": "2016-02-11 00:39:21+00:00", "text": "", "context": "<issue_start><issue_comment>Title: Document React.addons.batchedUpdates\nusername_0: I didn't realise this was in the addons bundle, would be good to get it added to the docs.\r\n\r\nhttp://facebook.github.io/react/docs/addons.html\r\nhttps://github.com/facebook/react/blob/master/src/browser/ReactWithAddons.js#L40\r\n\r\nWould be good to also include an example of using https://github.com/facebook/react/blob/master/src/addons/ReactRAFBatchingStrategy.js or something similar.\n<issue_comment>username_1: rAF batching isn't supported and won't go in the docs, but batchedUpdates should.\n<issue_comment>username_2: There are also known issues with rAF batching (mostly around controlled components).\n<issue_comment>username_3: Should we deprecate and eventually remove the RAF batching?\n<issue_comment>username_1: It's never been supported in any fashion.\n<issue_comment>username_1: (i.e., it's already deprecated?)\n<issue_comment>username_4: shouldn't this file be deleted in that case?\r\nhttps://github.com/facebook/react/blob/master/src/addons/ReactRAFBatchingStrategy.js\n<issue_comment>username_1: That's not exposed as a public API in any of our builds, but sure, we can delete it \u2013 PR incoming.\n<issue_comment>username_5: Killed in https://github.com/facebook/react/pull/6016<issue_closed>\n<issue_comment>username_1: We should document ReactDOM.unstable_batchedUpdates.\n<issue_comment>username_1: I didn't realise this was in the addons bundle, would be good to get it added to the docs.\r\n\r\nhttp://facebook.github.io/react/docs/addons.html\r\nhttps://github.com/facebook/react/blob/master/src/browser/ReactWithAddons.js#L40\r\n\r\nWould be good to also include an example of using https://github.com/facebook/react/blob/master/src/addons/ReactRAFBatchingStrategy.js or something similar.\n<issue_comment>username_5: @username_1 Since when did we start documenting `unstable_` features?  Haven't we always left them intentionally undocumented?  We also have unstable_renderSubtree and unstable_handleError and others, but I thought the decision was to not document them until they become stable.  We document them as part of the release process.\n<issue_comment>username_1: Sure.<issue_closed>"}
{"repo": "facebook/react", "issue_id": 389079792, "issue_number": 14410, "timestamp": "2018-12-09 23:51:46+00:00", "text": "<!--\r\n  Note: if the issue is about documentation or the website, please file it at:\r\n  https://github.com/reactjs/reactjs.org/issues/new\r\n-->\r\n\r\n**Do you want to request a *feature* or report a *bug*?** bug\r\n\r\n**What is the current behavior?** The react@16.6.3 package has a comment at the top claiming to be React 16.6.1.\r\n\r\n**If the current behavior is a bug, please provide the steps to reproduce and if possible a minimal demo of the problem. Your bug will get fixed much faster if we can run your code and it doesn't have dependencies other than React. Paste the link to your JSFiddle (https://jsfiddle.net/Luktwrdm/) or CodeSandbox (https://codesandbox.io/s/new) example below:**\r\n```shell\r\ncd /tmp\r\nnpm pack react@16.6.3\r\ntar xf react-16.6.3.tgz\r\nhead -1 package/cjs/react.development.js\r\n# output: /** @license React v16.6.1\r\n```\r\n\r\n**What is the expected behavior?**\r\nThe comment at the top of the build indicates the correct version.\r\n\r\n**Which versions of React, and which browser / OS are affected by this issue? Did this work in previous versions of React?**\r\n16.6.2 and 16.6.3 both have the wrong comment. 16.6.1 and 16.6.0 are correct.", "context": "<issue_start><issue_comment>Title: Comments in NPM package have wrong version\nusername_0: <!--\r\n  Note: if the issue is about documentation or the website, please file it at:\r\n  https://github.com/reactjs/reactjs.org/issues/new\r\n-->\r\n\r\n**Do you want to request a *feature* or report a *bug*?** bug\r\n\r\n**What is the current behavior?** The react@16.6.3 package has a comment at the top claiming to be React 16.6.1.\r\n\r\n**If the current behavior is a bug, please provide the steps to reproduce and if possible a minimal demo of the problem. Your bug will get fixed much faster if we can run your code and it doesn't have dependencies other than React. Paste the link to your JSFiddle (https://jsfiddle.net/Luktwrdm/) or CodeSandbox (https://codesandbox.io/s/new) example below:**\r\n```shell\r\ncd /tmp\r\nnpm pack react@16.6.3\r\ntar xf react-16.6.3.tgz\r\nhead -1 package/cjs/react.development.js\r\n# output: /** @license React v16.6.1\r\n```\r\n\r\n**What is the expected behavior?**\r\nThe comment at the top of the build indicates the correct version.\r\n\r\n**Which versions of React, and which browser / OS are affected by this issue? Did this work in previous versions of React?**\r\n16.6.2 and 16.6.3 both have the wrong comment. 16.6.1 and 16.6.0 are correct.\n<issue_comment>username_0: Also, https://github.com/facebook/react/blob/v16.6.3/packages/shared/ReactVersion.js claims to be `16.6.1`, but in the actual NPM package ReactVersion is set correctly.\n<issue_comment>username_1: @username_2 should this be handled as part of the release flow you've been working on?<issue_closed>\n<issue_comment>username_2: This was\u00a0a transient issue, caused by a somewhat manual release that Andrew ran. (There's no bug in the release script.) This issue should only affect 16.6.2 and 16.6.3. Fortunately it shouldn't cause any observable behavior unless someone reads the comments. \ud83d\ude04"}
{"repo": "facebook/react", "issue_id": 389079792, "issue_number": 14410, "timestamp": "2018-12-09 23:53:20+00:00", "text": "Also, https://github.com/facebook/react/blob/v16.6.3/packages/shared/ReactVersion.js claims to be `16.6.1`, but in the actual NPM package ReactVersion is set correctly.", "context": "<issue_start><issue_comment>Title: Comments in NPM package have wrong version\nusername_0: <!--\r\n  Note: if the issue is about documentation or the website, please file it at:\r\n  https://github.com/reactjs/reactjs.org/issues/new\r\n-->\r\n\r\n**Do you want to request a *feature* or report a *bug*?** bug\r\n\r\n**What is the current behavior?** The react@16.6.3 package has a comment at the top claiming to be React 16.6.1.\r\n\r\n**If the current behavior is a bug, please provide the steps to reproduce and if possible a minimal demo of the problem. Your bug will get fixed much faster if we can run your code and it doesn't have dependencies other than React. Paste the link to your JSFiddle (https://jsfiddle.net/Luktwrdm/) or CodeSandbox (https://codesandbox.io/s/new) example below:**\r\n```shell\r\ncd /tmp\r\nnpm pack react@16.6.3\r\ntar xf react-16.6.3.tgz\r\nhead -1 package/cjs/react.development.js\r\n# output: /** @license React v16.6.1\r\n```\r\n\r\n**What is the expected behavior?**\r\nThe comment at the top of the build indicates the correct version.\r\n\r\n**Which versions of React, and which browser / OS are affected by this issue? Did this work in previous versions of React?**\r\n16.6.2 and 16.6.3 both have the wrong comment. 16.6.1 and 16.6.0 are correct.\n<issue_comment>username_0: Also, https://github.com/facebook/react/blob/v16.6.3/packages/shared/ReactVersion.js claims to be `16.6.1`, but in the actual NPM package ReactVersion is set correctly.\n<issue_comment>username_1: @username_2 should this be handled as part of the release flow you've been working on?<issue_closed>\n<issue_comment>username_2: This was\u00a0a transient issue, caused by a somewhat manual release that Andrew ran. (There's no bug in the release script.) This issue should only affect 16.6.2 and 16.6.3. Fortunately it shouldn't cause any observable behavior unless someone reads the comments. \ud83d\ude04"}
{"repo": "facebook/react", "issue_id": 389079792, "issue_number": 14410, "timestamp": "2018-12-10 00:44:11+00:00", "text": "@username_2 should this be handled as part of the release flow you've been working on?", "context": "<issue_start><issue_comment>Title: Comments in NPM package have wrong version\nusername_0: <!--\r\n  Note: if the issue is about documentation or the website, please file it at:\r\n  https://github.com/reactjs/reactjs.org/issues/new\r\n-->\r\n\r\n**Do you want to request a *feature* or report a *bug*?** bug\r\n\r\n**What is the current behavior?** The react@16.6.3 package has a comment at the top claiming to be React 16.6.1.\r\n\r\n**If the current behavior is a bug, please provide the steps to reproduce and if possible a minimal demo of the problem. Your bug will get fixed much faster if we can run your code and it doesn't have dependencies other than React. Paste the link to your JSFiddle (https://jsfiddle.net/Luktwrdm/) or CodeSandbox (https://codesandbox.io/s/new) example below:**\r\n```shell\r\ncd /tmp\r\nnpm pack react@16.6.3\r\ntar xf react-16.6.3.tgz\r\nhead -1 package/cjs/react.development.js\r\n# output: /** @license React v16.6.1\r\n```\r\n\r\n**What is the expected behavior?**\r\nThe comment at the top of the build indicates the correct version.\r\n\r\n**Which versions of React, and which browser / OS are affected by this issue? Did this work in previous versions of React?**\r\n16.6.2 and 16.6.3 both have the wrong comment. 16.6.1 and 16.6.0 are correct.\n<issue_comment>username_0: Also, https://github.com/facebook/react/blob/v16.6.3/packages/shared/ReactVersion.js claims to be `16.6.1`, but in the actual NPM package ReactVersion is set correctly.\n<issue_comment>username_1: @username_2 should this be handled as part of the release flow you've been working on?<issue_closed>\n<issue_comment>username_2: This was\u00a0a transient issue, caused by a somewhat manual release that Andrew ran. (There's no bug in the release script.) This issue should only affect 16.6.2 and 16.6.3. Fortunately it shouldn't cause any observable behavior unless someone reads the comments. \ud83d\ude04"}
{"repo": "facebook/react", "issue_id": 389079792, "issue_number": 14410, "timestamp": "2018-12-10 03:35:36+00:00", "text": "", "context": "<issue_start><issue_comment>Title: Comments in NPM package have wrong version\nusername_0: <!--\r\n  Note: if the issue is about documentation or the website, please file it at:\r\n  https://github.com/reactjs/reactjs.org/issues/new\r\n-->\r\n\r\n**Do you want to request a *feature* or report a *bug*?** bug\r\n\r\n**What is the current behavior?** The react@16.6.3 package has a comment at the top claiming to be React 16.6.1.\r\n\r\n**If the current behavior is a bug, please provide the steps to reproduce and if possible a minimal demo of the problem. Your bug will get fixed much faster if we can run your code and it doesn't have dependencies other than React. Paste the link to your JSFiddle (https://jsfiddle.net/Luktwrdm/) or CodeSandbox (https://codesandbox.io/s/new) example below:**\r\n```shell\r\ncd /tmp\r\nnpm pack react@16.6.3\r\ntar xf react-16.6.3.tgz\r\nhead -1 package/cjs/react.development.js\r\n# output: /** @license React v16.6.1\r\n```\r\n\r\n**What is the expected behavior?**\r\nThe comment at the top of the build indicates the correct version.\r\n\r\n**Which versions of React, and which browser / OS are affected by this issue? Did this work in previous versions of React?**\r\n16.6.2 and 16.6.3 both have the wrong comment. 16.6.1 and 16.6.0 are correct.\n<issue_comment>username_0: Also, https://github.com/facebook/react/blob/v16.6.3/packages/shared/ReactVersion.js claims to be `16.6.1`, but in the actual NPM package ReactVersion is set correctly.\n<issue_comment>username_1: @username_2 should this be handled as part of the release flow you've been working on?<issue_closed>\n<issue_comment>username_2: This was\u00a0a transient issue, caused by a somewhat manual release that Andrew ran. (There's no bug in the release script.) This issue should only affect 16.6.2 and 16.6.3. Fortunately it shouldn't cause any observable behavior unless someone reads the comments. \ud83d\ude04"}
{"repo": "facebook/react", "issue_id": 389079792, "issue_number": 14410, "timestamp": "2018-12-10 03:35:36+00:00", "text": "This was\u00a0a transient issue, caused by a somewhat manual release that Andrew ran. (There's no bug in the release script.) This issue should only affect 16.6.2 and 16.6.3. Fortunately it shouldn't cause any observable behavior unless someone reads the comments. \ud83d\ude04", "context": "<issue_start><issue_comment>Title: Comments in NPM package have wrong version\nusername_0: <!--\r\n  Note: if the issue is about documentation or the website, please file it at:\r\n  https://github.com/reactjs/reactjs.org/issues/new\r\n-->\r\n\r\n**Do you want to request a *feature* or report a *bug*?** bug\r\n\r\n**What is the current behavior?** The react@16.6.3 package has a comment at the top claiming to be React 16.6.1.\r\n\r\n**If the current behavior is a bug, please provide the steps to reproduce and if possible a minimal demo of the problem. Your bug will get fixed much faster if we can run your code and it doesn't have dependencies other than React. Paste the link to your JSFiddle (https://jsfiddle.net/Luktwrdm/) or CodeSandbox (https://codesandbox.io/s/new) example below:**\r\n```shell\r\ncd /tmp\r\nnpm pack react@16.6.3\r\ntar xf react-16.6.3.tgz\r\nhead -1 package/cjs/react.development.js\r\n# output: /** @license React v16.6.1\r\n```\r\n\r\n**What is the expected behavior?**\r\nThe comment at the top of the build indicates the correct version.\r\n\r\n**Which versions of React, and which browser / OS are affected by this issue? Did this work in previous versions of React?**\r\n16.6.2 and 16.6.3 both have the wrong comment. 16.6.1 and 16.6.0 are correct.\n<issue_comment>username_0: Also, https://github.com/facebook/react/blob/v16.6.3/packages/shared/ReactVersion.js claims to be `16.6.1`, but in the actual NPM package ReactVersion is set correctly.\n<issue_comment>username_1: @username_2 should this be handled as part of the release flow you've been working on?<issue_closed>\n<issue_comment>username_2: This was\u00a0a transient issue, caused by a somewhat manual release that Andrew ran. (There's no bug in the release script.) This issue should only affect 16.6.2 and 16.6.3. Fortunately it shouldn't cause any observable behavior unless someone reads the comments. \ud83d\ude04"}
{"repo": "pytorch/pytorch", "issue_id": 408518494, "issue_number": 16934, "timestamp": "2019-02-10 09:46:45+00:00", "text": "The pytorch version is 1.0.0, when use 2 gpus on one machine it was blocked,but it run successfully when use 1 gpu\r\n\r\nThe call stack in python as follow:\r\n```\r\nFile \"/home/admin/code/PROPAGATE/tools/0_train_base.py\", line 319, in <module>\r\n    process(sys.argv[1], sys.argv[2], int(sys.argv[3]), sys.argv[4])\r\n  File \"/home/admin/code/PROPAGATE/tools/0_train_base.py\", line 241, in process\r\n    solver.solve()\r\n  File \"/home/admin/code/PROPAGATE/tools/0_train_base.py\", line 190, in solve\r\n    loss, accB, accN, lossC = self.iterate(XB, YB, XN, YN, True)\r\n  File \"/home/admin/code/PROPAGATE/tools/0_train_base.py\", line 140, in iterate\r\n    self.optimizer.step()\r\n  File \"/home/admin/code/PROPAGATE/rpm/anaconda/lib/python2.7/site-packages/torch/optim/sgd.py\", line 107, in step\r\n    p.data.add_(-group['lr'], d_p)\r\n  File \"<string>\", line 1, in <module>\r\n  File \"<string>\", line 1, in <module>\r\n```", "context": "<issue_start><issue_comment>Title: pytorch was blocked at optimizer.step\nusername_0: The pytorch version is 1.0.0, when use 2 gpus on one machine it was blocked,but it run successfully when use 1 gpu\r\n\r\nThe call stack in python as follow:\r\n```\r\nFile \"/home/admin/code/PROPAGATE/tools/0_train_base.py\", line 319, in <module>\r\n    process(sys.argv[1], sys.argv[2], int(sys.argv[3]), sys.argv[4])\r\n  File \"/home/admin/code/PROPAGATE/tools/0_train_base.py\", line 241, in process\r\n    solver.solve()\r\n  File \"/home/admin/code/PROPAGATE/tools/0_train_base.py\", line 190, in solve\r\n    loss, accB, accN, lossC = self.iterate(XB, YB, XN, YN, True)\r\n  File \"/home/admin/code/PROPAGATE/tools/0_train_base.py\", line 140, in iterate\r\n    self.optimizer.step()\r\n  File \"/home/admin/code/PROPAGATE/rpm/anaconda/lib/python2.7/site-packages/torch/optim/sgd.py\", line 107, in step\r\n    p.data.add_(-group['lr'], d_p)\r\n  File \"<string>\", line 1, in <module>\r\n  File \"<string>\", line 1, in <module>\r\n```\n<issue_comment>username_1: Please provide a minimal repro script. It's hard to debug without an repro here. Thanks!\n<issue_comment>username_2: Closing due to age and lack of repro script<issue_closed>"}
{"repo": "pytorch/pytorch", "issue_id": 408518494, "issue_number": 16934, "timestamp": "2019-02-10 23:24:59+00:00", "text": "Please provide a minimal repro script. It's hard to debug without an repro here. Thanks!", "context": "<issue_start><issue_comment>Title: pytorch was blocked at optimizer.step\nusername_0: The pytorch version is 1.0.0, when use 2 gpus on one machine it was blocked,but it run successfully when use 1 gpu\r\n\r\nThe call stack in python as follow:\r\n```\r\nFile \"/home/admin/code/PROPAGATE/tools/0_train_base.py\", line 319, in <module>\r\n    process(sys.argv[1], sys.argv[2], int(sys.argv[3]), sys.argv[4])\r\n  File \"/home/admin/code/PROPAGATE/tools/0_train_base.py\", line 241, in process\r\n    solver.solve()\r\n  File \"/home/admin/code/PROPAGATE/tools/0_train_base.py\", line 190, in solve\r\n    loss, accB, accN, lossC = self.iterate(XB, YB, XN, YN, True)\r\n  File \"/home/admin/code/PROPAGATE/tools/0_train_base.py\", line 140, in iterate\r\n    self.optimizer.step()\r\n  File \"/home/admin/code/PROPAGATE/rpm/anaconda/lib/python2.7/site-packages/torch/optim/sgd.py\", line 107, in step\r\n    p.data.add_(-group['lr'], d_p)\r\n  File \"<string>\", line 1, in <module>\r\n  File \"<string>\", line 1, in <module>\r\n```\n<issue_comment>username_1: Please provide a minimal repro script. It's hard to debug without an repro here. Thanks!\n<issue_comment>username_2: Closing due to age and lack of repro script<issue_closed>"}
{"repo": "pytorch/pytorch", "issue_id": 408518494, "issue_number": 16934, "timestamp": "2021-08-17 17:57:39+00:00", "text": "Closing due to age and lack of repro script", "context": "<issue_start><issue_comment>Title: pytorch was blocked at optimizer.step\nusername_0: The pytorch version is 1.0.0, when use 2 gpus on one machine it was blocked,but it run successfully when use 1 gpu\r\n\r\nThe call stack in python as follow:\r\n```\r\nFile \"/home/admin/code/PROPAGATE/tools/0_train_base.py\", line 319, in <module>\r\n    process(sys.argv[1], sys.argv[2], int(sys.argv[3]), sys.argv[4])\r\n  File \"/home/admin/code/PROPAGATE/tools/0_train_base.py\", line 241, in process\r\n    solver.solve()\r\n  File \"/home/admin/code/PROPAGATE/tools/0_train_base.py\", line 190, in solve\r\n    loss, accB, accN, lossC = self.iterate(XB, YB, XN, YN, True)\r\n  File \"/home/admin/code/PROPAGATE/tools/0_train_base.py\", line 140, in iterate\r\n    self.optimizer.step()\r\n  File \"/home/admin/code/PROPAGATE/rpm/anaconda/lib/python2.7/site-packages/torch/optim/sgd.py\", line 107, in step\r\n    p.data.add_(-group['lr'], d_p)\r\n  File \"<string>\", line 1, in <module>\r\n  File \"<string>\", line 1, in <module>\r\n```\n<issue_comment>username_1: Please provide a minimal repro script. It's hard to debug without an repro here. Thanks!\n<issue_comment>username_2: Closing due to age and lack of repro script<issue_closed>"}
{"repo": "pytorch/pytorch", "issue_id": 408518494, "issue_number": 16934, "timestamp": "2021-08-17 17:57:39+00:00", "text": "", "context": "<issue_start><issue_comment>Title: pytorch was blocked at optimizer.step\nusername_0: The pytorch version is 1.0.0, when use 2 gpus on one machine it was blocked,but it run successfully when use 1 gpu\r\n\r\nThe call stack in python as follow:\r\n```\r\nFile \"/home/admin/code/PROPAGATE/tools/0_train_base.py\", line 319, in <module>\r\n    process(sys.argv[1], sys.argv[2], int(sys.argv[3]), sys.argv[4])\r\n  File \"/home/admin/code/PROPAGATE/tools/0_train_base.py\", line 241, in process\r\n    solver.solve()\r\n  File \"/home/admin/code/PROPAGATE/tools/0_train_base.py\", line 190, in solve\r\n    loss, accB, accN, lossC = self.iterate(XB, YB, XN, YN, True)\r\n  File \"/home/admin/code/PROPAGATE/tools/0_train_base.py\", line 140, in iterate\r\n    self.optimizer.step()\r\n  File \"/home/admin/code/PROPAGATE/rpm/anaconda/lib/python2.7/site-packages/torch/optim/sgd.py\", line 107, in step\r\n    p.data.add_(-group['lr'], d_p)\r\n  File \"<string>\", line 1, in <module>\r\n  File \"<string>\", line 1, in <module>\r\n```\n<issue_comment>username_1: Please provide a minimal repro script. It's hard to debug without an repro here. Thanks!\n<issue_comment>username_2: Closing due to age and lack of repro script<issue_closed>"}
{"repo": "pytorch/pytorch", "issue_id": 518718409, "issue_number": 29319, "timestamp": "2019-11-06T20:29:35Z", "text": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#29319 Make all non-input arguments to functionals part of its options**\n* #29318 Pass F::*FuncOptions instead of torch::nn::*Options to functionals, and make F::*FuncOptions a different class when necessary", "context": "<issue_start><issue_comment>Title: Make all non-input arguments to functionals part of its options\nusername_0: Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#29319 Make all non-input arguments to functionals part of its options**\n* #29318 Pass F::*FuncOptions instead of torch::nn::*Options to functionals, and make F::*FuncOptions a different class when necessary\n<issue_comment>username_1: ## CircleCI build failures summary\nAs of commit 2e954dd:\n* 0/19 recognized as flaky\n* 19/19 failures introduced in this PR\n\n\n\nHere are the [reasons each build failed](https://dr.pytorch.org/commit-details.html?sha1=2e954dd7243d98f23b016bc55bfaff8aff11dba1).\n\n---\nThis comment was automatically generated by [Dr. CI](https://github.com/username_1/circleci-failure-tracker/tree/master/docs/from-pull-request-comment).\nFollow [this link to opt-out](https://dr.pytorch.org/admin/comments-opt-out.html) of these comments for your Pull Requests.\n\n\nPlease report bugs/suggestions on the [GitHub issue tracker](https://github.com/username_1/circleci-failure-tracker/issues)."}
{"repo": "pytorch/pytorch", "issue_id": 518718409, "issue_number": 29319, "timestamp": "2019-11-07 04:14:48+00:00", "text": "## CircleCI build failures summary\nAs of commit 2e954dd:\n* 0/19 recognized as flaky\n* 19/19 failures introduced in this PR\n\n\n\nHere are the [reasons each build failed](https://dr.pytorch.org/commit-details.html?sha1=2e954dd7243d98f23b016bc55bfaff8aff11dba1).\n\n---\nThis comment was automatically generated by [Dr. CI](https://github.com/username_1/circleci-failure-tracker/tree/master/docs/from-pull-request-comment).\nFollow [this link to opt-out](https://dr.pytorch.org/admin/comments-opt-out.html) of these comments for your Pull Requests.\n\n\nPlease report bugs/suggestions on the [GitHub issue tracker](https://github.com/username_1/circleci-failure-tracker/issues).", "context": "<issue_start><issue_comment>Title: Make all non-input arguments to functionals part of its options\nusername_0: Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#29319 Make all non-input arguments to functionals part of its options**\n* #29318 Pass F::*FuncOptions instead of torch::nn::*Options to functionals, and make F::*FuncOptions a different class when necessary\n<issue_comment>username_1: ## CircleCI build failures summary\nAs of commit 2e954dd:\n* 0/19 recognized as flaky\n* 19/19 failures introduced in this PR\n\n\n\nHere are the [reasons each build failed](https://dr.pytorch.org/commit-details.html?sha1=2e954dd7243d98f23b016bc55bfaff8aff11dba1).\n\n---\nThis comment was automatically generated by [Dr. CI](https://github.com/username_1/circleci-failure-tracker/tree/master/docs/from-pull-request-comment).\nFollow [this link to opt-out](https://dr.pytorch.org/admin/comments-opt-out.html) of these comments for your Pull Requests.\n\n\nPlease report bugs/suggestions on the [GitHub issue tracker](https://github.com/username_1/circleci-failure-tracker/issues)."}
{"repo": "pytorch/pytorch", "issue_id": 530629662, "issue_number": 30582, "timestamp": "2019-11-30T21:00:20Z", "text": "Update master Persons of Interest to match that from v1.2.0\r\nhttps://github.com/pytorch/pytorch/blob/v1.2.0/docs/source/community/persons_of_interest.rst", "context": "<issue_start><issue_comment>Title: Update master Persons of Interest to match v1.2.0\nusername_0: Update master Persons of Interest to match that from v1.2.0\r\nhttps://github.com/pytorch/pytorch/blob/v1.2.0/docs/source/community/persons_of_interest.rst\n<issue_comment>username_1: yeah, there is definitely a lot going on here. I think it might be better to can this PR and start with a new PR on top what we currently have. It would certainly be much more incremental."}
{"repo": "pytorch/pytorch", "issue_id": 530629662, "issue_number": 30582, "timestamp": "2019-12-03 03:59:56+00:00", "text": "yeah, there is definitely a lot going on here. I think it might be better to can this PR and start with a new PR on top what we currently have. It would certainly be much more incremental.", "context": "<issue_start><issue_comment>Title: Update master Persons of Interest to match v1.2.0\nusername_0: Update master Persons of Interest to match that from v1.2.0\r\nhttps://github.com/pytorch/pytorch/blob/v1.2.0/docs/source/community/persons_of_interest.rst\n<issue_comment>username_1: yeah, there is definitely a lot going on here. I think it might be better to can this PR and start with a new PR on top what we currently have. It would certainly be much more incremental."}
{"repo": "pytorch/pytorch", "issue_id": 546524878, "issue_number": 31924, "timestamp": "2020-01-07 21:47:10+00:00", "text": "https://app.circleci.com/jobs/github/pytorch/pytorch/4155093\r\n\r\n```\r\nJan 07 20:36:55 ======================================================================\r\nJan 07 20:36:55 FAIL [3.205s]: test_nccl_errors_blocking_clean_exit (__main__.NcclErrorHandlingTest)\r\nJan 07 20:36:55 ----------------------------------------------------------------------\r\nJan 07 20:36:55 Traceback (most recent call last):\r\nJan 07 20:36:55   File \"/var/lib/jenkins/workspace/test/common_distributed.py\", line 130, in wrapper\r\nJan 07 20:36:55     self._join_processes(fn)\r\nJan 07 20:36:55   File \"/var/lib/jenkins/workspace/test/common_distributed.py\", line 211, in _join_processes\r\nJan 07 20:36:55     self._check_return_codes(elapsed_time)\r\nJan 07 20:36:55   File \"/var/lib/jenkins/workspace/test/common_distributed.py\", line 231, in _check_return_codes\r\nJan 07 20:36:55     self.assertEqual(p.exitcode, first_process.exitcode)\r\nJan 07 20:36:55   File \"/var/lib/jenkins/workspace/test/common_utils.py\", line 888, in assertEqual\r\nJan 07 20:36:55     super(TestCase, self).assertLessEqual(abs(x - y), prec, message)\r\nJan 07 20:36:55 AssertionError: 11 not less than or equal to 1e-05 : \r\n```", "context": "<issue_start><issue_comment>Title: test_nccl_errors_blocking_clean_exit is flaky\nusername_0: https://app.circleci.com/jobs/github/pytorch/pytorch/4155093\r\n\r\n```\r\nJan 07 20:36:55 ======================================================================\r\nJan 07 20:36:55 FAIL [3.205s]: test_nccl_errors_blocking_clean_exit (__main__.NcclErrorHandlingTest)\r\nJan 07 20:36:55 ----------------------------------------------------------------------\r\nJan 07 20:36:55 Traceback (most recent call last):\r\nJan 07 20:36:55   File \"/var/lib/jenkins/workspace/test/common_distributed.py\", line 130, in wrapper\r\nJan 07 20:36:55     self._join_processes(fn)\r\nJan 07 20:36:55   File \"/var/lib/jenkins/workspace/test/common_distributed.py\", line 211, in _join_processes\r\nJan 07 20:36:55     self._check_return_codes(elapsed_time)\r\nJan 07 20:36:55   File \"/var/lib/jenkins/workspace/test/common_distributed.py\", line 231, in _check_return_codes\r\nJan 07 20:36:55     self.assertEqual(p.exitcode, first_process.exitcode)\r\nJan 07 20:36:55   File \"/var/lib/jenkins/workspace/test/common_utils.py\", line 888, in assertEqual\r\nJan 07 20:36:55     super(TestCase, self).assertLessEqual(abs(x - y), prec, message)\r\nJan 07 20:36:55 AssertionError: 11 not less than or equal to 1e-05 : \r\n```\n<issue_comment>username_1: Cannot reproduce this failure after running the same test 100 times on 8 GPUs.<issue_closed>"}
{"repo": "pytorch/pytorch", "issue_id": 546524878, "issue_number": 31924, "timestamp": "2020-09-22 08:39:15+00:00", "text": "Cannot reproduce this failure after running the same test 100 times on 8 GPUs.", "context": "<issue_start><issue_comment>Title: test_nccl_errors_blocking_clean_exit is flaky\nusername_0: https://app.circleci.com/jobs/github/pytorch/pytorch/4155093\r\n\r\n```\r\nJan 07 20:36:55 ======================================================================\r\nJan 07 20:36:55 FAIL [3.205s]: test_nccl_errors_blocking_clean_exit (__main__.NcclErrorHandlingTest)\r\nJan 07 20:36:55 ----------------------------------------------------------------------\r\nJan 07 20:36:55 Traceback (most recent call last):\r\nJan 07 20:36:55   File \"/var/lib/jenkins/workspace/test/common_distributed.py\", line 130, in wrapper\r\nJan 07 20:36:55     self._join_processes(fn)\r\nJan 07 20:36:55   File \"/var/lib/jenkins/workspace/test/common_distributed.py\", line 211, in _join_processes\r\nJan 07 20:36:55     self._check_return_codes(elapsed_time)\r\nJan 07 20:36:55   File \"/var/lib/jenkins/workspace/test/common_distributed.py\", line 231, in _check_return_codes\r\nJan 07 20:36:55     self.assertEqual(p.exitcode, first_process.exitcode)\r\nJan 07 20:36:55   File \"/var/lib/jenkins/workspace/test/common_utils.py\", line 888, in assertEqual\r\nJan 07 20:36:55     super(TestCase, self).assertLessEqual(abs(x - y), prec, message)\r\nJan 07 20:36:55 AssertionError: 11 not less than or equal to 1e-05 : \r\n```\n<issue_comment>username_1: Cannot reproduce this failure after running the same test 100 times on 8 GPUs.<issue_closed>"}
{"repo": "pytorch/pytorch", "issue_id": 546524878, "issue_number": 31924, "timestamp": "2020-10-12 10:10:13+00:00", "text": "", "context": "<issue_start><issue_comment>Title: test_nccl_errors_blocking_clean_exit is flaky\nusername_0: https://app.circleci.com/jobs/github/pytorch/pytorch/4155093\r\n\r\n```\r\nJan 07 20:36:55 ======================================================================\r\nJan 07 20:36:55 FAIL [3.205s]: test_nccl_errors_blocking_clean_exit (__main__.NcclErrorHandlingTest)\r\nJan 07 20:36:55 ----------------------------------------------------------------------\r\nJan 07 20:36:55 Traceback (most recent call last):\r\nJan 07 20:36:55   File \"/var/lib/jenkins/workspace/test/common_distributed.py\", line 130, in wrapper\r\nJan 07 20:36:55     self._join_processes(fn)\r\nJan 07 20:36:55   File \"/var/lib/jenkins/workspace/test/common_distributed.py\", line 211, in _join_processes\r\nJan 07 20:36:55     self._check_return_codes(elapsed_time)\r\nJan 07 20:36:55   File \"/var/lib/jenkins/workspace/test/common_distributed.py\", line 231, in _check_return_codes\r\nJan 07 20:36:55     self.assertEqual(p.exitcode, first_process.exitcode)\r\nJan 07 20:36:55   File \"/var/lib/jenkins/workspace/test/common_utils.py\", line 888, in assertEqual\r\nJan 07 20:36:55     super(TestCase, self).assertLessEqual(abs(x - y), prec, message)\r\nJan 07 20:36:55 AssertionError: 11 not less than or equal to 1e-05 : \r\n```\n<issue_comment>username_1: Cannot reproduce this failure after running the same test 100 times on 8 GPUs.<issue_closed>"}
{"repo": "pytorch/pytorch", "issue_id": 549943186, "issue_number": 32207, "timestamp": "2020-01-15T03:27:46Z", "text": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#32207 [DO NOT MERGE] iOS 1.4.0 binary push**\n* #32147 [iOS] Update Gemfile\n* #31912 [CI]Suppress pip logs", "context": "<issue_start><issue_comment>Title: [DO NOT MERGE] iOS 1.4.0 binary push\nusername_0: Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#32207 [DO NOT MERGE] iOS 1.4.0 binary push**\n* #32147 [iOS] Update Gemfile\n* #31912 [CI]Suppress pip logs\n<issue_comment>username_1: ## :pill: CircleCI build failures summary and remediations\nAs of commit 2f788305:\n\n\n* **1/1** failures introduced in this PR\n\n\n## Detailed failure analysis\n\nOne may explore the probable reasons each build failed interactively [on the Dr. CI website](https://dr.pytorch.org/commit-details.html?sha1=2f7883058b67b349112aba3524e2774237bd2e6c).\n\n### :detective: 1 new failure recognized by patterns\nThe following build failures do not appear to be due to upstream breakage:\n#### [![See CircleCI build](https://avatars0.githubusercontent.com/ml/7?s=12)](https://circleci.com/gh/pytorch/pytorch/4249792) binary_ios_upload (1/1)\n**Step:** \"Upload\" ([full log](https://dr.pytorch.org/api/view-log-full?build_id=86265225) | [pattern match details](https://dr.pytorch.org/build-details.html?build_id=86265225))\n<details>\n<summary>\n<code>fatal error: /Applications/Xcode-11.2.1.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/lipo: can't open input file: /Users/distiller/workspace/ios/x86_64/lib/libtorch_cpu.a (No such file or directory)</code>\n</summary>\n\n```\n++ lipo -create /Users/distiller/workspace/ios/x86_64/lib/libcpuinfo.a /Users/distiller/workspace/ios/arm64/lib/libcpuinfo.a -o /Users/distiller/workspace/zip/install/lib/libcpuinfo.a \n++ for lib in '${target_libs[*]}' \n++ libs=(${ARTIFACTS_DIR}/x86_64/lib/${lib} ${ARTIFACTS_DIR}/arm64/lib/${lib}) \n++ lipo -create /Users/distiller/workspace/ios/x86_64/lib/libeigen_blas.a /Users/distiller/workspace/ios/arm64/lib/libeigen_blas.a -o /Users/distiller/workspace/zip/install/lib/libeigen_blas.a \n++ for lib in '${target_libs[*]}' \n++ libs=(${ARTIFACTS_DIR}/x86_64/lib/${lib} ${ARTIFACTS_DIR}/arm64/lib/${lib}) \n++ lipo -create /Users/distiller/workspace/ios/x86_64/lib/libpytorch_qnnpack.a /Users/distiller/workspace/ios/arm64/lib/libpytorch_qnnpack.a -o /Users/distiller/workspace/zip/install/lib/libpytorch_qnnpack.a \n++ for lib in '${target_libs[*]}' \n++ libs=(${ARTIFACTS_DIR}/x86_64/lib/${lib} ${ARTIFACTS_DIR}/arm64/lib/${lib}) \n++ lipo -create /Users/distiller/workspace/ios/x86_64/lib/libtorch_cpu.a /Users/distiller/workspace/ios/arm64/lib/libtorch_cpu.a -o /Users/distiller/workspace/zip/install/lib/libtorch_cpu.a \nfatal error: /Applications/Xcode-11.2.1.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/lipo: can't open input file: /Users/distiller/workspace/ios/x86_64/lib/libtorch_cpu.a (No such file or directory) \n```\n\n</details>\n\n\n\n---\n\n<details><summary>This comment was automatically generated by <a href=\"https://github.com/username_1/circleci-failure-tracker/tree/master/docs/from-pull-request-comment\">Dr. CI</a> (expand for details).</summary>Follow <a href=\"https://dr.pytorch.org/admin/comments-opt-out.html\">this link to opt-out</a> of these comments for your Pull Requests.\n\nPlease report bugs/suggestions on the <a href=\"https://github.com/username_1/circleci-failure-tracker/issues\">GitHub issue tracker</a>.\n</details>"}
{"repo": "pytorch/pytorch", "issue_id": 549943186, "issue_number": 32207, "timestamp": "2020-01-15 03:48:50+00:00", "text": "## :pill: CircleCI build failures summary and remediations\nAs of commit 2f788305:\n\n\n* **1/1** failures introduced in this PR\n\n\n## Detailed failure analysis\n\nOne may explore the probable reasons each build failed interactively [on the Dr. CI website](https://dr.pytorch.org/commit-details.html?sha1=2f7883058b67b349112aba3524e2774237bd2e6c).\n\n### :detective: 1 new failure recognized by patterns\nThe following build failures do not appear to be due to upstream breakage:\n#### [![See CircleCI build](https://avatars0.githubusercontent.com/ml/7?s=12)](https://circleci.com/gh/pytorch/pytorch/4249792) binary_ios_upload (1/1)\n**Step:** \"Upload\" ([full log](https://dr.pytorch.org/api/view-log-full?build_id=86265225) | [pattern match details](https://dr.pytorch.org/build-details.html?build_id=86265225))\n<details>\n<summary>\n<code>fatal error: /Applications/Xcode-11.2.1.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/lipo: can't open input file: /Users/distiller/workspace/ios/x86_64/lib/libtorch_cpu.a (No such file or directory)</code>\n</summary>\n\n```\n++ lipo -create /Users/distiller/workspace/ios/x86_64/lib/libcpuinfo.a /Users/distiller/workspace/ios/arm64/lib/libcpuinfo.a -o /Users/distiller/workspace/zip/install/lib/libcpuinfo.a \n++ for lib in '${target_libs[*]}' \n++ libs=(${ARTIFACTS_DIR}/x86_64/lib/${lib} ${ARTIFACTS_DIR}/arm64/lib/${lib}) \n++ lipo -create /Users/distiller/workspace/ios/x86_64/lib/libeigen_blas.a /Users/distiller/workspace/ios/arm64/lib/libeigen_blas.a -o /Users/distiller/workspace/zip/install/lib/libeigen_blas.a \n++ for lib in '${target_libs[*]}' \n++ libs=(${ARTIFACTS_DIR}/x86_64/lib/${lib} ${ARTIFACTS_DIR}/arm64/lib/${lib}) \n++ lipo -create /Users/distiller/workspace/ios/x86_64/lib/libpytorch_qnnpack.a /Users/distiller/workspace/ios/arm64/lib/libpytorch_qnnpack.a -o /Users/distiller/workspace/zip/install/lib/libpytorch_qnnpack.a \n++ for lib in '${target_libs[*]}' \n++ libs=(${ARTIFACTS_DIR}/x86_64/lib/${lib} ${ARTIFACTS_DIR}/arm64/lib/${lib}) \n++ lipo -create /Users/distiller/workspace/ios/x86_64/lib/libtorch_cpu.a /Users/distiller/workspace/ios/arm64/lib/libtorch_cpu.a -o /Users/distiller/workspace/zip/install/lib/libtorch_cpu.a \nfatal error: /Applications/Xcode-11.2.1.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/lipo: can't open input file: /Users/distiller/workspace/ios/x86_64/lib/libtorch_cpu.a (No such file or directory) \n```\n\n</details>\n\n\n\n---\n\n<details><summary>This comment was automatically generated by <a href=\"https://github.com/username_1/circleci-failure-tracker/tree/master/docs/from-pull-request-comment\">Dr. CI</a> (expand for details).</summary>Follow <a href=\"https://dr.pytorch.org/admin/comments-opt-out.html\">this link to opt-out</a> of these comments for your Pull Requests.\n\nPlease report bugs/suggestions on the <a href=\"https://github.com/username_1/circleci-failure-tracker/issues\">GitHub issue tracker</a>.\n</details>", "context": "<issue_start><issue_comment>Title: [DO NOT MERGE] iOS 1.4.0 binary push\nusername_0: Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#32207 [DO NOT MERGE] iOS 1.4.0 binary push**\n* #32147 [iOS] Update Gemfile\n* #31912 [CI]Suppress pip logs\n<issue_comment>username_1: ## :pill: CircleCI build failures summary and remediations\nAs of commit 2f788305:\n\n\n* **1/1** failures introduced in this PR\n\n\n## Detailed failure analysis\n\nOne may explore the probable reasons each build failed interactively [on the Dr. CI website](https://dr.pytorch.org/commit-details.html?sha1=2f7883058b67b349112aba3524e2774237bd2e6c).\n\n### :detective: 1 new failure recognized by patterns\nThe following build failures do not appear to be due to upstream breakage:\n#### [![See CircleCI build](https://avatars0.githubusercontent.com/ml/7?s=12)](https://circleci.com/gh/pytorch/pytorch/4249792) binary_ios_upload (1/1)\n**Step:** \"Upload\" ([full log](https://dr.pytorch.org/api/view-log-full?build_id=86265225) | [pattern match details](https://dr.pytorch.org/build-details.html?build_id=86265225))\n<details>\n<summary>\n<code>fatal error: /Applications/Xcode-11.2.1.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/lipo: can't open input file: /Users/distiller/workspace/ios/x86_64/lib/libtorch_cpu.a (No such file or directory)</code>\n</summary>\n\n```\n++ lipo -create /Users/distiller/workspace/ios/x86_64/lib/libcpuinfo.a /Users/distiller/workspace/ios/arm64/lib/libcpuinfo.a -o /Users/distiller/workspace/zip/install/lib/libcpuinfo.a \n++ for lib in '${target_libs[*]}' \n++ libs=(${ARTIFACTS_DIR}/x86_64/lib/${lib} ${ARTIFACTS_DIR}/arm64/lib/${lib}) \n++ lipo -create /Users/distiller/workspace/ios/x86_64/lib/libeigen_blas.a /Users/distiller/workspace/ios/arm64/lib/libeigen_blas.a -o /Users/distiller/workspace/zip/install/lib/libeigen_blas.a \n++ for lib in '${target_libs[*]}' \n++ libs=(${ARTIFACTS_DIR}/x86_64/lib/${lib} ${ARTIFACTS_DIR}/arm64/lib/${lib}) \n++ lipo -create /Users/distiller/workspace/ios/x86_64/lib/libpytorch_qnnpack.a /Users/distiller/workspace/ios/arm64/lib/libpytorch_qnnpack.a -o /Users/distiller/workspace/zip/install/lib/libpytorch_qnnpack.a \n++ for lib in '${target_libs[*]}' \n++ libs=(${ARTIFACTS_DIR}/x86_64/lib/${lib} ${ARTIFACTS_DIR}/arm64/lib/${lib}) \n++ lipo -create /Users/distiller/workspace/ios/x86_64/lib/libtorch_cpu.a /Users/distiller/workspace/ios/arm64/lib/libtorch_cpu.a -o /Users/distiller/workspace/zip/install/lib/libtorch_cpu.a \nfatal error: /Applications/Xcode-11.2.1.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/lipo: can't open input file: /Users/distiller/workspace/ios/x86_64/lib/libtorch_cpu.a (No such file or directory) \n```\n\n</details>\n\n\n\n---\n\n<details><summary>This comment was automatically generated by <a href=\"https://github.com/username_1/circleci-failure-tracker/tree/master/docs/from-pull-request-comment\">Dr. CI</a> (expand for details).</summary>Follow <a href=\"https://dr.pytorch.org/admin/comments-opt-out.html\">this link to opt-out</a> of these comments for your Pull Requests.\n\nPlease report bugs/suggestions on the <a href=\"https://github.com/username_1/circleci-failure-tracker/issues\">GitHub issue tracker</a>.\n</details>"}
{"repo": "pytorch/pytorch", "issue_id": 564830266, "issue_number": 33299, "timestamp": "2020-02-13 17:12:05+00:00", "text": "```\r\nt = torch.empty(... , dtype=torch.int64)\r\nt.random_(torch.iinfo(torch.int64).min, None)\r\n```\r\nshould be able to fill tensor with all 64 bit number including `torch.iinfo(torch.int64).min` and `torch.iinfo(torch.int64).max`", "context": "<issue_start><issue_comment>Title: Tensor.random_ should be able to generate all 64 bit numbers including min and max value\nusername_0: ```\r\nt = torch.empty(... , dtype=torch.int64)\r\nt.random_(torch.iinfo(torch.int64).min, None)\r\n```\r\nshould be able to fill tensor with all 64 bit number including `torch.iinfo(torch.int64).min` and `torch.iinfo(torch.int64).max`\n<issue_comment>username_1: This is a dupe of #16944\n<issue_comment>username_0: Fixed in https://github.com/pytorch/pytorch/commit/095de1e872671c4da2cc0965976058cf752c3be0\r\nhttps://github.com/pytorch/pytorch/blob/master/aten/src/ATen/test/cpu_rng_test.cpp#L93 demonstrates how to generate uint64_t max value, (2^64 - 1)<issue_closed>\n<issue_comment>username_0: ```\r\nIn [1]: import torch\r\n\r\nIn [2]: \"{0:b}\".format(torch.empty(1, dtype=torch.int64, device='cpu').random_(torch.iinfo(torch.int64).max, None)[0])\r\nOut[2]: '111111111111111111111111111111111111111111111111111111111111111'\r\n\r\nIn [3]: \"{0:b}\".format(torch.empty(1, dtype=torch.int64, device='cuda').random_(torch.iinfo(torch.int64).max, None)[0])\r\nOut[3]: '111111111111111111111111111111111111111111111111111111111111111'\r\n```"}
{"repo": "pytorch/pytorch", "issue_id": 564830266, "issue_number": 33299, "timestamp": "2020-02-14 16:27:38+00:00", "text": "This is a dupe of #16944", "context": "<issue_start><issue_comment>Title: Tensor.random_ should be able to generate all 64 bit numbers including min and max value\nusername_0: ```\r\nt = torch.empty(... , dtype=torch.int64)\r\nt.random_(torch.iinfo(torch.int64).min, None)\r\n```\r\nshould be able to fill tensor with all 64 bit number including `torch.iinfo(torch.int64).min` and `torch.iinfo(torch.int64).max`\n<issue_comment>username_1: This is a dupe of #16944\n<issue_comment>username_0: Fixed in https://github.com/pytorch/pytorch/commit/095de1e872671c4da2cc0965976058cf752c3be0\r\nhttps://github.com/pytorch/pytorch/blob/master/aten/src/ATen/test/cpu_rng_test.cpp#L93 demonstrates how to generate uint64_t max value, (2^64 - 1)<issue_closed>\n<issue_comment>username_0: ```\r\nIn [1]: import torch\r\n\r\nIn [2]: \"{0:b}\".format(torch.empty(1, dtype=torch.int64, device='cpu').random_(torch.iinfo(torch.int64).max, None)[0])\r\nOut[2]: '111111111111111111111111111111111111111111111111111111111111111'\r\n\r\nIn [3]: \"{0:b}\".format(torch.empty(1, dtype=torch.int64, device='cuda').random_(torch.iinfo(torch.int64).max, None)[0])\r\nOut[3]: '111111111111111111111111111111111111111111111111111111111111111'\r\n```"}
{"repo": "pytorch/pytorch", "issue_id": 564830266, "issue_number": 33299, "timestamp": "2020-02-28 15:52:14+00:00", "text": "Fixed in https://github.com/pytorch/pytorch/commit/095de1e872671c4da2cc0965976058cf752c3be0\r\nhttps://github.com/pytorch/pytorch/blob/master/aten/src/ATen/test/cpu_rng_test.cpp#L93 demonstrates how to generate uint64_t max value, (2^64 - 1)", "context": "<issue_start><issue_comment>Title: Tensor.random_ should be able to generate all 64 bit numbers including min and max value\nusername_0: ```\r\nt = torch.empty(... , dtype=torch.int64)\r\nt.random_(torch.iinfo(torch.int64).min, None)\r\n```\r\nshould be able to fill tensor with all 64 bit number including `torch.iinfo(torch.int64).min` and `torch.iinfo(torch.int64).max`\n<issue_comment>username_1: This is a dupe of #16944\n<issue_comment>username_0: Fixed in https://github.com/pytorch/pytorch/commit/095de1e872671c4da2cc0965976058cf752c3be0\r\nhttps://github.com/pytorch/pytorch/blob/master/aten/src/ATen/test/cpu_rng_test.cpp#L93 demonstrates how to generate uint64_t max value, (2^64 - 1)<issue_closed>\n<issue_comment>username_0: ```\r\nIn [1]: import torch\r\n\r\nIn [2]: \"{0:b}\".format(torch.empty(1, dtype=torch.int64, device='cpu').random_(torch.iinfo(torch.int64).max, None)[0])\r\nOut[2]: '111111111111111111111111111111111111111111111111111111111111111'\r\n\r\nIn [3]: \"{0:b}\".format(torch.empty(1, dtype=torch.int64, device='cuda').random_(torch.iinfo(torch.int64).max, None)[0])\r\nOut[3]: '111111111111111111111111111111111111111111111111111111111111111'\r\n```"}
{"repo": "pytorch/pytorch", "issue_id": 564830266, "issue_number": 33299, "timestamp": "2020-02-28 15:52:15+00:00", "text": "", "context": "<issue_start><issue_comment>Title: Tensor.random_ should be able to generate all 64 bit numbers including min and max value\nusername_0: ```\r\nt = torch.empty(... , dtype=torch.int64)\r\nt.random_(torch.iinfo(torch.int64).min, None)\r\n```\r\nshould be able to fill tensor with all 64 bit number including `torch.iinfo(torch.int64).min` and `torch.iinfo(torch.int64).max`\n<issue_comment>username_1: This is a dupe of #16944\n<issue_comment>username_0: Fixed in https://github.com/pytorch/pytorch/commit/095de1e872671c4da2cc0965976058cf752c3be0\r\nhttps://github.com/pytorch/pytorch/blob/master/aten/src/ATen/test/cpu_rng_test.cpp#L93 demonstrates how to generate uint64_t max value, (2^64 - 1)<issue_closed>\n<issue_comment>username_0: ```\r\nIn [1]: import torch\r\n\r\nIn [2]: \"{0:b}\".format(torch.empty(1, dtype=torch.int64, device='cpu').random_(torch.iinfo(torch.int64).max, None)[0])\r\nOut[2]: '111111111111111111111111111111111111111111111111111111111111111'\r\n\r\nIn [3]: \"{0:b}\".format(torch.empty(1, dtype=torch.int64, device='cuda').random_(torch.iinfo(torch.int64).max, None)[0])\r\nOut[3]: '111111111111111111111111111111111111111111111111111111111111111'\r\n```"}
{"repo": "pytorch/pytorch", "issue_id": 564830266, "issue_number": 33299, "timestamp": "2020-02-28 16:12:44+00:00", "text": "```\r\nIn [1]: import torch\r\n\r\nIn [2]: \"{0:b}\".format(torch.empty(1, dtype=torch.int64, device='cpu').random_(torch.iinfo(torch.int64).max, None)[0])\r\nOut[2]: '111111111111111111111111111111111111111111111111111111111111111'\r\n\r\nIn [3]: \"{0:b}\".format(torch.empty(1, dtype=torch.int64, device='cuda').random_(torch.iinfo(torch.int64).max, None)[0])\r\nOut[3]: '111111111111111111111111111111111111111111111111111111111111111'\r\n```", "context": "<issue_start><issue_comment>Title: Tensor.random_ should be able to generate all 64 bit numbers including min and max value\nusername_0: ```\r\nt = torch.empty(... , dtype=torch.int64)\r\nt.random_(torch.iinfo(torch.int64).min, None)\r\n```\r\nshould be able to fill tensor with all 64 bit number including `torch.iinfo(torch.int64).min` and `torch.iinfo(torch.int64).max`\n<issue_comment>username_1: This is a dupe of #16944\n<issue_comment>username_0: Fixed in https://github.com/pytorch/pytorch/commit/095de1e872671c4da2cc0965976058cf752c3be0\r\nhttps://github.com/pytorch/pytorch/blob/master/aten/src/ATen/test/cpu_rng_test.cpp#L93 demonstrates how to generate uint64_t max value, (2^64 - 1)<issue_closed>\n<issue_comment>username_0: ```\r\nIn [1]: import torch\r\n\r\nIn [2]: \"{0:b}\".format(torch.empty(1, dtype=torch.int64, device='cpu').random_(torch.iinfo(torch.int64).max, None)[0])\r\nOut[2]: '111111111111111111111111111111111111111111111111111111111111111'\r\n\r\nIn [3]: \"{0:b}\".format(torch.empty(1, dtype=torch.int64, device='cuda').random_(torch.iinfo(torch.int64).max, None)[0])\r\nOut[3]: '111111111111111111111111111111111111111111111111111111111111111'\r\n```"}
{"repo": "pytorch/pytorch", "issue_id": 583995217, "issue_number": 34982, "timestamp": "2020-03-18T20:51:38Z", "text": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* #34983 Refactor test_jit_fuser legacy tests.\n* **#34982 Don't change global executor state in test_jit.py - rather do that in setUp and tearDown.**\n* #34981 Query graph executor mode in test fixture rather than relying on a global variable.\n* #34980 Fix warnings in test/test_jit_fuser.py\n\n\n\nDifferential Revision: [D20520370](https://our.internmc.facebook.com/intern/diff/D20520370)", "context": "<issue_start><issue_comment>Title: Don't change global executor state in test_jit.py - rather do that in setUp and tearDown.\nusername_0: Stack from [ghstack](https://github.com/ezyang/ghstack):\n* #34983 Refactor test_jit_fuser legacy tests.\n* **#34982 Don't change global executor state in test_jit.py - rather do that in setUp and tearDown.**\n* #34981 Query graph executor mode in test fixture rather than relying on a global variable.\n* #34980 Fix warnings in test/test_jit_fuser.py\n\n\n\nDifferential Revision: [D20520370](https://our.internmc.facebook.com/intern/diff/D20520370)"}
{"repo": "pytorch/pytorch", "issue_id": 893273577, "issue_number": 58391, "timestamp": "2021-05-17T12:19:23Z", "text": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* (to be filled)\n\nAn additional (and hopefully more robust) way of fixing the same problem https://github.com/pytorch/pytorch/pull/58382 fixed.\n\nDifferential Revision: [D28474154](https://our.internmc.facebook.com/intern/diff/D28474154/)", "context": "<issue_start><issue_comment>Title: Prevent lock inversions with GIL in Future\nusername_0: Stack from [ghstack](https://github.com/ezyang/ghstack):\n* (to be filled)\n\nAn additional (and hopefully more robust) way of fixing the same problem https://github.com/pytorch/pytorch/pull/58382 fixed.\n\nDifferential Revision: [D28474154](https://our.internmc.facebook.com/intern/diff/D28474154/)"}
{"repo": "pytorch/pytorch", "issue_id": 1052456616, "issue_number": 68291, "timestamp": "2021-11-12 22:38:26+00:00", "text": "## \ud83d\udc1b Bug\r\n\r\nCalling `torch.linalg.svd` on a large float64 tensor on the CPU (size 90k x ~34k) leads to this error. `torch.linalg.eigh` on `tensor.T@tensor` seg faults.\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. I don't know. I have a large tensor of type float64. Calling `torch.linalg.svd`on it produces this error.\r\n\r\n```\r\nIntel MKL ERROR: Parameter 12 was incorrect on entry to DGESDD.\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n/tmp/ipykernel_3300/2679862595.py in <module>\r\n----> 1 torch.linalg.svd(tensor)\r\n\r\nRuntimeError: falseINTERNAL ASSERT FAILED at \"/opt/conda/conda-bld/pytorch_1634272068185/work/aten/src/ATen/native/LinearAlgebraUtils.h\":244, please report a bug to PyTorch. svd_cpu: Argument 12 has illegal value. Most certainly there is a bug in the implementation calling the backend library.\r\n```\r\n## Expected behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n## Environment\r\n\r\nPlease copy and paste the output from our\r\n[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)\r\n(or fill out the checklist below manually).\r\n\r\nYou can get the script and run it with:\r\n```\r\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\r\n# For security purposes, please check the contents of collect_env.py before running it.\r\npython collect_env.py\r\n```\r\n\r\n - PyTorch Version (e.g., 1.0): '1.10.0'\r\n - OS (e.g., Linux): Linux\r\n - How you installed PyTorch (`conda`, `pip`, source): conda\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.9.5\r\n - CUDA/cuDNN version:\r\n - GPU models and configuration:On the CPU\r\n - Any other relevant information:\r\n\r\n## Additional context\r\n`torch.linalg.eigh` on `tensor.T@tensor` seg faults.", "context": "<issue_start><issue_comment>Title: INTERNAL ASSERT FAILED at \"/opt/conda/conda-bld/pytorch_1634272068185/work/aten/src/ATen/native/LinearAlgebraUtils.h\":244\nusername_0: ## \ud83d\udc1b Bug\r\n\r\nCalling `torch.linalg.svd` on a large float64 tensor on the CPU (size 90k x ~34k) leads to this error. `torch.linalg.eigh` on `tensor.T@tensor` seg faults.\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. I don't know. I have a large tensor of type float64. Calling `torch.linalg.svd`on it produces this error.\r\n\r\n```\r\nIntel MKL ERROR: Parameter 12 was incorrect on entry to DGESDD.\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n/tmp/ipykernel_3300/2679862595.py in <module>\r\n----> 1 torch.linalg.svd(tensor)\r\n\r\nRuntimeError: falseINTERNAL ASSERT FAILED at \"/opt/conda/conda-bld/pytorch_1634272068185/work/aten/src/ATen/native/LinearAlgebraUtils.h\":244, please report a bug to PyTorch. svd_cpu: Argument 12 has illegal value. Most certainly there is a bug in the implementation calling the backend library.\r\n```\r\n## Expected behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n## Environment\r\n\r\nPlease copy and paste the output from our\r\n[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)\r\n(or fill out the checklist below manually).\r\n\r\nYou can get the script and run it with:\r\n```\r\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\r\n# For security purposes, please check the contents of collect_env.py before running it.\r\npython collect_env.py\r\n```\r\n\r\n - PyTorch Version (e.g., 1.0): '1.10.0'\r\n - OS (e.g., Linux): Linux\r\n - How you installed PyTorch (`conda`, `pip`, source): conda\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.9.5\r\n - CUDA/cuDNN version:\r\n - GPU models and configuration:On the CPU\r\n - Any other relevant information:\r\n\r\n## Additional context\r\n`torch.linalg.eigh` on `tensor.T@tensor` seg faults.\n<issue_comment>username_0: I have been trying to figure out what is special about this tensor that is causing the algorithm to fail.\r\n\r\nI note that `torch.linalg.svdvals` is successful.\n<issue_comment>username_0: Ok, I am now able to replicate this problem using just `torch.rand` and `torch.linalg.svd`.\r\n\r\nThe following fails:\r\n```\r\nx = torch.rand(30000,30000)\r\nu,s,v = torch.linalg.svd(x)\r\n```\n<issue_comment>username_1: Thank you for the tiny repro! I can reproduce in master. cc @username_2\n<issue_comment>username_2: The problem is related to https://github.com/pytorch/pytorch/issues/51720.\r\n\r\nThe optimal workspace size for the input of this size is `2.70021e+09` which doesn't fit into `int` type. In PyTorch, we use a 32-bit interface to backend linear algebra libraries.\r\nWe can \"fix\" the internal assert with this patch\r\n```\r\ndiff --git a/aten/src/ATen/native/BatchLinearAlgebra.cpp b/aten/src/ATen/native/BatchLinearAlgebra.cpp\r\nindex 853f5c2cbe..8902958d98 100644\r\n--- a/aten/src/ATen/native/BatchLinearAlgebra.cpp\r\n+++ b/aten/src/ATen/native/BatchLinearAlgebra.cpp\r\n@@ -2984,7 +2984,9 @@ static void apply_svd(Tensor& self, Tensor& U, Tensor& S, Tensor& VT,\r\n   int lwork = -1;\r\n   scalar_t wkopt;\r\n   lapackSvd<scalar_t, value_t>(jobz, m, n, self_data, lda, S_data, U_data, lda, VT_data, ldvt, &wkopt, lwork, rwork_data, iwork_data, &info);\r\n-  lwork = std::max<int>(1, real_impl<scalar_t, value_t>(wkopt));\r\n+  TORCH_CHECK(\r\n+      (real_impl<scalar_t, value_t>(wkopt)) < INT_MAX,\r\n+      \"svd: The optimal workspace size is larger than allowed by 32-bit interface to backend math library.\");\r\n   Tensor work = at::empty({lwork}, self.options());\r\n   auto work_data = work.data_ptr<scalar_t>();\r\n```\r\nThe issue is likely there for other functions as well and we could include checks of the values before casting them to int.\n<issue_comment>username_0: @username_2 will there be any way around this in the future for large SVDs? Or is there an upper size limit due to the 32bit interface?\n<issue_comment>username_2: Unfortunately, there is no workaround available.\r\nFrom the MKL documentation the workspace size (`lwork`) can be computed using these guidelines:\r\n```\r\nFor real flavors:\r\n\r\nIf jobz = 'N', lwork \u2265 3*min(m, n) + max (max(m,n), 6*min(m, n));\r\n\r\nIf jobz = 'O', lwork \u2265 3*(min(m, n))2 + max (max(m, n), 5*(min(m, n))2 + 4*min(m, n));\r\n\r\nIf jobz = 'S' or 'A', lwork \u2265 3*(min(m, n))2 + max (max(m, n), 4*(min(m, n))2 + 4*min(m, n))\r\n\r\nFor complex flavors:\r\n\r\nIf jobz = 'N', lwork \u2265 2*min(m, n) + max(m, n);\r\n\r\nIf jobz = 'O', lwork \u2265 2*(min(m, n))2 + max(m, n) + 2*min(m, n);\r\n\r\nIf jobz = 'S' or 'A', lwork \u2265 (min(m, n))2 + max(m, n) + 2*min(m, n);\r\n```\r\n`lwork` must be smaller than `2147483647` (maximum `int`) in order to not hit the limitation of the 32-bit interface.\n<issue_comment>username_0: @username_2 Any clue as to why `svdvals` and `eigvalsh` work?\n<issue_comment>username_1: For what is worth, `svdvals` and `eigvalsh` may work in master when the input does not require gradients, as we do not compute the `U` and the `Vh` in those cases, so the working space needed is less than for the full SVD / eigendecomposition."}
{"repo": "pytorch/pytorch", "issue_id": 1052456616, "issue_number": 68291, "timestamp": "2021-11-15 22:05:51+00:00", "text": "I have been trying to figure out what is special about this tensor that is causing the algorithm to fail.\r\n\r\nI note that `torch.linalg.svdvals` is successful.", "context": "<issue_start><issue_comment>Title: INTERNAL ASSERT FAILED at \"/opt/conda/conda-bld/pytorch_1634272068185/work/aten/src/ATen/native/LinearAlgebraUtils.h\":244\nusername_0: ## \ud83d\udc1b Bug\r\n\r\nCalling `torch.linalg.svd` on a large float64 tensor on the CPU (size 90k x ~34k) leads to this error. `torch.linalg.eigh` on `tensor.T@tensor` seg faults.\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. I don't know. I have a large tensor of type float64. Calling `torch.linalg.svd`on it produces this error.\r\n\r\n```\r\nIntel MKL ERROR: Parameter 12 was incorrect on entry to DGESDD.\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n/tmp/ipykernel_3300/2679862595.py in <module>\r\n----> 1 torch.linalg.svd(tensor)\r\n\r\nRuntimeError: falseINTERNAL ASSERT FAILED at \"/opt/conda/conda-bld/pytorch_1634272068185/work/aten/src/ATen/native/LinearAlgebraUtils.h\":244, please report a bug to PyTorch. svd_cpu: Argument 12 has illegal value. Most certainly there is a bug in the implementation calling the backend library.\r\n```\r\n## Expected behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n## Environment\r\n\r\nPlease copy and paste the output from our\r\n[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)\r\n(or fill out the checklist below manually).\r\n\r\nYou can get the script and run it with:\r\n```\r\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\r\n# For security purposes, please check the contents of collect_env.py before running it.\r\npython collect_env.py\r\n```\r\n\r\n - PyTorch Version (e.g., 1.0): '1.10.0'\r\n - OS (e.g., Linux): Linux\r\n - How you installed PyTorch (`conda`, `pip`, source): conda\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.9.5\r\n - CUDA/cuDNN version:\r\n - GPU models and configuration:On the CPU\r\n - Any other relevant information:\r\n\r\n## Additional context\r\n`torch.linalg.eigh` on `tensor.T@tensor` seg faults.\n<issue_comment>username_0: I have been trying to figure out what is special about this tensor that is causing the algorithm to fail.\r\n\r\nI note that `torch.linalg.svdvals` is successful.\n<issue_comment>username_0: Ok, I am now able to replicate this problem using just `torch.rand` and `torch.linalg.svd`.\r\n\r\nThe following fails:\r\n```\r\nx = torch.rand(30000,30000)\r\nu,s,v = torch.linalg.svd(x)\r\n```\n<issue_comment>username_1: Thank you for the tiny repro! I can reproduce in master. cc @username_2\n<issue_comment>username_2: The problem is related to https://github.com/pytorch/pytorch/issues/51720.\r\n\r\nThe optimal workspace size for the input of this size is `2.70021e+09` which doesn't fit into `int` type. In PyTorch, we use a 32-bit interface to backend linear algebra libraries.\r\nWe can \"fix\" the internal assert with this patch\r\n```\r\ndiff --git a/aten/src/ATen/native/BatchLinearAlgebra.cpp b/aten/src/ATen/native/BatchLinearAlgebra.cpp\r\nindex 853f5c2cbe..8902958d98 100644\r\n--- a/aten/src/ATen/native/BatchLinearAlgebra.cpp\r\n+++ b/aten/src/ATen/native/BatchLinearAlgebra.cpp\r\n@@ -2984,7 +2984,9 @@ static void apply_svd(Tensor& self, Tensor& U, Tensor& S, Tensor& VT,\r\n   int lwork = -1;\r\n   scalar_t wkopt;\r\n   lapackSvd<scalar_t, value_t>(jobz, m, n, self_data, lda, S_data, U_data, lda, VT_data, ldvt, &wkopt, lwork, rwork_data, iwork_data, &info);\r\n-  lwork = std::max<int>(1, real_impl<scalar_t, value_t>(wkopt));\r\n+  TORCH_CHECK(\r\n+      (real_impl<scalar_t, value_t>(wkopt)) < INT_MAX,\r\n+      \"svd: The optimal workspace size is larger than allowed by 32-bit interface to backend math library.\");\r\n   Tensor work = at::empty({lwork}, self.options());\r\n   auto work_data = work.data_ptr<scalar_t>();\r\n```\r\nThe issue is likely there for other functions as well and we could include checks of the values before casting them to int.\n<issue_comment>username_0: @username_2 will there be any way around this in the future for large SVDs? Or is there an upper size limit due to the 32bit interface?\n<issue_comment>username_2: Unfortunately, there is no workaround available.\r\nFrom the MKL documentation the workspace size (`lwork`) can be computed using these guidelines:\r\n```\r\nFor real flavors:\r\n\r\nIf jobz = 'N', lwork \u2265 3*min(m, n) + max (max(m,n), 6*min(m, n));\r\n\r\nIf jobz = 'O', lwork \u2265 3*(min(m, n))2 + max (max(m, n), 5*(min(m, n))2 + 4*min(m, n));\r\n\r\nIf jobz = 'S' or 'A', lwork \u2265 3*(min(m, n))2 + max (max(m, n), 4*(min(m, n))2 + 4*min(m, n))\r\n\r\nFor complex flavors:\r\n\r\nIf jobz = 'N', lwork \u2265 2*min(m, n) + max(m, n);\r\n\r\nIf jobz = 'O', lwork \u2265 2*(min(m, n))2 + max(m, n) + 2*min(m, n);\r\n\r\nIf jobz = 'S' or 'A', lwork \u2265 (min(m, n))2 + max(m, n) + 2*min(m, n);\r\n```\r\n`lwork` must be smaller than `2147483647` (maximum `int`) in order to not hit the limitation of the 32-bit interface.\n<issue_comment>username_0: @username_2 Any clue as to why `svdvals` and `eigvalsh` work?\n<issue_comment>username_1: For what is worth, `svdvals` and `eigvalsh` may work in master when the input does not require gradients, as we do not compute the `U` and the `Vh` in those cases, so the working space needed is less than for the full SVD / eigendecomposition."}
{"repo": "pytorch/pytorch", "issue_id": 1052456616, "issue_number": 68291, "timestamp": "2021-11-15 22:51:50+00:00", "text": "Ok, I am now able to replicate this problem using just `torch.rand` and `torch.linalg.svd`.\r\n\r\nThe following fails:\r\n```\r\nx = torch.rand(30000,30000)\r\nu,s,v = torch.linalg.svd(x)\r\n```", "context": "<issue_start><issue_comment>Title: INTERNAL ASSERT FAILED at \"/opt/conda/conda-bld/pytorch_1634272068185/work/aten/src/ATen/native/LinearAlgebraUtils.h\":244\nusername_0: ## \ud83d\udc1b Bug\r\n\r\nCalling `torch.linalg.svd` on a large float64 tensor on the CPU (size 90k x ~34k) leads to this error. `torch.linalg.eigh` on `tensor.T@tensor` seg faults.\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. I don't know. I have a large tensor of type float64. Calling `torch.linalg.svd`on it produces this error.\r\n\r\n```\r\nIntel MKL ERROR: Parameter 12 was incorrect on entry to DGESDD.\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n/tmp/ipykernel_3300/2679862595.py in <module>\r\n----> 1 torch.linalg.svd(tensor)\r\n\r\nRuntimeError: falseINTERNAL ASSERT FAILED at \"/opt/conda/conda-bld/pytorch_1634272068185/work/aten/src/ATen/native/LinearAlgebraUtils.h\":244, please report a bug to PyTorch. svd_cpu: Argument 12 has illegal value. Most certainly there is a bug in the implementation calling the backend library.\r\n```\r\n## Expected behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n## Environment\r\n\r\nPlease copy and paste the output from our\r\n[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)\r\n(or fill out the checklist below manually).\r\n\r\nYou can get the script and run it with:\r\n```\r\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\r\n# For security purposes, please check the contents of collect_env.py before running it.\r\npython collect_env.py\r\n```\r\n\r\n - PyTorch Version (e.g., 1.0): '1.10.0'\r\n - OS (e.g., Linux): Linux\r\n - How you installed PyTorch (`conda`, `pip`, source): conda\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.9.5\r\n - CUDA/cuDNN version:\r\n - GPU models and configuration:On the CPU\r\n - Any other relevant information:\r\n\r\n## Additional context\r\n`torch.linalg.eigh` on `tensor.T@tensor` seg faults.\n<issue_comment>username_0: I have been trying to figure out what is special about this tensor that is causing the algorithm to fail.\r\n\r\nI note that `torch.linalg.svdvals` is successful.\n<issue_comment>username_0: Ok, I am now able to replicate this problem using just `torch.rand` and `torch.linalg.svd`.\r\n\r\nThe following fails:\r\n```\r\nx = torch.rand(30000,30000)\r\nu,s,v = torch.linalg.svd(x)\r\n```\n<issue_comment>username_1: Thank you for the tiny repro! I can reproduce in master. cc @username_2\n<issue_comment>username_2: The problem is related to https://github.com/pytorch/pytorch/issues/51720.\r\n\r\nThe optimal workspace size for the input of this size is `2.70021e+09` which doesn't fit into `int` type. In PyTorch, we use a 32-bit interface to backend linear algebra libraries.\r\nWe can \"fix\" the internal assert with this patch\r\n```\r\ndiff --git a/aten/src/ATen/native/BatchLinearAlgebra.cpp b/aten/src/ATen/native/BatchLinearAlgebra.cpp\r\nindex 853f5c2cbe..8902958d98 100644\r\n--- a/aten/src/ATen/native/BatchLinearAlgebra.cpp\r\n+++ b/aten/src/ATen/native/BatchLinearAlgebra.cpp\r\n@@ -2984,7 +2984,9 @@ static void apply_svd(Tensor& self, Tensor& U, Tensor& S, Tensor& VT,\r\n   int lwork = -1;\r\n   scalar_t wkopt;\r\n   lapackSvd<scalar_t, value_t>(jobz, m, n, self_data, lda, S_data, U_data, lda, VT_data, ldvt, &wkopt, lwork, rwork_data, iwork_data, &info);\r\n-  lwork = std::max<int>(1, real_impl<scalar_t, value_t>(wkopt));\r\n+  TORCH_CHECK(\r\n+      (real_impl<scalar_t, value_t>(wkopt)) < INT_MAX,\r\n+      \"svd: The optimal workspace size is larger than allowed by 32-bit interface to backend math library.\");\r\n   Tensor work = at::empty({lwork}, self.options());\r\n   auto work_data = work.data_ptr<scalar_t>();\r\n```\r\nThe issue is likely there for other functions as well and we could include checks of the values before casting them to int.\n<issue_comment>username_0: @username_2 will there be any way around this in the future for large SVDs? Or is there an upper size limit due to the 32bit interface?\n<issue_comment>username_2: Unfortunately, there is no workaround available.\r\nFrom the MKL documentation the workspace size (`lwork`) can be computed using these guidelines:\r\n```\r\nFor real flavors:\r\n\r\nIf jobz = 'N', lwork \u2265 3*min(m, n) + max (max(m,n), 6*min(m, n));\r\n\r\nIf jobz = 'O', lwork \u2265 3*(min(m, n))2 + max (max(m, n), 5*(min(m, n))2 + 4*min(m, n));\r\n\r\nIf jobz = 'S' or 'A', lwork \u2265 3*(min(m, n))2 + max (max(m, n), 4*(min(m, n))2 + 4*min(m, n))\r\n\r\nFor complex flavors:\r\n\r\nIf jobz = 'N', lwork \u2265 2*min(m, n) + max(m, n);\r\n\r\nIf jobz = 'O', lwork \u2265 2*(min(m, n))2 + max(m, n) + 2*min(m, n);\r\n\r\nIf jobz = 'S' or 'A', lwork \u2265 (min(m, n))2 + max(m, n) + 2*min(m, n);\r\n```\r\n`lwork` must be smaller than `2147483647` (maximum `int`) in order to not hit the limitation of the 32-bit interface.\n<issue_comment>username_0: @username_2 Any clue as to why `svdvals` and `eigvalsh` work?\n<issue_comment>username_1: For what is worth, `svdvals` and `eigvalsh` may work in master when the input does not require gradients, as we do not compute the `U` and the `Vh` in those cases, so the working space needed is less than for the full SVD / eigendecomposition."}
{"repo": "pytorch/pytorch", "issue_id": 1052456616, "issue_number": 68291, "timestamp": "2021-11-16 06:57:50+00:00", "text": "Thank you for the tiny repro! I can reproduce in master. cc @username_2", "context": "<issue_start><issue_comment>Title: INTERNAL ASSERT FAILED at \"/opt/conda/conda-bld/pytorch_1634272068185/work/aten/src/ATen/native/LinearAlgebraUtils.h\":244\nusername_0: ## \ud83d\udc1b Bug\r\n\r\nCalling `torch.linalg.svd` on a large float64 tensor on the CPU (size 90k x ~34k) leads to this error. `torch.linalg.eigh` on `tensor.T@tensor` seg faults.\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. I don't know. I have a large tensor of type float64. Calling `torch.linalg.svd`on it produces this error.\r\n\r\n```\r\nIntel MKL ERROR: Parameter 12 was incorrect on entry to DGESDD.\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n/tmp/ipykernel_3300/2679862595.py in <module>\r\n----> 1 torch.linalg.svd(tensor)\r\n\r\nRuntimeError: falseINTERNAL ASSERT FAILED at \"/opt/conda/conda-bld/pytorch_1634272068185/work/aten/src/ATen/native/LinearAlgebraUtils.h\":244, please report a bug to PyTorch. svd_cpu: Argument 12 has illegal value. Most certainly there is a bug in the implementation calling the backend library.\r\n```\r\n## Expected behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n## Environment\r\n\r\nPlease copy and paste the output from our\r\n[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)\r\n(or fill out the checklist below manually).\r\n\r\nYou can get the script and run it with:\r\n```\r\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\r\n# For security purposes, please check the contents of collect_env.py before running it.\r\npython collect_env.py\r\n```\r\n\r\n - PyTorch Version (e.g., 1.0): '1.10.0'\r\n - OS (e.g., Linux): Linux\r\n - How you installed PyTorch (`conda`, `pip`, source): conda\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.9.5\r\n - CUDA/cuDNN version:\r\n - GPU models and configuration:On the CPU\r\n - Any other relevant information:\r\n\r\n## Additional context\r\n`torch.linalg.eigh` on `tensor.T@tensor` seg faults.\n<issue_comment>username_0: I have been trying to figure out what is special about this tensor that is causing the algorithm to fail.\r\n\r\nI note that `torch.linalg.svdvals` is successful.\n<issue_comment>username_0: Ok, I am now able to replicate this problem using just `torch.rand` and `torch.linalg.svd`.\r\n\r\nThe following fails:\r\n```\r\nx = torch.rand(30000,30000)\r\nu,s,v = torch.linalg.svd(x)\r\n```\n<issue_comment>username_1: Thank you for the tiny repro! I can reproduce in master. cc @username_2\n<issue_comment>username_2: The problem is related to https://github.com/pytorch/pytorch/issues/51720.\r\n\r\nThe optimal workspace size for the input of this size is `2.70021e+09` which doesn't fit into `int` type. In PyTorch, we use a 32-bit interface to backend linear algebra libraries.\r\nWe can \"fix\" the internal assert with this patch\r\n```\r\ndiff --git a/aten/src/ATen/native/BatchLinearAlgebra.cpp b/aten/src/ATen/native/BatchLinearAlgebra.cpp\r\nindex 853f5c2cbe..8902958d98 100644\r\n--- a/aten/src/ATen/native/BatchLinearAlgebra.cpp\r\n+++ b/aten/src/ATen/native/BatchLinearAlgebra.cpp\r\n@@ -2984,7 +2984,9 @@ static void apply_svd(Tensor& self, Tensor& U, Tensor& S, Tensor& VT,\r\n   int lwork = -1;\r\n   scalar_t wkopt;\r\n   lapackSvd<scalar_t, value_t>(jobz, m, n, self_data, lda, S_data, U_data, lda, VT_data, ldvt, &wkopt, lwork, rwork_data, iwork_data, &info);\r\n-  lwork = std::max<int>(1, real_impl<scalar_t, value_t>(wkopt));\r\n+  TORCH_CHECK(\r\n+      (real_impl<scalar_t, value_t>(wkopt)) < INT_MAX,\r\n+      \"svd: The optimal workspace size is larger than allowed by 32-bit interface to backend math library.\");\r\n   Tensor work = at::empty({lwork}, self.options());\r\n   auto work_data = work.data_ptr<scalar_t>();\r\n```\r\nThe issue is likely there for other functions as well and we could include checks of the values before casting them to int.\n<issue_comment>username_0: @username_2 will there be any way around this in the future for large SVDs? Or is there an upper size limit due to the 32bit interface?\n<issue_comment>username_2: Unfortunately, there is no workaround available.\r\nFrom the MKL documentation the workspace size (`lwork`) can be computed using these guidelines:\r\n```\r\nFor real flavors:\r\n\r\nIf jobz = 'N', lwork \u2265 3*min(m, n) + max (max(m,n), 6*min(m, n));\r\n\r\nIf jobz = 'O', lwork \u2265 3*(min(m, n))2 + max (max(m, n), 5*(min(m, n))2 + 4*min(m, n));\r\n\r\nIf jobz = 'S' or 'A', lwork \u2265 3*(min(m, n))2 + max (max(m, n), 4*(min(m, n))2 + 4*min(m, n))\r\n\r\nFor complex flavors:\r\n\r\nIf jobz = 'N', lwork \u2265 2*min(m, n) + max(m, n);\r\n\r\nIf jobz = 'O', lwork \u2265 2*(min(m, n))2 + max(m, n) + 2*min(m, n);\r\n\r\nIf jobz = 'S' or 'A', lwork \u2265 (min(m, n))2 + max(m, n) + 2*min(m, n);\r\n```\r\n`lwork` must be smaller than `2147483647` (maximum `int`) in order to not hit the limitation of the 32-bit interface.\n<issue_comment>username_0: @username_2 Any clue as to why `svdvals` and `eigvalsh` work?\n<issue_comment>username_1: For what is worth, `svdvals` and `eigvalsh` may work in master when the input does not require gradients, as we do not compute the `U` and the `Vh` in those cases, so the working space needed is less than for the full SVD / eigendecomposition."}
{"repo": "pytorch/pytorch", "issue_id": 1052456616, "issue_number": 68291, "timestamp": "2021-11-16 08:26:42+00:00", "text": "The problem is related to https://github.com/pytorch/pytorch/issues/51720.\r\n\r\nThe optimal workspace size for the input of this size is `2.70021e+09` which doesn't fit into `int` type. In PyTorch, we use a 32-bit interface to backend linear algebra libraries.\r\nWe can \"fix\" the internal assert with this patch\r\n```\r\ndiff --git a/aten/src/ATen/native/BatchLinearAlgebra.cpp b/aten/src/ATen/native/BatchLinearAlgebra.cpp\r\nindex 853f5c2cbe..8902958d98 100644\r\n--- a/aten/src/ATen/native/BatchLinearAlgebra.cpp\r\n+++ b/aten/src/ATen/native/BatchLinearAlgebra.cpp\r\n@@ -2984,7 +2984,9 @@ static void apply_svd(Tensor& self, Tensor& U, Tensor& S, Tensor& VT,\r\n   int lwork = -1;\r\n   scalar_t wkopt;\r\n   lapackSvd<scalar_t, value_t>(jobz, m, n, self_data, lda, S_data, U_data, lda, VT_data, ldvt, &wkopt, lwork, rwork_data, iwork_data, &info);\r\n-  lwork = std::max<int>(1, real_impl<scalar_t, value_t>(wkopt));\r\n+  TORCH_CHECK(\r\n+      (real_impl<scalar_t, value_t>(wkopt)) < INT_MAX,\r\n+      \"svd: The optimal workspace size is larger than allowed by 32-bit interface to backend math library.\");\r\n   Tensor work = at::empty({lwork}, self.options());\r\n   auto work_data = work.data_ptr<scalar_t>();\r\n```\r\nThe issue is likely there for other functions as well and we could include checks of the values before casting them to int.", "context": "<issue_start><issue_comment>Title: INTERNAL ASSERT FAILED at \"/opt/conda/conda-bld/pytorch_1634272068185/work/aten/src/ATen/native/LinearAlgebraUtils.h\":244\nusername_0: ## \ud83d\udc1b Bug\r\n\r\nCalling `torch.linalg.svd` on a large float64 tensor on the CPU (size 90k x ~34k) leads to this error. `torch.linalg.eigh` on `tensor.T@tensor` seg faults.\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. I don't know. I have a large tensor of type float64. Calling `torch.linalg.svd`on it produces this error.\r\n\r\n```\r\nIntel MKL ERROR: Parameter 12 was incorrect on entry to DGESDD.\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n/tmp/ipykernel_3300/2679862595.py in <module>\r\n----> 1 torch.linalg.svd(tensor)\r\n\r\nRuntimeError: falseINTERNAL ASSERT FAILED at \"/opt/conda/conda-bld/pytorch_1634272068185/work/aten/src/ATen/native/LinearAlgebraUtils.h\":244, please report a bug to PyTorch. svd_cpu: Argument 12 has illegal value. Most certainly there is a bug in the implementation calling the backend library.\r\n```\r\n## Expected behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n## Environment\r\n\r\nPlease copy and paste the output from our\r\n[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)\r\n(or fill out the checklist below manually).\r\n\r\nYou can get the script and run it with:\r\n```\r\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\r\n# For security purposes, please check the contents of collect_env.py before running it.\r\npython collect_env.py\r\n```\r\n\r\n - PyTorch Version (e.g., 1.0): '1.10.0'\r\n - OS (e.g., Linux): Linux\r\n - How you installed PyTorch (`conda`, `pip`, source): conda\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.9.5\r\n - CUDA/cuDNN version:\r\n - GPU models and configuration:On the CPU\r\n - Any other relevant information:\r\n\r\n## Additional context\r\n`torch.linalg.eigh` on `tensor.T@tensor` seg faults.\n<issue_comment>username_0: I have been trying to figure out what is special about this tensor that is causing the algorithm to fail.\r\n\r\nI note that `torch.linalg.svdvals` is successful.\n<issue_comment>username_0: Ok, I am now able to replicate this problem using just `torch.rand` and `torch.linalg.svd`.\r\n\r\nThe following fails:\r\n```\r\nx = torch.rand(30000,30000)\r\nu,s,v = torch.linalg.svd(x)\r\n```\n<issue_comment>username_1: Thank you for the tiny repro! I can reproduce in master. cc @username_2\n<issue_comment>username_2: The problem is related to https://github.com/pytorch/pytorch/issues/51720.\r\n\r\nThe optimal workspace size for the input of this size is `2.70021e+09` which doesn't fit into `int` type. In PyTorch, we use a 32-bit interface to backend linear algebra libraries.\r\nWe can \"fix\" the internal assert with this patch\r\n```\r\ndiff --git a/aten/src/ATen/native/BatchLinearAlgebra.cpp b/aten/src/ATen/native/BatchLinearAlgebra.cpp\r\nindex 853f5c2cbe..8902958d98 100644\r\n--- a/aten/src/ATen/native/BatchLinearAlgebra.cpp\r\n+++ b/aten/src/ATen/native/BatchLinearAlgebra.cpp\r\n@@ -2984,7 +2984,9 @@ static void apply_svd(Tensor& self, Tensor& U, Tensor& S, Tensor& VT,\r\n   int lwork = -1;\r\n   scalar_t wkopt;\r\n   lapackSvd<scalar_t, value_t>(jobz, m, n, self_data, lda, S_data, U_data, lda, VT_data, ldvt, &wkopt, lwork, rwork_data, iwork_data, &info);\r\n-  lwork = std::max<int>(1, real_impl<scalar_t, value_t>(wkopt));\r\n+  TORCH_CHECK(\r\n+      (real_impl<scalar_t, value_t>(wkopt)) < INT_MAX,\r\n+      \"svd: The optimal workspace size is larger than allowed by 32-bit interface to backend math library.\");\r\n   Tensor work = at::empty({lwork}, self.options());\r\n   auto work_data = work.data_ptr<scalar_t>();\r\n```\r\nThe issue is likely there for other functions as well and we could include checks of the values before casting them to int.\n<issue_comment>username_0: @username_2 will there be any way around this in the future for large SVDs? Or is there an upper size limit due to the 32bit interface?\n<issue_comment>username_2: Unfortunately, there is no workaround available.\r\nFrom the MKL documentation the workspace size (`lwork`) can be computed using these guidelines:\r\n```\r\nFor real flavors:\r\n\r\nIf jobz = 'N', lwork \u2265 3*min(m, n) + max (max(m,n), 6*min(m, n));\r\n\r\nIf jobz = 'O', lwork \u2265 3*(min(m, n))2 + max (max(m, n), 5*(min(m, n))2 + 4*min(m, n));\r\n\r\nIf jobz = 'S' or 'A', lwork \u2265 3*(min(m, n))2 + max (max(m, n), 4*(min(m, n))2 + 4*min(m, n))\r\n\r\nFor complex flavors:\r\n\r\nIf jobz = 'N', lwork \u2265 2*min(m, n) + max(m, n);\r\n\r\nIf jobz = 'O', lwork \u2265 2*(min(m, n))2 + max(m, n) + 2*min(m, n);\r\n\r\nIf jobz = 'S' or 'A', lwork \u2265 (min(m, n))2 + max(m, n) + 2*min(m, n);\r\n```\r\n`lwork` must be smaller than `2147483647` (maximum `int`) in order to not hit the limitation of the 32-bit interface.\n<issue_comment>username_0: @username_2 Any clue as to why `svdvals` and `eigvalsh` work?\n<issue_comment>username_1: For what is worth, `svdvals` and `eigvalsh` may work in master when the input does not require gradients, as we do not compute the `U` and the `Vh` in those cases, so the working space needed is less than for the full SVD / eigendecomposition."}
{"repo": "pytorch/pytorch", "issue_id": 1052456616, "issue_number": 68291, "timestamp": "2021-11-16 13:18:24+00:00", "text": "@username_2 will there be any way around this in the future for large SVDs? Or is there an upper size limit due to the 32bit interface?", "context": "<issue_start><issue_comment>Title: INTERNAL ASSERT FAILED at \"/opt/conda/conda-bld/pytorch_1634272068185/work/aten/src/ATen/native/LinearAlgebraUtils.h\":244\nusername_0: ## \ud83d\udc1b Bug\r\n\r\nCalling `torch.linalg.svd` on a large float64 tensor on the CPU (size 90k x ~34k) leads to this error. `torch.linalg.eigh` on `tensor.T@tensor` seg faults.\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. I don't know. I have a large tensor of type float64. Calling `torch.linalg.svd`on it produces this error.\r\n\r\n```\r\nIntel MKL ERROR: Parameter 12 was incorrect on entry to DGESDD.\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n/tmp/ipykernel_3300/2679862595.py in <module>\r\n----> 1 torch.linalg.svd(tensor)\r\n\r\nRuntimeError: falseINTERNAL ASSERT FAILED at \"/opt/conda/conda-bld/pytorch_1634272068185/work/aten/src/ATen/native/LinearAlgebraUtils.h\":244, please report a bug to PyTorch. svd_cpu: Argument 12 has illegal value. Most certainly there is a bug in the implementation calling the backend library.\r\n```\r\n## Expected behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n## Environment\r\n\r\nPlease copy and paste the output from our\r\n[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)\r\n(or fill out the checklist below manually).\r\n\r\nYou can get the script and run it with:\r\n```\r\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\r\n# For security purposes, please check the contents of collect_env.py before running it.\r\npython collect_env.py\r\n```\r\n\r\n - PyTorch Version (e.g., 1.0): '1.10.0'\r\n - OS (e.g., Linux): Linux\r\n - How you installed PyTorch (`conda`, `pip`, source): conda\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.9.5\r\n - CUDA/cuDNN version:\r\n - GPU models and configuration:On the CPU\r\n - Any other relevant information:\r\n\r\n## Additional context\r\n`torch.linalg.eigh` on `tensor.T@tensor` seg faults.\n<issue_comment>username_0: I have been trying to figure out what is special about this tensor that is causing the algorithm to fail.\r\n\r\nI note that `torch.linalg.svdvals` is successful.\n<issue_comment>username_0: Ok, I am now able to replicate this problem using just `torch.rand` and `torch.linalg.svd`.\r\n\r\nThe following fails:\r\n```\r\nx = torch.rand(30000,30000)\r\nu,s,v = torch.linalg.svd(x)\r\n```\n<issue_comment>username_1: Thank you for the tiny repro! I can reproduce in master. cc @username_2\n<issue_comment>username_2: The problem is related to https://github.com/pytorch/pytorch/issues/51720.\r\n\r\nThe optimal workspace size for the input of this size is `2.70021e+09` which doesn't fit into `int` type. In PyTorch, we use a 32-bit interface to backend linear algebra libraries.\r\nWe can \"fix\" the internal assert with this patch\r\n```\r\ndiff --git a/aten/src/ATen/native/BatchLinearAlgebra.cpp b/aten/src/ATen/native/BatchLinearAlgebra.cpp\r\nindex 853f5c2cbe..8902958d98 100644\r\n--- a/aten/src/ATen/native/BatchLinearAlgebra.cpp\r\n+++ b/aten/src/ATen/native/BatchLinearAlgebra.cpp\r\n@@ -2984,7 +2984,9 @@ static void apply_svd(Tensor& self, Tensor& U, Tensor& S, Tensor& VT,\r\n   int lwork = -1;\r\n   scalar_t wkopt;\r\n   lapackSvd<scalar_t, value_t>(jobz, m, n, self_data, lda, S_data, U_data, lda, VT_data, ldvt, &wkopt, lwork, rwork_data, iwork_data, &info);\r\n-  lwork = std::max<int>(1, real_impl<scalar_t, value_t>(wkopt));\r\n+  TORCH_CHECK(\r\n+      (real_impl<scalar_t, value_t>(wkopt)) < INT_MAX,\r\n+      \"svd: The optimal workspace size is larger than allowed by 32-bit interface to backend math library.\");\r\n   Tensor work = at::empty({lwork}, self.options());\r\n   auto work_data = work.data_ptr<scalar_t>();\r\n```\r\nThe issue is likely there for other functions as well and we could include checks of the values before casting them to int.\n<issue_comment>username_0: @username_2 will there be any way around this in the future for large SVDs? Or is there an upper size limit due to the 32bit interface?\n<issue_comment>username_2: Unfortunately, there is no workaround available.\r\nFrom the MKL documentation the workspace size (`lwork`) can be computed using these guidelines:\r\n```\r\nFor real flavors:\r\n\r\nIf jobz = 'N', lwork \u2265 3*min(m, n) + max (max(m,n), 6*min(m, n));\r\n\r\nIf jobz = 'O', lwork \u2265 3*(min(m, n))2 + max (max(m, n), 5*(min(m, n))2 + 4*min(m, n));\r\n\r\nIf jobz = 'S' or 'A', lwork \u2265 3*(min(m, n))2 + max (max(m, n), 4*(min(m, n))2 + 4*min(m, n))\r\n\r\nFor complex flavors:\r\n\r\nIf jobz = 'N', lwork \u2265 2*min(m, n) + max(m, n);\r\n\r\nIf jobz = 'O', lwork \u2265 2*(min(m, n))2 + max(m, n) + 2*min(m, n);\r\n\r\nIf jobz = 'S' or 'A', lwork \u2265 (min(m, n))2 + max(m, n) + 2*min(m, n);\r\n```\r\n`lwork` must be smaller than `2147483647` (maximum `int`) in order to not hit the limitation of the 32-bit interface.\n<issue_comment>username_0: @username_2 Any clue as to why `svdvals` and `eigvalsh` work?\n<issue_comment>username_1: For what is worth, `svdvals` and `eigvalsh` may work in master when the input does not require gradients, as we do not compute the `U` and the `Vh` in those cases, so the working space needed is less than for the full SVD / eigendecomposition."}
{"repo": "pytorch/pytorch", "issue_id": 1052456616, "issue_number": 68291, "timestamp": "2021-11-16 13:35:01+00:00", "text": "Unfortunately, there is no workaround available.\r\nFrom the MKL documentation the workspace size (`lwork`) can be computed using these guidelines:\r\n```\r\nFor real flavors:\r\n\r\nIf jobz = 'N', lwork \u2265 3*min(m, n) + max (max(m,n), 6*min(m, n));\r\n\r\nIf jobz = 'O', lwork \u2265 3*(min(m, n))2 + max (max(m, n), 5*(min(m, n))2 + 4*min(m, n));\r\n\r\nIf jobz = 'S' or 'A', lwork \u2265 3*(min(m, n))2 + max (max(m, n), 4*(min(m, n))2 + 4*min(m, n))\r\n\r\nFor complex flavors:\r\n\r\nIf jobz = 'N', lwork \u2265 2*min(m, n) + max(m, n);\r\n\r\nIf jobz = 'O', lwork \u2265 2*(min(m, n))2 + max(m, n) + 2*min(m, n);\r\n\r\nIf jobz = 'S' or 'A', lwork \u2265 (min(m, n))2 + max(m, n) + 2*min(m, n);\r\n```\r\n`lwork` must be smaller than `2147483647` (maximum `int`) in order to not hit the limitation of the 32-bit interface.", "context": "<issue_start><issue_comment>Title: INTERNAL ASSERT FAILED at \"/opt/conda/conda-bld/pytorch_1634272068185/work/aten/src/ATen/native/LinearAlgebraUtils.h\":244\nusername_0: ## \ud83d\udc1b Bug\r\n\r\nCalling `torch.linalg.svd` on a large float64 tensor on the CPU (size 90k x ~34k) leads to this error. `torch.linalg.eigh` on `tensor.T@tensor` seg faults.\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. I don't know. I have a large tensor of type float64. Calling `torch.linalg.svd`on it produces this error.\r\n\r\n```\r\nIntel MKL ERROR: Parameter 12 was incorrect on entry to DGESDD.\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n/tmp/ipykernel_3300/2679862595.py in <module>\r\n----> 1 torch.linalg.svd(tensor)\r\n\r\nRuntimeError: falseINTERNAL ASSERT FAILED at \"/opt/conda/conda-bld/pytorch_1634272068185/work/aten/src/ATen/native/LinearAlgebraUtils.h\":244, please report a bug to PyTorch. svd_cpu: Argument 12 has illegal value. Most certainly there is a bug in the implementation calling the backend library.\r\n```\r\n## Expected behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n## Environment\r\n\r\nPlease copy and paste the output from our\r\n[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)\r\n(or fill out the checklist below manually).\r\n\r\nYou can get the script and run it with:\r\n```\r\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\r\n# For security purposes, please check the contents of collect_env.py before running it.\r\npython collect_env.py\r\n```\r\n\r\n - PyTorch Version (e.g., 1.0): '1.10.0'\r\n - OS (e.g., Linux): Linux\r\n - How you installed PyTorch (`conda`, `pip`, source): conda\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.9.5\r\n - CUDA/cuDNN version:\r\n - GPU models and configuration:On the CPU\r\n - Any other relevant information:\r\n\r\n## Additional context\r\n`torch.linalg.eigh` on `tensor.T@tensor` seg faults.\n<issue_comment>username_0: I have been trying to figure out what is special about this tensor that is causing the algorithm to fail.\r\n\r\nI note that `torch.linalg.svdvals` is successful.\n<issue_comment>username_0: Ok, I am now able to replicate this problem using just `torch.rand` and `torch.linalg.svd`.\r\n\r\nThe following fails:\r\n```\r\nx = torch.rand(30000,30000)\r\nu,s,v = torch.linalg.svd(x)\r\n```\n<issue_comment>username_1: Thank you for the tiny repro! I can reproduce in master. cc @username_2\n<issue_comment>username_2: The problem is related to https://github.com/pytorch/pytorch/issues/51720.\r\n\r\nThe optimal workspace size for the input of this size is `2.70021e+09` which doesn't fit into `int` type. In PyTorch, we use a 32-bit interface to backend linear algebra libraries.\r\nWe can \"fix\" the internal assert with this patch\r\n```\r\ndiff --git a/aten/src/ATen/native/BatchLinearAlgebra.cpp b/aten/src/ATen/native/BatchLinearAlgebra.cpp\r\nindex 853f5c2cbe..8902958d98 100644\r\n--- a/aten/src/ATen/native/BatchLinearAlgebra.cpp\r\n+++ b/aten/src/ATen/native/BatchLinearAlgebra.cpp\r\n@@ -2984,7 +2984,9 @@ static void apply_svd(Tensor& self, Tensor& U, Tensor& S, Tensor& VT,\r\n   int lwork = -1;\r\n   scalar_t wkopt;\r\n   lapackSvd<scalar_t, value_t>(jobz, m, n, self_data, lda, S_data, U_data, lda, VT_data, ldvt, &wkopt, lwork, rwork_data, iwork_data, &info);\r\n-  lwork = std::max<int>(1, real_impl<scalar_t, value_t>(wkopt));\r\n+  TORCH_CHECK(\r\n+      (real_impl<scalar_t, value_t>(wkopt)) < INT_MAX,\r\n+      \"svd: The optimal workspace size is larger than allowed by 32-bit interface to backend math library.\");\r\n   Tensor work = at::empty({lwork}, self.options());\r\n   auto work_data = work.data_ptr<scalar_t>();\r\n```\r\nThe issue is likely there for other functions as well and we could include checks of the values before casting them to int.\n<issue_comment>username_0: @username_2 will there be any way around this in the future for large SVDs? Or is there an upper size limit due to the 32bit interface?\n<issue_comment>username_2: Unfortunately, there is no workaround available.\r\nFrom the MKL documentation the workspace size (`lwork`) can be computed using these guidelines:\r\n```\r\nFor real flavors:\r\n\r\nIf jobz = 'N', lwork \u2265 3*min(m, n) + max (max(m,n), 6*min(m, n));\r\n\r\nIf jobz = 'O', lwork \u2265 3*(min(m, n))2 + max (max(m, n), 5*(min(m, n))2 + 4*min(m, n));\r\n\r\nIf jobz = 'S' or 'A', lwork \u2265 3*(min(m, n))2 + max (max(m, n), 4*(min(m, n))2 + 4*min(m, n))\r\n\r\nFor complex flavors:\r\n\r\nIf jobz = 'N', lwork \u2265 2*min(m, n) + max(m, n);\r\n\r\nIf jobz = 'O', lwork \u2265 2*(min(m, n))2 + max(m, n) + 2*min(m, n);\r\n\r\nIf jobz = 'S' or 'A', lwork \u2265 (min(m, n))2 + max(m, n) + 2*min(m, n);\r\n```\r\n`lwork` must be smaller than `2147483647` (maximum `int`) in order to not hit the limitation of the 32-bit interface.\n<issue_comment>username_0: @username_2 Any clue as to why `svdvals` and `eigvalsh` work?\n<issue_comment>username_1: For what is worth, `svdvals` and `eigvalsh` may work in master when the input does not require gradients, as we do not compute the `U` and the `Vh` in those cases, so the working space needed is less than for the full SVD / eigendecomposition."}
{"repo": "pytorch/pytorch", "issue_id": 1052456616, "issue_number": 68291, "timestamp": "2021-11-16 16:56:44+00:00", "text": "@username_2 Any clue as to why `svdvals` and `eigvalsh` work?", "context": "<issue_start><issue_comment>Title: INTERNAL ASSERT FAILED at \"/opt/conda/conda-bld/pytorch_1634272068185/work/aten/src/ATen/native/LinearAlgebraUtils.h\":244\nusername_0: ## \ud83d\udc1b Bug\r\n\r\nCalling `torch.linalg.svd` on a large float64 tensor on the CPU (size 90k x ~34k) leads to this error. `torch.linalg.eigh` on `tensor.T@tensor` seg faults.\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. I don't know. I have a large tensor of type float64. Calling `torch.linalg.svd`on it produces this error.\r\n\r\n```\r\nIntel MKL ERROR: Parameter 12 was incorrect on entry to DGESDD.\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n/tmp/ipykernel_3300/2679862595.py in <module>\r\n----> 1 torch.linalg.svd(tensor)\r\n\r\nRuntimeError: falseINTERNAL ASSERT FAILED at \"/opt/conda/conda-bld/pytorch_1634272068185/work/aten/src/ATen/native/LinearAlgebraUtils.h\":244, please report a bug to PyTorch. svd_cpu: Argument 12 has illegal value. Most certainly there is a bug in the implementation calling the backend library.\r\n```\r\n## Expected behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n## Environment\r\n\r\nPlease copy and paste the output from our\r\n[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)\r\n(or fill out the checklist below manually).\r\n\r\nYou can get the script and run it with:\r\n```\r\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\r\n# For security purposes, please check the contents of collect_env.py before running it.\r\npython collect_env.py\r\n```\r\n\r\n - PyTorch Version (e.g., 1.0): '1.10.0'\r\n - OS (e.g., Linux): Linux\r\n - How you installed PyTorch (`conda`, `pip`, source): conda\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.9.5\r\n - CUDA/cuDNN version:\r\n - GPU models and configuration:On the CPU\r\n - Any other relevant information:\r\n\r\n## Additional context\r\n`torch.linalg.eigh` on `tensor.T@tensor` seg faults.\n<issue_comment>username_0: I have been trying to figure out what is special about this tensor that is causing the algorithm to fail.\r\n\r\nI note that `torch.linalg.svdvals` is successful.\n<issue_comment>username_0: Ok, I am now able to replicate this problem using just `torch.rand` and `torch.linalg.svd`.\r\n\r\nThe following fails:\r\n```\r\nx = torch.rand(30000,30000)\r\nu,s,v = torch.linalg.svd(x)\r\n```\n<issue_comment>username_1: Thank you for the tiny repro! I can reproduce in master. cc @username_2\n<issue_comment>username_2: The problem is related to https://github.com/pytorch/pytorch/issues/51720.\r\n\r\nThe optimal workspace size for the input of this size is `2.70021e+09` which doesn't fit into `int` type. In PyTorch, we use a 32-bit interface to backend linear algebra libraries.\r\nWe can \"fix\" the internal assert with this patch\r\n```\r\ndiff --git a/aten/src/ATen/native/BatchLinearAlgebra.cpp b/aten/src/ATen/native/BatchLinearAlgebra.cpp\r\nindex 853f5c2cbe..8902958d98 100644\r\n--- a/aten/src/ATen/native/BatchLinearAlgebra.cpp\r\n+++ b/aten/src/ATen/native/BatchLinearAlgebra.cpp\r\n@@ -2984,7 +2984,9 @@ static void apply_svd(Tensor& self, Tensor& U, Tensor& S, Tensor& VT,\r\n   int lwork = -1;\r\n   scalar_t wkopt;\r\n   lapackSvd<scalar_t, value_t>(jobz, m, n, self_data, lda, S_data, U_data, lda, VT_data, ldvt, &wkopt, lwork, rwork_data, iwork_data, &info);\r\n-  lwork = std::max<int>(1, real_impl<scalar_t, value_t>(wkopt));\r\n+  TORCH_CHECK(\r\n+      (real_impl<scalar_t, value_t>(wkopt)) < INT_MAX,\r\n+      \"svd: The optimal workspace size is larger than allowed by 32-bit interface to backend math library.\");\r\n   Tensor work = at::empty({lwork}, self.options());\r\n   auto work_data = work.data_ptr<scalar_t>();\r\n```\r\nThe issue is likely there for other functions as well and we could include checks of the values before casting them to int.\n<issue_comment>username_0: @username_2 will there be any way around this in the future for large SVDs? Or is there an upper size limit due to the 32bit interface?\n<issue_comment>username_2: Unfortunately, there is no workaround available.\r\nFrom the MKL documentation the workspace size (`lwork`) can be computed using these guidelines:\r\n```\r\nFor real flavors:\r\n\r\nIf jobz = 'N', lwork \u2265 3*min(m, n) + max (max(m,n), 6*min(m, n));\r\n\r\nIf jobz = 'O', lwork \u2265 3*(min(m, n))2 + max (max(m, n), 5*(min(m, n))2 + 4*min(m, n));\r\n\r\nIf jobz = 'S' or 'A', lwork \u2265 3*(min(m, n))2 + max (max(m, n), 4*(min(m, n))2 + 4*min(m, n))\r\n\r\nFor complex flavors:\r\n\r\nIf jobz = 'N', lwork \u2265 2*min(m, n) + max(m, n);\r\n\r\nIf jobz = 'O', lwork \u2265 2*(min(m, n))2 + max(m, n) + 2*min(m, n);\r\n\r\nIf jobz = 'S' or 'A', lwork \u2265 (min(m, n))2 + max(m, n) + 2*min(m, n);\r\n```\r\n`lwork` must be smaller than `2147483647` (maximum `int`) in order to not hit the limitation of the 32-bit interface.\n<issue_comment>username_0: @username_2 Any clue as to why `svdvals` and `eigvalsh` work?\n<issue_comment>username_1: For what is worth, `svdvals` and `eigvalsh` may work in master when the input does not require gradients, as we do not compute the `U` and the `Vh` in those cases, so the working space needed is less than for the full SVD / eigendecomposition."}
{"repo": "pytorch/pytorch", "issue_id": 1052456616, "issue_number": 68291, "timestamp": "2022-02-08 18:27:41+00:00", "text": "For what is worth, `svdvals` and `eigvalsh` may work in master when the input does not require gradients, as we do not compute the `U` and the `Vh` in those cases, so the working space needed is less than for the full SVD / eigendecomposition.", "context": "<issue_start><issue_comment>Title: INTERNAL ASSERT FAILED at \"/opt/conda/conda-bld/pytorch_1634272068185/work/aten/src/ATen/native/LinearAlgebraUtils.h\":244\nusername_0: ## \ud83d\udc1b Bug\r\n\r\nCalling `torch.linalg.svd` on a large float64 tensor on the CPU (size 90k x ~34k) leads to this error. `torch.linalg.eigh` on `tensor.T@tensor` seg faults.\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. I don't know. I have a large tensor of type float64. Calling `torch.linalg.svd`on it produces this error.\r\n\r\n```\r\nIntel MKL ERROR: Parameter 12 was incorrect on entry to DGESDD.\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n/tmp/ipykernel_3300/2679862595.py in <module>\r\n----> 1 torch.linalg.svd(tensor)\r\n\r\nRuntimeError: falseINTERNAL ASSERT FAILED at \"/opt/conda/conda-bld/pytorch_1634272068185/work/aten/src/ATen/native/LinearAlgebraUtils.h\":244, please report a bug to PyTorch. svd_cpu: Argument 12 has illegal value. Most certainly there is a bug in the implementation calling the backend library.\r\n```\r\n## Expected behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n## Environment\r\n\r\nPlease copy and paste the output from our\r\n[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)\r\n(or fill out the checklist below manually).\r\n\r\nYou can get the script and run it with:\r\n```\r\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\r\n# For security purposes, please check the contents of collect_env.py before running it.\r\npython collect_env.py\r\n```\r\n\r\n - PyTorch Version (e.g., 1.0): '1.10.0'\r\n - OS (e.g., Linux): Linux\r\n - How you installed PyTorch (`conda`, `pip`, source): conda\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.9.5\r\n - CUDA/cuDNN version:\r\n - GPU models and configuration:On the CPU\r\n - Any other relevant information:\r\n\r\n## Additional context\r\n`torch.linalg.eigh` on `tensor.T@tensor` seg faults.\n<issue_comment>username_0: I have been trying to figure out what is special about this tensor that is causing the algorithm to fail.\r\n\r\nI note that `torch.linalg.svdvals` is successful.\n<issue_comment>username_0: Ok, I am now able to replicate this problem using just `torch.rand` and `torch.linalg.svd`.\r\n\r\nThe following fails:\r\n```\r\nx = torch.rand(30000,30000)\r\nu,s,v = torch.linalg.svd(x)\r\n```\n<issue_comment>username_1: Thank you for the tiny repro! I can reproduce in master. cc @username_2\n<issue_comment>username_2: The problem is related to https://github.com/pytorch/pytorch/issues/51720.\r\n\r\nThe optimal workspace size for the input of this size is `2.70021e+09` which doesn't fit into `int` type. In PyTorch, we use a 32-bit interface to backend linear algebra libraries.\r\nWe can \"fix\" the internal assert with this patch\r\n```\r\ndiff --git a/aten/src/ATen/native/BatchLinearAlgebra.cpp b/aten/src/ATen/native/BatchLinearAlgebra.cpp\r\nindex 853f5c2cbe..8902958d98 100644\r\n--- a/aten/src/ATen/native/BatchLinearAlgebra.cpp\r\n+++ b/aten/src/ATen/native/BatchLinearAlgebra.cpp\r\n@@ -2984,7 +2984,9 @@ static void apply_svd(Tensor& self, Tensor& U, Tensor& S, Tensor& VT,\r\n   int lwork = -1;\r\n   scalar_t wkopt;\r\n   lapackSvd<scalar_t, value_t>(jobz, m, n, self_data, lda, S_data, U_data, lda, VT_data, ldvt, &wkopt, lwork, rwork_data, iwork_data, &info);\r\n-  lwork = std::max<int>(1, real_impl<scalar_t, value_t>(wkopt));\r\n+  TORCH_CHECK(\r\n+      (real_impl<scalar_t, value_t>(wkopt)) < INT_MAX,\r\n+      \"svd: The optimal workspace size is larger than allowed by 32-bit interface to backend math library.\");\r\n   Tensor work = at::empty({lwork}, self.options());\r\n   auto work_data = work.data_ptr<scalar_t>();\r\n```\r\nThe issue is likely there for other functions as well and we could include checks of the values before casting them to int.\n<issue_comment>username_0: @username_2 will there be any way around this in the future for large SVDs? Or is there an upper size limit due to the 32bit interface?\n<issue_comment>username_2: Unfortunately, there is no workaround available.\r\nFrom the MKL documentation the workspace size (`lwork`) can be computed using these guidelines:\r\n```\r\nFor real flavors:\r\n\r\nIf jobz = 'N', lwork \u2265 3*min(m, n) + max (max(m,n), 6*min(m, n));\r\n\r\nIf jobz = 'O', lwork \u2265 3*(min(m, n))2 + max (max(m, n), 5*(min(m, n))2 + 4*min(m, n));\r\n\r\nIf jobz = 'S' or 'A', lwork \u2265 3*(min(m, n))2 + max (max(m, n), 4*(min(m, n))2 + 4*min(m, n))\r\n\r\nFor complex flavors:\r\n\r\nIf jobz = 'N', lwork \u2265 2*min(m, n) + max(m, n);\r\n\r\nIf jobz = 'O', lwork \u2265 2*(min(m, n))2 + max(m, n) + 2*min(m, n);\r\n\r\nIf jobz = 'S' or 'A', lwork \u2265 (min(m, n))2 + max(m, n) + 2*min(m, n);\r\n```\r\n`lwork` must be smaller than `2147483647` (maximum `int`) in order to not hit the limitation of the 32-bit interface.\n<issue_comment>username_0: @username_2 Any clue as to why `svdvals` and `eigvalsh` work?\n<issue_comment>username_1: For what is worth, `svdvals` and `eigvalsh` may work in master when the input does not require gradients, as we do not compute the `U` and the `Vh` in those cases, so the working space needed is less than for the full SVD / eigendecomposition."}
{"repo": "pytorch/pytorch", "issue_id": 1148648561, "issue_number": 73321, "timestamp": "2022-02-23T22:30:45Z", "text": "Stack from [ghstack](https://github.com/username_1/ghstack):\n* (to be filled)\n\nSimple refactoring change to ensure that methods with large bodies don't show up in `.h` (header) files. Instead they should be in `.cpp` (source) files to prevent slowing down compilation speed.\n\n#accept2ship\n\nDifferential Revision: [D34432507](https://our.internmc.facebook.com/intern/diff/D34432507/)", "context": "<issue_start><issue_comment>Title: [PyTorch] [Model Tracer] Refactor method bodies in various record function handlers into source (.cpp files)\nusername_0: Stack from [ghstack](https://github.com/username_1/ghstack):\n* (to be filled)\n\nSimple refactoring change to ensure that methods with large bodies don't show up in `.h` (header) files. Instead they should be in `.cpp` (source) files to prevent slowing down compilation speed.\n\n#accept2ship\n\nDifferential Revision: [D34432507](https://our.internmc.facebook.com/intern/diff/D34432507/)\n<issue_comment>username_1: @username_1 has imported this pull request.  If you are a Facebook employee, you can view this diff [on Phabricator](https://www.internalfb.com/diff/D34432507)."}
{"repo": "pytorch/pytorch", "issue_id": 1148648561, "issue_number": 73321, "timestamp": "2022-02-24 04:43:48+00:00", "text": "@username_1 has imported this pull request.  If you are a Facebook employee, you can view this diff [on Phabricator](https://www.internalfb.com/diff/D34432507).", "context": "<issue_start><issue_comment>Title: [PyTorch] [Model Tracer] Refactor method bodies in various record function handlers into source (.cpp files)\nusername_0: Stack from [ghstack](https://github.com/username_1/ghstack):\n* (to be filled)\n\nSimple refactoring change to ensure that methods with large bodies don't show up in `.h` (header) files. Instead they should be in `.cpp` (source) files to prevent slowing down compilation speed.\n\n#accept2ship\n\nDifferential Revision: [D34432507](https://our.internmc.facebook.com/intern/diff/D34432507/)\n<issue_comment>username_1: @username_1 has imported this pull request.  If you are a Facebook employee, you can view this diff [on Phabricator](https://www.internalfb.com/diff/D34432507)."}
{"repo": "facebook/react", "issue_id": 162356676, "issue_number": 7126, "timestamp": "2016-06-27T00:38:11Z", "text": "When merging, I didn\u2019t notice it uses old test format.\r\nFixing these to unbreak master.", "context": "<issue_start><issue_comment>Title: Fix tests from #6158 to use Jasmine 2\nusername_0: When merging, I didn\u2019t notice it uses old test format.\r\nFixing these to unbreak master.\n<issue_comment>username_1: (Not technically semver-minor on its own but bumping back to that since it otherwise thwarts my attempt to run a release with only semver-patch & semver-exempt changes)"}
{"repo": "facebook/react", "issue_id": 162356676, "issue_number": 7126, "timestamp": "2016-07-08 00:20:58+00:00", "text": "(Not technically semver-minor on its own but bumping back to that since it otherwise thwarts my attempt to run a release with only semver-patch & semver-exempt changes)", "context": "<issue_start><issue_comment>Title: Fix tests from #6158 to use Jasmine 2\nusername_0: When merging, I didn\u2019t notice it uses old test format.\r\nFixing these to unbreak master.\n<issue_comment>username_1: (Not technically semver-minor on its own but bumping back to that since it otherwise thwarts my attempt to run a release with only semver-patch & semver-exempt changes)"}
{"repo": "facebook/react", "issue_id": 259881446, "issue_number": 10781, "timestamp": "2017-09-22T17:17:50Z", "text": "**Before submitting a pull request,** please make sure the following is done:\r\n\r\n1. Fork [the repository](https://github.com/facebook/react) and create your branch from `master`.\r\n2. If you've added code that should be tested, add tests!\r\n3. If you've changed APIs, update the documentation.\r\n4. Ensure the test suite passes (`npm test`).\r\n5. Format your code with [prettier](https://github.com/prettier/prettier) (`npm run prettier`).\r\n6. Make sure your code lints (`npm run lint`).\r\n7. Run the [Flow](https://flowtype.org/) typechecks (`npm run flow`).\r\n8. If you haven't already, complete the CLA.", "context": "<issue_start><issue_comment>Title: Update conferences\nusername_0: **Before submitting a pull request,** please make sure the following is done:\r\n\r\n1. Fork [the repository](https://github.com/facebook/react) and create your branch from `master`.\r\n2. If you've added code that should be tested, add tests!\r\n3. If you've changed APIs, update the documentation.\r\n4. Ensure the test suite passes (`npm test`).\r\n5. Format your code with [prettier](https://github.com/prettier/prettier) (`npm run prettier`).\r\n6. Make sure your code lints (`npm run lint`).\r\n7. Run the [Flow](https://flowtype.org/) typechecks (`npm run flow`).\r\n8. If you haven't already, complete the CLA.\n<issue_comment>username_1: React Boston and React Alicante also ended.\n<issue_comment>username_0: Thanks!"}
{"repo": "facebook/react", "issue_id": 259881446, "issue_number": 10781, "timestamp": "2017-09-30 22:38:22+00:00", "text": "React Boston and React Alicante also ended.", "context": "<issue_start><issue_comment>Title: Update conferences\nusername_0: **Before submitting a pull request,** please make sure the following is done:\r\n\r\n1. Fork [the repository](https://github.com/facebook/react) and create your branch from `master`.\r\n2. If you've added code that should be tested, add tests!\r\n3. If you've changed APIs, update the documentation.\r\n4. Ensure the test suite passes (`npm test`).\r\n5. Format your code with [prettier](https://github.com/prettier/prettier) (`npm run prettier`).\r\n6. Make sure your code lints (`npm run lint`).\r\n7. Run the [Flow](https://flowtype.org/) typechecks (`npm run flow`).\r\n8. If you haven't already, complete the CLA.\n<issue_comment>username_1: React Boston and React Alicante also ended.\n<issue_comment>username_0: Thanks!"}
{"repo": "facebook/react", "issue_id": 259881446, "issue_number": 10781, "timestamp": "2017-10-03 13:33:40+00:00", "text": "Thanks!", "context": "<issue_start><issue_comment>Title: Update conferences\nusername_0: **Before submitting a pull request,** please make sure the following is done:\r\n\r\n1. Fork [the repository](https://github.com/facebook/react) and create your branch from `master`.\r\n2. If you've added code that should be tested, add tests!\r\n3. If you've changed APIs, update the documentation.\r\n4. Ensure the test suite passes (`npm test`).\r\n5. Format your code with [prettier](https://github.com/prettier/prettier) (`npm run prettier`).\r\n6. Make sure your code lints (`npm run lint`).\r\n7. Run the [Flow](https://flowtype.org/) typechecks (`npm run flow`).\r\n8. If you haven't already, complete the CLA.\n<issue_comment>username_1: React Boston and React Alicante also ended.\n<issue_comment>username_0: Thanks!"}
{"repo": "pytorch/pytorch", "issue_id": 304991946, "issue_number": 5761, "timestamp": "2018-03-14 01:29:09+00:00", "text": "- OS: Ubuntu 16.04\r\n- PyTorch version: torch-0.3.1-cp36-cp36-linux_x86_64.whl\r\n- How you installed PyTorch (conda, pip, source): pip3 from the binary\r\n- Python version: 3.6\r\n- CUDA/cuDNN version: 9.1\r\n- GPU models and configuration: GTX 1070 Ti\r\n\r\nAfter install using the wheel file, I have changed the torch/utils/data/Dataloader.py according to the changes shown at https://github.com/pytorch/pytorch/pull/4643\r\n\r\nBut the same error still happen when I try the tutorial example to load image database:\r\n\r\nException ignored in: <bound method DataLoaderIter.__del__ of <torch.utils.data.dataloader.DataLoaderIter object at 0x56090c8c7348>>\r\nTraceback (most recent call last):\r\n  File \"/home/username_0/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 375, in __del__\r\n  File \"/home/username_0/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 361, in _shutdown_workers\r\n  File \"/home/username_0/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 39, in _get_from_queue\r\n  File \"/usr/pkg/lib/python3.6/multiprocessing/queues.py\", line 337, in get\r\nImportError: sys.meta_path is None, Python is likely shutting down\r\n\r\nSo could anyone help me on that? Or is there any other file I should modify to get it run?", "context": "<issue_start><issue_comment>Title: Problem in DataLoader (same problem with #4643, but not solved by it)\nusername_0: - OS: Ubuntu 16.04\r\n- PyTorch version: torch-0.3.1-cp36-cp36-linux_x86_64.whl\r\n- How you installed PyTorch (conda, pip, source): pip3 from the binary\r\n- Python version: 3.6\r\n- CUDA/cuDNN version: 9.1\r\n- GPU models and configuration: GTX 1070 Ti\r\n\r\nAfter install using the wheel file, I have changed the torch/utils/data/Dataloader.py according to the changes shown at https://github.com/pytorch/pytorch/pull/4643\r\n\r\nBut the same error still happen when I try the tutorial example to load image database:\r\n\r\nException ignored in: <bound method DataLoaderIter.__del__ of <torch.utils.data.dataloader.DataLoaderIter object at 0x56090c8c7348>>\r\nTraceback (most recent call last):\r\n  File \"/home/username_0/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 375, in __del__\r\n  File \"/home/username_0/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 361, in _shutdown_workers\r\n  File \"/home/username_0/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 39, in _get_from_queue\r\n  File \"/usr/pkg/lib/python3.6/multiprocessing/queues.py\", line 337, in get\r\nImportError: sys.meta_path is None, Python is likely shutting down\r\n\r\nSo could anyone help me on that? Or is there any other file I should modify to get it run?\n<issue_comment>username_0: And, I am not sure how can I use the changes in torch/csrc/DataLoader.cpp, should I clone all the source code on GitHub and build the newest binary to fully fix this my problem?\n<issue_comment>username_1: when does this error happen? is it happening when you exit python?\n<issue_comment>username_2: @username_1 and @username_3,\r\nI am running this:\r\n\r\npython train.py --dataroot ./datasets/horse2zebra --name horse2zebra_cyclegan --model cycle_gan --no_dropout --loadSize 64 --nThreads 1\r\n\r\nfrom \r\n\r\nhttps://github.com/junyanz/pytorch-CycleGAN-and-pix2pix\r\n\r\nThe model is succefully created:\r\nmodel [CycleGANModel] was created\r\n\r\nI am on ubuntu 16.04, pytorch 3.1, cuda 9.0 and cuDnn 7.1\r\n\r\nThe error I get is traced below:\r\ncreate web directory ./checkpoints/horse2zebra_cyclegan/web...\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 22, in <module>\r\n    for i, data in enumerate(dataset):\r\n  File \"/home/sam/Documents/ComputerVision/pytorch-CycleGAN-and-pix2pix/data/custom_dataset_data_loader.py\", line 44, in __iter__\r\n    for i, data in enumerate(self.dataloader):\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 275, in __next__\r\n    idx, batch = self._get_batch()\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 254, in _get_batch\r\n    return self.data_queue.get()\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/queues.py\", line 335, in get\r\n    res = self._reader.recv_bytes()\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\r\n    buf = self._recv_bytes(maxlength)\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\r\n    buf = self._recv(4)\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\r\n    chunk = read(handle, remaining)\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 175, in handler\r\n    _error_if_any_worker_fails()\r\nRuntimeError: DataLoader worker (pid 3792) is killed by signal: Illegal instruction.\n<issue_comment>username_3: @username_2 that error is because you are running it on a computer that is too old and does not have SSE4 instruction on CPU.\n<issue_comment>username_1: @username_2 Also, it is more likely due to that your CPU doesn't support the operations done in the dataset.__getitem__.\n<issue_comment>username_2: @username_1 & @username_3,\r\n\r\nSome months ago I made it work in windows10 (I think)\r\nThe my windows machine crashed. So here I am. Sorry\r\n\r\nHere is what I see in cat /proc/cpuinfo:\r\n\r\nprocessor\t: 0\r\nvendor_id\t: AuthenticAMD\r\ncpu family\t: 21\r\nmodel\t\t: 2\r\nmodel name\t: AMD FX(tm)-4300 Quad-Core Processor\r\nstepping\t: 0\r\nmicrocode\t: 0x600081c\r\ncpu MHz\t\t: 1400.000\r\ncache size\t: 2048 KB\r\nphysical id\t: 0\r\nsiblings\t: 4\r\ncore id\t\t: 0\r\ncpu cores\t: 2\r\napicid\t\t: 16\r\ninitial apicid\t: 0\r\nfpu\t\t: yes\r\nfpu_exception\t: yes\r\ncpuid level\t: 13\r\nwp\t\t: yes\r\nflags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf pni pclmulqdq monitor ssse3 fma cx16 **sse4_1 sse4_2** popcnt aes xsave avx f16c lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs xop skinit wdt lwp fma4 tce nodeid_msr tbm topoext perfctr_core perfctr_nb cpb hw_pstate retpoline retpoline_amd rsb_ctxsw vmmcall bmi1 arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold\r\nbugs\t\t: fxsave_leak sysret_ss_attrs null_seg spectre_v1 spectre_v2\r\nbogomips\t: 7634.15\r\nTLB size\t: 1536 4K pages\r\nclflush size\t: 64\r\ncache_alignment\t: 64\r\naddress sizes\t: 48 bits physical, 48 bits virtual\r\npower management: ts ttp tm 100mhzsteps hwpstate cpb eff_freq_ro\r\n\r\n@username_1 how do I check if CPU support dataset.getitem?\n<issue_comment>username_1: @username_2 run with `nThreads=0`\n<issue_comment>username_2: tried it with nThreads=0\r\ndifferent error...\r\nmodel [CycleGANModel] was created\r\ncreate web directory ./checkpoints/horse2zebra_cyclegan/web...\r\nIllegal instruction (core dumped)\n<issue_comment>username_2: I see something I am uneasy about:\r\nsee bold below\r\n\r\nCustomDatasetDataLoader\r\ndataset [UnalignedDataset] was created\r\n/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torchvision-0.2.0-py3.6.egg/torchvision/transforms/transforms.py:156: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\r\n#training images = 1334\r\ncycle_gan\r\n**initialization method [normal]\r\ninitialization method [normal]\r\ninitialization method [normal]\r\ninitialization method [normal]**\n<issue_comment>username_1: @username_2 No it's same error. We just wrap it in a nice way so you can see it even if it's in the workers. \r\n\r\nIsn't that expected? CycleGAN has four networks: D_X D_Y G F .\n<issue_comment>username_1: @username_2 anyways, it confirms my hypothesis that your CPU doesn't support the operation in `dataset.__getitem__`.\n<issue_comment>username_2: Oh... Can you suggest a workaround?\n<issue_comment>username_1: @username_2 You can try to put debug statements in CycleGAN code and see which one isn't supported by your CPU (probably some PIL operations), and search for replacements. Sorry I don't have much else to offer.\n<issue_comment>username_2: @username_1 \r\nThank you sir !<issue_closed>\n<issue_comment>username_1: closed due to inactivity. feel free to reopen if you can reproduce with latest stable (0.4 at the moment of writing).\n<issue_comment>username_4: Still can't get this to work.\r\n\r\nhttps://stackoverflow.com/questions/53449927/runtimeerror-dataloader-worker-is-killed-by-signal-illegal-instruction\r\n\r\nPlease help."}
{"repo": "pytorch/pytorch", "issue_id": 304991946, "issue_number": 5761, "timestamp": "2018-03-14 01:38:31+00:00", "text": "And, I am not sure how can I use the changes in torch/csrc/DataLoader.cpp, should I clone all the source code on GitHub and build the newest binary to fully fix this my problem?", "context": "<issue_start><issue_comment>Title: Problem in DataLoader (same problem with #4643, but not solved by it)\nusername_0: - OS: Ubuntu 16.04\r\n- PyTorch version: torch-0.3.1-cp36-cp36-linux_x86_64.whl\r\n- How you installed PyTorch (conda, pip, source): pip3 from the binary\r\n- Python version: 3.6\r\n- CUDA/cuDNN version: 9.1\r\n- GPU models and configuration: GTX 1070 Ti\r\n\r\nAfter install using the wheel file, I have changed the torch/utils/data/Dataloader.py according to the changes shown at https://github.com/pytorch/pytorch/pull/4643\r\n\r\nBut the same error still happen when I try the tutorial example to load image database:\r\n\r\nException ignored in: <bound method DataLoaderIter.__del__ of <torch.utils.data.dataloader.DataLoaderIter object at 0x56090c8c7348>>\r\nTraceback (most recent call last):\r\n  File \"/home/username_0/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 375, in __del__\r\n  File \"/home/username_0/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 361, in _shutdown_workers\r\n  File \"/home/username_0/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 39, in _get_from_queue\r\n  File \"/usr/pkg/lib/python3.6/multiprocessing/queues.py\", line 337, in get\r\nImportError: sys.meta_path is None, Python is likely shutting down\r\n\r\nSo could anyone help me on that? Or is there any other file I should modify to get it run?\n<issue_comment>username_0: And, I am not sure how can I use the changes in torch/csrc/DataLoader.cpp, should I clone all the source code on GitHub and build the newest binary to fully fix this my problem?\n<issue_comment>username_1: when does this error happen? is it happening when you exit python?\n<issue_comment>username_2: @username_1 and @username_3,\r\nI am running this:\r\n\r\npython train.py --dataroot ./datasets/horse2zebra --name horse2zebra_cyclegan --model cycle_gan --no_dropout --loadSize 64 --nThreads 1\r\n\r\nfrom \r\n\r\nhttps://github.com/junyanz/pytorch-CycleGAN-and-pix2pix\r\n\r\nThe model is succefully created:\r\nmodel [CycleGANModel] was created\r\n\r\nI am on ubuntu 16.04, pytorch 3.1, cuda 9.0 and cuDnn 7.1\r\n\r\nThe error I get is traced below:\r\ncreate web directory ./checkpoints/horse2zebra_cyclegan/web...\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 22, in <module>\r\n    for i, data in enumerate(dataset):\r\n  File \"/home/sam/Documents/ComputerVision/pytorch-CycleGAN-and-pix2pix/data/custom_dataset_data_loader.py\", line 44, in __iter__\r\n    for i, data in enumerate(self.dataloader):\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 275, in __next__\r\n    idx, batch = self._get_batch()\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 254, in _get_batch\r\n    return self.data_queue.get()\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/queues.py\", line 335, in get\r\n    res = self._reader.recv_bytes()\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\r\n    buf = self._recv_bytes(maxlength)\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\r\n    buf = self._recv(4)\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\r\n    chunk = read(handle, remaining)\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 175, in handler\r\n    _error_if_any_worker_fails()\r\nRuntimeError: DataLoader worker (pid 3792) is killed by signal: Illegal instruction.\n<issue_comment>username_3: @username_2 that error is because you are running it on a computer that is too old and does not have SSE4 instruction on CPU.\n<issue_comment>username_1: @username_2 Also, it is more likely due to that your CPU doesn't support the operations done in the dataset.__getitem__.\n<issue_comment>username_2: @username_1 & @username_3,\r\n\r\nSome months ago I made it work in windows10 (I think)\r\nThe my windows machine crashed. So here I am. Sorry\r\n\r\nHere is what I see in cat /proc/cpuinfo:\r\n\r\nprocessor\t: 0\r\nvendor_id\t: AuthenticAMD\r\ncpu family\t: 21\r\nmodel\t\t: 2\r\nmodel name\t: AMD FX(tm)-4300 Quad-Core Processor\r\nstepping\t: 0\r\nmicrocode\t: 0x600081c\r\ncpu MHz\t\t: 1400.000\r\ncache size\t: 2048 KB\r\nphysical id\t: 0\r\nsiblings\t: 4\r\ncore id\t\t: 0\r\ncpu cores\t: 2\r\napicid\t\t: 16\r\ninitial apicid\t: 0\r\nfpu\t\t: yes\r\nfpu_exception\t: yes\r\ncpuid level\t: 13\r\nwp\t\t: yes\r\nflags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf pni pclmulqdq monitor ssse3 fma cx16 **sse4_1 sse4_2** popcnt aes xsave avx f16c lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs xop skinit wdt lwp fma4 tce nodeid_msr tbm topoext perfctr_core perfctr_nb cpb hw_pstate retpoline retpoline_amd rsb_ctxsw vmmcall bmi1 arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold\r\nbugs\t\t: fxsave_leak sysret_ss_attrs null_seg spectre_v1 spectre_v2\r\nbogomips\t: 7634.15\r\nTLB size\t: 1536 4K pages\r\nclflush size\t: 64\r\ncache_alignment\t: 64\r\naddress sizes\t: 48 bits physical, 48 bits virtual\r\npower management: ts ttp tm 100mhzsteps hwpstate cpb eff_freq_ro\r\n\r\n@username_1 how do I check if CPU support dataset.getitem?\n<issue_comment>username_1: @username_2 run with `nThreads=0`\n<issue_comment>username_2: tried it with nThreads=0\r\ndifferent error...\r\nmodel [CycleGANModel] was created\r\ncreate web directory ./checkpoints/horse2zebra_cyclegan/web...\r\nIllegal instruction (core dumped)\n<issue_comment>username_2: I see something I am uneasy about:\r\nsee bold below\r\n\r\nCustomDatasetDataLoader\r\ndataset [UnalignedDataset] was created\r\n/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torchvision-0.2.0-py3.6.egg/torchvision/transforms/transforms.py:156: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\r\n#training images = 1334\r\ncycle_gan\r\n**initialization method [normal]\r\ninitialization method [normal]\r\ninitialization method [normal]\r\ninitialization method [normal]**\n<issue_comment>username_1: @username_2 No it's same error. We just wrap it in a nice way so you can see it even if it's in the workers. \r\n\r\nIsn't that expected? CycleGAN has four networks: D_X D_Y G F .\n<issue_comment>username_1: @username_2 anyways, it confirms my hypothesis that your CPU doesn't support the operation in `dataset.__getitem__`.\n<issue_comment>username_2: Oh... Can you suggest a workaround?\n<issue_comment>username_1: @username_2 You can try to put debug statements in CycleGAN code and see which one isn't supported by your CPU (probably some PIL operations), and search for replacements. Sorry I don't have much else to offer.\n<issue_comment>username_2: @username_1 \r\nThank you sir !<issue_closed>\n<issue_comment>username_1: closed due to inactivity. feel free to reopen if you can reproduce with latest stable (0.4 at the moment of writing).\n<issue_comment>username_4: Still can't get this to work.\r\n\r\nhttps://stackoverflow.com/questions/53449927/runtimeerror-dataloader-worker-is-killed-by-signal-illegal-instruction\r\n\r\nPlease help."}
{"repo": "pytorch/pytorch", "issue_id": 304991946, "issue_number": 5761, "timestamp": "2018-03-16 14:45:31+00:00", "text": "when does this error happen? is it happening when you exit python?", "context": "<issue_start><issue_comment>Title: Problem in DataLoader (same problem with #4643, but not solved by it)\nusername_0: - OS: Ubuntu 16.04\r\n- PyTorch version: torch-0.3.1-cp36-cp36-linux_x86_64.whl\r\n- How you installed PyTorch (conda, pip, source): pip3 from the binary\r\n- Python version: 3.6\r\n- CUDA/cuDNN version: 9.1\r\n- GPU models and configuration: GTX 1070 Ti\r\n\r\nAfter install using the wheel file, I have changed the torch/utils/data/Dataloader.py according to the changes shown at https://github.com/pytorch/pytorch/pull/4643\r\n\r\nBut the same error still happen when I try the tutorial example to load image database:\r\n\r\nException ignored in: <bound method DataLoaderIter.__del__ of <torch.utils.data.dataloader.DataLoaderIter object at 0x56090c8c7348>>\r\nTraceback (most recent call last):\r\n  File \"/home/username_0/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 375, in __del__\r\n  File \"/home/username_0/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 361, in _shutdown_workers\r\n  File \"/home/username_0/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 39, in _get_from_queue\r\n  File \"/usr/pkg/lib/python3.6/multiprocessing/queues.py\", line 337, in get\r\nImportError: sys.meta_path is None, Python is likely shutting down\r\n\r\nSo could anyone help me on that? Or is there any other file I should modify to get it run?\n<issue_comment>username_0: And, I am not sure how can I use the changes in torch/csrc/DataLoader.cpp, should I clone all the source code on GitHub and build the newest binary to fully fix this my problem?\n<issue_comment>username_1: when does this error happen? is it happening when you exit python?\n<issue_comment>username_2: @username_1 and @username_3,\r\nI am running this:\r\n\r\npython train.py --dataroot ./datasets/horse2zebra --name horse2zebra_cyclegan --model cycle_gan --no_dropout --loadSize 64 --nThreads 1\r\n\r\nfrom \r\n\r\nhttps://github.com/junyanz/pytorch-CycleGAN-and-pix2pix\r\n\r\nThe model is succefully created:\r\nmodel [CycleGANModel] was created\r\n\r\nI am on ubuntu 16.04, pytorch 3.1, cuda 9.0 and cuDnn 7.1\r\n\r\nThe error I get is traced below:\r\ncreate web directory ./checkpoints/horse2zebra_cyclegan/web...\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 22, in <module>\r\n    for i, data in enumerate(dataset):\r\n  File \"/home/sam/Documents/ComputerVision/pytorch-CycleGAN-and-pix2pix/data/custom_dataset_data_loader.py\", line 44, in __iter__\r\n    for i, data in enumerate(self.dataloader):\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 275, in __next__\r\n    idx, batch = self._get_batch()\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 254, in _get_batch\r\n    return self.data_queue.get()\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/queues.py\", line 335, in get\r\n    res = self._reader.recv_bytes()\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\r\n    buf = self._recv_bytes(maxlength)\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\r\n    buf = self._recv(4)\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\r\n    chunk = read(handle, remaining)\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 175, in handler\r\n    _error_if_any_worker_fails()\r\nRuntimeError: DataLoader worker (pid 3792) is killed by signal: Illegal instruction.\n<issue_comment>username_3: @username_2 that error is because you are running it on a computer that is too old and does not have SSE4 instruction on CPU.\n<issue_comment>username_1: @username_2 Also, it is more likely due to that your CPU doesn't support the operations done in the dataset.__getitem__.\n<issue_comment>username_2: @username_1 & @username_3,\r\n\r\nSome months ago I made it work in windows10 (I think)\r\nThe my windows machine crashed. So here I am. Sorry\r\n\r\nHere is what I see in cat /proc/cpuinfo:\r\n\r\nprocessor\t: 0\r\nvendor_id\t: AuthenticAMD\r\ncpu family\t: 21\r\nmodel\t\t: 2\r\nmodel name\t: AMD FX(tm)-4300 Quad-Core Processor\r\nstepping\t: 0\r\nmicrocode\t: 0x600081c\r\ncpu MHz\t\t: 1400.000\r\ncache size\t: 2048 KB\r\nphysical id\t: 0\r\nsiblings\t: 4\r\ncore id\t\t: 0\r\ncpu cores\t: 2\r\napicid\t\t: 16\r\ninitial apicid\t: 0\r\nfpu\t\t: yes\r\nfpu_exception\t: yes\r\ncpuid level\t: 13\r\nwp\t\t: yes\r\nflags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf pni pclmulqdq monitor ssse3 fma cx16 **sse4_1 sse4_2** popcnt aes xsave avx f16c lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs xop skinit wdt lwp fma4 tce nodeid_msr tbm topoext perfctr_core perfctr_nb cpb hw_pstate retpoline retpoline_amd rsb_ctxsw vmmcall bmi1 arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold\r\nbugs\t\t: fxsave_leak sysret_ss_attrs null_seg spectre_v1 spectre_v2\r\nbogomips\t: 7634.15\r\nTLB size\t: 1536 4K pages\r\nclflush size\t: 64\r\ncache_alignment\t: 64\r\naddress sizes\t: 48 bits physical, 48 bits virtual\r\npower management: ts ttp tm 100mhzsteps hwpstate cpb eff_freq_ro\r\n\r\n@username_1 how do I check if CPU support dataset.getitem?\n<issue_comment>username_1: @username_2 run with `nThreads=0`\n<issue_comment>username_2: tried it with nThreads=0\r\ndifferent error...\r\nmodel [CycleGANModel] was created\r\ncreate web directory ./checkpoints/horse2zebra_cyclegan/web...\r\nIllegal instruction (core dumped)\n<issue_comment>username_2: I see something I am uneasy about:\r\nsee bold below\r\n\r\nCustomDatasetDataLoader\r\ndataset [UnalignedDataset] was created\r\n/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torchvision-0.2.0-py3.6.egg/torchvision/transforms/transforms.py:156: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\r\n#training images = 1334\r\ncycle_gan\r\n**initialization method [normal]\r\ninitialization method [normal]\r\ninitialization method [normal]\r\ninitialization method [normal]**\n<issue_comment>username_1: @username_2 No it's same error. We just wrap it in a nice way so you can see it even if it's in the workers. \r\n\r\nIsn't that expected? CycleGAN has four networks: D_X D_Y G F .\n<issue_comment>username_1: @username_2 anyways, it confirms my hypothesis that your CPU doesn't support the operation in `dataset.__getitem__`.\n<issue_comment>username_2: Oh... Can you suggest a workaround?\n<issue_comment>username_1: @username_2 You can try to put debug statements in CycleGAN code and see which one isn't supported by your CPU (probably some PIL operations), and search for replacements. Sorry I don't have much else to offer.\n<issue_comment>username_2: @username_1 \r\nThank you sir !<issue_closed>\n<issue_comment>username_1: closed due to inactivity. feel free to reopen if you can reproduce with latest stable (0.4 at the moment of writing).\n<issue_comment>username_4: Still can't get this to work.\r\n\r\nhttps://stackoverflow.com/questions/53449927/runtimeerror-dataloader-worker-is-killed-by-signal-illegal-instruction\r\n\r\nPlease help."}
{"repo": "pytorch/pytorch", "issue_id": 304991946, "issue_number": 5761, "timestamp": "2018-03-27 21:13:47+00:00", "text": "@username_1 and @username_3,\r\nI am running this:\r\n\r\npython train.py --dataroot ./datasets/horse2zebra --name horse2zebra_cyclegan --model cycle_gan --no_dropout --loadSize 64 --nThreads 1\r\n\r\nfrom \r\n\r\nhttps://github.com/junyanz/pytorch-CycleGAN-and-pix2pix\r\n\r\nThe model is succefully created:\r\nmodel [CycleGANModel] was created\r\n\r\nI am on ubuntu 16.04, pytorch 3.1, cuda 9.0 and cuDnn 7.1\r\n\r\nThe error I get is traced below:\r\ncreate web directory ./checkpoints/horse2zebra_cyclegan/web...\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 22, in <module>\r\n    for i, data in enumerate(dataset):\r\n  File \"/home/sam/Documents/ComputerVision/pytorch-CycleGAN-and-pix2pix/data/custom_dataset_data_loader.py\", line 44, in __iter__\r\n    for i, data in enumerate(self.dataloader):\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 275, in __next__\r\n    idx, batch = self._get_batch()\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 254, in _get_batch\r\n    return self.data_queue.get()\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/queues.py\", line 335, in get\r\n    res = self._reader.recv_bytes()\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\r\n    buf = self._recv_bytes(maxlength)\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\r\n    buf = self._recv(4)\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\r\n    chunk = read(handle, remaining)\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 175, in handler\r\n    _error_if_any_worker_fails()\r\nRuntimeError: DataLoader worker (pid 3792) is killed by signal: Illegal instruction.", "context": "<issue_start><issue_comment>Title: Problem in DataLoader (same problem with #4643, but not solved by it)\nusername_0: - OS: Ubuntu 16.04\r\n- PyTorch version: torch-0.3.1-cp36-cp36-linux_x86_64.whl\r\n- How you installed PyTorch (conda, pip, source): pip3 from the binary\r\n- Python version: 3.6\r\n- CUDA/cuDNN version: 9.1\r\n- GPU models and configuration: GTX 1070 Ti\r\n\r\nAfter install using the wheel file, I have changed the torch/utils/data/Dataloader.py according to the changes shown at https://github.com/pytorch/pytorch/pull/4643\r\n\r\nBut the same error still happen when I try the tutorial example to load image database:\r\n\r\nException ignored in: <bound method DataLoaderIter.__del__ of <torch.utils.data.dataloader.DataLoaderIter object at 0x56090c8c7348>>\r\nTraceback (most recent call last):\r\n  File \"/home/username_0/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 375, in __del__\r\n  File \"/home/username_0/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 361, in _shutdown_workers\r\n  File \"/home/username_0/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 39, in _get_from_queue\r\n  File \"/usr/pkg/lib/python3.6/multiprocessing/queues.py\", line 337, in get\r\nImportError: sys.meta_path is None, Python is likely shutting down\r\n\r\nSo could anyone help me on that? Or is there any other file I should modify to get it run?\n<issue_comment>username_0: And, I am not sure how can I use the changes in torch/csrc/DataLoader.cpp, should I clone all the source code on GitHub and build the newest binary to fully fix this my problem?\n<issue_comment>username_1: when does this error happen? is it happening when you exit python?\n<issue_comment>username_2: @username_1 and @username_3,\r\nI am running this:\r\n\r\npython train.py --dataroot ./datasets/horse2zebra --name horse2zebra_cyclegan --model cycle_gan --no_dropout --loadSize 64 --nThreads 1\r\n\r\nfrom \r\n\r\nhttps://github.com/junyanz/pytorch-CycleGAN-and-pix2pix\r\n\r\nThe model is succefully created:\r\nmodel [CycleGANModel] was created\r\n\r\nI am on ubuntu 16.04, pytorch 3.1, cuda 9.0 and cuDnn 7.1\r\n\r\nThe error I get is traced below:\r\ncreate web directory ./checkpoints/horse2zebra_cyclegan/web...\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 22, in <module>\r\n    for i, data in enumerate(dataset):\r\n  File \"/home/sam/Documents/ComputerVision/pytorch-CycleGAN-and-pix2pix/data/custom_dataset_data_loader.py\", line 44, in __iter__\r\n    for i, data in enumerate(self.dataloader):\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 275, in __next__\r\n    idx, batch = self._get_batch()\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 254, in _get_batch\r\n    return self.data_queue.get()\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/queues.py\", line 335, in get\r\n    res = self._reader.recv_bytes()\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\r\n    buf = self._recv_bytes(maxlength)\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\r\n    buf = self._recv(4)\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\r\n    chunk = read(handle, remaining)\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 175, in handler\r\n    _error_if_any_worker_fails()\r\nRuntimeError: DataLoader worker (pid 3792) is killed by signal: Illegal instruction.\n<issue_comment>username_3: @username_2 that error is because you are running it on a computer that is too old and does not have SSE4 instruction on CPU.\n<issue_comment>username_1: @username_2 Also, it is more likely due to that your CPU doesn't support the operations done in the dataset.__getitem__.\n<issue_comment>username_2: @username_1 & @username_3,\r\n\r\nSome months ago I made it work in windows10 (I think)\r\nThe my windows machine crashed. So here I am. Sorry\r\n\r\nHere is what I see in cat /proc/cpuinfo:\r\n\r\nprocessor\t: 0\r\nvendor_id\t: AuthenticAMD\r\ncpu family\t: 21\r\nmodel\t\t: 2\r\nmodel name\t: AMD FX(tm)-4300 Quad-Core Processor\r\nstepping\t: 0\r\nmicrocode\t: 0x600081c\r\ncpu MHz\t\t: 1400.000\r\ncache size\t: 2048 KB\r\nphysical id\t: 0\r\nsiblings\t: 4\r\ncore id\t\t: 0\r\ncpu cores\t: 2\r\napicid\t\t: 16\r\ninitial apicid\t: 0\r\nfpu\t\t: yes\r\nfpu_exception\t: yes\r\ncpuid level\t: 13\r\nwp\t\t: yes\r\nflags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf pni pclmulqdq monitor ssse3 fma cx16 **sse4_1 sse4_2** popcnt aes xsave avx f16c lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs xop skinit wdt lwp fma4 tce nodeid_msr tbm topoext perfctr_core perfctr_nb cpb hw_pstate retpoline retpoline_amd rsb_ctxsw vmmcall bmi1 arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold\r\nbugs\t\t: fxsave_leak sysret_ss_attrs null_seg spectre_v1 spectre_v2\r\nbogomips\t: 7634.15\r\nTLB size\t: 1536 4K pages\r\nclflush size\t: 64\r\ncache_alignment\t: 64\r\naddress sizes\t: 48 bits physical, 48 bits virtual\r\npower management: ts ttp tm 100mhzsteps hwpstate cpb eff_freq_ro\r\n\r\n@username_1 how do I check if CPU support dataset.getitem?\n<issue_comment>username_1: @username_2 run with `nThreads=0`\n<issue_comment>username_2: tried it with nThreads=0\r\ndifferent error...\r\nmodel [CycleGANModel] was created\r\ncreate web directory ./checkpoints/horse2zebra_cyclegan/web...\r\nIllegal instruction (core dumped)\n<issue_comment>username_2: I see something I am uneasy about:\r\nsee bold below\r\n\r\nCustomDatasetDataLoader\r\ndataset [UnalignedDataset] was created\r\n/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torchvision-0.2.0-py3.6.egg/torchvision/transforms/transforms.py:156: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\r\n#training images = 1334\r\ncycle_gan\r\n**initialization method [normal]\r\ninitialization method [normal]\r\ninitialization method [normal]\r\ninitialization method [normal]**\n<issue_comment>username_1: @username_2 No it's same error. We just wrap it in a nice way so you can see it even if it's in the workers. \r\n\r\nIsn't that expected? CycleGAN has four networks: D_X D_Y G F .\n<issue_comment>username_1: @username_2 anyways, it confirms my hypothesis that your CPU doesn't support the operation in `dataset.__getitem__`.\n<issue_comment>username_2: Oh... Can you suggest a workaround?\n<issue_comment>username_1: @username_2 You can try to put debug statements in CycleGAN code and see which one isn't supported by your CPU (probably some PIL operations), and search for replacements. Sorry I don't have much else to offer.\n<issue_comment>username_2: @username_1 \r\nThank you sir !<issue_closed>\n<issue_comment>username_1: closed due to inactivity. feel free to reopen if you can reproduce with latest stable (0.4 at the moment of writing).\n<issue_comment>username_4: Still can't get this to work.\r\n\r\nhttps://stackoverflow.com/questions/53449927/runtimeerror-dataloader-worker-is-killed-by-signal-illegal-instruction\r\n\r\nPlease help."}
{"repo": "pytorch/pytorch", "issue_id": 304991946, "issue_number": 5761, "timestamp": "2018-03-27 21:22:45+00:00", "text": "@username_2 that error is because you are running it on a computer that is too old and does not have SSE4 instruction on CPU.", "context": "<issue_start><issue_comment>Title: Problem in DataLoader (same problem with #4643, but not solved by it)\nusername_0: - OS: Ubuntu 16.04\r\n- PyTorch version: torch-0.3.1-cp36-cp36-linux_x86_64.whl\r\n- How you installed PyTorch (conda, pip, source): pip3 from the binary\r\n- Python version: 3.6\r\n- CUDA/cuDNN version: 9.1\r\n- GPU models and configuration: GTX 1070 Ti\r\n\r\nAfter install using the wheel file, I have changed the torch/utils/data/Dataloader.py according to the changes shown at https://github.com/pytorch/pytorch/pull/4643\r\n\r\nBut the same error still happen when I try the tutorial example to load image database:\r\n\r\nException ignored in: <bound method DataLoaderIter.__del__ of <torch.utils.data.dataloader.DataLoaderIter object at 0x56090c8c7348>>\r\nTraceback (most recent call last):\r\n  File \"/home/username_0/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 375, in __del__\r\n  File \"/home/username_0/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 361, in _shutdown_workers\r\n  File \"/home/username_0/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 39, in _get_from_queue\r\n  File \"/usr/pkg/lib/python3.6/multiprocessing/queues.py\", line 337, in get\r\nImportError: sys.meta_path is None, Python is likely shutting down\r\n\r\nSo could anyone help me on that? Or is there any other file I should modify to get it run?\n<issue_comment>username_0: And, I am not sure how can I use the changes in torch/csrc/DataLoader.cpp, should I clone all the source code on GitHub and build the newest binary to fully fix this my problem?\n<issue_comment>username_1: when does this error happen? is it happening when you exit python?\n<issue_comment>username_2: @username_1 and @username_3,\r\nI am running this:\r\n\r\npython train.py --dataroot ./datasets/horse2zebra --name horse2zebra_cyclegan --model cycle_gan --no_dropout --loadSize 64 --nThreads 1\r\n\r\nfrom \r\n\r\nhttps://github.com/junyanz/pytorch-CycleGAN-and-pix2pix\r\n\r\nThe model is succefully created:\r\nmodel [CycleGANModel] was created\r\n\r\nI am on ubuntu 16.04, pytorch 3.1, cuda 9.0 and cuDnn 7.1\r\n\r\nThe error I get is traced below:\r\ncreate web directory ./checkpoints/horse2zebra_cyclegan/web...\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 22, in <module>\r\n    for i, data in enumerate(dataset):\r\n  File \"/home/sam/Documents/ComputerVision/pytorch-CycleGAN-and-pix2pix/data/custom_dataset_data_loader.py\", line 44, in __iter__\r\n    for i, data in enumerate(self.dataloader):\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 275, in __next__\r\n    idx, batch = self._get_batch()\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 254, in _get_batch\r\n    return self.data_queue.get()\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/queues.py\", line 335, in get\r\n    res = self._reader.recv_bytes()\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\r\n    buf = self._recv_bytes(maxlength)\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\r\n    buf = self._recv(4)\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\r\n    chunk = read(handle, remaining)\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 175, in handler\r\n    _error_if_any_worker_fails()\r\nRuntimeError: DataLoader worker (pid 3792) is killed by signal: Illegal instruction.\n<issue_comment>username_3: @username_2 that error is because you are running it on a computer that is too old and does not have SSE4 instruction on CPU.\n<issue_comment>username_1: @username_2 Also, it is more likely due to that your CPU doesn't support the operations done in the dataset.__getitem__.\n<issue_comment>username_2: @username_1 & @username_3,\r\n\r\nSome months ago I made it work in windows10 (I think)\r\nThe my windows machine crashed. So here I am. Sorry\r\n\r\nHere is what I see in cat /proc/cpuinfo:\r\n\r\nprocessor\t: 0\r\nvendor_id\t: AuthenticAMD\r\ncpu family\t: 21\r\nmodel\t\t: 2\r\nmodel name\t: AMD FX(tm)-4300 Quad-Core Processor\r\nstepping\t: 0\r\nmicrocode\t: 0x600081c\r\ncpu MHz\t\t: 1400.000\r\ncache size\t: 2048 KB\r\nphysical id\t: 0\r\nsiblings\t: 4\r\ncore id\t\t: 0\r\ncpu cores\t: 2\r\napicid\t\t: 16\r\ninitial apicid\t: 0\r\nfpu\t\t: yes\r\nfpu_exception\t: yes\r\ncpuid level\t: 13\r\nwp\t\t: yes\r\nflags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf pni pclmulqdq monitor ssse3 fma cx16 **sse4_1 sse4_2** popcnt aes xsave avx f16c lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs xop skinit wdt lwp fma4 tce nodeid_msr tbm topoext perfctr_core perfctr_nb cpb hw_pstate retpoline retpoline_amd rsb_ctxsw vmmcall bmi1 arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold\r\nbugs\t\t: fxsave_leak sysret_ss_attrs null_seg spectre_v1 spectre_v2\r\nbogomips\t: 7634.15\r\nTLB size\t: 1536 4K pages\r\nclflush size\t: 64\r\ncache_alignment\t: 64\r\naddress sizes\t: 48 bits physical, 48 bits virtual\r\npower management: ts ttp tm 100mhzsteps hwpstate cpb eff_freq_ro\r\n\r\n@username_1 how do I check if CPU support dataset.getitem?\n<issue_comment>username_1: @username_2 run with `nThreads=0`\n<issue_comment>username_2: tried it with nThreads=0\r\ndifferent error...\r\nmodel [CycleGANModel] was created\r\ncreate web directory ./checkpoints/horse2zebra_cyclegan/web...\r\nIllegal instruction (core dumped)\n<issue_comment>username_2: I see something I am uneasy about:\r\nsee bold below\r\n\r\nCustomDatasetDataLoader\r\ndataset [UnalignedDataset] was created\r\n/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torchvision-0.2.0-py3.6.egg/torchvision/transforms/transforms.py:156: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\r\n#training images = 1334\r\ncycle_gan\r\n**initialization method [normal]\r\ninitialization method [normal]\r\ninitialization method [normal]\r\ninitialization method [normal]**\n<issue_comment>username_1: @username_2 No it's same error. We just wrap it in a nice way so you can see it even if it's in the workers. \r\n\r\nIsn't that expected? CycleGAN has four networks: D_X D_Y G F .\n<issue_comment>username_1: @username_2 anyways, it confirms my hypothesis that your CPU doesn't support the operation in `dataset.__getitem__`.\n<issue_comment>username_2: Oh... Can you suggest a workaround?\n<issue_comment>username_1: @username_2 You can try to put debug statements in CycleGAN code and see which one isn't supported by your CPU (probably some PIL operations), and search for replacements. Sorry I don't have much else to offer.\n<issue_comment>username_2: @username_1 \r\nThank you sir !<issue_closed>\n<issue_comment>username_1: closed due to inactivity. feel free to reopen if you can reproduce with latest stable (0.4 at the moment of writing).\n<issue_comment>username_4: Still can't get this to work.\r\n\r\nhttps://stackoverflow.com/questions/53449927/runtimeerror-dataloader-worker-is-killed-by-signal-illegal-instruction\r\n\r\nPlease help."}
{"repo": "pytorch/pytorch", "issue_id": 304991946, "issue_number": 5761, "timestamp": "2018-03-27 21:26:50+00:00", "text": "@username_2 Also, it is more likely due to that your CPU doesn't support the operations done in the dataset.__getitem__.", "context": "<issue_start><issue_comment>Title: Problem in DataLoader (same problem with #4643, but not solved by it)\nusername_0: - OS: Ubuntu 16.04\r\n- PyTorch version: torch-0.3.1-cp36-cp36-linux_x86_64.whl\r\n- How you installed PyTorch (conda, pip, source): pip3 from the binary\r\n- Python version: 3.6\r\n- CUDA/cuDNN version: 9.1\r\n- GPU models and configuration: GTX 1070 Ti\r\n\r\nAfter install using the wheel file, I have changed the torch/utils/data/Dataloader.py according to the changes shown at https://github.com/pytorch/pytorch/pull/4643\r\n\r\nBut the same error still happen when I try the tutorial example to load image database:\r\n\r\nException ignored in: <bound method DataLoaderIter.__del__ of <torch.utils.data.dataloader.DataLoaderIter object at 0x56090c8c7348>>\r\nTraceback (most recent call last):\r\n  File \"/home/username_0/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 375, in __del__\r\n  File \"/home/username_0/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 361, in _shutdown_workers\r\n  File \"/home/username_0/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 39, in _get_from_queue\r\n  File \"/usr/pkg/lib/python3.6/multiprocessing/queues.py\", line 337, in get\r\nImportError: sys.meta_path is None, Python is likely shutting down\r\n\r\nSo could anyone help me on that? Or is there any other file I should modify to get it run?\n<issue_comment>username_0: And, I am not sure how can I use the changes in torch/csrc/DataLoader.cpp, should I clone all the source code on GitHub and build the newest binary to fully fix this my problem?\n<issue_comment>username_1: when does this error happen? is it happening when you exit python?\n<issue_comment>username_2: @username_1 and @username_3,\r\nI am running this:\r\n\r\npython train.py --dataroot ./datasets/horse2zebra --name horse2zebra_cyclegan --model cycle_gan --no_dropout --loadSize 64 --nThreads 1\r\n\r\nfrom \r\n\r\nhttps://github.com/junyanz/pytorch-CycleGAN-and-pix2pix\r\n\r\nThe model is succefully created:\r\nmodel [CycleGANModel] was created\r\n\r\nI am on ubuntu 16.04, pytorch 3.1, cuda 9.0 and cuDnn 7.1\r\n\r\nThe error I get is traced below:\r\ncreate web directory ./checkpoints/horse2zebra_cyclegan/web...\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 22, in <module>\r\n    for i, data in enumerate(dataset):\r\n  File \"/home/sam/Documents/ComputerVision/pytorch-CycleGAN-and-pix2pix/data/custom_dataset_data_loader.py\", line 44, in __iter__\r\n    for i, data in enumerate(self.dataloader):\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 275, in __next__\r\n    idx, batch = self._get_batch()\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 254, in _get_batch\r\n    return self.data_queue.get()\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/queues.py\", line 335, in get\r\n    res = self._reader.recv_bytes()\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\r\n    buf = self._recv_bytes(maxlength)\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\r\n    buf = self._recv(4)\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\r\n    chunk = read(handle, remaining)\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 175, in handler\r\n    _error_if_any_worker_fails()\r\nRuntimeError: DataLoader worker (pid 3792) is killed by signal: Illegal instruction.\n<issue_comment>username_3: @username_2 that error is because you are running it on a computer that is too old and does not have SSE4 instruction on CPU.\n<issue_comment>username_1: @username_2 Also, it is more likely due to that your CPU doesn't support the operations done in the dataset.__getitem__.\n<issue_comment>username_2: @username_1 & @username_3,\r\n\r\nSome months ago I made it work in windows10 (I think)\r\nThe my windows machine crashed. So here I am. Sorry\r\n\r\nHere is what I see in cat /proc/cpuinfo:\r\n\r\nprocessor\t: 0\r\nvendor_id\t: AuthenticAMD\r\ncpu family\t: 21\r\nmodel\t\t: 2\r\nmodel name\t: AMD FX(tm)-4300 Quad-Core Processor\r\nstepping\t: 0\r\nmicrocode\t: 0x600081c\r\ncpu MHz\t\t: 1400.000\r\ncache size\t: 2048 KB\r\nphysical id\t: 0\r\nsiblings\t: 4\r\ncore id\t\t: 0\r\ncpu cores\t: 2\r\napicid\t\t: 16\r\ninitial apicid\t: 0\r\nfpu\t\t: yes\r\nfpu_exception\t: yes\r\ncpuid level\t: 13\r\nwp\t\t: yes\r\nflags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf pni pclmulqdq monitor ssse3 fma cx16 **sse4_1 sse4_2** popcnt aes xsave avx f16c lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs xop skinit wdt lwp fma4 tce nodeid_msr tbm topoext perfctr_core perfctr_nb cpb hw_pstate retpoline retpoline_amd rsb_ctxsw vmmcall bmi1 arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold\r\nbugs\t\t: fxsave_leak sysret_ss_attrs null_seg spectre_v1 spectre_v2\r\nbogomips\t: 7634.15\r\nTLB size\t: 1536 4K pages\r\nclflush size\t: 64\r\ncache_alignment\t: 64\r\naddress sizes\t: 48 bits physical, 48 bits virtual\r\npower management: ts ttp tm 100mhzsteps hwpstate cpb eff_freq_ro\r\n\r\n@username_1 how do I check if CPU support dataset.getitem?\n<issue_comment>username_1: @username_2 run with `nThreads=0`\n<issue_comment>username_2: tried it with nThreads=0\r\ndifferent error...\r\nmodel [CycleGANModel] was created\r\ncreate web directory ./checkpoints/horse2zebra_cyclegan/web...\r\nIllegal instruction (core dumped)\n<issue_comment>username_2: I see something I am uneasy about:\r\nsee bold below\r\n\r\nCustomDatasetDataLoader\r\ndataset [UnalignedDataset] was created\r\n/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torchvision-0.2.0-py3.6.egg/torchvision/transforms/transforms.py:156: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\r\n#training images = 1334\r\ncycle_gan\r\n**initialization method [normal]\r\ninitialization method [normal]\r\ninitialization method [normal]\r\ninitialization method [normal]**\n<issue_comment>username_1: @username_2 No it's same error. We just wrap it in a nice way so you can see it even if it's in the workers. \r\n\r\nIsn't that expected? CycleGAN has four networks: D_X D_Y G F .\n<issue_comment>username_1: @username_2 anyways, it confirms my hypothesis that your CPU doesn't support the operation in `dataset.__getitem__`.\n<issue_comment>username_2: Oh... Can you suggest a workaround?\n<issue_comment>username_1: @username_2 You can try to put debug statements in CycleGAN code and see which one isn't supported by your CPU (probably some PIL operations), and search for replacements. Sorry I don't have much else to offer.\n<issue_comment>username_2: @username_1 \r\nThank you sir !<issue_closed>\n<issue_comment>username_1: closed due to inactivity. feel free to reopen if you can reproduce with latest stable (0.4 at the moment of writing).\n<issue_comment>username_4: Still can't get this to work.\r\n\r\nhttps://stackoverflow.com/questions/53449927/runtimeerror-dataloader-worker-is-killed-by-signal-illegal-instruction\r\n\r\nPlease help."}
{"repo": "pytorch/pytorch", "issue_id": 304991946, "issue_number": 5761, "timestamp": "2018-03-27 21:31:13+00:00", "text": "@username_1 & @username_3,\r\n\r\nSome months ago I made it work in windows10 (I think)\r\nThe my windows machine crashed. So here I am. Sorry\r\n\r\nHere is what I see in cat /proc/cpuinfo:\r\n\r\nprocessor\t: 0\r\nvendor_id\t: AuthenticAMD\r\ncpu family\t: 21\r\nmodel\t\t: 2\r\nmodel name\t: AMD FX(tm)-4300 Quad-Core Processor\r\nstepping\t: 0\r\nmicrocode\t: 0x600081c\r\ncpu MHz\t\t: 1400.000\r\ncache size\t: 2048 KB\r\nphysical id\t: 0\r\nsiblings\t: 4\r\ncore id\t\t: 0\r\ncpu cores\t: 2\r\napicid\t\t: 16\r\ninitial apicid\t: 0\r\nfpu\t\t: yes\r\nfpu_exception\t: yes\r\ncpuid level\t: 13\r\nwp\t\t: yes\r\nflags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf pni pclmulqdq monitor ssse3 fma cx16 **sse4_1 sse4_2** popcnt aes xsave avx f16c lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs xop skinit wdt lwp fma4 tce nodeid_msr tbm topoext perfctr_core perfctr_nb cpb hw_pstate retpoline retpoline_amd rsb_ctxsw vmmcall bmi1 arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold\r\nbugs\t\t: fxsave_leak sysret_ss_attrs null_seg spectre_v1 spectre_v2\r\nbogomips\t: 7634.15\r\nTLB size\t: 1536 4K pages\r\nclflush size\t: 64\r\ncache_alignment\t: 64\r\naddress sizes\t: 48 bits physical, 48 bits virtual\r\npower management: ts ttp tm 100mhzsteps hwpstate cpb eff_freq_ro\r\n\r\n@username_1 how do I check if CPU support dataset.getitem?", "context": "<issue_start><issue_comment>Title: Problem in DataLoader (same problem with #4643, but not solved by it)\nusername_0: - OS: Ubuntu 16.04\r\n- PyTorch version: torch-0.3.1-cp36-cp36-linux_x86_64.whl\r\n- How you installed PyTorch (conda, pip, source): pip3 from the binary\r\n- Python version: 3.6\r\n- CUDA/cuDNN version: 9.1\r\n- GPU models and configuration: GTX 1070 Ti\r\n\r\nAfter install using the wheel file, I have changed the torch/utils/data/Dataloader.py according to the changes shown at https://github.com/pytorch/pytorch/pull/4643\r\n\r\nBut the same error still happen when I try the tutorial example to load image database:\r\n\r\nException ignored in: <bound method DataLoaderIter.__del__ of <torch.utils.data.dataloader.DataLoaderIter object at 0x56090c8c7348>>\r\nTraceback (most recent call last):\r\n  File \"/home/username_0/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 375, in __del__\r\n  File \"/home/username_0/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 361, in _shutdown_workers\r\n  File \"/home/username_0/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 39, in _get_from_queue\r\n  File \"/usr/pkg/lib/python3.6/multiprocessing/queues.py\", line 337, in get\r\nImportError: sys.meta_path is None, Python is likely shutting down\r\n\r\nSo could anyone help me on that? Or is there any other file I should modify to get it run?\n<issue_comment>username_0: And, I am not sure how can I use the changes in torch/csrc/DataLoader.cpp, should I clone all the source code on GitHub and build the newest binary to fully fix this my problem?\n<issue_comment>username_1: when does this error happen? is it happening when you exit python?\n<issue_comment>username_2: @username_1 and @username_3,\r\nI am running this:\r\n\r\npython train.py --dataroot ./datasets/horse2zebra --name horse2zebra_cyclegan --model cycle_gan --no_dropout --loadSize 64 --nThreads 1\r\n\r\nfrom \r\n\r\nhttps://github.com/junyanz/pytorch-CycleGAN-and-pix2pix\r\n\r\nThe model is succefully created:\r\nmodel [CycleGANModel] was created\r\n\r\nI am on ubuntu 16.04, pytorch 3.1, cuda 9.0 and cuDnn 7.1\r\n\r\nThe error I get is traced below:\r\ncreate web directory ./checkpoints/horse2zebra_cyclegan/web...\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 22, in <module>\r\n    for i, data in enumerate(dataset):\r\n  File \"/home/sam/Documents/ComputerVision/pytorch-CycleGAN-and-pix2pix/data/custom_dataset_data_loader.py\", line 44, in __iter__\r\n    for i, data in enumerate(self.dataloader):\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 275, in __next__\r\n    idx, batch = self._get_batch()\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 254, in _get_batch\r\n    return self.data_queue.get()\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/queues.py\", line 335, in get\r\n    res = self._reader.recv_bytes()\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\r\n    buf = self._recv_bytes(maxlength)\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\r\n    buf = self._recv(4)\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\r\n    chunk = read(handle, remaining)\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 175, in handler\r\n    _error_if_any_worker_fails()\r\nRuntimeError: DataLoader worker (pid 3792) is killed by signal: Illegal instruction.\n<issue_comment>username_3: @username_2 that error is because you are running it on a computer that is too old and does not have SSE4 instruction on CPU.\n<issue_comment>username_1: @username_2 Also, it is more likely due to that your CPU doesn't support the operations done in the dataset.__getitem__.\n<issue_comment>username_2: @username_1 & @username_3,\r\n\r\nSome months ago I made it work in windows10 (I think)\r\nThe my windows machine crashed. So here I am. Sorry\r\n\r\nHere is what I see in cat /proc/cpuinfo:\r\n\r\nprocessor\t: 0\r\nvendor_id\t: AuthenticAMD\r\ncpu family\t: 21\r\nmodel\t\t: 2\r\nmodel name\t: AMD FX(tm)-4300 Quad-Core Processor\r\nstepping\t: 0\r\nmicrocode\t: 0x600081c\r\ncpu MHz\t\t: 1400.000\r\ncache size\t: 2048 KB\r\nphysical id\t: 0\r\nsiblings\t: 4\r\ncore id\t\t: 0\r\ncpu cores\t: 2\r\napicid\t\t: 16\r\ninitial apicid\t: 0\r\nfpu\t\t: yes\r\nfpu_exception\t: yes\r\ncpuid level\t: 13\r\nwp\t\t: yes\r\nflags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf pni pclmulqdq monitor ssse3 fma cx16 **sse4_1 sse4_2** popcnt aes xsave avx f16c lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs xop skinit wdt lwp fma4 tce nodeid_msr tbm topoext perfctr_core perfctr_nb cpb hw_pstate retpoline retpoline_amd rsb_ctxsw vmmcall bmi1 arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold\r\nbugs\t\t: fxsave_leak sysret_ss_attrs null_seg spectre_v1 spectre_v2\r\nbogomips\t: 7634.15\r\nTLB size\t: 1536 4K pages\r\nclflush size\t: 64\r\ncache_alignment\t: 64\r\naddress sizes\t: 48 bits physical, 48 bits virtual\r\npower management: ts ttp tm 100mhzsteps hwpstate cpb eff_freq_ro\r\n\r\n@username_1 how do I check if CPU support dataset.getitem?\n<issue_comment>username_1: @username_2 run with `nThreads=0`\n<issue_comment>username_2: tried it with nThreads=0\r\ndifferent error...\r\nmodel [CycleGANModel] was created\r\ncreate web directory ./checkpoints/horse2zebra_cyclegan/web...\r\nIllegal instruction (core dumped)\n<issue_comment>username_2: I see something I am uneasy about:\r\nsee bold below\r\n\r\nCustomDatasetDataLoader\r\ndataset [UnalignedDataset] was created\r\n/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torchvision-0.2.0-py3.6.egg/torchvision/transforms/transforms.py:156: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\r\n#training images = 1334\r\ncycle_gan\r\n**initialization method [normal]\r\ninitialization method [normal]\r\ninitialization method [normal]\r\ninitialization method [normal]**\n<issue_comment>username_1: @username_2 No it's same error. We just wrap it in a nice way so you can see it even if it's in the workers. \r\n\r\nIsn't that expected? CycleGAN has four networks: D_X D_Y G F .\n<issue_comment>username_1: @username_2 anyways, it confirms my hypothesis that your CPU doesn't support the operation in `dataset.__getitem__`.\n<issue_comment>username_2: Oh... Can you suggest a workaround?\n<issue_comment>username_1: @username_2 You can try to put debug statements in CycleGAN code and see which one isn't supported by your CPU (probably some PIL operations), and search for replacements. Sorry I don't have much else to offer.\n<issue_comment>username_2: @username_1 \r\nThank you sir !<issue_closed>\n<issue_comment>username_1: closed due to inactivity. feel free to reopen if you can reproduce with latest stable (0.4 at the moment of writing).\n<issue_comment>username_4: Still can't get this to work.\r\n\r\nhttps://stackoverflow.com/questions/53449927/runtimeerror-dataloader-worker-is-killed-by-signal-illegal-instruction\r\n\r\nPlease help."}
{"repo": "pytorch/pytorch", "issue_id": 304991946, "issue_number": 5761, "timestamp": "2018-03-27 21:32:37+00:00", "text": "@username_2 run with `nThreads=0`", "context": "<issue_start><issue_comment>Title: Problem in DataLoader (same problem with #4643, but not solved by it)\nusername_0: - OS: Ubuntu 16.04\r\n- PyTorch version: torch-0.3.1-cp36-cp36-linux_x86_64.whl\r\n- How you installed PyTorch (conda, pip, source): pip3 from the binary\r\n- Python version: 3.6\r\n- CUDA/cuDNN version: 9.1\r\n- GPU models and configuration: GTX 1070 Ti\r\n\r\nAfter install using the wheel file, I have changed the torch/utils/data/Dataloader.py according to the changes shown at https://github.com/pytorch/pytorch/pull/4643\r\n\r\nBut the same error still happen when I try the tutorial example to load image database:\r\n\r\nException ignored in: <bound method DataLoaderIter.__del__ of <torch.utils.data.dataloader.DataLoaderIter object at 0x56090c8c7348>>\r\nTraceback (most recent call last):\r\n  File \"/home/username_0/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 375, in __del__\r\n  File \"/home/username_0/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 361, in _shutdown_workers\r\n  File \"/home/username_0/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 39, in _get_from_queue\r\n  File \"/usr/pkg/lib/python3.6/multiprocessing/queues.py\", line 337, in get\r\nImportError: sys.meta_path is None, Python is likely shutting down\r\n\r\nSo could anyone help me on that? Or is there any other file I should modify to get it run?\n<issue_comment>username_0: And, I am not sure how can I use the changes in torch/csrc/DataLoader.cpp, should I clone all the source code on GitHub and build the newest binary to fully fix this my problem?\n<issue_comment>username_1: when does this error happen? is it happening when you exit python?\n<issue_comment>username_2: @username_1 and @username_3,\r\nI am running this:\r\n\r\npython train.py --dataroot ./datasets/horse2zebra --name horse2zebra_cyclegan --model cycle_gan --no_dropout --loadSize 64 --nThreads 1\r\n\r\nfrom \r\n\r\nhttps://github.com/junyanz/pytorch-CycleGAN-and-pix2pix\r\n\r\nThe model is succefully created:\r\nmodel [CycleGANModel] was created\r\n\r\nI am on ubuntu 16.04, pytorch 3.1, cuda 9.0 and cuDnn 7.1\r\n\r\nThe error I get is traced below:\r\ncreate web directory ./checkpoints/horse2zebra_cyclegan/web...\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 22, in <module>\r\n    for i, data in enumerate(dataset):\r\n  File \"/home/sam/Documents/ComputerVision/pytorch-CycleGAN-and-pix2pix/data/custom_dataset_data_loader.py\", line 44, in __iter__\r\n    for i, data in enumerate(self.dataloader):\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 275, in __next__\r\n    idx, batch = self._get_batch()\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 254, in _get_batch\r\n    return self.data_queue.get()\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/queues.py\", line 335, in get\r\n    res = self._reader.recv_bytes()\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\r\n    buf = self._recv_bytes(maxlength)\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\r\n    buf = self._recv(4)\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\r\n    chunk = read(handle, remaining)\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 175, in handler\r\n    _error_if_any_worker_fails()\r\nRuntimeError: DataLoader worker (pid 3792) is killed by signal: Illegal instruction.\n<issue_comment>username_3: @username_2 that error is because you are running it on a computer that is too old and does not have SSE4 instruction on CPU.\n<issue_comment>username_1: @username_2 Also, it is more likely due to that your CPU doesn't support the operations done in the dataset.__getitem__.\n<issue_comment>username_2: @username_1 & @username_3,\r\n\r\nSome months ago I made it work in windows10 (I think)\r\nThe my windows machine crashed. So here I am. Sorry\r\n\r\nHere is what I see in cat /proc/cpuinfo:\r\n\r\nprocessor\t: 0\r\nvendor_id\t: AuthenticAMD\r\ncpu family\t: 21\r\nmodel\t\t: 2\r\nmodel name\t: AMD FX(tm)-4300 Quad-Core Processor\r\nstepping\t: 0\r\nmicrocode\t: 0x600081c\r\ncpu MHz\t\t: 1400.000\r\ncache size\t: 2048 KB\r\nphysical id\t: 0\r\nsiblings\t: 4\r\ncore id\t\t: 0\r\ncpu cores\t: 2\r\napicid\t\t: 16\r\ninitial apicid\t: 0\r\nfpu\t\t: yes\r\nfpu_exception\t: yes\r\ncpuid level\t: 13\r\nwp\t\t: yes\r\nflags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf pni pclmulqdq monitor ssse3 fma cx16 **sse4_1 sse4_2** popcnt aes xsave avx f16c lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs xop skinit wdt lwp fma4 tce nodeid_msr tbm topoext perfctr_core perfctr_nb cpb hw_pstate retpoline retpoline_amd rsb_ctxsw vmmcall bmi1 arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold\r\nbugs\t\t: fxsave_leak sysret_ss_attrs null_seg spectre_v1 spectre_v2\r\nbogomips\t: 7634.15\r\nTLB size\t: 1536 4K pages\r\nclflush size\t: 64\r\ncache_alignment\t: 64\r\naddress sizes\t: 48 bits physical, 48 bits virtual\r\npower management: ts ttp tm 100mhzsteps hwpstate cpb eff_freq_ro\r\n\r\n@username_1 how do I check if CPU support dataset.getitem?\n<issue_comment>username_1: @username_2 run with `nThreads=0`\n<issue_comment>username_2: tried it with nThreads=0\r\ndifferent error...\r\nmodel [CycleGANModel] was created\r\ncreate web directory ./checkpoints/horse2zebra_cyclegan/web...\r\nIllegal instruction (core dumped)\n<issue_comment>username_2: I see something I am uneasy about:\r\nsee bold below\r\n\r\nCustomDatasetDataLoader\r\ndataset [UnalignedDataset] was created\r\n/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torchvision-0.2.0-py3.6.egg/torchvision/transforms/transforms.py:156: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\r\n#training images = 1334\r\ncycle_gan\r\n**initialization method [normal]\r\ninitialization method [normal]\r\ninitialization method [normal]\r\ninitialization method [normal]**\n<issue_comment>username_1: @username_2 No it's same error. We just wrap it in a nice way so you can see it even if it's in the workers. \r\n\r\nIsn't that expected? CycleGAN has four networks: D_X D_Y G F .\n<issue_comment>username_1: @username_2 anyways, it confirms my hypothesis that your CPU doesn't support the operation in `dataset.__getitem__`.\n<issue_comment>username_2: Oh... Can you suggest a workaround?\n<issue_comment>username_1: @username_2 You can try to put debug statements in CycleGAN code and see which one isn't supported by your CPU (probably some PIL operations), and search for replacements. Sorry I don't have much else to offer.\n<issue_comment>username_2: @username_1 \r\nThank you sir !<issue_closed>\n<issue_comment>username_1: closed due to inactivity. feel free to reopen if you can reproduce with latest stable (0.4 at the moment of writing).\n<issue_comment>username_4: Still can't get this to work.\r\n\r\nhttps://stackoverflow.com/questions/53449927/runtimeerror-dataloader-worker-is-killed-by-signal-illegal-instruction\r\n\r\nPlease help."}
{"repo": "pytorch/pytorch", "issue_id": 304991946, "issue_number": 5761, "timestamp": "2018-03-27 21:37:23+00:00", "text": "tried it with nThreads=0\r\ndifferent error...\r\nmodel [CycleGANModel] was created\r\ncreate web directory ./checkpoints/horse2zebra_cyclegan/web...\r\nIllegal instruction (core dumped)", "context": "<issue_start><issue_comment>Title: Problem in DataLoader (same problem with #4643, but not solved by it)\nusername_0: - OS: Ubuntu 16.04\r\n- PyTorch version: torch-0.3.1-cp36-cp36-linux_x86_64.whl\r\n- How you installed PyTorch (conda, pip, source): pip3 from the binary\r\n- Python version: 3.6\r\n- CUDA/cuDNN version: 9.1\r\n- GPU models and configuration: GTX 1070 Ti\r\n\r\nAfter install using the wheel file, I have changed the torch/utils/data/Dataloader.py according to the changes shown at https://github.com/pytorch/pytorch/pull/4643\r\n\r\nBut the same error still happen when I try the tutorial example to load image database:\r\n\r\nException ignored in: <bound method DataLoaderIter.__del__ of <torch.utils.data.dataloader.DataLoaderIter object at 0x56090c8c7348>>\r\nTraceback (most recent call last):\r\n  File \"/home/username_0/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 375, in __del__\r\n  File \"/home/username_0/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 361, in _shutdown_workers\r\n  File \"/home/username_0/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 39, in _get_from_queue\r\n  File \"/usr/pkg/lib/python3.6/multiprocessing/queues.py\", line 337, in get\r\nImportError: sys.meta_path is None, Python is likely shutting down\r\n\r\nSo could anyone help me on that? Or is there any other file I should modify to get it run?\n<issue_comment>username_0: And, I am not sure how can I use the changes in torch/csrc/DataLoader.cpp, should I clone all the source code on GitHub and build the newest binary to fully fix this my problem?\n<issue_comment>username_1: when does this error happen? is it happening when you exit python?\n<issue_comment>username_2: @username_1 and @username_3,\r\nI am running this:\r\n\r\npython train.py --dataroot ./datasets/horse2zebra --name horse2zebra_cyclegan --model cycle_gan --no_dropout --loadSize 64 --nThreads 1\r\n\r\nfrom \r\n\r\nhttps://github.com/junyanz/pytorch-CycleGAN-and-pix2pix\r\n\r\nThe model is succefully created:\r\nmodel [CycleGANModel] was created\r\n\r\nI am on ubuntu 16.04, pytorch 3.1, cuda 9.0 and cuDnn 7.1\r\n\r\nThe error I get is traced below:\r\ncreate web directory ./checkpoints/horse2zebra_cyclegan/web...\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 22, in <module>\r\n    for i, data in enumerate(dataset):\r\n  File \"/home/sam/Documents/ComputerVision/pytorch-CycleGAN-and-pix2pix/data/custom_dataset_data_loader.py\", line 44, in __iter__\r\n    for i, data in enumerate(self.dataloader):\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 275, in __next__\r\n    idx, batch = self._get_batch()\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 254, in _get_batch\r\n    return self.data_queue.get()\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/queues.py\", line 335, in get\r\n    res = self._reader.recv_bytes()\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\r\n    buf = self._recv_bytes(maxlength)\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\r\n    buf = self._recv(4)\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\r\n    chunk = read(handle, remaining)\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 175, in handler\r\n    _error_if_any_worker_fails()\r\nRuntimeError: DataLoader worker (pid 3792) is killed by signal: Illegal instruction.\n<issue_comment>username_3: @username_2 that error is because you are running it on a computer that is too old and does not have SSE4 instruction on CPU.\n<issue_comment>username_1: @username_2 Also, it is more likely due to that your CPU doesn't support the operations done in the dataset.__getitem__.\n<issue_comment>username_2: @username_1 & @username_3,\r\n\r\nSome months ago I made it work in windows10 (I think)\r\nThe my windows machine crashed. So here I am. Sorry\r\n\r\nHere is what I see in cat /proc/cpuinfo:\r\n\r\nprocessor\t: 0\r\nvendor_id\t: AuthenticAMD\r\ncpu family\t: 21\r\nmodel\t\t: 2\r\nmodel name\t: AMD FX(tm)-4300 Quad-Core Processor\r\nstepping\t: 0\r\nmicrocode\t: 0x600081c\r\ncpu MHz\t\t: 1400.000\r\ncache size\t: 2048 KB\r\nphysical id\t: 0\r\nsiblings\t: 4\r\ncore id\t\t: 0\r\ncpu cores\t: 2\r\napicid\t\t: 16\r\ninitial apicid\t: 0\r\nfpu\t\t: yes\r\nfpu_exception\t: yes\r\ncpuid level\t: 13\r\nwp\t\t: yes\r\nflags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf pni pclmulqdq monitor ssse3 fma cx16 **sse4_1 sse4_2** popcnt aes xsave avx f16c lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs xop skinit wdt lwp fma4 tce nodeid_msr tbm topoext perfctr_core perfctr_nb cpb hw_pstate retpoline retpoline_amd rsb_ctxsw vmmcall bmi1 arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold\r\nbugs\t\t: fxsave_leak sysret_ss_attrs null_seg spectre_v1 spectre_v2\r\nbogomips\t: 7634.15\r\nTLB size\t: 1536 4K pages\r\nclflush size\t: 64\r\ncache_alignment\t: 64\r\naddress sizes\t: 48 bits physical, 48 bits virtual\r\npower management: ts ttp tm 100mhzsteps hwpstate cpb eff_freq_ro\r\n\r\n@username_1 how do I check if CPU support dataset.getitem?\n<issue_comment>username_1: @username_2 run with `nThreads=0`\n<issue_comment>username_2: tried it with nThreads=0\r\ndifferent error...\r\nmodel [CycleGANModel] was created\r\ncreate web directory ./checkpoints/horse2zebra_cyclegan/web...\r\nIllegal instruction (core dumped)\n<issue_comment>username_2: I see something I am uneasy about:\r\nsee bold below\r\n\r\nCustomDatasetDataLoader\r\ndataset [UnalignedDataset] was created\r\n/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torchvision-0.2.0-py3.6.egg/torchvision/transforms/transforms.py:156: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\r\n#training images = 1334\r\ncycle_gan\r\n**initialization method [normal]\r\ninitialization method [normal]\r\ninitialization method [normal]\r\ninitialization method [normal]**\n<issue_comment>username_1: @username_2 No it's same error. We just wrap it in a nice way so you can see it even if it's in the workers. \r\n\r\nIsn't that expected? CycleGAN has four networks: D_X D_Y G F .\n<issue_comment>username_1: @username_2 anyways, it confirms my hypothesis that your CPU doesn't support the operation in `dataset.__getitem__`.\n<issue_comment>username_2: Oh... Can you suggest a workaround?\n<issue_comment>username_1: @username_2 You can try to put debug statements in CycleGAN code and see which one isn't supported by your CPU (probably some PIL operations), and search for replacements. Sorry I don't have much else to offer.\n<issue_comment>username_2: @username_1 \r\nThank you sir !<issue_closed>\n<issue_comment>username_1: closed due to inactivity. feel free to reopen if you can reproduce with latest stable (0.4 at the moment of writing).\n<issue_comment>username_4: Still can't get this to work.\r\n\r\nhttps://stackoverflow.com/questions/53449927/runtimeerror-dataloader-worker-is-killed-by-signal-illegal-instruction\r\n\r\nPlease help."}
{"repo": "pytorch/pytorch", "issue_id": 304991946, "issue_number": 5761, "timestamp": "2018-03-27 21:45:37+00:00", "text": "I see something I am uneasy about:\r\nsee bold below\r\n\r\nCustomDatasetDataLoader\r\ndataset [UnalignedDataset] was created\r\n/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torchvision-0.2.0-py3.6.egg/torchvision/transforms/transforms.py:156: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\r\n#training images = 1334\r\ncycle_gan\r\n**initialization method [normal]\r\ninitialization method [normal]\r\ninitialization method [normal]\r\ninitialization method [normal]**", "context": "<issue_start><issue_comment>Title: Problem in DataLoader (same problem with #4643, but not solved by it)\nusername_0: - OS: Ubuntu 16.04\r\n- PyTorch version: torch-0.3.1-cp36-cp36-linux_x86_64.whl\r\n- How you installed PyTorch (conda, pip, source): pip3 from the binary\r\n- Python version: 3.6\r\n- CUDA/cuDNN version: 9.1\r\n- GPU models and configuration: GTX 1070 Ti\r\n\r\nAfter install using the wheel file, I have changed the torch/utils/data/Dataloader.py according to the changes shown at https://github.com/pytorch/pytorch/pull/4643\r\n\r\nBut the same error still happen when I try the tutorial example to load image database:\r\n\r\nException ignored in: <bound method DataLoaderIter.__del__ of <torch.utils.data.dataloader.DataLoaderIter object at 0x56090c8c7348>>\r\nTraceback (most recent call last):\r\n  File \"/home/username_0/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 375, in __del__\r\n  File \"/home/username_0/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 361, in _shutdown_workers\r\n  File \"/home/username_0/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 39, in _get_from_queue\r\n  File \"/usr/pkg/lib/python3.6/multiprocessing/queues.py\", line 337, in get\r\nImportError: sys.meta_path is None, Python is likely shutting down\r\n\r\nSo could anyone help me on that? Or is there any other file I should modify to get it run?\n<issue_comment>username_0: And, I am not sure how can I use the changes in torch/csrc/DataLoader.cpp, should I clone all the source code on GitHub and build the newest binary to fully fix this my problem?\n<issue_comment>username_1: when does this error happen? is it happening when you exit python?\n<issue_comment>username_2: @username_1 and @username_3,\r\nI am running this:\r\n\r\npython train.py --dataroot ./datasets/horse2zebra --name horse2zebra_cyclegan --model cycle_gan --no_dropout --loadSize 64 --nThreads 1\r\n\r\nfrom \r\n\r\nhttps://github.com/junyanz/pytorch-CycleGAN-and-pix2pix\r\n\r\nThe model is succefully created:\r\nmodel [CycleGANModel] was created\r\n\r\nI am on ubuntu 16.04, pytorch 3.1, cuda 9.0 and cuDnn 7.1\r\n\r\nThe error I get is traced below:\r\ncreate web directory ./checkpoints/horse2zebra_cyclegan/web...\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 22, in <module>\r\n    for i, data in enumerate(dataset):\r\n  File \"/home/sam/Documents/ComputerVision/pytorch-CycleGAN-and-pix2pix/data/custom_dataset_data_loader.py\", line 44, in __iter__\r\n    for i, data in enumerate(self.dataloader):\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 275, in __next__\r\n    idx, batch = self._get_batch()\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 254, in _get_batch\r\n    return self.data_queue.get()\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/queues.py\", line 335, in get\r\n    res = self._reader.recv_bytes()\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\r\n    buf = self._recv_bytes(maxlength)\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\r\n    buf = self._recv(4)\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\r\n    chunk = read(handle, remaining)\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 175, in handler\r\n    _error_if_any_worker_fails()\r\nRuntimeError: DataLoader worker (pid 3792) is killed by signal: Illegal instruction.\n<issue_comment>username_3: @username_2 that error is because you are running it on a computer that is too old and does not have SSE4 instruction on CPU.\n<issue_comment>username_1: @username_2 Also, it is more likely due to that your CPU doesn't support the operations done in the dataset.__getitem__.\n<issue_comment>username_2: @username_1 & @username_3,\r\n\r\nSome months ago I made it work in windows10 (I think)\r\nThe my windows machine crashed. So here I am. Sorry\r\n\r\nHere is what I see in cat /proc/cpuinfo:\r\n\r\nprocessor\t: 0\r\nvendor_id\t: AuthenticAMD\r\ncpu family\t: 21\r\nmodel\t\t: 2\r\nmodel name\t: AMD FX(tm)-4300 Quad-Core Processor\r\nstepping\t: 0\r\nmicrocode\t: 0x600081c\r\ncpu MHz\t\t: 1400.000\r\ncache size\t: 2048 KB\r\nphysical id\t: 0\r\nsiblings\t: 4\r\ncore id\t\t: 0\r\ncpu cores\t: 2\r\napicid\t\t: 16\r\ninitial apicid\t: 0\r\nfpu\t\t: yes\r\nfpu_exception\t: yes\r\ncpuid level\t: 13\r\nwp\t\t: yes\r\nflags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf pni pclmulqdq monitor ssse3 fma cx16 **sse4_1 sse4_2** popcnt aes xsave avx f16c lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs xop skinit wdt lwp fma4 tce nodeid_msr tbm topoext perfctr_core perfctr_nb cpb hw_pstate retpoline retpoline_amd rsb_ctxsw vmmcall bmi1 arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold\r\nbugs\t\t: fxsave_leak sysret_ss_attrs null_seg spectre_v1 spectre_v2\r\nbogomips\t: 7634.15\r\nTLB size\t: 1536 4K pages\r\nclflush size\t: 64\r\ncache_alignment\t: 64\r\naddress sizes\t: 48 bits physical, 48 bits virtual\r\npower management: ts ttp tm 100mhzsteps hwpstate cpb eff_freq_ro\r\n\r\n@username_1 how do I check if CPU support dataset.getitem?\n<issue_comment>username_1: @username_2 run with `nThreads=0`\n<issue_comment>username_2: tried it with nThreads=0\r\ndifferent error...\r\nmodel [CycleGANModel] was created\r\ncreate web directory ./checkpoints/horse2zebra_cyclegan/web...\r\nIllegal instruction (core dumped)\n<issue_comment>username_2: I see something I am uneasy about:\r\nsee bold below\r\n\r\nCustomDatasetDataLoader\r\ndataset [UnalignedDataset] was created\r\n/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torchvision-0.2.0-py3.6.egg/torchvision/transforms/transforms.py:156: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\r\n#training images = 1334\r\ncycle_gan\r\n**initialization method [normal]\r\ninitialization method [normal]\r\ninitialization method [normal]\r\ninitialization method [normal]**\n<issue_comment>username_1: @username_2 No it's same error. We just wrap it in a nice way so you can see it even if it's in the workers. \r\n\r\nIsn't that expected? CycleGAN has four networks: D_X D_Y G F .\n<issue_comment>username_1: @username_2 anyways, it confirms my hypothesis that your CPU doesn't support the operation in `dataset.__getitem__`.\n<issue_comment>username_2: Oh... Can you suggest a workaround?\n<issue_comment>username_1: @username_2 You can try to put debug statements in CycleGAN code and see which one isn't supported by your CPU (probably some PIL operations), and search for replacements. Sorry I don't have much else to offer.\n<issue_comment>username_2: @username_1 \r\nThank you sir !<issue_closed>\n<issue_comment>username_1: closed due to inactivity. feel free to reopen if you can reproduce with latest stable (0.4 at the moment of writing).\n<issue_comment>username_4: Still can't get this to work.\r\n\r\nhttps://stackoverflow.com/questions/53449927/runtimeerror-dataloader-worker-is-killed-by-signal-illegal-instruction\r\n\r\nPlease help."}
{"repo": "pytorch/pytorch", "issue_id": 304991946, "issue_number": 5761, "timestamp": "2018-03-27 21:47:52+00:00", "text": "@username_2 No it's same error. We just wrap it in a nice way so you can see it even if it's in the workers. \r\n\r\nIsn't that expected? CycleGAN has four networks: D_X D_Y G F .", "context": "<issue_start><issue_comment>Title: Problem in DataLoader (same problem with #4643, but not solved by it)\nusername_0: - OS: Ubuntu 16.04\r\n- PyTorch version: torch-0.3.1-cp36-cp36-linux_x86_64.whl\r\n- How you installed PyTorch (conda, pip, source): pip3 from the binary\r\n- Python version: 3.6\r\n- CUDA/cuDNN version: 9.1\r\n- GPU models and configuration: GTX 1070 Ti\r\n\r\nAfter install using the wheel file, I have changed the torch/utils/data/Dataloader.py according to the changes shown at https://github.com/pytorch/pytorch/pull/4643\r\n\r\nBut the same error still happen when I try the tutorial example to load image database:\r\n\r\nException ignored in: <bound method DataLoaderIter.__del__ of <torch.utils.data.dataloader.DataLoaderIter object at 0x56090c8c7348>>\r\nTraceback (most recent call last):\r\n  File \"/home/username_0/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 375, in __del__\r\n  File \"/home/username_0/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 361, in _shutdown_workers\r\n  File \"/home/username_0/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 39, in _get_from_queue\r\n  File \"/usr/pkg/lib/python3.6/multiprocessing/queues.py\", line 337, in get\r\nImportError: sys.meta_path is None, Python is likely shutting down\r\n\r\nSo could anyone help me on that? Or is there any other file I should modify to get it run?\n<issue_comment>username_0: And, I am not sure how can I use the changes in torch/csrc/DataLoader.cpp, should I clone all the source code on GitHub and build the newest binary to fully fix this my problem?\n<issue_comment>username_1: when does this error happen? is it happening when you exit python?\n<issue_comment>username_2: @username_1 and @username_3,\r\nI am running this:\r\n\r\npython train.py --dataroot ./datasets/horse2zebra --name horse2zebra_cyclegan --model cycle_gan --no_dropout --loadSize 64 --nThreads 1\r\n\r\nfrom \r\n\r\nhttps://github.com/junyanz/pytorch-CycleGAN-and-pix2pix\r\n\r\nThe model is succefully created:\r\nmodel [CycleGANModel] was created\r\n\r\nI am on ubuntu 16.04, pytorch 3.1, cuda 9.0 and cuDnn 7.1\r\n\r\nThe error I get is traced below:\r\ncreate web directory ./checkpoints/horse2zebra_cyclegan/web...\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 22, in <module>\r\n    for i, data in enumerate(dataset):\r\n  File \"/home/sam/Documents/ComputerVision/pytorch-CycleGAN-and-pix2pix/data/custom_dataset_data_loader.py\", line 44, in __iter__\r\n    for i, data in enumerate(self.dataloader):\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 275, in __next__\r\n    idx, batch = self._get_batch()\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 254, in _get_batch\r\n    return self.data_queue.get()\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/queues.py\", line 335, in get\r\n    res = self._reader.recv_bytes()\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\r\n    buf = self._recv_bytes(maxlength)\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\r\n    buf = self._recv(4)\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\r\n    chunk = read(handle, remaining)\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 175, in handler\r\n    _error_if_any_worker_fails()\r\nRuntimeError: DataLoader worker (pid 3792) is killed by signal: Illegal instruction.\n<issue_comment>username_3: @username_2 that error is because you are running it on a computer that is too old and does not have SSE4 instruction on CPU.\n<issue_comment>username_1: @username_2 Also, it is more likely due to that your CPU doesn't support the operations done in the dataset.__getitem__.\n<issue_comment>username_2: @username_1 & @username_3,\r\n\r\nSome months ago I made it work in windows10 (I think)\r\nThe my windows machine crashed. So here I am. Sorry\r\n\r\nHere is what I see in cat /proc/cpuinfo:\r\n\r\nprocessor\t: 0\r\nvendor_id\t: AuthenticAMD\r\ncpu family\t: 21\r\nmodel\t\t: 2\r\nmodel name\t: AMD FX(tm)-4300 Quad-Core Processor\r\nstepping\t: 0\r\nmicrocode\t: 0x600081c\r\ncpu MHz\t\t: 1400.000\r\ncache size\t: 2048 KB\r\nphysical id\t: 0\r\nsiblings\t: 4\r\ncore id\t\t: 0\r\ncpu cores\t: 2\r\napicid\t\t: 16\r\ninitial apicid\t: 0\r\nfpu\t\t: yes\r\nfpu_exception\t: yes\r\ncpuid level\t: 13\r\nwp\t\t: yes\r\nflags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf pni pclmulqdq monitor ssse3 fma cx16 **sse4_1 sse4_2** popcnt aes xsave avx f16c lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs xop skinit wdt lwp fma4 tce nodeid_msr tbm topoext perfctr_core perfctr_nb cpb hw_pstate retpoline retpoline_amd rsb_ctxsw vmmcall bmi1 arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold\r\nbugs\t\t: fxsave_leak sysret_ss_attrs null_seg spectre_v1 spectre_v2\r\nbogomips\t: 7634.15\r\nTLB size\t: 1536 4K pages\r\nclflush size\t: 64\r\ncache_alignment\t: 64\r\naddress sizes\t: 48 bits physical, 48 bits virtual\r\npower management: ts ttp tm 100mhzsteps hwpstate cpb eff_freq_ro\r\n\r\n@username_1 how do I check if CPU support dataset.getitem?\n<issue_comment>username_1: @username_2 run with `nThreads=0`\n<issue_comment>username_2: tried it with nThreads=0\r\ndifferent error...\r\nmodel [CycleGANModel] was created\r\ncreate web directory ./checkpoints/horse2zebra_cyclegan/web...\r\nIllegal instruction (core dumped)\n<issue_comment>username_2: I see something I am uneasy about:\r\nsee bold below\r\n\r\nCustomDatasetDataLoader\r\ndataset [UnalignedDataset] was created\r\n/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torchvision-0.2.0-py3.6.egg/torchvision/transforms/transforms.py:156: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\r\n#training images = 1334\r\ncycle_gan\r\n**initialization method [normal]\r\ninitialization method [normal]\r\ninitialization method [normal]\r\ninitialization method [normal]**\n<issue_comment>username_1: @username_2 No it's same error. We just wrap it in a nice way so you can see it even if it's in the workers. \r\n\r\nIsn't that expected? CycleGAN has four networks: D_X D_Y G F .\n<issue_comment>username_1: @username_2 anyways, it confirms my hypothesis that your CPU doesn't support the operation in `dataset.__getitem__`.\n<issue_comment>username_2: Oh... Can you suggest a workaround?\n<issue_comment>username_1: @username_2 You can try to put debug statements in CycleGAN code and see which one isn't supported by your CPU (probably some PIL operations), and search for replacements. Sorry I don't have much else to offer.\n<issue_comment>username_2: @username_1 \r\nThank you sir !<issue_closed>\n<issue_comment>username_1: closed due to inactivity. feel free to reopen if you can reproduce with latest stable (0.4 at the moment of writing).\n<issue_comment>username_4: Still can't get this to work.\r\n\r\nhttps://stackoverflow.com/questions/53449927/runtimeerror-dataloader-worker-is-killed-by-signal-illegal-instruction\r\n\r\nPlease help."}
{"repo": "pytorch/pytorch", "issue_id": 304991946, "issue_number": 5761, "timestamp": "2018-03-27 21:48:34+00:00", "text": "@username_2 anyways, it confirms my hypothesis that your CPU doesn't support the operation in `dataset.__getitem__`.", "context": "<issue_start><issue_comment>Title: Problem in DataLoader (same problem with #4643, but not solved by it)\nusername_0: - OS: Ubuntu 16.04\r\n- PyTorch version: torch-0.3.1-cp36-cp36-linux_x86_64.whl\r\n- How you installed PyTorch (conda, pip, source): pip3 from the binary\r\n- Python version: 3.6\r\n- CUDA/cuDNN version: 9.1\r\n- GPU models and configuration: GTX 1070 Ti\r\n\r\nAfter install using the wheel file, I have changed the torch/utils/data/Dataloader.py according to the changes shown at https://github.com/pytorch/pytorch/pull/4643\r\n\r\nBut the same error still happen when I try the tutorial example to load image database:\r\n\r\nException ignored in: <bound method DataLoaderIter.__del__ of <torch.utils.data.dataloader.DataLoaderIter object at 0x56090c8c7348>>\r\nTraceback (most recent call last):\r\n  File \"/home/username_0/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 375, in __del__\r\n  File \"/home/username_0/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 361, in _shutdown_workers\r\n  File \"/home/username_0/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 39, in _get_from_queue\r\n  File \"/usr/pkg/lib/python3.6/multiprocessing/queues.py\", line 337, in get\r\nImportError: sys.meta_path is None, Python is likely shutting down\r\n\r\nSo could anyone help me on that? Or is there any other file I should modify to get it run?\n<issue_comment>username_0: And, I am not sure how can I use the changes in torch/csrc/DataLoader.cpp, should I clone all the source code on GitHub and build the newest binary to fully fix this my problem?\n<issue_comment>username_1: when does this error happen? is it happening when you exit python?\n<issue_comment>username_2: @username_1 and @username_3,\r\nI am running this:\r\n\r\npython train.py --dataroot ./datasets/horse2zebra --name horse2zebra_cyclegan --model cycle_gan --no_dropout --loadSize 64 --nThreads 1\r\n\r\nfrom \r\n\r\nhttps://github.com/junyanz/pytorch-CycleGAN-and-pix2pix\r\n\r\nThe model is succefully created:\r\nmodel [CycleGANModel] was created\r\n\r\nI am on ubuntu 16.04, pytorch 3.1, cuda 9.0 and cuDnn 7.1\r\n\r\nThe error I get is traced below:\r\ncreate web directory ./checkpoints/horse2zebra_cyclegan/web...\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 22, in <module>\r\n    for i, data in enumerate(dataset):\r\n  File \"/home/sam/Documents/ComputerVision/pytorch-CycleGAN-and-pix2pix/data/custom_dataset_data_loader.py\", line 44, in __iter__\r\n    for i, data in enumerate(self.dataloader):\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 275, in __next__\r\n    idx, batch = self._get_batch()\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 254, in _get_batch\r\n    return self.data_queue.get()\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/queues.py\", line 335, in get\r\n    res = self._reader.recv_bytes()\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\r\n    buf = self._recv_bytes(maxlength)\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\r\n    buf = self._recv(4)\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\r\n    chunk = read(handle, remaining)\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 175, in handler\r\n    _error_if_any_worker_fails()\r\nRuntimeError: DataLoader worker (pid 3792) is killed by signal: Illegal instruction.\n<issue_comment>username_3: @username_2 that error is because you are running it on a computer that is too old and does not have SSE4 instruction on CPU.\n<issue_comment>username_1: @username_2 Also, it is more likely due to that your CPU doesn't support the operations done in the dataset.__getitem__.\n<issue_comment>username_2: @username_1 & @username_3,\r\n\r\nSome months ago I made it work in windows10 (I think)\r\nThe my windows machine crashed. So here I am. Sorry\r\n\r\nHere is what I see in cat /proc/cpuinfo:\r\n\r\nprocessor\t: 0\r\nvendor_id\t: AuthenticAMD\r\ncpu family\t: 21\r\nmodel\t\t: 2\r\nmodel name\t: AMD FX(tm)-4300 Quad-Core Processor\r\nstepping\t: 0\r\nmicrocode\t: 0x600081c\r\ncpu MHz\t\t: 1400.000\r\ncache size\t: 2048 KB\r\nphysical id\t: 0\r\nsiblings\t: 4\r\ncore id\t\t: 0\r\ncpu cores\t: 2\r\napicid\t\t: 16\r\ninitial apicid\t: 0\r\nfpu\t\t: yes\r\nfpu_exception\t: yes\r\ncpuid level\t: 13\r\nwp\t\t: yes\r\nflags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf pni pclmulqdq monitor ssse3 fma cx16 **sse4_1 sse4_2** popcnt aes xsave avx f16c lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs xop skinit wdt lwp fma4 tce nodeid_msr tbm topoext perfctr_core perfctr_nb cpb hw_pstate retpoline retpoline_amd rsb_ctxsw vmmcall bmi1 arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold\r\nbugs\t\t: fxsave_leak sysret_ss_attrs null_seg spectre_v1 spectre_v2\r\nbogomips\t: 7634.15\r\nTLB size\t: 1536 4K pages\r\nclflush size\t: 64\r\ncache_alignment\t: 64\r\naddress sizes\t: 48 bits physical, 48 bits virtual\r\npower management: ts ttp tm 100mhzsteps hwpstate cpb eff_freq_ro\r\n\r\n@username_1 how do I check if CPU support dataset.getitem?\n<issue_comment>username_1: @username_2 run with `nThreads=0`\n<issue_comment>username_2: tried it with nThreads=0\r\ndifferent error...\r\nmodel [CycleGANModel] was created\r\ncreate web directory ./checkpoints/horse2zebra_cyclegan/web...\r\nIllegal instruction (core dumped)\n<issue_comment>username_2: I see something I am uneasy about:\r\nsee bold below\r\n\r\nCustomDatasetDataLoader\r\ndataset [UnalignedDataset] was created\r\n/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torchvision-0.2.0-py3.6.egg/torchvision/transforms/transforms.py:156: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\r\n#training images = 1334\r\ncycle_gan\r\n**initialization method [normal]\r\ninitialization method [normal]\r\ninitialization method [normal]\r\ninitialization method [normal]**\n<issue_comment>username_1: @username_2 No it's same error. We just wrap it in a nice way so you can see it even if it's in the workers. \r\n\r\nIsn't that expected? CycleGAN has four networks: D_X D_Y G F .\n<issue_comment>username_1: @username_2 anyways, it confirms my hypothesis that your CPU doesn't support the operation in `dataset.__getitem__`.\n<issue_comment>username_2: Oh... Can you suggest a workaround?\n<issue_comment>username_1: @username_2 You can try to put debug statements in CycleGAN code and see which one isn't supported by your CPU (probably some PIL operations), and search for replacements. Sorry I don't have much else to offer.\n<issue_comment>username_2: @username_1 \r\nThank you sir !<issue_closed>\n<issue_comment>username_1: closed due to inactivity. feel free to reopen if you can reproduce with latest stable (0.4 at the moment of writing).\n<issue_comment>username_4: Still can't get this to work.\r\n\r\nhttps://stackoverflow.com/questions/53449927/runtimeerror-dataloader-worker-is-killed-by-signal-illegal-instruction\r\n\r\nPlease help."}
{"repo": "pytorch/pytorch", "issue_id": 304991946, "issue_number": 5761, "timestamp": "2018-03-27 21:49:07+00:00", "text": "Oh... Can you suggest a workaround?", "context": "<issue_start><issue_comment>Title: Problem in DataLoader (same problem with #4643, but not solved by it)\nusername_0: - OS: Ubuntu 16.04\r\n- PyTorch version: torch-0.3.1-cp36-cp36-linux_x86_64.whl\r\n- How you installed PyTorch (conda, pip, source): pip3 from the binary\r\n- Python version: 3.6\r\n- CUDA/cuDNN version: 9.1\r\n- GPU models and configuration: GTX 1070 Ti\r\n\r\nAfter install using the wheel file, I have changed the torch/utils/data/Dataloader.py according to the changes shown at https://github.com/pytorch/pytorch/pull/4643\r\n\r\nBut the same error still happen when I try the tutorial example to load image database:\r\n\r\nException ignored in: <bound method DataLoaderIter.__del__ of <torch.utils.data.dataloader.DataLoaderIter object at 0x56090c8c7348>>\r\nTraceback (most recent call last):\r\n  File \"/home/username_0/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 375, in __del__\r\n  File \"/home/username_0/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 361, in _shutdown_workers\r\n  File \"/home/username_0/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 39, in _get_from_queue\r\n  File \"/usr/pkg/lib/python3.6/multiprocessing/queues.py\", line 337, in get\r\nImportError: sys.meta_path is None, Python is likely shutting down\r\n\r\nSo could anyone help me on that? Or is there any other file I should modify to get it run?\n<issue_comment>username_0: And, I am not sure how can I use the changes in torch/csrc/DataLoader.cpp, should I clone all the source code on GitHub and build the newest binary to fully fix this my problem?\n<issue_comment>username_1: when does this error happen? is it happening when you exit python?\n<issue_comment>username_2: @username_1 and @username_3,\r\nI am running this:\r\n\r\npython train.py --dataroot ./datasets/horse2zebra --name horse2zebra_cyclegan --model cycle_gan --no_dropout --loadSize 64 --nThreads 1\r\n\r\nfrom \r\n\r\nhttps://github.com/junyanz/pytorch-CycleGAN-and-pix2pix\r\n\r\nThe model is succefully created:\r\nmodel [CycleGANModel] was created\r\n\r\nI am on ubuntu 16.04, pytorch 3.1, cuda 9.0 and cuDnn 7.1\r\n\r\nThe error I get is traced below:\r\ncreate web directory ./checkpoints/horse2zebra_cyclegan/web...\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 22, in <module>\r\n    for i, data in enumerate(dataset):\r\n  File \"/home/sam/Documents/ComputerVision/pytorch-CycleGAN-and-pix2pix/data/custom_dataset_data_loader.py\", line 44, in __iter__\r\n    for i, data in enumerate(self.dataloader):\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 275, in __next__\r\n    idx, batch = self._get_batch()\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 254, in _get_batch\r\n    return self.data_queue.get()\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/queues.py\", line 335, in get\r\n    res = self._reader.recv_bytes()\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\r\n    buf = self._recv_bytes(maxlength)\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\r\n    buf = self._recv(4)\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\r\n    chunk = read(handle, remaining)\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 175, in handler\r\n    _error_if_any_worker_fails()\r\nRuntimeError: DataLoader worker (pid 3792) is killed by signal: Illegal instruction.\n<issue_comment>username_3: @username_2 that error is because you are running it on a computer that is too old and does not have SSE4 instruction on CPU.\n<issue_comment>username_1: @username_2 Also, it is more likely due to that your CPU doesn't support the operations done in the dataset.__getitem__.\n<issue_comment>username_2: @username_1 & @username_3,\r\n\r\nSome months ago I made it work in windows10 (I think)\r\nThe my windows machine crashed. So here I am. Sorry\r\n\r\nHere is what I see in cat /proc/cpuinfo:\r\n\r\nprocessor\t: 0\r\nvendor_id\t: AuthenticAMD\r\ncpu family\t: 21\r\nmodel\t\t: 2\r\nmodel name\t: AMD FX(tm)-4300 Quad-Core Processor\r\nstepping\t: 0\r\nmicrocode\t: 0x600081c\r\ncpu MHz\t\t: 1400.000\r\ncache size\t: 2048 KB\r\nphysical id\t: 0\r\nsiblings\t: 4\r\ncore id\t\t: 0\r\ncpu cores\t: 2\r\napicid\t\t: 16\r\ninitial apicid\t: 0\r\nfpu\t\t: yes\r\nfpu_exception\t: yes\r\ncpuid level\t: 13\r\nwp\t\t: yes\r\nflags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf pni pclmulqdq monitor ssse3 fma cx16 **sse4_1 sse4_2** popcnt aes xsave avx f16c lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs xop skinit wdt lwp fma4 tce nodeid_msr tbm topoext perfctr_core perfctr_nb cpb hw_pstate retpoline retpoline_amd rsb_ctxsw vmmcall bmi1 arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold\r\nbugs\t\t: fxsave_leak sysret_ss_attrs null_seg spectre_v1 spectre_v2\r\nbogomips\t: 7634.15\r\nTLB size\t: 1536 4K pages\r\nclflush size\t: 64\r\ncache_alignment\t: 64\r\naddress sizes\t: 48 bits physical, 48 bits virtual\r\npower management: ts ttp tm 100mhzsteps hwpstate cpb eff_freq_ro\r\n\r\n@username_1 how do I check if CPU support dataset.getitem?\n<issue_comment>username_1: @username_2 run with `nThreads=0`\n<issue_comment>username_2: tried it with nThreads=0\r\ndifferent error...\r\nmodel [CycleGANModel] was created\r\ncreate web directory ./checkpoints/horse2zebra_cyclegan/web...\r\nIllegal instruction (core dumped)\n<issue_comment>username_2: I see something I am uneasy about:\r\nsee bold below\r\n\r\nCustomDatasetDataLoader\r\ndataset [UnalignedDataset] was created\r\n/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torchvision-0.2.0-py3.6.egg/torchvision/transforms/transforms.py:156: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\r\n#training images = 1334\r\ncycle_gan\r\n**initialization method [normal]\r\ninitialization method [normal]\r\ninitialization method [normal]\r\ninitialization method [normal]**\n<issue_comment>username_1: @username_2 No it's same error. We just wrap it in a nice way so you can see it even if it's in the workers. \r\n\r\nIsn't that expected? CycleGAN has four networks: D_X D_Y G F .\n<issue_comment>username_1: @username_2 anyways, it confirms my hypothesis that your CPU doesn't support the operation in `dataset.__getitem__`.\n<issue_comment>username_2: Oh... Can you suggest a workaround?\n<issue_comment>username_1: @username_2 You can try to put debug statements in CycleGAN code and see which one isn't supported by your CPU (probably some PIL operations), and search for replacements. Sorry I don't have much else to offer.\n<issue_comment>username_2: @username_1 \r\nThank you sir !<issue_closed>\n<issue_comment>username_1: closed due to inactivity. feel free to reopen if you can reproduce with latest stable (0.4 at the moment of writing).\n<issue_comment>username_4: Still can't get this to work.\r\n\r\nhttps://stackoverflow.com/questions/53449927/runtimeerror-dataloader-worker-is-killed-by-signal-illegal-instruction\r\n\r\nPlease help."}
{"repo": "pytorch/pytorch", "issue_id": 304991946, "issue_number": 5761, "timestamp": "2018-03-27 21:50:56+00:00", "text": "@username_2 You can try to put debug statements in CycleGAN code and see which one isn't supported by your CPU (probably some PIL operations), and search for replacements. Sorry I don't have much else to offer.", "context": "<issue_start><issue_comment>Title: Problem in DataLoader (same problem with #4643, but not solved by it)\nusername_0: - OS: Ubuntu 16.04\r\n- PyTorch version: torch-0.3.1-cp36-cp36-linux_x86_64.whl\r\n- How you installed PyTorch (conda, pip, source): pip3 from the binary\r\n- Python version: 3.6\r\n- CUDA/cuDNN version: 9.1\r\n- GPU models and configuration: GTX 1070 Ti\r\n\r\nAfter install using the wheel file, I have changed the torch/utils/data/Dataloader.py according to the changes shown at https://github.com/pytorch/pytorch/pull/4643\r\n\r\nBut the same error still happen when I try the tutorial example to load image database:\r\n\r\nException ignored in: <bound method DataLoaderIter.__del__ of <torch.utils.data.dataloader.DataLoaderIter object at 0x56090c8c7348>>\r\nTraceback (most recent call last):\r\n  File \"/home/username_0/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 375, in __del__\r\n  File \"/home/username_0/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 361, in _shutdown_workers\r\n  File \"/home/username_0/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 39, in _get_from_queue\r\n  File \"/usr/pkg/lib/python3.6/multiprocessing/queues.py\", line 337, in get\r\nImportError: sys.meta_path is None, Python is likely shutting down\r\n\r\nSo could anyone help me on that? Or is there any other file I should modify to get it run?\n<issue_comment>username_0: And, I am not sure how can I use the changes in torch/csrc/DataLoader.cpp, should I clone all the source code on GitHub and build the newest binary to fully fix this my problem?\n<issue_comment>username_1: when does this error happen? is it happening when you exit python?\n<issue_comment>username_2: @username_1 and @username_3,\r\nI am running this:\r\n\r\npython train.py --dataroot ./datasets/horse2zebra --name horse2zebra_cyclegan --model cycle_gan --no_dropout --loadSize 64 --nThreads 1\r\n\r\nfrom \r\n\r\nhttps://github.com/junyanz/pytorch-CycleGAN-and-pix2pix\r\n\r\nThe model is succefully created:\r\nmodel [CycleGANModel] was created\r\n\r\nI am on ubuntu 16.04, pytorch 3.1, cuda 9.0 and cuDnn 7.1\r\n\r\nThe error I get is traced below:\r\ncreate web directory ./checkpoints/horse2zebra_cyclegan/web...\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 22, in <module>\r\n    for i, data in enumerate(dataset):\r\n  File \"/home/sam/Documents/ComputerVision/pytorch-CycleGAN-and-pix2pix/data/custom_dataset_data_loader.py\", line 44, in __iter__\r\n    for i, data in enumerate(self.dataloader):\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 275, in __next__\r\n    idx, batch = self._get_batch()\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 254, in _get_batch\r\n    return self.data_queue.get()\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/queues.py\", line 335, in get\r\n    res = self._reader.recv_bytes()\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\r\n    buf = self._recv_bytes(maxlength)\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\r\n    buf = self._recv(4)\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\r\n    chunk = read(handle, remaining)\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 175, in handler\r\n    _error_if_any_worker_fails()\r\nRuntimeError: DataLoader worker (pid 3792) is killed by signal: Illegal instruction.\n<issue_comment>username_3: @username_2 that error is because you are running it on a computer that is too old and does not have SSE4 instruction on CPU.\n<issue_comment>username_1: @username_2 Also, it is more likely due to that your CPU doesn't support the operations done in the dataset.__getitem__.\n<issue_comment>username_2: @username_1 & @username_3,\r\n\r\nSome months ago I made it work in windows10 (I think)\r\nThe my windows machine crashed. So here I am. Sorry\r\n\r\nHere is what I see in cat /proc/cpuinfo:\r\n\r\nprocessor\t: 0\r\nvendor_id\t: AuthenticAMD\r\ncpu family\t: 21\r\nmodel\t\t: 2\r\nmodel name\t: AMD FX(tm)-4300 Quad-Core Processor\r\nstepping\t: 0\r\nmicrocode\t: 0x600081c\r\ncpu MHz\t\t: 1400.000\r\ncache size\t: 2048 KB\r\nphysical id\t: 0\r\nsiblings\t: 4\r\ncore id\t\t: 0\r\ncpu cores\t: 2\r\napicid\t\t: 16\r\ninitial apicid\t: 0\r\nfpu\t\t: yes\r\nfpu_exception\t: yes\r\ncpuid level\t: 13\r\nwp\t\t: yes\r\nflags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf pni pclmulqdq monitor ssse3 fma cx16 **sse4_1 sse4_2** popcnt aes xsave avx f16c lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs xop skinit wdt lwp fma4 tce nodeid_msr tbm topoext perfctr_core perfctr_nb cpb hw_pstate retpoline retpoline_amd rsb_ctxsw vmmcall bmi1 arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold\r\nbugs\t\t: fxsave_leak sysret_ss_attrs null_seg spectre_v1 spectre_v2\r\nbogomips\t: 7634.15\r\nTLB size\t: 1536 4K pages\r\nclflush size\t: 64\r\ncache_alignment\t: 64\r\naddress sizes\t: 48 bits physical, 48 bits virtual\r\npower management: ts ttp tm 100mhzsteps hwpstate cpb eff_freq_ro\r\n\r\n@username_1 how do I check if CPU support dataset.getitem?\n<issue_comment>username_1: @username_2 run with `nThreads=0`\n<issue_comment>username_2: tried it with nThreads=0\r\ndifferent error...\r\nmodel [CycleGANModel] was created\r\ncreate web directory ./checkpoints/horse2zebra_cyclegan/web...\r\nIllegal instruction (core dumped)\n<issue_comment>username_2: I see something I am uneasy about:\r\nsee bold below\r\n\r\nCustomDatasetDataLoader\r\ndataset [UnalignedDataset] was created\r\n/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torchvision-0.2.0-py3.6.egg/torchvision/transforms/transforms.py:156: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\r\n#training images = 1334\r\ncycle_gan\r\n**initialization method [normal]\r\ninitialization method [normal]\r\ninitialization method [normal]\r\ninitialization method [normal]**\n<issue_comment>username_1: @username_2 No it's same error. We just wrap it in a nice way so you can see it even if it's in the workers. \r\n\r\nIsn't that expected? CycleGAN has four networks: D_X D_Y G F .\n<issue_comment>username_1: @username_2 anyways, it confirms my hypothesis that your CPU doesn't support the operation in `dataset.__getitem__`.\n<issue_comment>username_2: Oh... Can you suggest a workaround?\n<issue_comment>username_1: @username_2 You can try to put debug statements in CycleGAN code and see which one isn't supported by your CPU (probably some PIL operations), and search for replacements. Sorry I don't have much else to offer.\n<issue_comment>username_2: @username_1 \r\nThank you sir !<issue_closed>\n<issue_comment>username_1: closed due to inactivity. feel free to reopen if you can reproduce with latest stable (0.4 at the moment of writing).\n<issue_comment>username_4: Still can't get this to work.\r\n\r\nhttps://stackoverflow.com/questions/53449927/runtimeerror-dataloader-worker-is-killed-by-signal-illegal-instruction\r\n\r\nPlease help."}
{"repo": "pytorch/pytorch", "issue_id": 304991946, "issue_number": 5761, "timestamp": "2018-03-27 21:51:53+00:00", "text": "@username_1 \r\nThank you sir !", "context": "<issue_start><issue_comment>Title: Problem in DataLoader (same problem with #4643, but not solved by it)\nusername_0: - OS: Ubuntu 16.04\r\n- PyTorch version: torch-0.3.1-cp36-cp36-linux_x86_64.whl\r\n- How you installed PyTorch (conda, pip, source): pip3 from the binary\r\n- Python version: 3.6\r\n- CUDA/cuDNN version: 9.1\r\n- GPU models and configuration: GTX 1070 Ti\r\n\r\nAfter install using the wheel file, I have changed the torch/utils/data/Dataloader.py according to the changes shown at https://github.com/pytorch/pytorch/pull/4643\r\n\r\nBut the same error still happen when I try the tutorial example to load image database:\r\n\r\nException ignored in: <bound method DataLoaderIter.__del__ of <torch.utils.data.dataloader.DataLoaderIter object at 0x56090c8c7348>>\r\nTraceback (most recent call last):\r\n  File \"/home/username_0/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 375, in __del__\r\n  File \"/home/username_0/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 361, in _shutdown_workers\r\n  File \"/home/username_0/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 39, in _get_from_queue\r\n  File \"/usr/pkg/lib/python3.6/multiprocessing/queues.py\", line 337, in get\r\nImportError: sys.meta_path is None, Python is likely shutting down\r\n\r\nSo could anyone help me on that? Or is there any other file I should modify to get it run?\n<issue_comment>username_0: And, I am not sure how can I use the changes in torch/csrc/DataLoader.cpp, should I clone all the source code on GitHub and build the newest binary to fully fix this my problem?\n<issue_comment>username_1: when does this error happen? is it happening when you exit python?\n<issue_comment>username_2: @username_1 and @username_3,\r\nI am running this:\r\n\r\npython train.py --dataroot ./datasets/horse2zebra --name horse2zebra_cyclegan --model cycle_gan --no_dropout --loadSize 64 --nThreads 1\r\n\r\nfrom \r\n\r\nhttps://github.com/junyanz/pytorch-CycleGAN-and-pix2pix\r\n\r\nThe model is succefully created:\r\nmodel [CycleGANModel] was created\r\n\r\nI am on ubuntu 16.04, pytorch 3.1, cuda 9.0 and cuDnn 7.1\r\n\r\nThe error I get is traced below:\r\ncreate web directory ./checkpoints/horse2zebra_cyclegan/web...\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 22, in <module>\r\n    for i, data in enumerate(dataset):\r\n  File \"/home/sam/Documents/ComputerVision/pytorch-CycleGAN-and-pix2pix/data/custom_dataset_data_loader.py\", line 44, in __iter__\r\n    for i, data in enumerate(self.dataloader):\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 275, in __next__\r\n    idx, batch = self._get_batch()\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 254, in _get_batch\r\n    return self.data_queue.get()\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/queues.py\", line 335, in get\r\n    res = self._reader.recv_bytes()\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\r\n    buf = self._recv_bytes(maxlength)\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\r\n    buf = self._recv(4)\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\r\n    chunk = read(handle, remaining)\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 175, in handler\r\n    _error_if_any_worker_fails()\r\nRuntimeError: DataLoader worker (pid 3792) is killed by signal: Illegal instruction.\n<issue_comment>username_3: @username_2 that error is because you are running it on a computer that is too old and does not have SSE4 instruction on CPU.\n<issue_comment>username_1: @username_2 Also, it is more likely due to that your CPU doesn't support the operations done in the dataset.__getitem__.\n<issue_comment>username_2: @username_1 & @username_3,\r\n\r\nSome months ago I made it work in windows10 (I think)\r\nThe my windows machine crashed. So here I am. Sorry\r\n\r\nHere is what I see in cat /proc/cpuinfo:\r\n\r\nprocessor\t: 0\r\nvendor_id\t: AuthenticAMD\r\ncpu family\t: 21\r\nmodel\t\t: 2\r\nmodel name\t: AMD FX(tm)-4300 Quad-Core Processor\r\nstepping\t: 0\r\nmicrocode\t: 0x600081c\r\ncpu MHz\t\t: 1400.000\r\ncache size\t: 2048 KB\r\nphysical id\t: 0\r\nsiblings\t: 4\r\ncore id\t\t: 0\r\ncpu cores\t: 2\r\napicid\t\t: 16\r\ninitial apicid\t: 0\r\nfpu\t\t: yes\r\nfpu_exception\t: yes\r\ncpuid level\t: 13\r\nwp\t\t: yes\r\nflags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf pni pclmulqdq monitor ssse3 fma cx16 **sse4_1 sse4_2** popcnt aes xsave avx f16c lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs xop skinit wdt lwp fma4 tce nodeid_msr tbm topoext perfctr_core perfctr_nb cpb hw_pstate retpoline retpoline_amd rsb_ctxsw vmmcall bmi1 arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold\r\nbugs\t\t: fxsave_leak sysret_ss_attrs null_seg spectre_v1 spectre_v2\r\nbogomips\t: 7634.15\r\nTLB size\t: 1536 4K pages\r\nclflush size\t: 64\r\ncache_alignment\t: 64\r\naddress sizes\t: 48 bits physical, 48 bits virtual\r\npower management: ts ttp tm 100mhzsteps hwpstate cpb eff_freq_ro\r\n\r\n@username_1 how do I check if CPU support dataset.getitem?\n<issue_comment>username_1: @username_2 run with `nThreads=0`\n<issue_comment>username_2: tried it with nThreads=0\r\ndifferent error...\r\nmodel [CycleGANModel] was created\r\ncreate web directory ./checkpoints/horse2zebra_cyclegan/web...\r\nIllegal instruction (core dumped)\n<issue_comment>username_2: I see something I am uneasy about:\r\nsee bold below\r\n\r\nCustomDatasetDataLoader\r\ndataset [UnalignedDataset] was created\r\n/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torchvision-0.2.0-py3.6.egg/torchvision/transforms/transforms.py:156: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\r\n#training images = 1334\r\ncycle_gan\r\n**initialization method [normal]\r\ninitialization method [normal]\r\ninitialization method [normal]\r\ninitialization method [normal]**\n<issue_comment>username_1: @username_2 No it's same error. We just wrap it in a nice way so you can see it even if it's in the workers. \r\n\r\nIsn't that expected? CycleGAN has four networks: D_X D_Y G F .\n<issue_comment>username_1: @username_2 anyways, it confirms my hypothesis that your CPU doesn't support the operation in `dataset.__getitem__`.\n<issue_comment>username_2: Oh... Can you suggest a workaround?\n<issue_comment>username_1: @username_2 You can try to put debug statements in CycleGAN code and see which one isn't supported by your CPU (probably some PIL operations), and search for replacements. Sorry I don't have much else to offer.\n<issue_comment>username_2: @username_1 \r\nThank you sir !<issue_closed>\n<issue_comment>username_1: closed due to inactivity. feel free to reopen if you can reproduce with latest stable (0.4 at the moment of writing).\n<issue_comment>username_4: Still can't get this to work.\r\n\r\nhttps://stackoverflow.com/questions/53449927/runtimeerror-dataloader-worker-is-killed-by-signal-illegal-instruction\r\n\r\nPlease help."}
{"repo": "pytorch/pytorch", "issue_id": 304991946, "issue_number": 5761, "timestamp": "2018-04-28 03:32:18+00:00", "text": "", "context": "<issue_start><issue_comment>Title: Problem in DataLoader (same problem with #4643, but not solved by it)\nusername_0: - OS: Ubuntu 16.04\r\n- PyTorch version: torch-0.3.1-cp36-cp36-linux_x86_64.whl\r\n- How you installed PyTorch (conda, pip, source): pip3 from the binary\r\n- Python version: 3.6\r\n- CUDA/cuDNN version: 9.1\r\n- GPU models and configuration: GTX 1070 Ti\r\n\r\nAfter install using the wheel file, I have changed the torch/utils/data/Dataloader.py according to the changes shown at https://github.com/pytorch/pytorch/pull/4643\r\n\r\nBut the same error still happen when I try the tutorial example to load image database:\r\n\r\nException ignored in: <bound method DataLoaderIter.__del__ of <torch.utils.data.dataloader.DataLoaderIter object at 0x56090c8c7348>>\r\nTraceback (most recent call last):\r\n  File \"/home/username_0/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 375, in __del__\r\n  File \"/home/username_0/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 361, in _shutdown_workers\r\n  File \"/home/username_0/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 39, in _get_from_queue\r\n  File \"/usr/pkg/lib/python3.6/multiprocessing/queues.py\", line 337, in get\r\nImportError: sys.meta_path is None, Python is likely shutting down\r\n\r\nSo could anyone help me on that? Or is there any other file I should modify to get it run?\n<issue_comment>username_0: And, I am not sure how can I use the changes in torch/csrc/DataLoader.cpp, should I clone all the source code on GitHub and build the newest binary to fully fix this my problem?\n<issue_comment>username_1: when does this error happen? is it happening when you exit python?\n<issue_comment>username_2: @username_1 and @username_3,\r\nI am running this:\r\n\r\npython train.py --dataroot ./datasets/horse2zebra --name horse2zebra_cyclegan --model cycle_gan --no_dropout --loadSize 64 --nThreads 1\r\n\r\nfrom \r\n\r\nhttps://github.com/junyanz/pytorch-CycleGAN-and-pix2pix\r\n\r\nThe model is succefully created:\r\nmodel [CycleGANModel] was created\r\n\r\nI am on ubuntu 16.04, pytorch 3.1, cuda 9.0 and cuDnn 7.1\r\n\r\nThe error I get is traced below:\r\ncreate web directory ./checkpoints/horse2zebra_cyclegan/web...\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 22, in <module>\r\n    for i, data in enumerate(dataset):\r\n  File \"/home/sam/Documents/ComputerVision/pytorch-CycleGAN-and-pix2pix/data/custom_dataset_data_loader.py\", line 44, in __iter__\r\n    for i, data in enumerate(self.dataloader):\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 275, in __next__\r\n    idx, batch = self._get_batch()\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 254, in _get_batch\r\n    return self.data_queue.get()\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/queues.py\", line 335, in get\r\n    res = self._reader.recv_bytes()\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\r\n    buf = self._recv_bytes(maxlength)\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\r\n    buf = self._recv(4)\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\r\n    chunk = read(handle, remaining)\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 175, in handler\r\n    _error_if_any_worker_fails()\r\nRuntimeError: DataLoader worker (pid 3792) is killed by signal: Illegal instruction.\n<issue_comment>username_3: @username_2 that error is because you are running it on a computer that is too old and does not have SSE4 instruction on CPU.\n<issue_comment>username_1: @username_2 Also, it is more likely due to that your CPU doesn't support the operations done in the dataset.__getitem__.\n<issue_comment>username_2: @username_1 & @username_3,\r\n\r\nSome months ago I made it work in windows10 (I think)\r\nThe my windows machine crashed. So here I am. Sorry\r\n\r\nHere is what I see in cat /proc/cpuinfo:\r\n\r\nprocessor\t: 0\r\nvendor_id\t: AuthenticAMD\r\ncpu family\t: 21\r\nmodel\t\t: 2\r\nmodel name\t: AMD FX(tm)-4300 Quad-Core Processor\r\nstepping\t: 0\r\nmicrocode\t: 0x600081c\r\ncpu MHz\t\t: 1400.000\r\ncache size\t: 2048 KB\r\nphysical id\t: 0\r\nsiblings\t: 4\r\ncore id\t\t: 0\r\ncpu cores\t: 2\r\napicid\t\t: 16\r\ninitial apicid\t: 0\r\nfpu\t\t: yes\r\nfpu_exception\t: yes\r\ncpuid level\t: 13\r\nwp\t\t: yes\r\nflags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf pni pclmulqdq monitor ssse3 fma cx16 **sse4_1 sse4_2** popcnt aes xsave avx f16c lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs xop skinit wdt lwp fma4 tce nodeid_msr tbm topoext perfctr_core perfctr_nb cpb hw_pstate retpoline retpoline_amd rsb_ctxsw vmmcall bmi1 arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold\r\nbugs\t\t: fxsave_leak sysret_ss_attrs null_seg spectre_v1 spectre_v2\r\nbogomips\t: 7634.15\r\nTLB size\t: 1536 4K pages\r\nclflush size\t: 64\r\ncache_alignment\t: 64\r\naddress sizes\t: 48 bits physical, 48 bits virtual\r\npower management: ts ttp tm 100mhzsteps hwpstate cpb eff_freq_ro\r\n\r\n@username_1 how do I check if CPU support dataset.getitem?\n<issue_comment>username_1: @username_2 run with `nThreads=0`\n<issue_comment>username_2: tried it with nThreads=0\r\ndifferent error...\r\nmodel [CycleGANModel] was created\r\ncreate web directory ./checkpoints/horse2zebra_cyclegan/web...\r\nIllegal instruction (core dumped)\n<issue_comment>username_2: I see something I am uneasy about:\r\nsee bold below\r\n\r\nCustomDatasetDataLoader\r\ndataset [UnalignedDataset] was created\r\n/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torchvision-0.2.0-py3.6.egg/torchvision/transforms/transforms.py:156: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\r\n#training images = 1334\r\ncycle_gan\r\n**initialization method [normal]\r\ninitialization method [normal]\r\ninitialization method [normal]\r\ninitialization method [normal]**\n<issue_comment>username_1: @username_2 No it's same error. We just wrap it in a nice way so you can see it even if it's in the workers. \r\n\r\nIsn't that expected? CycleGAN has four networks: D_X D_Y G F .\n<issue_comment>username_1: @username_2 anyways, it confirms my hypothesis that your CPU doesn't support the operation in `dataset.__getitem__`.\n<issue_comment>username_2: Oh... Can you suggest a workaround?\n<issue_comment>username_1: @username_2 You can try to put debug statements in CycleGAN code and see which one isn't supported by your CPU (probably some PIL operations), and search for replacements. Sorry I don't have much else to offer.\n<issue_comment>username_2: @username_1 \r\nThank you sir !<issue_closed>\n<issue_comment>username_1: closed due to inactivity. feel free to reopen if you can reproduce with latest stable (0.4 at the moment of writing).\n<issue_comment>username_4: Still can't get this to work.\r\n\r\nhttps://stackoverflow.com/questions/53449927/runtimeerror-dataloader-worker-is-killed-by-signal-illegal-instruction\r\n\r\nPlease help."}
{"repo": "pytorch/pytorch", "issue_id": 304991946, "issue_number": 5761, "timestamp": "2018-04-28 03:32:18+00:00", "text": "closed due to inactivity. feel free to reopen if you can reproduce with latest stable (0.4 at the moment of writing).", "context": "<issue_start><issue_comment>Title: Problem in DataLoader (same problem with #4643, but not solved by it)\nusername_0: - OS: Ubuntu 16.04\r\n- PyTorch version: torch-0.3.1-cp36-cp36-linux_x86_64.whl\r\n- How you installed PyTorch (conda, pip, source): pip3 from the binary\r\n- Python version: 3.6\r\n- CUDA/cuDNN version: 9.1\r\n- GPU models and configuration: GTX 1070 Ti\r\n\r\nAfter install using the wheel file, I have changed the torch/utils/data/Dataloader.py according to the changes shown at https://github.com/pytorch/pytorch/pull/4643\r\n\r\nBut the same error still happen when I try the tutorial example to load image database:\r\n\r\nException ignored in: <bound method DataLoaderIter.__del__ of <torch.utils.data.dataloader.DataLoaderIter object at 0x56090c8c7348>>\r\nTraceback (most recent call last):\r\n  File \"/home/username_0/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 375, in __del__\r\n  File \"/home/username_0/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 361, in _shutdown_workers\r\n  File \"/home/username_0/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 39, in _get_from_queue\r\n  File \"/usr/pkg/lib/python3.6/multiprocessing/queues.py\", line 337, in get\r\nImportError: sys.meta_path is None, Python is likely shutting down\r\n\r\nSo could anyone help me on that? Or is there any other file I should modify to get it run?\n<issue_comment>username_0: And, I am not sure how can I use the changes in torch/csrc/DataLoader.cpp, should I clone all the source code on GitHub and build the newest binary to fully fix this my problem?\n<issue_comment>username_1: when does this error happen? is it happening when you exit python?\n<issue_comment>username_2: @username_1 and @username_3,\r\nI am running this:\r\n\r\npython train.py --dataroot ./datasets/horse2zebra --name horse2zebra_cyclegan --model cycle_gan --no_dropout --loadSize 64 --nThreads 1\r\n\r\nfrom \r\n\r\nhttps://github.com/junyanz/pytorch-CycleGAN-and-pix2pix\r\n\r\nThe model is succefully created:\r\nmodel [CycleGANModel] was created\r\n\r\nI am on ubuntu 16.04, pytorch 3.1, cuda 9.0 and cuDnn 7.1\r\n\r\nThe error I get is traced below:\r\ncreate web directory ./checkpoints/horse2zebra_cyclegan/web...\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 22, in <module>\r\n    for i, data in enumerate(dataset):\r\n  File \"/home/sam/Documents/ComputerVision/pytorch-CycleGAN-and-pix2pix/data/custom_dataset_data_loader.py\", line 44, in __iter__\r\n    for i, data in enumerate(self.dataloader):\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 275, in __next__\r\n    idx, batch = self._get_batch()\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 254, in _get_batch\r\n    return self.data_queue.get()\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/queues.py\", line 335, in get\r\n    res = self._reader.recv_bytes()\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\r\n    buf = self._recv_bytes(maxlength)\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\r\n    buf = self._recv(4)\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\r\n    chunk = read(handle, remaining)\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 175, in handler\r\n    _error_if_any_worker_fails()\r\nRuntimeError: DataLoader worker (pid 3792) is killed by signal: Illegal instruction.\n<issue_comment>username_3: @username_2 that error is because you are running it on a computer that is too old and does not have SSE4 instruction on CPU.\n<issue_comment>username_1: @username_2 Also, it is more likely due to that your CPU doesn't support the operations done in the dataset.__getitem__.\n<issue_comment>username_2: @username_1 & @username_3,\r\n\r\nSome months ago I made it work in windows10 (I think)\r\nThe my windows machine crashed. So here I am. Sorry\r\n\r\nHere is what I see in cat /proc/cpuinfo:\r\n\r\nprocessor\t: 0\r\nvendor_id\t: AuthenticAMD\r\ncpu family\t: 21\r\nmodel\t\t: 2\r\nmodel name\t: AMD FX(tm)-4300 Quad-Core Processor\r\nstepping\t: 0\r\nmicrocode\t: 0x600081c\r\ncpu MHz\t\t: 1400.000\r\ncache size\t: 2048 KB\r\nphysical id\t: 0\r\nsiblings\t: 4\r\ncore id\t\t: 0\r\ncpu cores\t: 2\r\napicid\t\t: 16\r\ninitial apicid\t: 0\r\nfpu\t\t: yes\r\nfpu_exception\t: yes\r\ncpuid level\t: 13\r\nwp\t\t: yes\r\nflags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf pni pclmulqdq monitor ssse3 fma cx16 **sse4_1 sse4_2** popcnt aes xsave avx f16c lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs xop skinit wdt lwp fma4 tce nodeid_msr tbm topoext perfctr_core perfctr_nb cpb hw_pstate retpoline retpoline_amd rsb_ctxsw vmmcall bmi1 arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold\r\nbugs\t\t: fxsave_leak sysret_ss_attrs null_seg spectre_v1 spectre_v2\r\nbogomips\t: 7634.15\r\nTLB size\t: 1536 4K pages\r\nclflush size\t: 64\r\ncache_alignment\t: 64\r\naddress sizes\t: 48 bits physical, 48 bits virtual\r\npower management: ts ttp tm 100mhzsteps hwpstate cpb eff_freq_ro\r\n\r\n@username_1 how do I check if CPU support dataset.getitem?\n<issue_comment>username_1: @username_2 run with `nThreads=0`\n<issue_comment>username_2: tried it with nThreads=0\r\ndifferent error...\r\nmodel [CycleGANModel] was created\r\ncreate web directory ./checkpoints/horse2zebra_cyclegan/web...\r\nIllegal instruction (core dumped)\n<issue_comment>username_2: I see something I am uneasy about:\r\nsee bold below\r\n\r\nCustomDatasetDataLoader\r\ndataset [UnalignedDataset] was created\r\n/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torchvision-0.2.0-py3.6.egg/torchvision/transforms/transforms.py:156: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\r\n#training images = 1334\r\ncycle_gan\r\n**initialization method [normal]\r\ninitialization method [normal]\r\ninitialization method [normal]\r\ninitialization method [normal]**\n<issue_comment>username_1: @username_2 No it's same error. We just wrap it in a nice way so you can see it even if it's in the workers. \r\n\r\nIsn't that expected? CycleGAN has four networks: D_X D_Y G F .\n<issue_comment>username_1: @username_2 anyways, it confirms my hypothesis that your CPU doesn't support the operation in `dataset.__getitem__`.\n<issue_comment>username_2: Oh... Can you suggest a workaround?\n<issue_comment>username_1: @username_2 You can try to put debug statements in CycleGAN code and see which one isn't supported by your CPU (probably some PIL operations), and search for replacements. Sorry I don't have much else to offer.\n<issue_comment>username_2: @username_1 \r\nThank you sir !<issue_closed>\n<issue_comment>username_1: closed due to inactivity. feel free to reopen if you can reproduce with latest stable (0.4 at the moment of writing).\n<issue_comment>username_4: Still can't get this to work.\r\n\r\nhttps://stackoverflow.com/questions/53449927/runtimeerror-dataloader-worker-is-killed-by-signal-illegal-instruction\r\n\r\nPlease help."}
{"repo": "pytorch/pytorch", "issue_id": 304991946, "issue_number": 5761, "timestamp": "2018-11-23 16:31:47+00:00", "text": "Still can't get this to work.\r\n\r\nhttps://stackoverflow.com/questions/53449927/runtimeerror-dataloader-worker-is-killed-by-signal-illegal-instruction\r\n\r\nPlease help.", "context": "<issue_start><issue_comment>Title: Problem in DataLoader (same problem with #4643, but not solved by it)\nusername_0: - OS: Ubuntu 16.04\r\n- PyTorch version: torch-0.3.1-cp36-cp36-linux_x86_64.whl\r\n- How you installed PyTorch (conda, pip, source): pip3 from the binary\r\n- Python version: 3.6\r\n- CUDA/cuDNN version: 9.1\r\n- GPU models and configuration: GTX 1070 Ti\r\n\r\nAfter install using the wheel file, I have changed the torch/utils/data/Dataloader.py according to the changes shown at https://github.com/pytorch/pytorch/pull/4643\r\n\r\nBut the same error still happen when I try the tutorial example to load image database:\r\n\r\nException ignored in: <bound method DataLoaderIter.__del__ of <torch.utils.data.dataloader.DataLoaderIter object at 0x56090c8c7348>>\r\nTraceback (most recent call last):\r\n  File \"/home/username_0/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 375, in __del__\r\n  File \"/home/username_0/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 361, in _shutdown_workers\r\n  File \"/home/username_0/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 39, in _get_from_queue\r\n  File \"/usr/pkg/lib/python3.6/multiprocessing/queues.py\", line 337, in get\r\nImportError: sys.meta_path is None, Python is likely shutting down\r\n\r\nSo could anyone help me on that? Or is there any other file I should modify to get it run?\n<issue_comment>username_0: And, I am not sure how can I use the changes in torch/csrc/DataLoader.cpp, should I clone all the source code on GitHub and build the newest binary to fully fix this my problem?\n<issue_comment>username_1: when does this error happen? is it happening when you exit python?\n<issue_comment>username_2: @username_1 and @username_3,\r\nI am running this:\r\n\r\npython train.py --dataroot ./datasets/horse2zebra --name horse2zebra_cyclegan --model cycle_gan --no_dropout --loadSize 64 --nThreads 1\r\n\r\nfrom \r\n\r\nhttps://github.com/junyanz/pytorch-CycleGAN-and-pix2pix\r\n\r\nThe model is succefully created:\r\nmodel [CycleGANModel] was created\r\n\r\nI am on ubuntu 16.04, pytorch 3.1, cuda 9.0 and cuDnn 7.1\r\n\r\nThe error I get is traced below:\r\ncreate web directory ./checkpoints/horse2zebra_cyclegan/web...\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 22, in <module>\r\n    for i, data in enumerate(dataset):\r\n  File \"/home/sam/Documents/ComputerVision/pytorch-CycleGAN-and-pix2pix/data/custom_dataset_data_loader.py\", line 44, in __iter__\r\n    for i, data in enumerate(self.dataloader):\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 275, in __next__\r\n    idx, batch = self._get_batch()\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 254, in _get_batch\r\n    return self.data_queue.get()\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/queues.py\", line 335, in get\r\n    res = self._reader.recv_bytes()\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\r\n    buf = self._recv_bytes(maxlength)\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\r\n    buf = self._recv(4)\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\r\n    chunk = read(handle, remaining)\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 175, in handler\r\n    _error_if_any_worker_fails()\r\nRuntimeError: DataLoader worker (pid 3792) is killed by signal: Illegal instruction.\n<issue_comment>username_3: @username_2 that error is because you are running it on a computer that is too old and does not have SSE4 instruction on CPU.\n<issue_comment>username_1: @username_2 Also, it is more likely due to that your CPU doesn't support the operations done in the dataset.__getitem__.\n<issue_comment>username_2: @username_1 & @username_3,\r\n\r\nSome months ago I made it work in windows10 (I think)\r\nThe my windows machine crashed. So here I am. Sorry\r\n\r\nHere is what I see in cat /proc/cpuinfo:\r\n\r\nprocessor\t: 0\r\nvendor_id\t: AuthenticAMD\r\ncpu family\t: 21\r\nmodel\t\t: 2\r\nmodel name\t: AMD FX(tm)-4300 Quad-Core Processor\r\nstepping\t: 0\r\nmicrocode\t: 0x600081c\r\ncpu MHz\t\t: 1400.000\r\ncache size\t: 2048 KB\r\nphysical id\t: 0\r\nsiblings\t: 4\r\ncore id\t\t: 0\r\ncpu cores\t: 2\r\napicid\t\t: 16\r\ninitial apicid\t: 0\r\nfpu\t\t: yes\r\nfpu_exception\t: yes\r\ncpuid level\t: 13\r\nwp\t\t: yes\r\nflags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf pni pclmulqdq monitor ssse3 fma cx16 **sse4_1 sse4_2** popcnt aes xsave avx f16c lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs xop skinit wdt lwp fma4 tce nodeid_msr tbm topoext perfctr_core perfctr_nb cpb hw_pstate retpoline retpoline_amd rsb_ctxsw vmmcall bmi1 arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold\r\nbugs\t\t: fxsave_leak sysret_ss_attrs null_seg spectre_v1 spectre_v2\r\nbogomips\t: 7634.15\r\nTLB size\t: 1536 4K pages\r\nclflush size\t: 64\r\ncache_alignment\t: 64\r\naddress sizes\t: 48 bits physical, 48 bits virtual\r\npower management: ts ttp tm 100mhzsteps hwpstate cpb eff_freq_ro\r\n\r\n@username_1 how do I check if CPU support dataset.getitem?\n<issue_comment>username_1: @username_2 run with `nThreads=0`\n<issue_comment>username_2: tried it with nThreads=0\r\ndifferent error...\r\nmodel [CycleGANModel] was created\r\ncreate web directory ./checkpoints/horse2zebra_cyclegan/web...\r\nIllegal instruction (core dumped)\n<issue_comment>username_2: I see something I am uneasy about:\r\nsee bold below\r\n\r\nCustomDatasetDataLoader\r\ndataset [UnalignedDataset] was created\r\n/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torchvision-0.2.0-py3.6.egg/torchvision/transforms/transforms.py:156: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\r\n#training images = 1334\r\ncycle_gan\r\n**initialization method [normal]\r\ninitialization method [normal]\r\ninitialization method [normal]\r\ninitialization method [normal]**\n<issue_comment>username_1: @username_2 No it's same error. We just wrap it in a nice way so you can see it even if it's in the workers. \r\n\r\nIsn't that expected? CycleGAN has four networks: D_X D_Y G F .\n<issue_comment>username_1: @username_2 anyways, it confirms my hypothesis that your CPU doesn't support the operation in `dataset.__getitem__`.\n<issue_comment>username_2: Oh... Can you suggest a workaround?\n<issue_comment>username_1: @username_2 You can try to put debug statements in CycleGAN code and see which one isn't supported by your CPU (probably some PIL operations), and search for replacements. Sorry I don't have much else to offer.\n<issue_comment>username_2: @username_1 \r\nThank you sir !<issue_closed>\n<issue_comment>username_1: closed due to inactivity. feel free to reopen if you can reproduce with latest stable (0.4 at the moment of writing).\n<issue_comment>username_4: Still can't get this to work.\r\n\r\nhttps://stackoverflow.com/questions/53449927/runtimeerror-dataloader-worker-is-killed-by-signal-illegal-instruction\r\n\r\nPlease help."}
{"repo": "pytorch/pytorch", "issue_id": 391131022, "issue_number": 15222, "timestamp": "2018-12-14 14:24:20+00:00", "text": "\u5df2\u5b8c\u6210\u751f\u6210\u9879\u76ee\u201cE:\\SOFTWARE\\pytorch\\build\\caffe2\\torch\\lib\\libshm_windows\\shm.vcxproj\u201d(\u9ed8\u8ba4\u76ee\u6807)\u7684\u64cd\u4f5c\u3002\r\n\r\n\u5df2\u5b8c\u6210\u751f\u6210\u9879\u76ee\u201cE:\\SOFTWARE\\pytorch\\build\\ALL_BUILD.vcxproj\u201d(\u9ed8\u8ba4\u76ee\u6807)\u7684\u64cd\u4f5c - \u5931\u8d25\u3002\r\n\r\n\u5df2\u5b8c\u6210\u751f\u6210\u9879\u76ee\u201cE:\\SOFTWARE\\pytorch\\build\\INSTALL.vcxproj\u201d(\u9ed8\u8ba4\u76ee\u6807)\u7684\u64cd\u4f5c - \u5931\u8d25\u3002\r\n\r\n\r\n\u751f\u6210\u5931\u8d25\u3002\r\n\r\n\u201cE:\\SOFTWARE\\pytorch\\build\\INSTALL.vcxproj\u201d(\u9ed8\u8ba4\u76ee\u6807) (1) ->\r\n\u201cE:\\SOFTWARE\\pytorch\\build\\ALL_BUILD.vcxproj\u201d(\u9ed8\u8ba4\u76ee\u6807) (3) ->\r\n\u201cE:\\SOFTWARE\\pytorch\\build\\caffe2\\AlgorithmsTest.vcxproj\u201d(\u9ed8\u8ba4\u76ee\u6807) (4) ->\r\n\u201cE:\\SOFTWARE\\pytorch\\build\\caffe2\\caffe2_gpu.vcxproj\u201d(\u9ed8\u8ba4\u76ee\u6807) (22) ->\r\n(Link \u76ee\u6807) ->\r\n  C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\lib\\x64.obj : fatal error LNK1136: \u65e0\u6548\u6216\u635f\u574f\u7684\u6587\u4ef6 [E:\\SOFTWARE\\pyt\r\norch\\build\\caffe2\\caffe2_gpu.vcxproj]\r\n\r\n    0 \u4e2a\u8b66\u544a\r\n    1 \u4e2a\u9519\u8bef\r\n\r\n\u5df2\u7528\u65f6\u95f4 00:00:16.23\r\n\r\nE:\\SOFTWARE\\pytorch\\build>IF ERRORLEVEL 1 exit 1\r\nFailed to run 'tools\\build_pytorch_libs.bat --use-cuda --use-fbgemm --use-nnpack --use-mkldnn --use-qnnpack caffe2'\r\n\r\npytorch version\uff1astable\uff081.0\uff09\r\nos\uff1aWindows 10 Pro Version 1803\r\npython version:3.7\r\nCUDA:10\r\ncudnn:7.4.1\r\nVisual Studio version: Microsoft Visual Studio 2017 Community Edition\r\n\r\nCan anyone help me solve this problem? Thanks !", "context": "<issue_start><issue_comment>Title: Can't open x64.obj while installing pytorch with win10 \nusername_0: \u5df2\u5b8c\u6210\u751f\u6210\u9879\u76ee\u201cE:\\SOFTWARE\\pytorch\\build\\caffe2\\torch\\lib\\libshm_windows\\shm.vcxproj\u201d(\u9ed8\u8ba4\u76ee\u6807)\u7684\u64cd\u4f5c\u3002\r\n\r\n\u5df2\u5b8c\u6210\u751f\u6210\u9879\u76ee\u201cE:\\SOFTWARE\\pytorch\\build\\ALL_BUILD.vcxproj\u201d(\u9ed8\u8ba4\u76ee\u6807)\u7684\u64cd\u4f5c - \u5931\u8d25\u3002\r\n\r\n\u5df2\u5b8c\u6210\u751f\u6210\u9879\u76ee\u201cE:\\SOFTWARE\\pytorch\\build\\INSTALL.vcxproj\u201d(\u9ed8\u8ba4\u76ee\u6807)\u7684\u64cd\u4f5c - \u5931\u8d25\u3002\r\n\r\n\r\n\u751f\u6210\u5931\u8d25\u3002\r\n\r\n\u201cE:\\SOFTWARE\\pytorch\\build\\INSTALL.vcxproj\u201d(\u9ed8\u8ba4\u76ee\u6807) (1) ->\r\n\u201cE:\\SOFTWARE\\pytorch\\build\\ALL_BUILD.vcxproj\u201d(\u9ed8\u8ba4\u76ee\u6807) (3) ->\r\n\u201cE:\\SOFTWARE\\pytorch\\build\\caffe2\\AlgorithmsTest.vcxproj\u201d(\u9ed8\u8ba4\u76ee\u6807) (4) ->\r\n\u201cE:\\SOFTWARE\\pytorch\\build\\caffe2\\caffe2_gpu.vcxproj\u201d(\u9ed8\u8ba4\u76ee\u6807) (22) ->\r\n(Link \u76ee\u6807) ->\r\n  C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\lib\\x64.obj : fatal error LNK1136: \u65e0\u6548\u6216\u635f\u574f\u7684\u6587\u4ef6 [E:\\SOFTWARE\\pyt\r\norch\\build\\caffe2\\caffe2_gpu.vcxproj]\r\n\r\n    0 \u4e2a\u8b66\u544a\r\n    1 \u4e2a\u9519\u8bef\r\n\r\n\u5df2\u7528\u65f6\u95f4 00:00:16.23\r\n\r\nE:\\SOFTWARE\\pytorch\\build>IF ERRORLEVEL 1 exit 1\r\nFailed to run 'tools\\build_pytorch_libs.bat --use-cuda --use-fbgemm --use-nnpack --use-mkldnn --use-qnnpack caffe2'\r\n\r\npytorch version\uff1astable\uff081.0\uff09\r\nos\uff1aWindows 10 Pro Version 1803\r\npython version:3.7\r\nCUDA:10\r\ncudnn:7.4.1\r\nVisual Studio version: Microsoft Visual Studio 2017 Community Edition\r\n\r\nCan anyone help me solve this problem? Thanks !\n<issue_comment>username_1: Please provide the full log.\n<issue_comment>username_0: the full log\uff1a\r\n\r\nBuilding wheel torch-1.0.0a0+df61437\r\nrunning install\r\nsetup.py::run()\r\nrunning build_deps\r\nsetup.py::build_deps::run()\r\n\r\nE:\\SOFTWARE\\pytorch>cd \"E:\\SOFTWARE\\pytorch\\tools\\\\..\"\r\n\r\nE:\\SOFTWARE\\pytorch>set PATH=\\bin;E:\\SOFTWARE\\Anaconda\\DLLs;E:\\SOFTWARE\\Visual Studio\\VC\\Tools\\MSVC\\14.11.25503\\bin\\HostX64\\x64;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\VC\\VCPackages;C:\\Program Files (x86)\\Microsoft SDKs\\TypeScript\\3.1;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\TestWindow;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\TeamFoundation\\Team Explorer;E:\\SOFTWARE\\Visual Studio\\MSBuild\\15.0\\bin\\Roslyn;E:\\SOFTWARE\\Visual Studio\\Team Tools\\Performance Tools\\x64;E:\\SOFTWARE\\Visual Studio\\Team Tools\\Performance Tools;E:\\SOFTWARE\\Visual Studio\\share\\Common\\VSPerfCollectionTools\\\\x64;E:\\SOFTWARE\\Visual Studio\\share\\Common\\VSPerfCollectionTools\\;C:\\Program Files (x86)\\Microsoft SDKs\\Windows\\v10.0A\\bin\\NETFX 4.6.1 Tools\\x64\\;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\10.0.17763.0\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x64;E:\\SOFTWARE\\Visual Studio\\\\MSBuild\\15.0\\bin;C:\\WINDOWS\\Microsoft.NET\\Framework64\\v4.0.30319;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\;E:\\SOFTWARE\\Visual Studio\\Common7\\Tools\\;E:\\SOFTWARE\\Anaconda;E:\\SOFTWARE\\Anaconda\\Library\\mingw-w64\\bin;E:\\SOFTWARE\\Anaconda\\Library\\usr\\bin;E:\\SOFTWARE\\Anaconda\\Library\\bin;E:\\SOFTWARE\\Anaconda\\Scripts;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\bin;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\libnvvp;C:\\WINDOWS\\system32;C:\\WINDOWS;C:\\WINDOWS\\System32\\Wbem;C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0\\;C:\\WINDOWS\\System32\\OpenSSH\\;E:\\\u8f6f\u4ef6\\Git\\cmd;E:\\SOFTWARE\\cmake\\bin;C:\\Program Files (x86)\\Windows Kits\\8.1\\Windows Performance Toolkit\\;C:\\Program Files (x86)\\NVIDIA Corporation\\PhysX\\Common;C:\\Program Files\\dotnet\\;C:\\Program Files\\NVIDIA Corporation\\NVIDIA NvDLISR;C:\\Windows\\Microsoft.NET\\Framework\\v4.0.30319;C:\\Windows\\Microsoft.NET\\Framework64\\v4.0.30319;C:\\Users\\11838\\AppData\\Local\\Microsoft\\WindowsApps;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\CMake\\bin;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\Ninja\r\n\r\nE:\\SOFTWARE\\pytorch>set BASE_DIR=E:/SOFTWARE/pytorch\r\n\r\nE:\\SOFTWARE\\pytorch>set TORCH_LIB_DIR=E:/SOFTWARE/pytorch/torch/lib\r\n\r\nE:\\SOFTWARE\\pytorch>set THIRD_PARTY_DIR=E:/SOFTWARE/pytorch/third_party\r\n\r\nE:\\SOFTWARE\\pytorch>set BASIC_C_FLAGS=\r\n\r\nE:\\SOFTWARE\\pytorch>set BASIC_CUDA_FLAGS=\r\n\r\nE:\\SOFTWARE\\pytorch>IF NOT DEFINED INSTALL_DIR (set \"INSTALL_DIR=E:/SOFTWARE/pytorch/torch/lib/tmp_install\" )  ELSE (set \"INSTALL_DIR=\\=/\" )\r\n\r\nE:\\SOFTWARE\\pytorch>set LDFLAGS=/LIBPATH:E:/SOFTWARE/pytorch/torch/lib/tmp_install/lib\r\n\r\nE:\\SOFTWARE\\pytorch>set C_FLAGS= /D_WIN32 /Z7 /EHa /DNOMINMAX\r\n\r\nE:\\SOFTWARE\\pytorch>set LINK_FLAGS=/DEBUG:FULL\r\n\r\nE:\\SOFTWARE\\pytorch>if not exist torch\\lib\\tmp_install mkdir torch\\lib\\tmp_install\r\n\r\nE:\\SOFTWARE\\pytorch>set /a USE_CUDA=0\r\n\r\nE:\\SOFTWARE\\pytorch>set /a USE_FBGEMM=1\r\n\r\nE:\\SOFTWARE\\pytorch>set /a USE_ROCM=0\r\n\r\nE:\\SOFTWARE\\pytorch>set /a USE_NNPACK=0\r\n\r\nE:\\SOFTWARE\\pytorch>set /a USE_QNNPACK=0\r\n\r\nE:\\SOFTWARE\\pytorch>set /a USE_GLOO_IBVERBS=0\r\n\r\nE:\\SOFTWARE\\pytorch>set /a USE_MKLDNN=0\r\n\r\nE:\\SOFTWARE\\pytorch>set _BUILD_ARGS=\r\n\r\nE:\\SOFTWARE\\pytorch>if \"--use-cuda\" == \"\" (goto :process_args_exit )\r\n\r\nE:\\SOFTWARE\\pytorch>if \"--use-cuda\" == \"--use-cuda\" (\r\nset /a USE_CUDA=1\r\n goto :process_args_processed\r\n)\r\n\r\nE:\\SOFTWARE\\pytorch>shift\r\n\r\nE:\\SOFTWARE\\pytorch>goto :process_args\r\n\r\nE:\\SOFTWARE\\pytorch>if \"--use-fbgemm\" == \"\" (goto :process_args_exit )\r\n\r\nE:\\SOFTWARE\\pytorch>if \"--use-fbgemm\" == \"--use-cuda\" (\r\nset /a USE_CUDA=1\r\n goto :process_args_processed\r\n)\r\n\r\nE:\\SOFTWARE\\pytorch>if \"--use-fbgemm\" == \"--use-fbgemm\" (\r\nset /a USE_FBGEMM=1\r\n goto :process_args_processed\r\n)\r\n\r\nE:\\SOFTWARE\\pytorch>shift\r\n\r\nE:\\SOFTWARE\\pytorch>goto :process_args\r\n\r\nE:\\SOFTWARE\\pytorch>if \"--use-nnpack\" == \"\" (goto :process_args_exit )\r\n\r\nE:\\SOFTWARE\\pytorch>if \"--use-nnpack\" == \"--use-cuda\" (\r\nset /a USE_CUDA=1\r\n goto :process_args_processed\r\n[Truncated]\nDone Building Project \"E:\\SOFTWARE\\pytorch\\build\\INSTALL.vcxproj\" (default targets) -- FAILED.\r\n\r\n\r\nBuild FAILED.\r\n\r\n\"E:\\SOFTWARE\\pytorch\\build\\INSTALL.vcxproj\" (default target) (1) ->\r\n\"E:\\SOFTWARE\\pytorch\\build\\ALL_BUILD.vcxproj\" (default target) (3) ->\r\n\"E:\\SOFTWARE\\pytorch\\build\\caffe2\\AlgorithmsTest.vcxproj\" (default target) (4) ->\r\n\"E:\\SOFTWARE\\pytorch\\build\\caffe2\\caffe2_gpu.vcxproj\" (default target) (22) ->\r\n(Link target) ->\r\n  LINK : fatal error LNK1181: \u65e0\u6cd5\u6253\u5f00\u8f93\u5165\u6587\u4ef6\u201cC:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\lib\\x64.obj\u201d [E:\\SOFTWA\r\nRE\\pytorch\\build\\caffe2\\caffe2_gpu.vcxproj]\r\n\r\n    0 Warning(s)\r\n    1 Error(s)\r\n\r\nTime Elapsed 00:00:22.22\r\n\r\nE:\\SOFTWARE\\pytorch\\build>IF ERRORLEVEL 1 exit 1\r\nFailed to run 'tools\\build_pytorch_libs.bat --use-cuda --use-fbgemm --use-nnpack --use-mkldnn --use-qnnpack caffe2'\n<issue_comment>username_1: The error is caused by the misconfiguration of the flag `CUDNN_LIBRARY`. Could you please run `tools/setup_helpers/cudnn.py` and see which code path it goes through?\n<issue_comment>username_0: My environmental variables is :\r\nCUDA_PATH=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\r\nCUDA_PATH_V10_0=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\r\nCUDNN_INCLUDE_DIR=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\include\r\nCUDNN_LIBRARY=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\lib\\x64\r\nNVCUDASAMPLES10_0_ROOT=C:\\ProgramData\\NVIDIA Corporation\\CUDA Samples\\v10.0\r\nNVCUDASAMPLES_ROOT=C:\\ProgramData\\NVIDIA Corporation\\CUDA Samples\\v10.0\r\nNVTOOLSEXT_PATH=C:\\Program Files\\NVIDIA Corporation\\NvToolsExt\\\r\nPath=E:\\SOFTWARE\\Visual Studio\\VC\\Tools\\MSVC\\14.11.25503\\bin\\HostX64\\x64;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\VC\\VCPackages;C:\\Program Files (x86)\\Microsoft SDKs\\TypeScript\\3.1;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\TestWindow;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\TeamFoundation\\Team Explorer;E:\\SOFTWARE\\Visual Studio\\MSBuild\\15.0\\bin\\Roslyn;E:\\SOFTWARE\\Visual Studio\\Team Tools\\Performance Tools\\x64;E:\\SOFTWARE\\Visual Studio\\Team Tools\\Performance Tools;E:\\SOFTWARE\\Visual Studio\\share\\Common\\VSPerfCollectionTools\\\\x64;E:\\SOFTWARE\\Visual Studio\\share\\Common\\VSPerfCollectionTools\\;C:\\Program Files (x86)\\Microsoft SDKs\\Windows\\v10.0A\\bin\\NETFX 4.6.1 Tools\\x64\\;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\10.0.17763.0\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x64;E:\\SOFTWARE\\Visual Studio\\\\MSBuild\\15.0\\bin;C:\\WINDOWS\\Microsoft.NET\\Framework64\\v4.0.30319;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\;E:\\SOFTWARE\\Visual Studio\\Common7\\Tools\\;E:\\SOFTWARE\\Anaconda;E:\\SOFTWARE\\Anaconda\\Library\\mingw-w64\\bin;E:\\SOFTWARE\\Anaconda\\Library\\usr\\bin;E:\\SOFTWARE\\Anaconda\\Library\\bin;E:\\SOFTWARE\\Anaconda\\Scripts;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\bin;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\libnvvp;C:\\WINDOWS\\system32;C:\\WINDOWS;C:\\WINDOWS\\System32\\Wbem;C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0\\;C:\\WINDOWS\\System32\\OpenSSH\\;E:\\\u8f6f\u4ef6\\Git\\cmd;E:\\SOFTWARE\\cmake\\bin;C:\\Program Files (x86)\\Windows Kits\\8.1\\Windows Performance Toolkit\\;C:\\Program Files (x86)\\NVIDIA Corporation\\PhysX\\Common;C:\\Program Files\\dotnet\\;C:\\Program Files\\NVIDIA Corporation\\NVIDIA NvDLISR;C:\\Windows\\Microsoft.NET\\Framework\\v4.0.30319;C:\\Windows\\Microsoft.NET\\Framework64\\v4.0.30319;C:\\Users\\11838\\AppData\\Local\\Microsoft\\WindowsApps;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\CMake\\bin;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\Ninja\r\n\r\nAND the cudnn.py shows:\r\nimport os\r\nimport glob\r\n\r\nfrom .env import IS_WINDOWS, IS_CONDA, CONDA_DIR, check_negative_env_flag, gather_paths, lib_paths_from_base\r\nfrom .cuda import USE_CUDA, CUDA_HOME\r\n\r\n\r\nUSE_CUDNN = False\r\nCUDNN_LIB_DIR = None\r\nCUDNN_INCLUDE_DIR = None\r\nCUDNN_LIBRARY = None\r\nWITH_STATIC_CUDNN = os.getenv(\"USE_STATIC_CUDNN\")\r\n\r\nif USE_CUDA and not check_negative_env_flag('USE_CUDNN'):\r\n    lib_paths = list(filter(bool, [\r\n        os.getenv('CUDNN_LIB_DIR')\r\n    ] + lib_paths_from_base(CUDA_HOME) + [\r\n        '/usr/lib/x86_64-linux-gnu/',\r\n        '/usr/lib/powerpc64le-linux-gnu/',\r\n        '/usr/lib/aarch64-linux-gnu/',\r\n    ] + gather_paths([\r\n        'LIBRARY_PATH',\r\n    ]) + gather_paths([\r\n        'LD_LIBRARY_PATH',\r\n    ])))\r\n    include_paths = list(filter(bool, [\r\n        os.getenv('CUDNN_INCLUDE_DIR'),\r\n        os.path.join(CUDA_HOME, 'include'),\r\n        '/usr/include/',\r\n    ] + gather_paths([\r\n        'CPATH',\r\n        'C_INCLUDE_PATH',\r\n        'CPLUS_INCLUDE_PATH',\r\n    ])))\r\n    # Add CUDA related dirs to candidate list\r\n    if IS_CONDA:\r\n        lib_paths.append(os.path.join(CONDA_DIR, 'lib'))\r\n        include_paths.append(os.path.join(CONDA_DIR, 'include'))\r\n    for path in include_paths:\r\n        if path is None or not os.path.exists(path):\r\n            continue\r\n        include_file_path = os.path.join(path, 'cudnn.h')\r\n        CUDNN_INCLUDE_VERSION = None\r\n        if os.path.exists(include_file_path):\r\n            CUDNN_INCLUDE_DIR = path\r\n            with open(include_file_path) as f:\r\n                for line in f:\r\n                    if \"#define CUDNN_MAJOR\" in line:\r\n                        CUDNN_INCLUDE_VERSION = int(line.split()[-1])\r\n                        break\r\n            if CUDNN_INCLUDE_VERSION is None:\r\n                raise AssertionError(\"Could not find #define CUDNN_MAJOR in \" + include_file_path)\r\n            break\r\n\r\n    if CUDNN_INCLUDE_VERSION is None:\r\n        pass\r\n\r\n    # Check for standalone cuDNN libraries\r\n    if CUDNN_INCLUDE_DIR is not None:\r\n        cudnn_path = os.path.join(os.path.dirname(CUDNN_INCLUDE_DIR))\r\n        cudnn_lib_paths = lib_paths_from_base(cudnn_path)\r\n        lib_paths.extend(cudnn_lib_paths)\r\n\r\n    for path in lib_paths:\r\n        if path is None or not os.path.exists(path):\r\n            continue\r\n        if IS_WINDOWS:\r\n            library = os.path.join(path, 'cudnn.lib')\r\n            if os.path.exists(library):\r\n[Truncated]\n                search_name = 'libcudnn*' + str(CUDNN_INCLUDE_VERSION) + \"*\"\r\n            libraries = sorted(glob.glob(os.path.join(path, search_name)))\r\n            if libraries:\r\n                CUDNN_LIBRARY = libraries[0]\r\n                CUDNN_LIB_DIR = path\r\n                break\r\n    # Specifying the library directly will overwrite the lib directory\r\n    library = os.getenv('CUDNN_LIBRARY')\r\n    if library is not None and os.path.exists(library):\r\n        CUDNN_LIBRARY = library\r\n        CUDNN_LIB_DIR = os.path.dirname(CUDNN_LIBRARY)\r\n\r\n    if not all([CUDNN_LIBRARY, CUDNN_LIB_DIR, CUDNN_INCLUDE_DIR]):\r\n        CUDNN_LIBRARY = CUDNN_LIB_DIR = CUDNN_INCLUDE_DIR = None\r\n    else:\r\n        real_cudnn_library = os.path.realpath(CUDNN_LIBRARY)\r\n        real_cudnn_lib_dir = os.path.realpath(CUDNN_LIB_DIR)\r\n        assert os.path.dirname(real_cudnn_library) == real_cudnn_lib_dir, (\r\n            'cudnn library and lib_dir must agree')\r\n        USE_CUDNN = True\n<issue_comment>username_1: Please remove `CUDNN_INCLUDE_DIR` and `CUDNN_LIBRARY` by using the following commands.\r\n```cmd\r\nset CUDNN_INCLUDE_DIR=\r\nset CUDNN_LIBRARY=\r\n```\n<issue_comment>username_0: It works! Thanks a lot!<issue_closed>"}
{"repo": "pytorch/pytorch", "issue_id": 391131022, "issue_number": 15222, "timestamp": "2018-12-17 05:37:53+00:00", "text": "Please provide the full log.", "context": "<issue_start><issue_comment>Title: Can't open x64.obj while installing pytorch with win10 \nusername_0: \u5df2\u5b8c\u6210\u751f\u6210\u9879\u76ee\u201cE:\\SOFTWARE\\pytorch\\build\\caffe2\\torch\\lib\\libshm_windows\\shm.vcxproj\u201d(\u9ed8\u8ba4\u76ee\u6807)\u7684\u64cd\u4f5c\u3002\r\n\r\n\u5df2\u5b8c\u6210\u751f\u6210\u9879\u76ee\u201cE:\\SOFTWARE\\pytorch\\build\\ALL_BUILD.vcxproj\u201d(\u9ed8\u8ba4\u76ee\u6807)\u7684\u64cd\u4f5c - \u5931\u8d25\u3002\r\n\r\n\u5df2\u5b8c\u6210\u751f\u6210\u9879\u76ee\u201cE:\\SOFTWARE\\pytorch\\build\\INSTALL.vcxproj\u201d(\u9ed8\u8ba4\u76ee\u6807)\u7684\u64cd\u4f5c - \u5931\u8d25\u3002\r\n\r\n\r\n\u751f\u6210\u5931\u8d25\u3002\r\n\r\n\u201cE:\\SOFTWARE\\pytorch\\build\\INSTALL.vcxproj\u201d(\u9ed8\u8ba4\u76ee\u6807) (1) ->\r\n\u201cE:\\SOFTWARE\\pytorch\\build\\ALL_BUILD.vcxproj\u201d(\u9ed8\u8ba4\u76ee\u6807) (3) ->\r\n\u201cE:\\SOFTWARE\\pytorch\\build\\caffe2\\AlgorithmsTest.vcxproj\u201d(\u9ed8\u8ba4\u76ee\u6807) (4) ->\r\n\u201cE:\\SOFTWARE\\pytorch\\build\\caffe2\\caffe2_gpu.vcxproj\u201d(\u9ed8\u8ba4\u76ee\u6807) (22) ->\r\n(Link \u76ee\u6807) ->\r\n  C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\lib\\x64.obj : fatal error LNK1136: \u65e0\u6548\u6216\u635f\u574f\u7684\u6587\u4ef6 [E:\\SOFTWARE\\pyt\r\norch\\build\\caffe2\\caffe2_gpu.vcxproj]\r\n\r\n    0 \u4e2a\u8b66\u544a\r\n    1 \u4e2a\u9519\u8bef\r\n\r\n\u5df2\u7528\u65f6\u95f4 00:00:16.23\r\n\r\nE:\\SOFTWARE\\pytorch\\build>IF ERRORLEVEL 1 exit 1\r\nFailed to run 'tools\\build_pytorch_libs.bat --use-cuda --use-fbgemm --use-nnpack --use-mkldnn --use-qnnpack caffe2'\r\n\r\npytorch version\uff1astable\uff081.0\uff09\r\nos\uff1aWindows 10 Pro Version 1803\r\npython version:3.7\r\nCUDA:10\r\ncudnn:7.4.1\r\nVisual Studio version: Microsoft Visual Studio 2017 Community Edition\r\n\r\nCan anyone help me solve this problem? Thanks !\n<issue_comment>username_1: Please provide the full log.\n<issue_comment>username_0: the full log\uff1a\r\n\r\nBuilding wheel torch-1.0.0a0+df61437\r\nrunning install\r\nsetup.py::run()\r\nrunning build_deps\r\nsetup.py::build_deps::run()\r\n\r\nE:\\SOFTWARE\\pytorch>cd \"E:\\SOFTWARE\\pytorch\\tools\\\\..\"\r\n\r\nE:\\SOFTWARE\\pytorch>set PATH=\\bin;E:\\SOFTWARE\\Anaconda\\DLLs;E:\\SOFTWARE\\Visual Studio\\VC\\Tools\\MSVC\\14.11.25503\\bin\\HostX64\\x64;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\VC\\VCPackages;C:\\Program Files (x86)\\Microsoft SDKs\\TypeScript\\3.1;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\TestWindow;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\TeamFoundation\\Team Explorer;E:\\SOFTWARE\\Visual Studio\\MSBuild\\15.0\\bin\\Roslyn;E:\\SOFTWARE\\Visual Studio\\Team Tools\\Performance Tools\\x64;E:\\SOFTWARE\\Visual Studio\\Team Tools\\Performance Tools;E:\\SOFTWARE\\Visual Studio\\share\\Common\\VSPerfCollectionTools\\\\x64;E:\\SOFTWARE\\Visual Studio\\share\\Common\\VSPerfCollectionTools\\;C:\\Program Files (x86)\\Microsoft SDKs\\Windows\\v10.0A\\bin\\NETFX 4.6.1 Tools\\x64\\;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\10.0.17763.0\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x64;E:\\SOFTWARE\\Visual Studio\\\\MSBuild\\15.0\\bin;C:\\WINDOWS\\Microsoft.NET\\Framework64\\v4.0.30319;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\;E:\\SOFTWARE\\Visual Studio\\Common7\\Tools\\;E:\\SOFTWARE\\Anaconda;E:\\SOFTWARE\\Anaconda\\Library\\mingw-w64\\bin;E:\\SOFTWARE\\Anaconda\\Library\\usr\\bin;E:\\SOFTWARE\\Anaconda\\Library\\bin;E:\\SOFTWARE\\Anaconda\\Scripts;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\bin;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\libnvvp;C:\\WINDOWS\\system32;C:\\WINDOWS;C:\\WINDOWS\\System32\\Wbem;C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0\\;C:\\WINDOWS\\System32\\OpenSSH\\;E:\\\u8f6f\u4ef6\\Git\\cmd;E:\\SOFTWARE\\cmake\\bin;C:\\Program Files (x86)\\Windows Kits\\8.1\\Windows Performance Toolkit\\;C:\\Program Files (x86)\\NVIDIA Corporation\\PhysX\\Common;C:\\Program Files\\dotnet\\;C:\\Program Files\\NVIDIA Corporation\\NVIDIA NvDLISR;C:\\Windows\\Microsoft.NET\\Framework\\v4.0.30319;C:\\Windows\\Microsoft.NET\\Framework64\\v4.0.30319;C:\\Users\\11838\\AppData\\Local\\Microsoft\\WindowsApps;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\CMake\\bin;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\Ninja\r\n\r\nE:\\SOFTWARE\\pytorch>set BASE_DIR=E:/SOFTWARE/pytorch\r\n\r\nE:\\SOFTWARE\\pytorch>set TORCH_LIB_DIR=E:/SOFTWARE/pytorch/torch/lib\r\n\r\nE:\\SOFTWARE\\pytorch>set THIRD_PARTY_DIR=E:/SOFTWARE/pytorch/third_party\r\n\r\nE:\\SOFTWARE\\pytorch>set BASIC_C_FLAGS=\r\n\r\nE:\\SOFTWARE\\pytorch>set BASIC_CUDA_FLAGS=\r\n\r\nE:\\SOFTWARE\\pytorch>IF NOT DEFINED INSTALL_DIR (set \"INSTALL_DIR=E:/SOFTWARE/pytorch/torch/lib/tmp_install\" )  ELSE (set \"INSTALL_DIR=\\=/\" )\r\n\r\nE:\\SOFTWARE\\pytorch>set LDFLAGS=/LIBPATH:E:/SOFTWARE/pytorch/torch/lib/tmp_install/lib\r\n\r\nE:\\SOFTWARE\\pytorch>set C_FLAGS= /D_WIN32 /Z7 /EHa /DNOMINMAX\r\n\r\nE:\\SOFTWARE\\pytorch>set LINK_FLAGS=/DEBUG:FULL\r\n\r\nE:\\SOFTWARE\\pytorch>if not exist torch\\lib\\tmp_install mkdir torch\\lib\\tmp_install\r\n\r\nE:\\SOFTWARE\\pytorch>set /a USE_CUDA=0\r\n\r\nE:\\SOFTWARE\\pytorch>set /a USE_FBGEMM=1\r\n\r\nE:\\SOFTWARE\\pytorch>set /a USE_ROCM=0\r\n\r\nE:\\SOFTWARE\\pytorch>set /a USE_NNPACK=0\r\n\r\nE:\\SOFTWARE\\pytorch>set /a USE_QNNPACK=0\r\n\r\nE:\\SOFTWARE\\pytorch>set /a USE_GLOO_IBVERBS=0\r\n\r\nE:\\SOFTWARE\\pytorch>set /a USE_MKLDNN=0\r\n\r\nE:\\SOFTWARE\\pytorch>set _BUILD_ARGS=\r\n\r\nE:\\SOFTWARE\\pytorch>if \"--use-cuda\" == \"\" (goto :process_args_exit )\r\n\r\nE:\\SOFTWARE\\pytorch>if \"--use-cuda\" == \"--use-cuda\" (\r\nset /a USE_CUDA=1\r\n goto :process_args_processed\r\n)\r\n\r\nE:\\SOFTWARE\\pytorch>shift\r\n\r\nE:\\SOFTWARE\\pytorch>goto :process_args\r\n\r\nE:\\SOFTWARE\\pytorch>if \"--use-fbgemm\" == \"\" (goto :process_args_exit )\r\n\r\nE:\\SOFTWARE\\pytorch>if \"--use-fbgemm\" == \"--use-cuda\" (\r\nset /a USE_CUDA=1\r\n goto :process_args_processed\r\n)\r\n\r\nE:\\SOFTWARE\\pytorch>if \"--use-fbgemm\" == \"--use-fbgemm\" (\r\nset /a USE_FBGEMM=1\r\n goto :process_args_processed\r\n)\r\n\r\nE:\\SOFTWARE\\pytorch>shift\r\n\r\nE:\\SOFTWARE\\pytorch>goto :process_args\r\n\r\nE:\\SOFTWARE\\pytorch>if \"--use-nnpack\" == \"\" (goto :process_args_exit )\r\n\r\nE:\\SOFTWARE\\pytorch>if \"--use-nnpack\" == \"--use-cuda\" (\r\nset /a USE_CUDA=1\r\n goto :process_args_processed\r\n[Truncated]\nDone Building Project \"E:\\SOFTWARE\\pytorch\\build\\INSTALL.vcxproj\" (default targets) -- FAILED.\r\n\r\n\r\nBuild FAILED.\r\n\r\n\"E:\\SOFTWARE\\pytorch\\build\\INSTALL.vcxproj\" (default target) (1) ->\r\n\"E:\\SOFTWARE\\pytorch\\build\\ALL_BUILD.vcxproj\" (default target) (3) ->\r\n\"E:\\SOFTWARE\\pytorch\\build\\caffe2\\AlgorithmsTest.vcxproj\" (default target) (4) ->\r\n\"E:\\SOFTWARE\\pytorch\\build\\caffe2\\caffe2_gpu.vcxproj\" (default target) (22) ->\r\n(Link target) ->\r\n  LINK : fatal error LNK1181: \u65e0\u6cd5\u6253\u5f00\u8f93\u5165\u6587\u4ef6\u201cC:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\lib\\x64.obj\u201d [E:\\SOFTWA\r\nRE\\pytorch\\build\\caffe2\\caffe2_gpu.vcxproj]\r\n\r\n    0 Warning(s)\r\n    1 Error(s)\r\n\r\nTime Elapsed 00:00:22.22\r\n\r\nE:\\SOFTWARE\\pytorch\\build>IF ERRORLEVEL 1 exit 1\r\nFailed to run 'tools\\build_pytorch_libs.bat --use-cuda --use-fbgemm --use-nnpack --use-mkldnn --use-qnnpack caffe2'\n<issue_comment>username_1: The error is caused by the misconfiguration of the flag `CUDNN_LIBRARY`. Could you please run `tools/setup_helpers/cudnn.py` and see which code path it goes through?\n<issue_comment>username_0: My environmental variables is :\r\nCUDA_PATH=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\r\nCUDA_PATH_V10_0=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\r\nCUDNN_INCLUDE_DIR=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\include\r\nCUDNN_LIBRARY=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\lib\\x64\r\nNVCUDASAMPLES10_0_ROOT=C:\\ProgramData\\NVIDIA Corporation\\CUDA Samples\\v10.0\r\nNVCUDASAMPLES_ROOT=C:\\ProgramData\\NVIDIA Corporation\\CUDA Samples\\v10.0\r\nNVTOOLSEXT_PATH=C:\\Program Files\\NVIDIA Corporation\\NvToolsExt\\\r\nPath=E:\\SOFTWARE\\Visual Studio\\VC\\Tools\\MSVC\\14.11.25503\\bin\\HostX64\\x64;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\VC\\VCPackages;C:\\Program Files (x86)\\Microsoft SDKs\\TypeScript\\3.1;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\TestWindow;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\TeamFoundation\\Team Explorer;E:\\SOFTWARE\\Visual Studio\\MSBuild\\15.0\\bin\\Roslyn;E:\\SOFTWARE\\Visual Studio\\Team Tools\\Performance Tools\\x64;E:\\SOFTWARE\\Visual Studio\\Team Tools\\Performance Tools;E:\\SOFTWARE\\Visual Studio\\share\\Common\\VSPerfCollectionTools\\\\x64;E:\\SOFTWARE\\Visual Studio\\share\\Common\\VSPerfCollectionTools\\;C:\\Program Files (x86)\\Microsoft SDKs\\Windows\\v10.0A\\bin\\NETFX 4.6.1 Tools\\x64\\;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\10.0.17763.0\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x64;E:\\SOFTWARE\\Visual Studio\\\\MSBuild\\15.0\\bin;C:\\WINDOWS\\Microsoft.NET\\Framework64\\v4.0.30319;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\;E:\\SOFTWARE\\Visual Studio\\Common7\\Tools\\;E:\\SOFTWARE\\Anaconda;E:\\SOFTWARE\\Anaconda\\Library\\mingw-w64\\bin;E:\\SOFTWARE\\Anaconda\\Library\\usr\\bin;E:\\SOFTWARE\\Anaconda\\Library\\bin;E:\\SOFTWARE\\Anaconda\\Scripts;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\bin;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\libnvvp;C:\\WINDOWS\\system32;C:\\WINDOWS;C:\\WINDOWS\\System32\\Wbem;C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0\\;C:\\WINDOWS\\System32\\OpenSSH\\;E:\\\u8f6f\u4ef6\\Git\\cmd;E:\\SOFTWARE\\cmake\\bin;C:\\Program Files (x86)\\Windows Kits\\8.1\\Windows Performance Toolkit\\;C:\\Program Files (x86)\\NVIDIA Corporation\\PhysX\\Common;C:\\Program Files\\dotnet\\;C:\\Program Files\\NVIDIA Corporation\\NVIDIA NvDLISR;C:\\Windows\\Microsoft.NET\\Framework\\v4.0.30319;C:\\Windows\\Microsoft.NET\\Framework64\\v4.0.30319;C:\\Users\\11838\\AppData\\Local\\Microsoft\\WindowsApps;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\CMake\\bin;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\Ninja\r\n\r\nAND the cudnn.py shows:\r\nimport os\r\nimport glob\r\n\r\nfrom .env import IS_WINDOWS, IS_CONDA, CONDA_DIR, check_negative_env_flag, gather_paths, lib_paths_from_base\r\nfrom .cuda import USE_CUDA, CUDA_HOME\r\n\r\n\r\nUSE_CUDNN = False\r\nCUDNN_LIB_DIR = None\r\nCUDNN_INCLUDE_DIR = None\r\nCUDNN_LIBRARY = None\r\nWITH_STATIC_CUDNN = os.getenv(\"USE_STATIC_CUDNN\")\r\n\r\nif USE_CUDA and not check_negative_env_flag('USE_CUDNN'):\r\n    lib_paths = list(filter(bool, [\r\n        os.getenv('CUDNN_LIB_DIR')\r\n    ] + lib_paths_from_base(CUDA_HOME) + [\r\n        '/usr/lib/x86_64-linux-gnu/',\r\n        '/usr/lib/powerpc64le-linux-gnu/',\r\n        '/usr/lib/aarch64-linux-gnu/',\r\n    ] + gather_paths([\r\n        'LIBRARY_PATH',\r\n    ]) + gather_paths([\r\n        'LD_LIBRARY_PATH',\r\n    ])))\r\n    include_paths = list(filter(bool, [\r\n        os.getenv('CUDNN_INCLUDE_DIR'),\r\n        os.path.join(CUDA_HOME, 'include'),\r\n        '/usr/include/',\r\n    ] + gather_paths([\r\n        'CPATH',\r\n        'C_INCLUDE_PATH',\r\n        'CPLUS_INCLUDE_PATH',\r\n    ])))\r\n    # Add CUDA related dirs to candidate list\r\n    if IS_CONDA:\r\n        lib_paths.append(os.path.join(CONDA_DIR, 'lib'))\r\n        include_paths.append(os.path.join(CONDA_DIR, 'include'))\r\n    for path in include_paths:\r\n        if path is None or not os.path.exists(path):\r\n            continue\r\n        include_file_path = os.path.join(path, 'cudnn.h')\r\n        CUDNN_INCLUDE_VERSION = None\r\n        if os.path.exists(include_file_path):\r\n            CUDNN_INCLUDE_DIR = path\r\n            with open(include_file_path) as f:\r\n                for line in f:\r\n                    if \"#define CUDNN_MAJOR\" in line:\r\n                        CUDNN_INCLUDE_VERSION = int(line.split()[-1])\r\n                        break\r\n            if CUDNN_INCLUDE_VERSION is None:\r\n                raise AssertionError(\"Could not find #define CUDNN_MAJOR in \" + include_file_path)\r\n            break\r\n\r\n    if CUDNN_INCLUDE_VERSION is None:\r\n        pass\r\n\r\n    # Check for standalone cuDNN libraries\r\n    if CUDNN_INCLUDE_DIR is not None:\r\n        cudnn_path = os.path.join(os.path.dirname(CUDNN_INCLUDE_DIR))\r\n        cudnn_lib_paths = lib_paths_from_base(cudnn_path)\r\n        lib_paths.extend(cudnn_lib_paths)\r\n\r\n    for path in lib_paths:\r\n        if path is None or not os.path.exists(path):\r\n            continue\r\n        if IS_WINDOWS:\r\n            library = os.path.join(path, 'cudnn.lib')\r\n            if os.path.exists(library):\r\n[Truncated]\n                search_name = 'libcudnn*' + str(CUDNN_INCLUDE_VERSION) + \"*\"\r\n            libraries = sorted(glob.glob(os.path.join(path, search_name)))\r\n            if libraries:\r\n                CUDNN_LIBRARY = libraries[0]\r\n                CUDNN_LIB_DIR = path\r\n                break\r\n    # Specifying the library directly will overwrite the lib directory\r\n    library = os.getenv('CUDNN_LIBRARY')\r\n    if library is not None and os.path.exists(library):\r\n        CUDNN_LIBRARY = library\r\n        CUDNN_LIB_DIR = os.path.dirname(CUDNN_LIBRARY)\r\n\r\n    if not all([CUDNN_LIBRARY, CUDNN_LIB_DIR, CUDNN_INCLUDE_DIR]):\r\n        CUDNN_LIBRARY = CUDNN_LIB_DIR = CUDNN_INCLUDE_DIR = None\r\n    else:\r\n        real_cudnn_library = os.path.realpath(CUDNN_LIBRARY)\r\n        real_cudnn_lib_dir = os.path.realpath(CUDNN_LIB_DIR)\r\n        assert os.path.dirname(real_cudnn_library) == real_cudnn_lib_dir, (\r\n            'cudnn library and lib_dir must agree')\r\n        USE_CUDNN = True\n<issue_comment>username_1: Please remove `CUDNN_INCLUDE_DIR` and `CUDNN_LIBRARY` by using the following commands.\r\n```cmd\r\nset CUDNN_INCLUDE_DIR=\r\nset CUDNN_LIBRARY=\r\n```\n<issue_comment>username_0: It works! Thanks a lot!<issue_closed>"}
{"repo": "pytorch/pytorch", "issue_id": 391131022, "issue_number": 15222, "timestamp": "2018-12-17 15:39:08+00:00", "text": "the full log\uff1a\r\n\r\nBuilding wheel torch-1.0.0a0+df61437\r\nrunning install\r\nsetup.py::run()\r\nrunning build_deps\r\nsetup.py::build_deps::run()\r\n\r\nE:\\SOFTWARE\\pytorch>cd \"E:\\SOFTWARE\\pytorch\\tools\\\\..\"\r\n\r\nE:\\SOFTWARE\\pytorch>set PATH=\\bin;E:\\SOFTWARE\\Anaconda\\DLLs;E:\\SOFTWARE\\Visual Studio\\VC\\Tools\\MSVC\\14.11.25503\\bin\\HostX64\\x64;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\VC\\VCPackages;C:\\Program Files (x86)\\Microsoft SDKs\\TypeScript\\3.1;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\TestWindow;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\TeamFoundation\\Team Explorer;E:\\SOFTWARE\\Visual Studio\\MSBuild\\15.0\\bin\\Roslyn;E:\\SOFTWARE\\Visual Studio\\Team Tools\\Performance Tools\\x64;E:\\SOFTWARE\\Visual Studio\\Team Tools\\Performance Tools;E:\\SOFTWARE\\Visual Studio\\share\\Common\\VSPerfCollectionTools\\\\x64;E:\\SOFTWARE\\Visual Studio\\share\\Common\\VSPerfCollectionTools\\;C:\\Program Files (x86)\\Microsoft SDKs\\Windows\\v10.0A\\bin\\NETFX 4.6.1 Tools\\x64\\;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\10.0.17763.0\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x64;E:\\SOFTWARE\\Visual Studio\\\\MSBuild\\15.0\\bin;C:\\WINDOWS\\Microsoft.NET\\Framework64\\v4.0.30319;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\;E:\\SOFTWARE\\Visual Studio\\Common7\\Tools\\;E:\\SOFTWARE\\Anaconda;E:\\SOFTWARE\\Anaconda\\Library\\mingw-w64\\bin;E:\\SOFTWARE\\Anaconda\\Library\\usr\\bin;E:\\SOFTWARE\\Anaconda\\Library\\bin;E:\\SOFTWARE\\Anaconda\\Scripts;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\bin;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\libnvvp;C:\\WINDOWS\\system32;C:\\WINDOWS;C:\\WINDOWS\\System32\\Wbem;C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0\\;C:\\WINDOWS\\System32\\OpenSSH\\;E:\\\u8f6f\u4ef6\\Git\\cmd;E:\\SOFTWARE\\cmake\\bin;C:\\Program Files (x86)\\Windows Kits\\8.1\\Windows Performance Toolkit\\;C:\\Program Files (x86)\\NVIDIA Corporation\\PhysX\\Common;C:\\Program Files\\dotnet\\;C:\\Program Files\\NVIDIA Corporation\\NVIDIA NvDLISR;C:\\Windows\\Microsoft.NET\\Framework\\v4.0.30319;C:\\Windows\\Microsoft.NET\\Framework64\\v4.0.30319;C:\\Users\\11838\\AppData\\Local\\Microsoft\\WindowsApps;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\CMake\\bin;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\Ninja\r\n\r\nE:\\SOFTWARE\\pytorch>set BASE_DIR=E:/SOFTWARE/pytorch\r\n\r\nE:\\SOFTWARE\\pytorch>set TORCH_LIB_DIR=E:/SOFTWARE/pytorch/torch/lib\r\n\r\nE:\\SOFTWARE\\pytorch>set THIRD_PARTY_DIR=E:/SOFTWARE/pytorch/third_party\r\n\r\nE:\\SOFTWARE\\pytorch>set BASIC_C_FLAGS=\r\n\r\nE:\\SOFTWARE\\pytorch>set BASIC_CUDA_FLAGS=\r\n\r\nE:\\SOFTWARE\\pytorch>IF NOT DEFINED INSTALL_DIR (set \"INSTALL_DIR=E:/SOFTWARE/pytorch/torch/lib/tmp_install\" )  ELSE (set \"INSTALL_DIR=\\=/\" )\r\n\r\nE:\\SOFTWARE\\pytorch>set LDFLAGS=/LIBPATH:E:/SOFTWARE/pytorch/torch/lib/tmp_install/lib\r\n\r\nE:\\SOFTWARE\\pytorch>set C_FLAGS= /D_WIN32 /Z7 /EHa /DNOMINMAX\r\n\r\nE:\\SOFTWARE\\pytorch>set LINK_FLAGS=/DEBUG:FULL\r\n\r\nE:\\SOFTWARE\\pytorch>if not exist torch\\lib\\tmp_install mkdir torch\\lib\\tmp_install\r\n\r\nE:\\SOFTWARE\\pytorch>set /a USE_CUDA=0\r\n\r\nE:\\SOFTWARE\\pytorch>set /a USE_FBGEMM=1\r\n\r\nE:\\SOFTWARE\\pytorch>set /a USE_ROCM=0\r\n\r\nE:\\SOFTWARE\\pytorch>set /a USE_NNPACK=0\r\n\r\nE:\\SOFTWARE\\pytorch>set /a USE_QNNPACK=0\r\n\r\nE:\\SOFTWARE\\pytorch>set /a USE_GLOO_IBVERBS=0\r\n\r\nE:\\SOFTWARE\\pytorch>set /a USE_MKLDNN=0\r\n\r\nE:\\SOFTWARE\\pytorch>set _BUILD_ARGS=\r\n\r\nE:\\SOFTWARE\\pytorch>if \"--use-cuda\" == \"\" (goto :process_args_exit )\r\n\r\nE:\\SOFTWARE\\pytorch>if \"--use-cuda\" == \"--use-cuda\" (\r\nset /a USE_CUDA=1\r\n goto :process_args_processed\r\n)\r\n\r\nE:\\SOFTWARE\\pytorch>shift\r\n\r\nE:\\SOFTWARE\\pytorch>goto :process_args\r\n\r\nE:\\SOFTWARE\\pytorch>if \"--use-fbgemm\" == \"\" (goto :process_args_exit )\r\n\r\nE:\\SOFTWARE\\pytorch>if \"--use-fbgemm\" == \"--use-cuda\" (\r\nset /a USE_CUDA=1\r\n goto :process_args_processed\r\n)\r\n\r\nE:\\SOFTWARE\\pytorch>if \"--use-fbgemm\" == \"--use-fbgemm\" (\r\nset /a USE_FBGEMM=1\r\n goto :process_args_processed\r\n)\r\n\r\nE:\\SOFTWARE\\pytorch>shift\r\n\r\nE:\\SOFTWARE\\pytorch>goto :process_args\r\n\r\nE:\\SOFTWARE\\pytorch>if \"--use-nnpack\" == \"\" (goto :process_args_exit )\r\n\r\nE:\\SOFTWARE\\pytorch>if \"--use-nnpack\" == \"--use-cuda\" (\r\nset /a USE_CUDA=1\r\n goto :process_args_processed\r\n[Truncated]\nDone Building Project \"E:\\SOFTWARE\\pytorch\\build\\INSTALL.vcxproj\" (default targets) -- FAILED.\r\n\r\n\r\nBuild FAILED.\r\n\r\n\"E:\\SOFTWARE\\pytorch\\build\\INSTALL.vcxproj\" (default target) (1) ->\r\n\"E:\\SOFTWARE\\pytorch\\build\\ALL_BUILD.vcxproj\" (default target) (3) ->\r\n\"E:\\SOFTWARE\\pytorch\\build\\caffe2\\AlgorithmsTest.vcxproj\" (default target) (4) ->\r\n\"E:\\SOFTWARE\\pytorch\\build\\caffe2\\caffe2_gpu.vcxproj\" (default target) (22) ->\r\n(Link target) ->\r\n  LINK : fatal error LNK1181: \u65e0\u6cd5\u6253\u5f00\u8f93\u5165\u6587\u4ef6\u201cC:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\lib\\x64.obj\u201d [E:\\SOFTWA\r\nRE\\pytorch\\build\\caffe2\\caffe2_gpu.vcxproj]\r\n\r\n    0 Warning(s)\r\n    1 Error(s)\r\n\r\nTime Elapsed 00:00:22.22\r\n\r\nE:\\SOFTWARE\\pytorch\\build>IF ERRORLEVEL 1 exit 1\r\nFailed to run 'tools\\build_pytorch_libs.bat --use-cuda --use-fbgemm --use-nnpack --use-mkldnn --use-qnnpack caffe2'", "context": "<issue_start><issue_comment>Title: Can't open x64.obj while installing pytorch with win10 \nusername_0: \u5df2\u5b8c\u6210\u751f\u6210\u9879\u76ee\u201cE:\\SOFTWARE\\pytorch\\build\\caffe2\\torch\\lib\\libshm_windows\\shm.vcxproj\u201d(\u9ed8\u8ba4\u76ee\u6807)\u7684\u64cd\u4f5c\u3002\r\n\r\n\u5df2\u5b8c\u6210\u751f\u6210\u9879\u76ee\u201cE:\\SOFTWARE\\pytorch\\build\\ALL_BUILD.vcxproj\u201d(\u9ed8\u8ba4\u76ee\u6807)\u7684\u64cd\u4f5c - \u5931\u8d25\u3002\r\n\r\n\u5df2\u5b8c\u6210\u751f\u6210\u9879\u76ee\u201cE:\\SOFTWARE\\pytorch\\build\\INSTALL.vcxproj\u201d(\u9ed8\u8ba4\u76ee\u6807)\u7684\u64cd\u4f5c - \u5931\u8d25\u3002\r\n\r\n\r\n\u751f\u6210\u5931\u8d25\u3002\r\n\r\n\u201cE:\\SOFTWARE\\pytorch\\build\\INSTALL.vcxproj\u201d(\u9ed8\u8ba4\u76ee\u6807) (1) ->\r\n\u201cE:\\SOFTWARE\\pytorch\\build\\ALL_BUILD.vcxproj\u201d(\u9ed8\u8ba4\u76ee\u6807) (3) ->\r\n\u201cE:\\SOFTWARE\\pytorch\\build\\caffe2\\AlgorithmsTest.vcxproj\u201d(\u9ed8\u8ba4\u76ee\u6807) (4) ->\r\n\u201cE:\\SOFTWARE\\pytorch\\build\\caffe2\\caffe2_gpu.vcxproj\u201d(\u9ed8\u8ba4\u76ee\u6807) (22) ->\r\n(Link \u76ee\u6807) ->\r\n  C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\lib\\x64.obj : fatal error LNK1136: \u65e0\u6548\u6216\u635f\u574f\u7684\u6587\u4ef6 [E:\\SOFTWARE\\pyt\r\norch\\build\\caffe2\\caffe2_gpu.vcxproj]\r\n\r\n    0 \u4e2a\u8b66\u544a\r\n    1 \u4e2a\u9519\u8bef\r\n\r\n\u5df2\u7528\u65f6\u95f4 00:00:16.23\r\n\r\nE:\\SOFTWARE\\pytorch\\build>IF ERRORLEVEL 1 exit 1\r\nFailed to run 'tools\\build_pytorch_libs.bat --use-cuda --use-fbgemm --use-nnpack --use-mkldnn --use-qnnpack caffe2'\r\n\r\npytorch version\uff1astable\uff081.0\uff09\r\nos\uff1aWindows 10 Pro Version 1803\r\npython version:3.7\r\nCUDA:10\r\ncudnn:7.4.1\r\nVisual Studio version: Microsoft Visual Studio 2017 Community Edition\r\n\r\nCan anyone help me solve this problem? Thanks !\n<issue_comment>username_1: Please provide the full log.\n<issue_comment>username_0: the full log\uff1a\r\n\r\nBuilding wheel torch-1.0.0a0+df61437\r\nrunning install\r\nsetup.py::run()\r\nrunning build_deps\r\nsetup.py::build_deps::run()\r\n\r\nE:\\SOFTWARE\\pytorch>cd \"E:\\SOFTWARE\\pytorch\\tools\\\\..\"\r\n\r\nE:\\SOFTWARE\\pytorch>set PATH=\\bin;E:\\SOFTWARE\\Anaconda\\DLLs;E:\\SOFTWARE\\Visual Studio\\VC\\Tools\\MSVC\\14.11.25503\\bin\\HostX64\\x64;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\VC\\VCPackages;C:\\Program Files (x86)\\Microsoft SDKs\\TypeScript\\3.1;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\TestWindow;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\TeamFoundation\\Team Explorer;E:\\SOFTWARE\\Visual Studio\\MSBuild\\15.0\\bin\\Roslyn;E:\\SOFTWARE\\Visual Studio\\Team Tools\\Performance Tools\\x64;E:\\SOFTWARE\\Visual Studio\\Team Tools\\Performance Tools;E:\\SOFTWARE\\Visual Studio\\share\\Common\\VSPerfCollectionTools\\\\x64;E:\\SOFTWARE\\Visual Studio\\share\\Common\\VSPerfCollectionTools\\;C:\\Program Files (x86)\\Microsoft SDKs\\Windows\\v10.0A\\bin\\NETFX 4.6.1 Tools\\x64\\;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\10.0.17763.0\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x64;E:\\SOFTWARE\\Visual Studio\\\\MSBuild\\15.0\\bin;C:\\WINDOWS\\Microsoft.NET\\Framework64\\v4.0.30319;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\;E:\\SOFTWARE\\Visual Studio\\Common7\\Tools\\;E:\\SOFTWARE\\Anaconda;E:\\SOFTWARE\\Anaconda\\Library\\mingw-w64\\bin;E:\\SOFTWARE\\Anaconda\\Library\\usr\\bin;E:\\SOFTWARE\\Anaconda\\Library\\bin;E:\\SOFTWARE\\Anaconda\\Scripts;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\bin;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\libnvvp;C:\\WINDOWS\\system32;C:\\WINDOWS;C:\\WINDOWS\\System32\\Wbem;C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0\\;C:\\WINDOWS\\System32\\OpenSSH\\;E:\\\u8f6f\u4ef6\\Git\\cmd;E:\\SOFTWARE\\cmake\\bin;C:\\Program Files (x86)\\Windows Kits\\8.1\\Windows Performance Toolkit\\;C:\\Program Files (x86)\\NVIDIA Corporation\\PhysX\\Common;C:\\Program Files\\dotnet\\;C:\\Program Files\\NVIDIA Corporation\\NVIDIA NvDLISR;C:\\Windows\\Microsoft.NET\\Framework\\v4.0.30319;C:\\Windows\\Microsoft.NET\\Framework64\\v4.0.30319;C:\\Users\\11838\\AppData\\Local\\Microsoft\\WindowsApps;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\CMake\\bin;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\Ninja\r\n\r\nE:\\SOFTWARE\\pytorch>set BASE_DIR=E:/SOFTWARE/pytorch\r\n\r\nE:\\SOFTWARE\\pytorch>set TORCH_LIB_DIR=E:/SOFTWARE/pytorch/torch/lib\r\n\r\nE:\\SOFTWARE\\pytorch>set THIRD_PARTY_DIR=E:/SOFTWARE/pytorch/third_party\r\n\r\nE:\\SOFTWARE\\pytorch>set BASIC_C_FLAGS=\r\n\r\nE:\\SOFTWARE\\pytorch>set BASIC_CUDA_FLAGS=\r\n\r\nE:\\SOFTWARE\\pytorch>IF NOT DEFINED INSTALL_DIR (set \"INSTALL_DIR=E:/SOFTWARE/pytorch/torch/lib/tmp_install\" )  ELSE (set \"INSTALL_DIR=\\=/\" )\r\n\r\nE:\\SOFTWARE\\pytorch>set LDFLAGS=/LIBPATH:E:/SOFTWARE/pytorch/torch/lib/tmp_install/lib\r\n\r\nE:\\SOFTWARE\\pytorch>set C_FLAGS= /D_WIN32 /Z7 /EHa /DNOMINMAX\r\n\r\nE:\\SOFTWARE\\pytorch>set LINK_FLAGS=/DEBUG:FULL\r\n\r\nE:\\SOFTWARE\\pytorch>if not exist torch\\lib\\tmp_install mkdir torch\\lib\\tmp_install\r\n\r\nE:\\SOFTWARE\\pytorch>set /a USE_CUDA=0\r\n\r\nE:\\SOFTWARE\\pytorch>set /a USE_FBGEMM=1\r\n\r\nE:\\SOFTWARE\\pytorch>set /a USE_ROCM=0\r\n\r\nE:\\SOFTWARE\\pytorch>set /a USE_NNPACK=0\r\n\r\nE:\\SOFTWARE\\pytorch>set /a USE_QNNPACK=0\r\n\r\nE:\\SOFTWARE\\pytorch>set /a USE_GLOO_IBVERBS=0\r\n\r\nE:\\SOFTWARE\\pytorch>set /a USE_MKLDNN=0\r\n\r\nE:\\SOFTWARE\\pytorch>set _BUILD_ARGS=\r\n\r\nE:\\SOFTWARE\\pytorch>if \"--use-cuda\" == \"\" (goto :process_args_exit )\r\n\r\nE:\\SOFTWARE\\pytorch>if \"--use-cuda\" == \"--use-cuda\" (\r\nset /a USE_CUDA=1\r\n goto :process_args_processed\r\n)\r\n\r\nE:\\SOFTWARE\\pytorch>shift\r\n\r\nE:\\SOFTWARE\\pytorch>goto :process_args\r\n\r\nE:\\SOFTWARE\\pytorch>if \"--use-fbgemm\" == \"\" (goto :process_args_exit )\r\n\r\nE:\\SOFTWARE\\pytorch>if \"--use-fbgemm\" == \"--use-cuda\" (\r\nset /a USE_CUDA=1\r\n goto :process_args_processed\r\n)\r\n\r\nE:\\SOFTWARE\\pytorch>if \"--use-fbgemm\" == \"--use-fbgemm\" (\r\nset /a USE_FBGEMM=1\r\n goto :process_args_processed\r\n)\r\n\r\nE:\\SOFTWARE\\pytorch>shift\r\n\r\nE:\\SOFTWARE\\pytorch>goto :process_args\r\n\r\nE:\\SOFTWARE\\pytorch>if \"--use-nnpack\" == \"\" (goto :process_args_exit )\r\n\r\nE:\\SOFTWARE\\pytorch>if \"--use-nnpack\" == \"--use-cuda\" (\r\nset /a USE_CUDA=1\r\n goto :process_args_processed\r\n[Truncated]\nDone Building Project \"E:\\SOFTWARE\\pytorch\\build\\INSTALL.vcxproj\" (default targets) -- FAILED.\r\n\r\n\r\nBuild FAILED.\r\n\r\n\"E:\\SOFTWARE\\pytorch\\build\\INSTALL.vcxproj\" (default target) (1) ->\r\n\"E:\\SOFTWARE\\pytorch\\build\\ALL_BUILD.vcxproj\" (default target) (3) ->\r\n\"E:\\SOFTWARE\\pytorch\\build\\caffe2\\AlgorithmsTest.vcxproj\" (default target) (4) ->\r\n\"E:\\SOFTWARE\\pytorch\\build\\caffe2\\caffe2_gpu.vcxproj\" (default target) (22) ->\r\n(Link target) ->\r\n  LINK : fatal error LNK1181: \u65e0\u6cd5\u6253\u5f00\u8f93\u5165\u6587\u4ef6\u201cC:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\lib\\x64.obj\u201d [E:\\SOFTWA\r\nRE\\pytorch\\build\\caffe2\\caffe2_gpu.vcxproj]\r\n\r\n    0 Warning(s)\r\n    1 Error(s)\r\n\r\nTime Elapsed 00:00:22.22\r\n\r\nE:\\SOFTWARE\\pytorch\\build>IF ERRORLEVEL 1 exit 1\r\nFailed to run 'tools\\build_pytorch_libs.bat --use-cuda --use-fbgemm --use-nnpack --use-mkldnn --use-qnnpack caffe2'\n<issue_comment>username_1: The error is caused by the misconfiguration of the flag `CUDNN_LIBRARY`. Could you please run `tools/setup_helpers/cudnn.py` and see which code path it goes through?\n<issue_comment>username_0: My environmental variables is :\r\nCUDA_PATH=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\r\nCUDA_PATH_V10_0=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\r\nCUDNN_INCLUDE_DIR=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\include\r\nCUDNN_LIBRARY=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\lib\\x64\r\nNVCUDASAMPLES10_0_ROOT=C:\\ProgramData\\NVIDIA Corporation\\CUDA Samples\\v10.0\r\nNVCUDASAMPLES_ROOT=C:\\ProgramData\\NVIDIA Corporation\\CUDA Samples\\v10.0\r\nNVTOOLSEXT_PATH=C:\\Program Files\\NVIDIA Corporation\\NvToolsExt\\\r\nPath=E:\\SOFTWARE\\Visual Studio\\VC\\Tools\\MSVC\\14.11.25503\\bin\\HostX64\\x64;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\VC\\VCPackages;C:\\Program Files (x86)\\Microsoft SDKs\\TypeScript\\3.1;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\TestWindow;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\TeamFoundation\\Team Explorer;E:\\SOFTWARE\\Visual Studio\\MSBuild\\15.0\\bin\\Roslyn;E:\\SOFTWARE\\Visual Studio\\Team Tools\\Performance Tools\\x64;E:\\SOFTWARE\\Visual Studio\\Team Tools\\Performance Tools;E:\\SOFTWARE\\Visual Studio\\share\\Common\\VSPerfCollectionTools\\\\x64;E:\\SOFTWARE\\Visual Studio\\share\\Common\\VSPerfCollectionTools\\;C:\\Program Files (x86)\\Microsoft SDKs\\Windows\\v10.0A\\bin\\NETFX 4.6.1 Tools\\x64\\;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\10.0.17763.0\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x64;E:\\SOFTWARE\\Visual Studio\\\\MSBuild\\15.0\\bin;C:\\WINDOWS\\Microsoft.NET\\Framework64\\v4.0.30319;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\;E:\\SOFTWARE\\Visual Studio\\Common7\\Tools\\;E:\\SOFTWARE\\Anaconda;E:\\SOFTWARE\\Anaconda\\Library\\mingw-w64\\bin;E:\\SOFTWARE\\Anaconda\\Library\\usr\\bin;E:\\SOFTWARE\\Anaconda\\Library\\bin;E:\\SOFTWARE\\Anaconda\\Scripts;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\bin;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\libnvvp;C:\\WINDOWS\\system32;C:\\WINDOWS;C:\\WINDOWS\\System32\\Wbem;C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0\\;C:\\WINDOWS\\System32\\OpenSSH\\;E:\\\u8f6f\u4ef6\\Git\\cmd;E:\\SOFTWARE\\cmake\\bin;C:\\Program Files (x86)\\Windows Kits\\8.1\\Windows Performance Toolkit\\;C:\\Program Files (x86)\\NVIDIA Corporation\\PhysX\\Common;C:\\Program Files\\dotnet\\;C:\\Program Files\\NVIDIA Corporation\\NVIDIA NvDLISR;C:\\Windows\\Microsoft.NET\\Framework\\v4.0.30319;C:\\Windows\\Microsoft.NET\\Framework64\\v4.0.30319;C:\\Users\\11838\\AppData\\Local\\Microsoft\\WindowsApps;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\CMake\\bin;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\Ninja\r\n\r\nAND the cudnn.py shows:\r\nimport os\r\nimport glob\r\n\r\nfrom .env import IS_WINDOWS, IS_CONDA, CONDA_DIR, check_negative_env_flag, gather_paths, lib_paths_from_base\r\nfrom .cuda import USE_CUDA, CUDA_HOME\r\n\r\n\r\nUSE_CUDNN = False\r\nCUDNN_LIB_DIR = None\r\nCUDNN_INCLUDE_DIR = None\r\nCUDNN_LIBRARY = None\r\nWITH_STATIC_CUDNN = os.getenv(\"USE_STATIC_CUDNN\")\r\n\r\nif USE_CUDA and not check_negative_env_flag('USE_CUDNN'):\r\n    lib_paths = list(filter(bool, [\r\n        os.getenv('CUDNN_LIB_DIR')\r\n    ] + lib_paths_from_base(CUDA_HOME) + [\r\n        '/usr/lib/x86_64-linux-gnu/',\r\n        '/usr/lib/powerpc64le-linux-gnu/',\r\n        '/usr/lib/aarch64-linux-gnu/',\r\n    ] + gather_paths([\r\n        'LIBRARY_PATH',\r\n    ]) + gather_paths([\r\n        'LD_LIBRARY_PATH',\r\n    ])))\r\n    include_paths = list(filter(bool, [\r\n        os.getenv('CUDNN_INCLUDE_DIR'),\r\n        os.path.join(CUDA_HOME, 'include'),\r\n        '/usr/include/',\r\n    ] + gather_paths([\r\n        'CPATH',\r\n        'C_INCLUDE_PATH',\r\n        'CPLUS_INCLUDE_PATH',\r\n    ])))\r\n    # Add CUDA related dirs to candidate list\r\n    if IS_CONDA:\r\n        lib_paths.append(os.path.join(CONDA_DIR, 'lib'))\r\n        include_paths.append(os.path.join(CONDA_DIR, 'include'))\r\n    for path in include_paths:\r\n        if path is None or not os.path.exists(path):\r\n            continue\r\n        include_file_path = os.path.join(path, 'cudnn.h')\r\n        CUDNN_INCLUDE_VERSION = None\r\n        if os.path.exists(include_file_path):\r\n            CUDNN_INCLUDE_DIR = path\r\n            with open(include_file_path) as f:\r\n                for line in f:\r\n                    if \"#define CUDNN_MAJOR\" in line:\r\n                        CUDNN_INCLUDE_VERSION = int(line.split()[-1])\r\n                        break\r\n            if CUDNN_INCLUDE_VERSION is None:\r\n                raise AssertionError(\"Could not find #define CUDNN_MAJOR in \" + include_file_path)\r\n            break\r\n\r\n    if CUDNN_INCLUDE_VERSION is None:\r\n        pass\r\n\r\n    # Check for standalone cuDNN libraries\r\n    if CUDNN_INCLUDE_DIR is not None:\r\n        cudnn_path = os.path.join(os.path.dirname(CUDNN_INCLUDE_DIR))\r\n        cudnn_lib_paths = lib_paths_from_base(cudnn_path)\r\n        lib_paths.extend(cudnn_lib_paths)\r\n\r\n    for path in lib_paths:\r\n        if path is None or not os.path.exists(path):\r\n            continue\r\n        if IS_WINDOWS:\r\n            library = os.path.join(path, 'cudnn.lib')\r\n            if os.path.exists(library):\r\n[Truncated]\n                search_name = 'libcudnn*' + str(CUDNN_INCLUDE_VERSION) + \"*\"\r\n            libraries = sorted(glob.glob(os.path.join(path, search_name)))\r\n            if libraries:\r\n                CUDNN_LIBRARY = libraries[0]\r\n                CUDNN_LIB_DIR = path\r\n                break\r\n    # Specifying the library directly will overwrite the lib directory\r\n    library = os.getenv('CUDNN_LIBRARY')\r\n    if library is not None and os.path.exists(library):\r\n        CUDNN_LIBRARY = library\r\n        CUDNN_LIB_DIR = os.path.dirname(CUDNN_LIBRARY)\r\n\r\n    if not all([CUDNN_LIBRARY, CUDNN_LIB_DIR, CUDNN_INCLUDE_DIR]):\r\n        CUDNN_LIBRARY = CUDNN_LIB_DIR = CUDNN_INCLUDE_DIR = None\r\n    else:\r\n        real_cudnn_library = os.path.realpath(CUDNN_LIBRARY)\r\n        real_cudnn_lib_dir = os.path.realpath(CUDNN_LIB_DIR)\r\n        assert os.path.dirname(real_cudnn_library) == real_cudnn_lib_dir, (\r\n            'cudnn library and lib_dir must agree')\r\n        USE_CUDNN = True\n<issue_comment>username_1: Please remove `CUDNN_INCLUDE_DIR` and `CUDNN_LIBRARY` by using the following commands.\r\n```cmd\r\nset CUDNN_INCLUDE_DIR=\r\nset CUDNN_LIBRARY=\r\n```\n<issue_comment>username_0: It works! Thanks a lot!<issue_closed>"}
{"repo": "pytorch/pytorch", "issue_id": 391131022, "issue_number": 15222, "timestamp": "2018-12-18 04:58:23+00:00", "text": "The error is caused by the misconfiguration of the flag `CUDNN_LIBRARY`. Could you please run `tools/setup_helpers/cudnn.py` and see which code path it goes through?", "context": "<issue_start><issue_comment>Title: Can't open x64.obj while installing pytorch with win10 \nusername_0: \u5df2\u5b8c\u6210\u751f\u6210\u9879\u76ee\u201cE:\\SOFTWARE\\pytorch\\build\\caffe2\\torch\\lib\\libshm_windows\\shm.vcxproj\u201d(\u9ed8\u8ba4\u76ee\u6807)\u7684\u64cd\u4f5c\u3002\r\n\r\n\u5df2\u5b8c\u6210\u751f\u6210\u9879\u76ee\u201cE:\\SOFTWARE\\pytorch\\build\\ALL_BUILD.vcxproj\u201d(\u9ed8\u8ba4\u76ee\u6807)\u7684\u64cd\u4f5c - \u5931\u8d25\u3002\r\n\r\n\u5df2\u5b8c\u6210\u751f\u6210\u9879\u76ee\u201cE:\\SOFTWARE\\pytorch\\build\\INSTALL.vcxproj\u201d(\u9ed8\u8ba4\u76ee\u6807)\u7684\u64cd\u4f5c - \u5931\u8d25\u3002\r\n\r\n\r\n\u751f\u6210\u5931\u8d25\u3002\r\n\r\n\u201cE:\\SOFTWARE\\pytorch\\build\\INSTALL.vcxproj\u201d(\u9ed8\u8ba4\u76ee\u6807) (1) ->\r\n\u201cE:\\SOFTWARE\\pytorch\\build\\ALL_BUILD.vcxproj\u201d(\u9ed8\u8ba4\u76ee\u6807) (3) ->\r\n\u201cE:\\SOFTWARE\\pytorch\\build\\caffe2\\AlgorithmsTest.vcxproj\u201d(\u9ed8\u8ba4\u76ee\u6807) (4) ->\r\n\u201cE:\\SOFTWARE\\pytorch\\build\\caffe2\\caffe2_gpu.vcxproj\u201d(\u9ed8\u8ba4\u76ee\u6807) (22) ->\r\n(Link \u76ee\u6807) ->\r\n  C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\lib\\x64.obj : fatal error LNK1136: \u65e0\u6548\u6216\u635f\u574f\u7684\u6587\u4ef6 [E:\\SOFTWARE\\pyt\r\norch\\build\\caffe2\\caffe2_gpu.vcxproj]\r\n\r\n    0 \u4e2a\u8b66\u544a\r\n    1 \u4e2a\u9519\u8bef\r\n\r\n\u5df2\u7528\u65f6\u95f4 00:00:16.23\r\n\r\nE:\\SOFTWARE\\pytorch\\build>IF ERRORLEVEL 1 exit 1\r\nFailed to run 'tools\\build_pytorch_libs.bat --use-cuda --use-fbgemm --use-nnpack --use-mkldnn --use-qnnpack caffe2'\r\n\r\npytorch version\uff1astable\uff081.0\uff09\r\nos\uff1aWindows 10 Pro Version 1803\r\npython version:3.7\r\nCUDA:10\r\ncudnn:7.4.1\r\nVisual Studio version: Microsoft Visual Studio 2017 Community Edition\r\n\r\nCan anyone help me solve this problem? Thanks !\n<issue_comment>username_1: Please provide the full log.\n<issue_comment>username_0: the full log\uff1a\r\n\r\nBuilding wheel torch-1.0.0a0+df61437\r\nrunning install\r\nsetup.py::run()\r\nrunning build_deps\r\nsetup.py::build_deps::run()\r\n\r\nE:\\SOFTWARE\\pytorch>cd \"E:\\SOFTWARE\\pytorch\\tools\\\\..\"\r\n\r\nE:\\SOFTWARE\\pytorch>set PATH=\\bin;E:\\SOFTWARE\\Anaconda\\DLLs;E:\\SOFTWARE\\Visual Studio\\VC\\Tools\\MSVC\\14.11.25503\\bin\\HostX64\\x64;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\VC\\VCPackages;C:\\Program Files (x86)\\Microsoft SDKs\\TypeScript\\3.1;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\TestWindow;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\TeamFoundation\\Team Explorer;E:\\SOFTWARE\\Visual Studio\\MSBuild\\15.0\\bin\\Roslyn;E:\\SOFTWARE\\Visual Studio\\Team Tools\\Performance Tools\\x64;E:\\SOFTWARE\\Visual Studio\\Team Tools\\Performance Tools;E:\\SOFTWARE\\Visual Studio\\share\\Common\\VSPerfCollectionTools\\\\x64;E:\\SOFTWARE\\Visual Studio\\share\\Common\\VSPerfCollectionTools\\;C:\\Program Files (x86)\\Microsoft SDKs\\Windows\\v10.0A\\bin\\NETFX 4.6.1 Tools\\x64\\;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\10.0.17763.0\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x64;E:\\SOFTWARE\\Visual Studio\\\\MSBuild\\15.0\\bin;C:\\WINDOWS\\Microsoft.NET\\Framework64\\v4.0.30319;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\;E:\\SOFTWARE\\Visual Studio\\Common7\\Tools\\;E:\\SOFTWARE\\Anaconda;E:\\SOFTWARE\\Anaconda\\Library\\mingw-w64\\bin;E:\\SOFTWARE\\Anaconda\\Library\\usr\\bin;E:\\SOFTWARE\\Anaconda\\Library\\bin;E:\\SOFTWARE\\Anaconda\\Scripts;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\bin;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\libnvvp;C:\\WINDOWS\\system32;C:\\WINDOWS;C:\\WINDOWS\\System32\\Wbem;C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0\\;C:\\WINDOWS\\System32\\OpenSSH\\;E:\\\u8f6f\u4ef6\\Git\\cmd;E:\\SOFTWARE\\cmake\\bin;C:\\Program Files (x86)\\Windows Kits\\8.1\\Windows Performance Toolkit\\;C:\\Program Files (x86)\\NVIDIA Corporation\\PhysX\\Common;C:\\Program Files\\dotnet\\;C:\\Program Files\\NVIDIA Corporation\\NVIDIA NvDLISR;C:\\Windows\\Microsoft.NET\\Framework\\v4.0.30319;C:\\Windows\\Microsoft.NET\\Framework64\\v4.0.30319;C:\\Users\\11838\\AppData\\Local\\Microsoft\\WindowsApps;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\CMake\\bin;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\Ninja\r\n\r\nE:\\SOFTWARE\\pytorch>set BASE_DIR=E:/SOFTWARE/pytorch\r\n\r\nE:\\SOFTWARE\\pytorch>set TORCH_LIB_DIR=E:/SOFTWARE/pytorch/torch/lib\r\n\r\nE:\\SOFTWARE\\pytorch>set THIRD_PARTY_DIR=E:/SOFTWARE/pytorch/third_party\r\n\r\nE:\\SOFTWARE\\pytorch>set BASIC_C_FLAGS=\r\n\r\nE:\\SOFTWARE\\pytorch>set BASIC_CUDA_FLAGS=\r\n\r\nE:\\SOFTWARE\\pytorch>IF NOT DEFINED INSTALL_DIR (set \"INSTALL_DIR=E:/SOFTWARE/pytorch/torch/lib/tmp_install\" )  ELSE (set \"INSTALL_DIR=\\=/\" )\r\n\r\nE:\\SOFTWARE\\pytorch>set LDFLAGS=/LIBPATH:E:/SOFTWARE/pytorch/torch/lib/tmp_install/lib\r\n\r\nE:\\SOFTWARE\\pytorch>set C_FLAGS= /D_WIN32 /Z7 /EHa /DNOMINMAX\r\n\r\nE:\\SOFTWARE\\pytorch>set LINK_FLAGS=/DEBUG:FULL\r\n\r\nE:\\SOFTWARE\\pytorch>if not exist torch\\lib\\tmp_install mkdir torch\\lib\\tmp_install\r\n\r\nE:\\SOFTWARE\\pytorch>set /a USE_CUDA=0\r\n\r\nE:\\SOFTWARE\\pytorch>set /a USE_FBGEMM=1\r\n\r\nE:\\SOFTWARE\\pytorch>set /a USE_ROCM=0\r\n\r\nE:\\SOFTWARE\\pytorch>set /a USE_NNPACK=0\r\n\r\nE:\\SOFTWARE\\pytorch>set /a USE_QNNPACK=0\r\n\r\nE:\\SOFTWARE\\pytorch>set /a USE_GLOO_IBVERBS=0\r\n\r\nE:\\SOFTWARE\\pytorch>set /a USE_MKLDNN=0\r\n\r\nE:\\SOFTWARE\\pytorch>set _BUILD_ARGS=\r\n\r\nE:\\SOFTWARE\\pytorch>if \"--use-cuda\" == \"\" (goto :process_args_exit )\r\n\r\nE:\\SOFTWARE\\pytorch>if \"--use-cuda\" == \"--use-cuda\" (\r\nset /a USE_CUDA=1\r\n goto :process_args_processed\r\n)\r\n\r\nE:\\SOFTWARE\\pytorch>shift\r\n\r\nE:\\SOFTWARE\\pytorch>goto :process_args\r\n\r\nE:\\SOFTWARE\\pytorch>if \"--use-fbgemm\" == \"\" (goto :process_args_exit )\r\n\r\nE:\\SOFTWARE\\pytorch>if \"--use-fbgemm\" == \"--use-cuda\" (\r\nset /a USE_CUDA=1\r\n goto :process_args_processed\r\n)\r\n\r\nE:\\SOFTWARE\\pytorch>if \"--use-fbgemm\" == \"--use-fbgemm\" (\r\nset /a USE_FBGEMM=1\r\n goto :process_args_processed\r\n)\r\n\r\nE:\\SOFTWARE\\pytorch>shift\r\n\r\nE:\\SOFTWARE\\pytorch>goto :process_args\r\n\r\nE:\\SOFTWARE\\pytorch>if \"--use-nnpack\" == \"\" (goto :process_args_exit )\r\n\r\nE:\\SOFTWARE\\pytorch>if \"--use-nnpack\" == \"--use-cuda\" (\r\nset /a USE_CUDA=1\r\n goto :process_args_processed\r\n[Truncated]\nDone Building Project \"E:\\SOFTWARE\\pytorch\\build\\INSTALL.vcxproj\" (default targets) -- FAILED.\r\n\r\n\r\nBuild FAILED.\r\n\r\n\"E:\\SOFTWARE\\pytorch\\build\\INSTALL.vcxproj\" (default target) (1) ->\r\n\"E:\\SOFTWARE\\pytorch\\build\\ALL_BUILD.vcxproj\" (default target) (3) ->\r\n\"E:\\SOFTWARE\\pytorch\\build\\caffe2\\AlgorithmsTest.vcxproj\" (default target) (4) ->\r\n\"E:\\SOFTWARE\\pytorch\\build\\caffe2\\caffe2_gpu.vcxproj\" (default target) (22) ->\r\n(Link target) ->\r\n  LINK : fatal error LNK1181: \u65e0\u6cd5\u6253\u5f00\u8f93\u5165\u6587\u4ef6\u201cC:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\lib\\x64.obj\u201d [E:\\SOFTWA\r\nRE\\pytorch\\build\\caffe2\\caffe2_gpu.vcxproj]\r\n\r\n    0 Warning(s)\r\n    1 Error(s)\r\n\r\nTime Elapsed 00:00:22.22\r\n\r\nE:\\SOFTWARE\\pytorch\\build>IF ERRORLEVEL 1 exit 1\r\nFailed to run 'tools\\build_pytorch_libs.bat --use-cuda --use-fbgemm --use-nnpack --use-mkldnn --use-qnnpack caffe2'\n<issue_comment>username_1: The error is caused by the misconfiguration of the flag `CUDNN_LIBRARY`. Could you please run `tools/setup_helpers/cudnn.py` and see which code path it goes through?\n<issue_comment>username_0: My environmental variables is :\r\nCUDA_PATH=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\r\nCUDA_PATH_V10_0=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\r\nCUDNN_INCLUDE_DIR=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\include\r\nCUDNN_LIBRARY=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\lib\\x64\r\nNVCUDASAMPLES10_0_ROOT=C:\\ProgramData\\NVIDIA Corporation\\CUDA Samples\\v10.0\r\nNVCUDASAMPLES_ROOT=C:\\ProgramData\\NVIDIA Corporation\\CUDA Samples\\v10.0\r\nNVTOOLSEXT_PATH=C:\\Program Files\\NVIDIA Corporation\\NvToolsExt\\\r\nPath=E:\\SOFTWARE\\Visual Studio\\VC\\Tools\\MSVC\\14.11.25503\\bin\\HostX64\\x64;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\VC\\VCPackages;C:\\Program Files (x86)\\Microsoft SDKs\\TypeScript\\3.1;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\TestWindow;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\TeamFoundation\\Team Explorer;E:\\SOFTWARE\\Visual Studio\\MSBuild\\15.0\\bin\\Roslyn;E:\\SOFTWARE\\Visual Studio\\Team Tools\\Performance Tools\\x64;E:\\SOFTWARE\\Visual Studio\\Team Tools\\Performance Tools;E:\\SOFTWARE\\Visual Studio\\share\\Common\\VSPerfCollectionTools\\\\x64;E:\\SOFTWARE\\Visual Studio\\share\\Common\\VSPerfCollectionTools\\;C:\\Program Files (x86)\\Microsoft SDKs\\Windows\\v10.0A\\bin\\NETFX 4.6.1 Tools\\x64\\;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\10.0.17763.0\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x64;E:\\SOFTWARE\\Visual Studio\\\\MSBuild\\15.0\\bin;C:\\WINDOWS\\Microsoft.NET\\Framework64\\v4.0.30319;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\;E:\\SOFTWARE\\Visual Studio\\Common7\\Tools\\;E:\\SOFTWARE\\Anaconda;E:\\SOFTWARE\\Anaconda\\Library\\mingw-w64\\bin;E:\\SOFTWARE\\Anaconda\\Library\\usr\\bin;E:\\SOFTWARE\\Anaconda\\Library\\bin;E:\\SOFTWARE\\Anaconda\\Scripts;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\bin;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\libnvvp;C:\\WINDOWS\\system32;C:\\WINDOWS;C:\\WINDOWS\\System32\\Wbem;C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0\\;C:\\WINDOWS\\System32\\OpenSSH\\;E:\\\u8f6f\u4ef6\\Git\\cmd;E:\\SOFTWARE\\cmake\\bin;C:\\Program Files (x86)\\Windows Kits\\8.1\\Windows Performance Toolkit\\;C:\\Program Files (x86)\\NVIDIA Corporation\\PhysX\\Common;C:\\Program Files\\dotnet\\;C:\\Program Files\\NVIDIA Corporation\\NVIDIA NvDLISR;C:\\Windows\\Microsoft.NET\\Framework\\v4.0.30319;C:\\Windows\\Microsoft.NET\\Framework64\\v4.0.30319;C:\\Users\\11838\\AppData\\Local\\Microsoft\\WindowsApps;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\CMake\\bin;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\Ninja\r\n\r\nAND the cudnn.py shows:\r\nimport os\r\nimport glob\r\n\r\nfrom .env import IS_WINDOWS, IS_CONDA, CONDA_DIR, check_negative_env_flag, gather_paths, lib_paths_from_base\r\nfrom .cuda import USE_CUDA, CUDA_HOME\r\n\r\n\r\nUSE_CUDNN = False\r\nCUDNN_LIB_DIR = None\r\nCUDNN_INCLUDE_DIR = None\r\nCUDNN_LIBRARY = None\r\nWITH_STATIC_CUDNN = os.getenv(\"USE_STATIC_CUDNN\")\r\n\r\nif USE_CUDA and not check_negative_env_flag('USE_CUDNN'):\r\n    lib_paths = list(filter(bool, [\r\n        os.getenv('CUDNN_LIB_DIR')\r\n    ] + lib_paths_from_base(CUDA_HOME) + [\r\n        '/usr/lib/x86_64-linux-gnu/',\r\n        '/usr/lib/powerpc64le-linux-gnu/',\r\n        '/usr/lib/aarch64-linux-gnu/',\r\n    ] + gather_paths([\r\n        'LIBRARY_PATH',\r\n    ]) + gather_paths([\r\n        'LD_LIBRARY_PATH',\r\n    ])))\r\n    include_paths = list(filter(bool, [\r\n        os.getenv('CUDNN_INCLUDE_DIR'),\r\n        os.path.join(CUDA_HOME, 'include'),\r\n        '/usr/include/',\r\n    ] + gather_paths([\r\n        'CPATH',\r\n        'C_INCLUDE_PATH',\r\n        'CPLUS_INCLUDE_PATH',\r\n    ])))\r\n    # Add CUDA related dirs to candidate list\r\n    if IS_CONDA:\r\n        lib_paths.append(os.path.join(CONDA_DIR, 'lib'))\r\n        include_paths.append(os.path.join(CONDA_DIR, 'include'))\r\n    for path in include_paths:\r\n        if path is None or not os.path.exists(path):\r\n            continue\r\n        include_file_path = os.path.join(path, 'cudnn.h')\r\n        CUDNN_INCLUDE_VERSION = None\r\n        if os.path.exists(include_file_path):\r\n            CUDNN_INCLUDE_DIR = path\r\n            with open(include_file_path) as f:\r\n                for line in f:\r\n                    if \"#define CUDNN_MAJOR\" in line:\r\n                        CUDNN_INCLUDE_VERSION = int(line.split()[-1])\r\n                        break\r\n            if CUDNN_INCLUDE_VERSION is None:\r\n                raise AssertionError(\"Could not find #define CUDNN_MAJOR in \" + include_file_path)\r\n            break\r\n\r\n    if CUDNN_INCLUDE_VERSION is None:\r\n        pass\r\n\r\n    # Check for standalone cuDNN libraries\r\n    if CUDNN_INCLUDE_DIR is not None:\r\n        cudnn_path = os.path.join(os.path.dirname(CUDNN_INCLUDE_DIR))\r\n        cudnn_lib_paths = lib_paths_from_base(cudnn_path)\r\n        lib_paths.extend(cudnn_lib_paths)\r\n\r\n    for path in lib_paths:\r\n        if path is None or not os.path.exists(path):\r\n            continue\r\n        if IS_WINDOWS:\r\n            library = os.path.join(path, 'cudnn.lib')\r\n            if os.path.exists(library):\r\n[Truncated]\n                search_name = 'libcudnn*' + str(CUDNN_INCLUDE_VERSION) + \"*\"\r\n            libraries = sorted(glob.glob(os.path.join(path, search_name)))\r\n            if libraries:\r\n                CUDNN_LIBRARY = libraries[0]\r\n                CUDNN_LIB_DIR = path\r\n                break\r\n    # Specifying the library directly will overwrite the lib directory\r\n    library = os.getenv('CUDNN_LIBRARY')\r\n    if library is not None and os.path.exists(library):\r\n        CUDNN_LIBRARY = library\r\n        CUDNN_LIB_DIR = os.path.dirname(CUDNN_LIBRARY)\r\n\r\n    if not all([CUDNN_LIBRARY, CUDNN_LIB_DIR, CUDNN_INCLUDE_DIR]):\r\n        CUDNN_LIBRARY = CUDNN_LIB_DIR = CUDNN_INCLUDE_DIR = None\r\n    else:\r\n        real_cudnn_library = os.path.realpath(CUDNN_LIBRARY)\r\n        real_cudnn_lib_dir = os.path.realpath(CUDNN_LIB_DIR)\r\n        assert os.path.dirname(real_cudnn_library) == real_cudnn_lib_dir, (\r\n            'cudnn library and lib_dir must agree')\r\n        USE_CUDNN = True\n<issue_comment>username_1: Please remove `CUDNN_INCLUDE_DIR` and `CUDNN_LIBRARY` by using the following commands.\r\n```cmd\r\nset CUDNN_INCLUDE_DIR=\r\nset CUDNN_LIBRARY=\r\n```\n<issue_comment>username_0: It works! Thanks a lot!<issue_closed>"}
{"repo": "pytorch/pytorch", "issue_id": 391131022, "issue_number": 15222, "timestamp": "2018-12-18 14:55:29+00:00", "text": "My environmental variables is :\r\nCUDA_PATH=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\r\nCUDA_PATH_V10_0=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\r\nCUDNN_INCLUDE_DIR=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\include\r\nCUDNN_LIBRARY=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\lib\\x64\r\nNVCUDASAMPLES10_0_ROOT=C:\\ProgramData\\NVIDIA Corporation\\CUDA Samples\\v10.0\r\nNVCUDASAMPLES_ROOT=C:\\ProgramData\\NVIDIA Corporation\\CUDA Samples\\v10.0\r\nNVTOOLSEXT_PATH=C:\\Program Files\\NVIDIA Corporation\\NvToolsExt\\\r\nPath=E:\\SOFTWARE\\Visual Studio\\VC\\Tools\\MSVC\\14.11.25503\\bin\\HostX64\\x64;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\VC\\VCPackages;C:\\Program Files (x86)\\Microsoft SDKs\\TypeScript\\3.1;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\TestWindow;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\TeamFoundation\\Team Explorer;E:\\SOFTWARE\\Visual Studio\\MSBuild\\15.0\\bin\\Roslyn;E:\\SOFTWARE\\Visual Studio\\Team Tools\\Performance Tools\\x64;E:\\SOFTWARE\\Visual Studio\\Team Tools\\Performance Tools;E:\\SOFTWARE\\Visual Studio\\share\\Common\\VSPerfCollectionTools\\\\x64;E:\\SOFTWARE\\Visual Studio\\share\\Common\\VSPerfCollectionTools\\;C:\\Program Files (x86)\\Microsoft SDKs\\Windows\\v10.0A\\bin\\NETFX 4.6.1 Tools\\x64\\;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\10.0.17763.0\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x64;E:\\SOFTWARE\\Visual Studio\\\\MSBuild\\15.0\\bin;C:\\WINDOWS\\Microsoft.NET\\Framework64\\v4.0.30319;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\;E:\\SOFTWARE\\Visual Studio\\Common7\\Tools\\;E:\\SOFTWARE\\Anaconda;E:\\SOFTWARE\\Anaconda\\Library\\mingw-w64\\bin;E:\\SOFTWARE\\Anaconda\\Library\\usr\\bin;E:\\SOFTWARE\\Anaconda\\Library\\bin;E:\\SOFTWARE\\Anaconda\\Scripts;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\bin;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\libnvvp;C:\\WINDOWS\\system32;C:\\WINDOWS;C:\\WINDOWS\\System32\\Wbem;C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0\\;C:\\WINDOWS\\System32\\OpenSSH\\;E:\\\u8f6f\u4ef6\\Git\\cmd;E:\\SOFTWARE\\cmake\\bin;C:\\Program Files (x86)\\Windows Kits\\8.1\\Windows Performance Toolkit\\;C:\\Program Files (x86)\\NVIDIA Corporation\\PhysX\\Common;C:\\Program Files\\dotnet\\;C:\\Program Files\\NVIDIA Corporation\\NVIDIA NvDLISR;C:\\Windows\\Microsoft.NET\\Framework\\v4.0.30319;C:\\Windows\\Microsoft.NET\\Framework64\\v4.0.30319;C:\\Users\\11838\\AppData\\Local\\Microsoft\\WindowsApps;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\CMake\\bin;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\Ninja\r\n\r\nAND the cudnn.py shows:\r\nimport os\r\nimport glob\r\n\r\nfrom .env import IS_WINDOWS, IS_CONDA, CONDA_DIR, check_negative_env_flag, gather_paths, lib_paths_from_base\r\nfrom .cuda import USE_CUDA, CUDA_HOME\r\n\r\n\r\nUSE_CUDNN = False\r\nCUDNN_LIB_DIR = None\r\nCUDNN_INCLUDE_DIR = None\r\nCUDNN_LIBRARY = None\r\nWITH_STATIC_CUDNN = os.getenv(\"USE_STATIC_CUDNN\")\r\n\r\nif USE_CUDA and not check_negative_env_flag('USE_CUDNN'):\r\n    lib_paths = list(filter(bool, [\r\n        os.getenv('CUDNN_LIB_DIR')\r\n    ] + lib_paths_from_base(CUDA_HOME) + [\r\n        '/usr/lib/x86_64-linux-gnu/',\r\n        '/usr/lib/powerpc64le-linux-gnu/',\r\n        '/usr/lib/aarch64-linux-gnu/',\r\n    ] + gather_paths([\r\n        'LIBRARY_PATH',\r\n    ]) + gather_paths([\r\n        'LD_LIBRARY_PATH',\r\n    ])))\r\n    include_paths = list(filter(bool, [\r\n        os.getenv('CUDNN_INCLUDE_DIR'),\r\n        os.path.join(CUDA_HOME, 'include'),\r\n        '/usr/include/',\r\n    ] + gather_paths([\r\n        'CPATH',\r\n        'C_INCLUDE_PATH',\r\n        'CPLUS_INCLUDE_PATH',\r\n    ])))\r\n    # Add CUDA related dirs to candidate list\r\n    if IS_CONDA:\r\n        lib_paths.append(os.path.join(CONDA_DIR, 'lib'))\r\n        include_paths.append(os.path.join(CONDA_DIR, 'include'))\r\n    for path in include_paths:\r\n        if path is None or not os.path.exists(path):\r\n            continue\r\n        include_file_path = os.path.join(path, 'cudnn.h')\r\n        CUDNN_INCLUDE_VERSION = None\r\n        if os.path.exists(include_file_path):\r\n            CUDNN_INCLUDE_DIR = path\r\n            with open(include_file_path) as f:\r\n                for line in f:\r\n                    if \"#define CUDNN_MAJOR\" in line:\r\n                        CUDNN_INCLUDE_VERSION = int(line.split()[-1])\r\n                        break\r\n            if CUDNN_INCLUDE_VERSION is None:\r\n                raise AssertionError(\"Could not find #define CUDNN_MAJOR in \" + include_file_path)\r\n            break\r\n\r\n    if CUDNN_INCLUDE_VERSION is None:\r\n        pass\r\n\r\n    # Check for standalone cuDNN libraries\r\n    if CUDNN_INCLUDE_DIR is not None:\r\n        cudnn_path = os.path.join(os.path.dirname(CUDNN_INCLUDE_DIR))\r\n        cudnn_lib_paths = lib_paths_from_base(cudnn_path)\r\n        lib_paths.extend(cudnn_lib_paths)\r\n\r\n    for path in lib_paths:\r\n        if path is None or not os.path.exists(path):\r\n            continue\r\n        if IS_WINDOWS:\r\n            library = os.path.join(path, 'cudnn.lib')\r\n            if os.path.exists(library):\r\n[Truncated]\n                search_name = 'libcudnn*' + str(CUDNN_INCLUDE_VERSION) + \"*\"\r\n            libraries = sorted(glob.glob(os.path.join(path, search_name)))\r\n            if libraries:\r\n                CUDNN_LIBRARY = libraries[0]\r\n                CUDNN_LIB_DIR = path\r\n                break\r\n    # Specifying the library directly will overwrite the lib directory\r\n    library = os.getenv('CUDNN_LIBRARY')\r\n    if library is not None and os.path.exists(library):\r\n        CUDNN_LIBRARY = library\r\n        CUDNN_LIB_DIR = os.path.dirname(CUDNN_LIBRARY)\r\n\r\n    if not all([CUDNN_LIBRARY, CUDNN_LIB_DIR, CUDNN_INCLUDE_DIR]):\r\n        CUDNN_LIBRARY = CUDNN_LIB_DIR = CUDNN_INCLUDE_DIR = None\r\n    else:\r\n        real_cudnn_library = os.path.realpath(CUDNN_LIBRARY)\r\n        real_cudnn_lib_dir = os.path.realpath(CUDNN_LIB_DIR)\r\n        assert os.path.dirname(real_cudnn_library) == real_cudnn_lib_dir, (\r\n            'cudnn library and lib_dir must agree')\r\n        USE_CUDNN = True", "context": "<issue_start><issue_comment>Title: Can't open x64.obj while installing pytorch with win10 \nusername_0: \u5df2\u5b8c\u6210\u751f\u6210\u9879\u76ee\u201cE:\\SOFTWARE\\pytorch\\build\\caffe2\\torch\\lib\\libshm_windows\\shm.vcxproj\u201d(\u9ed8\u8ba4\u76ee\u6807)\u7684\u64cd\u4f5c\u3002\r\n\r\n\u5df2\u5b8c\u6210\u751f\u6210\u9879\u76ee\u201cE:\\SOFTWARE\\pytorch\\build\\ALL_BUILD.vcxproj\u201d(\u9ed8\u8ba4\u76ee\u6807)\u7684\u64cd\u4f5c - \u5931\u8d25\u3002\r\n\r\n\u5df2\u5b8c\u6210\u751f\u6210\u9879\u76ee\u201cE:\\SOFTWARE\\pytorch\\build\\INSTALL.vcxproj\u201d(\u9ed8\u8ba4\u76ee\u6807)\u7684\u64cd\u4f5c - \u5931\u8d25\u3002\r\n\r\n\r\n\u751f\u6210\u5931\u8d25\u3002\r\n\r\n\u201cE:\\SOFTWARE\\pytorch\\build\\INSTALL.vcxproj\u201d(\u9ed8\u8ba4\u76ee\u6807) (1) ->\r\n\u201cE:\\SOFTWARE\\pytorch\\build\\ALL_BUILD.vcxproj\u201d(\u9ed8\u8ba4\u76ee\u6807) (3) ->\r\n\u201cE:\\SOFTWARE\\pytorch\\build\\caffe2\\AlgorithmsTest.vcxproj\u201d(\u9ed8\u8ba4\u76ee\u6807) (4) ->\r\n\u201cE:\\SOFTWARE\\pytorch\\build\\caffe2\\caffe2_gpu.vcxproj\u201d(\u9ed8\u8ba4\u76ee\u6807) (22) ->\r\n(Link \u76ee\u6807) ->\r\n  C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\lib\\x64.obj : fatal error LNK1136: \u65e0\u6548\u6216\u635f\u574f\u7684\u6587\u4ef6 [E:\\SOFTWARE\\pyt\r\norch\\build\\caffe2\\caffe2_gpu.vcxproj]\r\n\r\n    0 \u4e2a\u8b66\u544a\r\n    1 \u4e2a\u9519\u8bef\r\n\r\n\u5df2\u7528\u65f6\u95f4 00:00:16.23\r\n\r\nE:\\SOFTWARE\\pytorch\\build>IF ERRORLEVEL 1 exit 1\r\nFailed to run 'tools\\build_pytorch_libs.bat --use-cuda --use-fbgemm --use-nnpack --use-mkldnn --use-qnnpack caffe2'\r\n\r\npytorch version\uff1astable\uff081.0\uff09\r\nos\uff1aWindows 10 Pro Version 1803\r\npython version:3.7\r\nCUDA:10\r\ncudnn:7.4.1\r\nVisual Studio version: Microsoft Visual Studio 2017 Community Edition\r\n\r\nCan anyone help me solve this problem? Thanks !\n<issue_comment>username_1: Please provide the full log.\n<issue_comment>username_0: the full log\uff1a\r\n\r\nBuilding wheel torch-1.0.0a0+df61437\r\nrunning install\r\nsetup.py::run()\r\nrunning build_deps\r\nsetup.py::build_deps::run()\r\n\r\nE:\\SOFTWARE\\pytorch>cd \"E:\\SOFTWARE\\pytorch\\tools\\\\..\"\r\n\r\nE:\\SOFTWARE\\pytorch>set PATH=\\bin;E:\\SOFTWARE\\Anaconda\\DLLs;E:\\SOFTWARE\\Visual Studio\\VC\\Tools\\MSVC\\14.11.25503\\bin\\HostX64\\x64;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\VC\\VCPackages;C:\\Program Files (x86)\\Microsoft SDKs\\TypeScript\\3.1;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\TestWindow;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\TeamFoundation\\Team Explorer;E:\\SOFTWARE\\Visual Studio\\MSBuild\\15.0\\bin\\Roslyn;E:\\SOFTWARE\\Visual Studio\\Team Tools\\Performance Tools\\x64;E:\\SOFTWARE\\Visual Studio\\Team Tools\\Performance Tools;E:\\SOFTWARE\\Visual Studio\\share\\Common\\VSPerfCollectionTools\\\\x64;E:\\SOFTWARE\\Visual Studio\\share\\Common\\VSPerfCollectionTools\\;C:\\Program Files (x86)\\Microsoft SDKs\\Windows\\v10.0A\\bin\\NETFX 4.6.1 Tools\\x64\\;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\10.0.17763.0\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x64;E:\\SOFTWARE\\Visual Studio\\\\MSBuild\\15.0\\bin;C:\\WINDOWS\\Microsoft.NET\\Framework64\\v4.0.30319;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\;E:\\SOFTWARE\\Visual Studio\\Common7\\Tools\\;E:\\SOFTWARE\\Anaconda;E:\\SOFTWARE\\Anaconda\\Library\\mingw-w64\\bin;E:\\SOFTWARE\\Anaconda\\Library\\usr\\bin;E:\\SOFTWARE\\Anaconda\\Library\\bin;E:\\SOFTWARE\\Anaconda\\Scripts;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\bin;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\libnvvp;C:\\WINDOWS\\system32;C:\\WINDOWS;C:\\WINDOWS\\System32\\Wbem;C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0\\;C:\\WINDOWS\\System32\\OpenSSH\\;E:\\\u8f6f\u4ef6\\Git\\cmd;E:\\SOFTWARE\\cmake\\bin;C:\\Program Files (x86)\\Windows Kits\\8.1\\Windows Performance Toolkit\\;C:\\Program Files (x86)\\NVIDIA Corporation\\PhysX\\Common;C:\\Program Files\\dotnet\\;C:\\Program Files\\NVIDIA Corporation\\NVIDIA NvDLISR;C:\\Windows\\Microsoft.NET\\Framework\\v4.0.30319;C:\\Windows\\Microsoft.NET\\Framework64\\v4.0.30319;C:\\Users\\11838\\AppData\\Local\\Microsoft\\WindowsApps;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\CMake\\bin;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\Ninja\r\n\r\nE:\\SOFTWARE\\pytorch>set BASE_DIR=E:/SOFTWARE/pytorch\r\n\r\nE:\\SOFTWARE\\pytorch>set TORCH_LIB_DIR=E:/SOFTWARE/pytorch/torch/lib\r\n\r\nE:\\SOFTWARE\\pytorch>set THIRD_PARTY_DIR=E:/SOFTWARE/pytorch/third_party\r\n\r\nE:\\SOFTWARE\\pytorch>set BASIC_C_FLAGS=\r\n\r\nE:\\SOFTWARE\\pytorch>set BASIC_CUDA_FLAGS=\r\n\r\nE:\\SOFTWARE\\pytorch>IF NOT DEFINED INSTALL_DIR (set \"INSTALL_DIR=E:/SOFTWARE/pytorch/torch/lib/tmp_install\" )  ELSE (set \"INSTALL_DIR=\\=/\" )\r\n\r\nE:\\SOFTWARE\\pytorch>set LDFLAGS=/LIBPATH:E:/SOFTWARE/pytorch/torch/lib/tmp_install/lib\r\n\r\nE:\\SOFTWARE\\pytorch>set C_FLAGS= /D_WIN32 /Z7 /EHa /DNOMINMAX\r\n\r\nE:\\SOFTWARE\\pytorch>set LINK_FLAGS=/DEBUG:FULL\r\n\r\nE:\\SOFTWARE\\pytorch>if not exist torch\\lib\\tmp_install mkdir torch\\lib\\tmp_install\r\n\r\nE:\\SOFTWARE\\pytorch>set /a USE_CUDA=0\r\n\r\nE:\\SOFTWARE\\pytorch>set /a USE_FBGEMM=1\r\n\r\nE:\\SOFTWARE\\pytorch>set /a USE_ROCM=0\r\n\r\nE:\\SOFTWARE\\pytorch>set /a USE_NNPACK=0\r\n\r\nE:\\SOFTWARE\\pytorch>set /a USE_QNNPACK=0\r\n\r\nE:\\SOFTWARE\\pytorch>set /a USE_GLOO_IBVERBS=0\r\n\r\nE:\\SOFTWARE\\pytorch>set /a USE_MKLDNN=0\r\n\r\nE:\\SOFTWARE\\pytorch>set _BUILD_ARGS=\r\n\r\nE:\\SOFTWARE\\pytorch>if \"--use-cuda\" == \"\" (goto :process_args_exit )\r\n\r\nE:\\SOFTWARE\\pytorch>if \"--use-cuda\" == \"--use-cuda\" (\r\nset /a USE_CUDA=1\r\n goto :process_args_processed\r\n)\r\n\r\nE:\\SOFTWARE\\pytorch>shift\r\n\r\nE:\\SOFTWARE\\pytorch>goto :process_args\r\n\r\nE:\\SOFTWARE\\pytorch>if \"--use-fbgemm\" == \"\" (goto :process_args_exit )\r\n\r\nE:\\SOFTWARE\\pytorch>if \"--use-fbgemm\" == \"--use-cuda\" (\r\nset /a USE_CUDA=1\r\n goto :process_args_processed\r\n)\r\n\r\nE:\\SOFTWARE\\pytorch>if \"--use-fbgemm\" == \"--use-fbgemm\" (\r\nset /a USE_FBGEMM=1\r\n goto :process_args_processed\r\n)\r\n\r\nE:\\SOFTWARE\\pytorch>shift\r\n\r\nE:\\SOFTWARE\\pytorch>goto :process_args\r\n\r\nE:\\SOFTWARE\\pytorch>if \"--use-nnpack\" == \"\" (goto :process_args_exit )\r\n\r\nE:\\SOFTWARE\\pytorch>if \"--use-nnpack\" == \"--use-cuda\" (\r\nset /a USE_CUDA=1\r\n goto :process_args_processed\r\n[Truncated]\nDone Building Project \"E:\\SOFTWARE\\pytorch\\build\\INSTALL.vcxproj\" (default targets) -- FAILED.\r\n\r\n\r\nBuild FAILED.\r\n\r\n\"E:\\SOFTWARE\\pytorch\\build\\INSTALL.vcxproj\" (default target) (1) ->\r\n\"E:\\SOFTWARE\\pytorch\\build\\ALL_BUILD.vcxproj\" (default target) (3) ->\r\n\"E:\\SOFTWARE\\pytorch\\build\\caffe2\\AlgorithmsTest.vcxproj\" (default target) (4) ->\r\n\"E:\\SOFTWARE\\pytorch\\build\\caffe2\\caffe2_gpu.vcxproj\" (default target) (22) ->\r\n(Link target) ->\r\n  LINK : fatal error LNK1181: \u65e0\u6cd5\u6253\u5f00\u8f93\u5165\u6587\u4ef6\u201cC:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\lib\\x64.obj\u201d [E:\\SOFTWA\r\nRE\\pytorch\\build\\caffe2\\caffe2_gpu.vcxproj]\r\n\r\n    0 Warning(s)\r\n    1 Error(s)\r\n\r\nTime Elapsed 00:00:22.22\r\n\r\nE:\\SOFTWARE\\pytorch\\build>IF ERRORLEVEL 1 exit 1\r\nFailed to run 'tools\\build_pytorch_libs.bat --use-cuda --use-fbgemm --use-nnpack --use-mkldnn --use-qnnpack caffe2'\n<issue_comment>username_1: The error is caused by the misconfiguration of the flag `CUDNN_LIBRARY`. Could you please run `tools/setup_helpers/cudnn.py` and see which code path it goes through?\n<issue_comment>username_0: My environmental variables is :\r\nCUDA_PATH=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\r\nCUDA_PATH_V10_0=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\r\nCUDNN_INCLUDE_DIR=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\include\r\nCUDNN_LIBRARY=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\lib\\x64\r\nNVCUDASAMPLES10_0_ROOT=C:\\ProgramData\\NVIDIA Corporation\\CUDA Samples\\v10.0\r\nNVCUDASAMPLES_ROOT=C:\\ProgramData\\NVIDIA Corporation\\CUDA Samples\\v10.0\r\nNVTOOLSEXT_PATH=C:\\Program Files\\NVIDIA Corporation\\NvToolsExt\\\r\nPath=E:\\SOFTWARE\\Visual Studio\\VC\\Tools\\MSVC\\14.11.25503\\bin\\HostX64\\x64;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\VC\\VCPackages;C:\\Program Files (x86)\\Microsoft SDKs\\TypeScript\\3.1;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\TestWindow;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\TeamFoundation\\Team Explorer;E:\\SOFTWARE\\Visual Studio\\MSBuild\\15.0\\bin\\Roslyn;E:\\SOFTWARE\\Visual Studio\\Team Tools\\Performance Tools\\x64;E:\\SOFTWARE\\Visual Studio\\Team Tools\\Performance Tools;E:\\SOFTWARE\\Visual Studio\\share\\Common\\VSPerfCollectionTools\\\\x64;E:\\SOFTWARE\\Visual Studio\\share\\Common\\VSPerfCollectionTools\\;C:\\Program Files (x86)\\Microsoft SDKs\\Windows\\v10.0A\\bin\\NETFX 4.6.1 Tools\\x64\\;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\10.0.17763.0\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x64;E:\\SOFTWARE\\Visual Studio\\\\MSBuild\\15.0\\bin;C:\\WINDOWS\\Microsoft.NET\\Framework64\\v4.0.30319;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\;E:\\SOFTWARE\\Visual Studio\\Common7\\Tools\\;E:\\SOFTWARE\\Anaconda;E:\\SOFTWARE\\Anaconda\\Library\\mingw-w64\\bin;E:\\SOFTWARE\\Anaconda\\Library\\usr\\bin;E:\\SOFTWARE\\Anaconda\\Library\\bin;E:\\SOFTWARE\\Anaconda\\Scripts;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\bin;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\libnvvp;C:\\WINDOWS\\system32;C:\\WINDOWS;C:\\WINDOWS\\System32\\Wbem;C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0\\;C:\\WINDOWS\\System32\\OpenSSH\\;E:\\\u8f6f\u4ef6\\Git\\cmd;E:\\SOFTWARE\\cmake\\bin;C:\\Program Files (x86)\\Windows Kits\\8.1\\Windows Performance Toolkit\\;C:\\Program Files (x86)\\NVIDIA Corporation\\PhysX\\Common;C:\\Program Files\\dotnet\\;C:\\Program Files\\NVIDIA Corporation\\NVIDIA NvDLISR;C:\\Windows\\Microsoft.NET\\Framework\\v4.0.30319;C:\\Windows\\Microsoft.NET\\Framework64\\v4.0.30319;C:\\Users\\11838\\AppData\\Local\\Microsoft\\WindowsApps;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\CMake\\bin;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\Ninja\r\n\r\nAND the cudnn.py shows:\r\nimport os\r\nimport glob\r\n\r\nfrom .env import IS_WINDOWS, IS_CONDA, CONDA_DIR, check_negative_env_flag, gather_paths, lib_paths_from_base\r\nfrom .cuda import USE_CUDA, CUDA_HOME\r\n\r\n\r\nUSE_CUDNN = False\r\nCUDNN_LIB_DIR = None\r\nCUDNN_INCLUDE_DIR = None\r\nCUDNN_LIBRARY = None\r\nWITH_STATIC_CUDNN = os.getenv(\"USE_STATIC_CUDNN\")\r\n\r\nif USE_CUDA and not check_negative_env_flag('USE_CUDNN'):\r\n    lib_paths = list(filter(bool, [\r\n        os.getenv('CUDNN_LIB_DIR')\r\n    ] + lib_paths_from_base(CUDA_HOME) + [\r\n        '/usr/lib/x86_64-linux-gnu/',\r\n        '/usr/lib/powerpc64le-linux-gnu/',\r\n        '/usr/lib/aarch64-linux-gnu/',\r\n    ] + gather_paths([\r\n        'LIBRARY_PATH',\r\n    ]) + gather_paths([\r\n        'LD_LIBRARY_PATH',\r\n    ])))\r\n    include_paths = list(filter(bool, [\r\n        os.getenv('CUDNN_INCLUDE_DIR'),\r\n        os.path.join(CUDA_HOME, 'include'),\r\n        '/usr/include/',\r\n    ] + gather_paths([\r\n        'CPATH',\r\n        'C_INCLUDE_PATH',\r\n        'CPLUS_INCLUDE_PATH',\r\n    ])))\r\n    # Add CUDA related dirs to candidate list\r\n    if IS_CONDA:\r\n        lib_paths.append(os.path.join(CONDA_DIR, 'lib'))\r\n        include_paths.append(os.path.join(CONDA_DIR, 'include'))\r\n    for path in include_paths:\r\n        if path is None or not os.path.exists(path):\r\n            continue\r\n        include_file_path = os.path.join(path, 'cudnn.h')\r\n        CUDNN_INCLUDE_VERSION = None\r\n        if os.path.exists(include_file_path):\r\n            CUDNN_INCLUDE_DIR = path\r\n            with open(include_file_path) as f:\r\n                for line in f:\r\n                    if \"#define CUDNN_MAJOR\" in line:\r\n                        CUDNN_INCLUDE_VERSION = int(line.split()[-1])\r\n                        break\r\n            if CUDNN_INCLUDE_VERSION is None:\r\n                raise AssertionError(\"Could not find #define CUDNN_MAJOR in \" + include_file_path)\r\n            break\r\n\r\n    if CUDNN_INCLUDE_VERSION is None:\r\n        pass\r\n\r\n    # Check for standalone cuDNN libraries\r\n    if CUDNN_INCLUDE_DIR is not None:\r\n        cudnn_path = os.path.join(os.path.dirname(CUDNN_INCLUDE_DIR))\r\n        cudnn_lib_paths = lib_paths_from_base(cudnn_path)\r\n        lib_paths.extend(cudnn_lib_paths)\r\n\r\n    for path in lib_paths:\r\n        if path is None or not os.path.exists(path):\r\n            continue\r\n        if IS_WINDOWS:\r\n            library = os.path.join(path, 'cudnn.lib')\r\n            if os.path.exists(library):\r\n[Truncated]\n                search_name = 'libcudnn*' + str(CUDNN_INCLUDE_VERSION) + \"*\"\r\n            libraries = sorted(glob.glob(os.path.join(path, search_name)))\r\n            if libraries:\r\n                CUDNN_LIBRARY = libraries[0]\r\n                CUDNN_LIB_DIR = path\r\n                break\r\n    # Specifying the library directly will overwrite the lib directory\r\n    library = os.getenv('CUDNN_LIBRARY')\r\n    if library is not None and os.path.exists(library):\r\n        CUDNN_LIBRARY = library\r\n        CUDNN_LIB_DIR = os.path.dirname(CUDNN_LIBRARY)\r\n\r\n    if not all([CUDNN_LIBRARY, CUDNN_LIB_DIR, CUDNN_INCLUDE_DIR]):\r\n        CUDNN_LIBRARY = CUDNN_LIB_DIR = CUDNN_INCLUDE_DIR = None\r\n    else:\r\n        real_cudnn_library = os.path.realpath(CUDNN_LIBRARY)\r\n        real_cudnn_lib_dir = os.path.realpath(CUDNN_LIB_DIR)\r\n        assert os.path.dirname(real_cudnn_library) == real_cudnn_lib_dir, (\r\n            'cudnn library and lib_dir must agree')\r\n        USE_CUDNN = True\n<issue_comment>username_1: Please remove `CUDNN_INCLUDE_DIR` and `CUDNN_LIBRARY` by using the following commands.\r\n```cmd\r\nset CUDNN_INCLUDE_DIR=\r\nset CUDNN_LIBRARY=\r\n```\n<issue_comment>username_0: It works! Thanks a lot!<issue_closed>"}
{"repo": "pytorch/pytorch", "issue_id": 391131022, "issue_number": 15222, "timestamp": "2018-12-18 15:05:11+00:00", "text": "Please remove `CUDNN_INCLUDE_DIR` and `CUDNN_LIBRARY` by using the following commands.\r\n```cmd\r\nset CUDNN_INCLUDE_DIR=\r\nset CUDNN_LIBRARY=\r\n```", "context": "<issue_start><issue_comment>Title: Can't open x64.obj while installing pytorch with win10 \nusername_0: \u5df2\u5b8c\u6210\u751f\u6210\u9879\u76ee\u201cE:\\SOFTWARE\\pytorch\\build\\caffe2\\torch\\lib\\libshm_windows\\shm.vcxproj\u201d(\u9ed8\u8ba4\u76ee\u6807)\u7684\u64cd\u4f5c\u3002\r\n\r\n\u5df2\u5b8c\u6210\u751f\u6210\u9879\u76ee\u201cE:\\SOFTWARE\\pytorch\\build\\ALL_BUILD.vcxproj\u201d(\u9ed8\u8ba4\u76ee\u6807)\u7684\u64cd\u4f5c - \u5931\u8d25\u3002\r\n\r\n\u5df2\u5b8c\u6210\u751f\u6210\u9879\u76ee\u201cE:\\SOFTWARE\\pytorch\\build\\INSTALL.vcxproj\u201d(\u9ed8\u8ba4\u76ee\u6807)\u7684\u64cd\u4f5c - \u5931\u8d25\u3002\r\n\r\n\r\n\u751f\u6210\u5931\u8d25\u3002\r\n\r\n\u201cE:\\SOFTWARE\\pytorch\\build\\INSTALL.vcxproj\u201d(\u9ed8\u8ba4\u76ee\u6807) (1) ->\r\n\u201cE:\\SOFTWARE\\pytorch\\build\\ALL_BUILD.vcxproj\u201d(\u9ed8\u8ba4\u76ee\u6807) (3) ->\r\n\u201cE:\\SOFTWARE\\pytorch\\build\\caffe2\\AlgorithmsTest.vcxproj\u201d(\u9ed8\u8ba4\u76ee\u6807) (4) ->\r\n\u201cE:\\SOFTWARE\\pytorch\\build\\caffe2\\caffe2_gpu.vcxproj\u201d(\u9ed8\u8ba4\u76ee\u6807) (22) ->\r\n(Link \u76ee\u6807) ->\r\n  C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\lib\\x64.obj : fatal error LNK1136: \u65e0\u6548\u6216\u635f\u574f\u7684\u6587\u4ef6 [E:\\SOFTWARE\\pyt\r\norch\\build\\caffe2\\caffe2_gpu.vcxproj]\r\n\r\n    0 \u4e2a\u8b66\u544a\r\n    1 \u4e2a\u9519\u8bef\r\n\r\n\u5df2\u7528\u65f6\u95f4 00:00:16.23\r\n\r\nE:\\SOFTWARE\\pytorch\\build>IF ERRORLEVEL 1 exit 1\r\nFailed to run 'tools\\build_pytorch_libs.bat --use-cuda --use-fbgemm --use-nnpack --use-mkldnn --use-qnnpack caffe2'\r\n\r\npytorch version\uff1astable\uff081.0\uff09\r\nos\uff1aWindows 10 Pro Version 1803\r\npython version:3.7\r\nCUDA:10\r\ncudnn:7.4.1\r\nVisual Studio version: Microsoft Visual Studio 2017 Community Edition\r\n\r\nCan anyone help me solve this problem? Thanks !\n<issue_comment>username_1: Please provide the full log.\n<issue_comment>username_0: the full log\uff1a\r\n\r\nBuilding wheel torch-1.0.0a0+df61437\r\nrunning install\r\nsetup.py::run()\r\nrunning build_deps\r\nsetup.py::build_deps::run()\r\n\r\nE:\\SOFTWARE\\pytorch>cd \"E:\\SOFTWARE\\pytorch\\tools\\\\..\"\r\n\r\nE:\\SOFTWARE\\pytorch>set PATH=\\bin;E:\\SOFTWARE\\Anaconda\\DLLs;E:\\SOFTWARE\\Visual Studio\\VC\\Tools\\MSVC\\14.11.25503\\bin\\HostX64\\x64;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\VC\\VCPackages;C:\\Program Files (x86)\\Microsoft SDKs\\TypeScript\\3.1;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\TestWindow;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\TeamFoundation\\Team Explorer;E:\\SOFTWARE\\Visual Studio\\MSBuild\\15.0\\bin\\Roslyn;E:\\SOFTWARE\\Visual Studio\\Team Tools\\Performance Tools\\x64;E:\\SOFTWARE\\Visual Studio\\Team Tools\\Performance Tools;E:\\SOFTWARE\\Visual Studio\\share\\Common\\VSPerfCollectionTools\\\\x64;E:\\SOFTWARE\\Visual Studio\\share\\Common\\VSPerfCollectionTools\\;C:\\Program Files (x86)\\Microsoft SDKs\\Windows\\v10.0A\\bin\\NETFX 4.6.1 Tools\\x64\\;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\10.0.17763.0\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x64;E:\\SOFTWARE\\Visual Studio\\\\MSBuild\\15.0\\bin;C:\\WINDOWS\\Microsoft.NET\\Framework64\\v4.0.30319;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\;E:\\SOFTWARE\\Visual Studio\\Common7\\Tools\\;E:\\SOFTWARE\\Anaconda;E:\\SOFTWARE\\Anaconda\\Library\\mingw-w64\\bin;E:\\SOFTWARE\\Anaconda\\Library\\usr\\bin;E:\\SOFTWARE\\Anaconda\\Library\\bin;E:\\SOFTWARE\\Anaconda\\Scripts;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\bin;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\libnvvp;C:\\WINDOWS\\system32;C:\\WINDOWS;C:\\WINDOWS\\System32\\Wbem;C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0\\;C:\\WINDOWS\\System32\\OpenSSH\\;E:\\\u8f6f\u4ef6\\Git\\cmd;E:\\SOFTWARE\\cmake\\bin;C:\\Program Files (x86)\\Windows Kits\\8.1\\Windows Performance Toolkit\\;C:\\Program Files (x86)\\NVIDIA Corporation\\PhysX\\Common;C:\\Program Files\\dotnet\\;C:\\Program Files\\NVIDIA Corporation\\NVIDIA NvDLISR;C:\\Windows\\Microsoft.NET\\Framework\\v4.0.30319;C:\\Windows\\Microsoft.NET\\Framework64\\v4.0.30319;C:\\Users\\11838\\AppData\\Local\\Microsoft\\WindowsApps;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\CMake\\bin;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\Ninja\r\n\r\nE:\\SOFTWARE\\pytorch>set BASE_DIR=E:/SOFTWARE/pytorch\r\n\r\nE:\\SOFTWARE\\pytorch>set TORCH_LIB_DIR=E:/SOFTWARE/pytorch/torch/lib\r\n\r\nE:\\SOFTWARE\\pytorch>set THIRD_PARTY_DIR=E:/SOFTWARE/pytorch/third_party\r\n\r\nE:\\SOFTWARE\\pytorch>set BASIC_C_FLAGS=\r\n\r\nE:\\SOFTWARE\\pytorch>set BASIC_CUDA_FLAGS=\r\n\r\nE:\\SOFTWARE\\pytorch>IF NOT DEFINED INSTALL_DIR (set \"INSTALL_DIR=E:/SOFTWARE/pytorch/torch/lib/tmp_install\" )  ELSE (set \"INSTALL_DIR=\\=/\" )\r\n\r\nE:\\SOFTWARE\\pytorch>set LDFLAGS=/LIBPATH:E:/SOFTWARE/pytorch/torch/lib/tmp_install/lib\r\n\r\nE:\\SOFTWARE\\pytorch>set C_FLAGS= /D_WIN32 /Z7 /EHa /DNOMINMAX\r\n\r\nE:\\SOFTWARE\\pytorch>set LINK_FLAGS=/DEBUG:FULL\r\n\r\nE:\\SOFTWARE\\pytorch>if not exist torch\\lib\\tmp_install mkdir torch\\lib\\tmp_install\r\n\r\nE:\\SOFTWARE\\pytorch>set /a USE_CUDA=0\r\n\r\nE:\\SOFTWARE\\pytorch>set /a USE_FBGEMM=1\r\n\r\nE:\\SOFTWARE\\pytorch>set /a USE_ROCM=0\r\n\r\nE:\\SOFTWARE\\pytorch>set /a USE_NNPACK=0\r\n\r\nE:\\SOFTWARE\\pytorch>set /a USE_QNNPACK=0\r\n\r\nE:\\SOFTWARE\\pytorch>set /a USE_GLOO_IBVERBS=0\r\n\r\nE:\\SOFTWARE\\pytorch>set /a USE_MKLDNN=0\r\n\r\nE:\\SOFTWARE\\pytorch>set _BUILD_ARGS=\r\n\r\nE:\\SOFTWARE\\pytorch>if \"--use-cuda\" == \"\" (goto :process_args_exit )\r\n\r\nE:\\SOFTWARE\\pytorch>if \"--use-cuda\" == \"--use-cuda\" (\r\nset /a USE_CUDA=1\r\n goto :process_args_processed\r\n)\r\n\r\nE:\\SOFTWARE\\pytorch>shift\r\n\r\nE:\\SOFTWARE\\pytorch>goto :process_args\r\n\r\nE:\\SOFTWARE\\pytorch>if \"--use-fbgemm\" == \"\" (goto :process_args_exit )\r\n\r\nE:\\SOFTWARE\\pytorch>if \"--use-fbgemm\" == \"--use-cuda\" (\r\nset /a USE_CUDA=1\r\n goto :process_args_processed\r\n)\r\n\r\nE:\\SOFTWARE\\pytorch>if \"--use-fbgemm\" == \"--use-fbgemm\" (\r\nset /a USE_FBGEMM=1\r\n goto :process_args_processed\r\n)\r\n\r\nE:\\SOFTWARE\\pytorch>shift\r\n\r\nE:\\SOFTWARE\\pytorch>goto :process_args\r\n\r\nE:\\SOFTWARE\\pytorch>if \"--use-nnpack\" == \"\" (goto :process_args_exit )\r\n\r\nE:\\SOFTWARE\\pytorch>if \"--use-nnpack\" == \"--use-cuda\" (\r\nset /a USE_CUDA=1\r\n goto :process_args_processed\r\n[Truncated]\nDone Building Project \"E:\\SOFTWARE\\pytorch\\build\\INSTALL.vcxproj\" (default targets) -- FAILED.\r\n\r\n\r\nBuild FAILED.\r\n\r\n\"E:\\SOFTWARE\\pytorch\\build\\INSTALL.vcxproj\" (default target) (1) ->\r\n\"E:\\SOFTWARE\\pytorch\\build\\ALL_BUILD.vcxproj\" (default target) (3) ->\r\n\"E:\\SOFTWARE\\pytorch\\build\\caffe2\\AlgorithmsTest.vcxproj\" (default target) (4) ->\r\n\"E:\\SOFTWARE\\pytorch\\build\\caffe2\\caffe2_gpu.vcxproj\" (default target) (22) ->\r\n(Link target) ->\r\n  LINK : fatal error LNK1181: \u65e0\u6cd5\u6253\u5f00\u8f93\u5165\u6587\u4ef6\u201cC:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\lib\\x64.obj\u201d [E:\\SOFTWA\r\nRE\\pytorch\\build\\caffe2\\caffe2_gpu.vcxproj]\r\n\r\n    0 Warning(s)\r\n    1 Error(s)\r\n\r\nTime Elapsed 00:00:22.22\r\n\r\nE:\\SOFTWARE\\pytorch\\build>IF ERRORLEVEL 1 exit 1\r\nFailed to run 'tools\\build_pytorch_libs.bat --use-cuda --use-fbgemm --use-nnpack --use-mkldnn --use-qnnpack caffe2'\n<issue_comment>username_1: The error is caused by the misconfiguration of the flag `CUDNN_LIBRARY`. Could you please run `tools/setup_helpers/cudnn.py` and see which code path it goes through?\n<issue_comment>username_0: My environmental variables is :\r\nCUDA_PATH=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\r\nCUDA_PATH_V10_0=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\r\nCUDNN_INCLUDE_DIR=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\include\r\nCUDNN_LIBRARY=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\lib\\x64\r\nNVCUDASAMPLES10_0_ROOT=C:\\ProgramData\\NVIDIA Corporation\\CUDA Samples\\v10.0\r\nNVCUDASAMPLES_ROOT=C:\\ProgramData\\NVIDIA Corporation\\CUDA Samples\\v10.0\r\nNVTOOLSEXT_PATH=C:\\Program Files\\NVIDIA Corporation\\NvToolsExt\\\r\nPath=E:\\SOFTWARE\\Visual Studio\\VC\\Tools\\MSVC\\14.11.25503\\bin\\HostX64\\x64;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\VC\\VCPackages;C:\\Program Files (x86)\\Microsoft SDKs\\TypeScript\\3.1;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\TestWindow;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\TeamFoundation\\Team Explorer;E:\\SOFTWARE\\Visual Studio\\MSBuild\\15.0\\bin\\Roslyn;E:\\SOFTWARE\\Visual Studio\\Team Tools\\Performance Tools\\x64;E:\\SOFTWARE\\Visual Studio\\Team Tools\\Performance Tools;E:\\SOFTWARE\\Visual Studio\\share\\Common\\VSPerfCollectionTools\\\\x64;E:\\SOFTWARE\\Visual Studio\\share\\Common\\VSPerfCollectionTools\\;C:\\Program Files (x86)\\Microsoft SDKs\\Windows\\v10.0A\\bin\\NETFX 4.6.1 Tools\\x64\\;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\10.0.17763.0\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x64;E:\\SOFTWARE\\Visual Studio\\\\MSBuild\\15.0\\bin;C:\\WINDOWS\\Microsoft.NET\\Framework64\\v4.0.30319;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\;E:\\SOFTWARE\\Visual Studio\\Common7\\Tools\\;E:\\SOFTWARE\\Anaconda;E:\\SOFTWARE\\Anaconda\\Library\\mingw-w64\\bin;E:\\SOFTWARE\\Anaconda\\Library\\usr\\bin;E:\\SOFTWARE\\Anaconda\\Library\\bin;E:\\SOFTWARE\\Anaconda\\Scripts;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\bin;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\libnvvp;C:\\WINDOWS\\system32;C:\\WINDOWS;C:\\WINDOWS\\System32\\Wbem;C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0\\;C:\\WINDOWS\\System32\\OpenSSH\\;E:\\\u8f6f\u4ef6\\Git\\cmd;E:\\SOFTWARE\\cmake\\bin;C:\\Program Files (x86)\\Windows Kits\\8.1\\Windows Performance Toolkit\\;C:\\Program Files (x86)\\NVIDIA Corporation\\PhysX\\Common;C:\\Program Files\\dotnet\\;C:\\Program Files\\NVIDIA Corporation\\NVIDIA NvDLISR;C:\\Windows\\Microsoft.NET\\Framework\\v4.0.30319;C:\\Windows\\Microsoft.NET\\Framework64\\v4.0.30319;C:\\Users\\11838\\AppData\\Local\\Microsoft\\WindowsApps;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\CMake\\bin;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\Ninja\r\n\r\nAND the cudnn.py shows:\r\nimport os\r\nimport glob\r\n\r\nfrom .env import IS_WINDOWS, IS_CONDA, CONDA_DIR, check_negative_env_flag, gather_paths, lib_paths_from_base\r\nfrom .cuda import USE_CUDA, CUDA_HOME\r\n\r\n\r\nUSE_CUDNN = False\r\nCUDNN_LIB_DIR = None\r\nCUDNN_INCLUDE_DIR = None\r\nCUDNN_LIBRARY = None\r\nWITH_STATIC_CUDNN = os.getenv(\"USE_STATIC_CUDNN\")\r\n\r\nif USE_CUDA and not check_negative_env_flag('USE_CUDNN'):\r\n    lib_paths = list(filter(bool, [\r\n        os.getenv('CUDNN_LIB_DIR')\r\n    ] + lib_paths_from_base(CUDA_HOME) + [\r\n        '/usr/lib/x86_64-linux-gnu/',\r\n        '/usr/lib/powerpc64le-linux-gnu/',\r\n        '/usr/lib/aarch64-linux-gnu/',\r\n    ] + gather_paths([\r\n        'LIBRARY_PATH',\r\n    ]) + gather_paths([\r\n        'LD_LIBRARY_PATH',\r\n    ])))\r\n    include_paths = list(filter(bool, [\r\n        os.getenv('CUDNN_INCLUDE_DIR'),\r\n        os.path.join(CUDA_HOME, 'include'),\r\n        '/usr/include/',\r\n    ] + gather_paths([\r\n        'CPATH',\r\n        'C_INCLUDE_PATH',\r\n        'CPLUS_INCLUDE_PATH',\r\n    ])))\r\n    # Add CUDA related dirs to candidate list\r\n    if IS_CONDA:\r\n        lib_paths.append(os.path.join(CONDA_DIR, 'lib'))\r\n        include_paths.append(os.path.join(CONDA_DIR, 'include'))\r\n    for path in include_paths:\r\n        if path is None or not os.path.exists(path):\r\n            continue\r\n        include_file_path = os.path.join(path, 'cudnn.h')\r\n        CUDNN_INCLUDE_VERSION = None\r\n        if os.path.exists(include_file_path):\r\n            CUDNN_INCLUDE_DIR = path\r\n            with open(include_file_path) as f:\r\n                for line in f:\r\n                    if \"#define CUDNN_MAJOR\" in line:\r\n                        CUDNN_INCLUDE_VERSION = int(line.split()[-1])\r\n                        break\r\n            if CUDNN_INCLUDE_VERSION is None:\r\n                raise AssertionError(\"Could not find #define CUDNN_MAJOR in \" + include_file_path)\r\n            break\r\n\r\n    if CUDNN_INCLUDE_VERSION is None:\r\n        pass\r\n\r\n    # Check for standalone cuDNN libraries\r\n    if CUDNN_INCLUDE_DIR is not None:\r\n        cudnn_path = os.path.join(os.path.dirname(CUDNN_INCLUDE_DIR))\r\n        cudnn_lib_paths = lib_paths_from_base(cudnn_path)\r\n        lib_paths.extend(cudnn_lib_paths)\r\n\r\n    for path in lib_paths:\r\n        if path is None or not os.path.exists(path):\r\n            continue\r\n        if IS_WINDOWS:\r\n            library = os.path.join(path, 'cudnn.lib')\r\n            if os.path.exists(library):\r\n[Truncated]\n                search_name = 'libcudnn*' + str(CUDNN_INCLUDE_VERSION) + \"*\"\r\n            libraries = sorted(glob.glob(os.path.join(path, search_name)))\r\n            if libraries:\r\n                CUDNN_LIBRARY = libraries[0]\r\n                CUDNN_LIB_DIR = path\r\n                break\r\n    # Specifying the library directly will overwrite the lib directory\r\n    library = os.getenv('CUDNN_LIBRARY')\r\n    if library is not None and os.path.exists(library):\r\n        CUDNN_LIBRARY = library\r\n        CUDNN_LIB_DIR = os.path.dirname(CUDNN_LIBRARY)\r\n\r\n    if not all([CUDNN_LIBRARY, CUDNN_LIB_DIR, CUDNN_INCLUDE_DIR]):\r\n        CUDNN_LIBRARY = CUDNN_LIB_DIR = CUDNN_INCLUDE_DIR = None\r\n    else:\r\n        real_cudnn_library = os.path.realpath(CUDNN_LIBRARY)\r\n        real_cudnn_lib_dir = os.path.realpath(CUDNN_LIB_DIR)\r\n        assert os.path.dirname(real_cudnn_library) == real_cudnn_lib_dir, (\r\n            'cudnn library and lib_dir must agree')\r\n        USE_CUDNN = True\n<issue_comment>username_1: Please remove `CUDNN_INCLUDE_DIR` and `CUDNN_LIBRARY` by using the following commands.\r\n```cmd\r\nset CUDNN_INCLUDE_DIR=\r\nset CUDNN_LIBRARY=\r\n```\n<issue_comment>username_0: It works! Thanks a lot!<issue_closed>"}
{"repo": "pytorch/pytorch", "issue_id": 391131022, "issue_number": 15222, "timestamp": "2018-12-19 15:03:34+00:00", "text": "It works! Thanks a lot!", "context": "<issue_start><issue_comment>Title: Can't open x64.obj while installing pytorch with win10 \nusername_0: \u5df2\u5b8c\u6210\u751f\u6210\u9879\u76ee\u201cE:\\SOFTWARE\\pytorch\\build\\caffe2\\torch\\lib\\libshm_windows\\shm.vcxproj\u201d(\u9ed8\u8ba4\u76ee\u6807)\u7684\u64cd\u4f5c\u3002\r\n\r\n\u5df2\u5b8c\u6210\u751f\u6210\u9879\u76ee\u201cE:\\SOFTWARE\\pytorch\\build\\ALL_BUILD.vcxproj\u201d(\u9ed8\u8ba4\u76ee\u6807)\u7684\u64cd\u4f5c - \u5931\u8d25\u3002\r\n\r\n\u5df2\u5b8c\u6210\u751f\u6210\u9879\u76ee\u201cE:\\SOFTWARE\\pytorch\\build\\INSTALL.vcxproj\u201d(\u9ed8\u8ba4\u76ee\u6807)\u7684\u64cd\u4f5c - \u5931\u8d25\u3002\r\n\r\n\r\n\u751f\u6210\u5931\u8d25\u3002\r\n\r\n\u201cE:\\SOFTWARE\\pytorch\\build\\INSTALL.vcxproj\u201d(\u9ed8\u8ba4\u76ee\u6807) (1) ->\r\n\u201cE:\\SOFTWARE\\pytorch\\build\\ALL_BUILD.vcxproj\u201d(\u9ed8\u8ba4\u76ee\u6807) (3) ->\r\n\u201cE:\\SOFTWARE\\pytorch\\build\\caffe2\\AlgorithmsTest.vcxproj\u201d(\u9ed8\u8ba4\u76ee\u6807) (4) ->\r\n\u201cE:\\SOFTWARE\\pytorch\\build\\caffe2\\caffe2_gpu.vcxproj\u201d(\u9ed8\u8ba4\u76ee\u6807) (22) ->\r\n(Link \u76ee\u6807) ->\r\n  C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\lib\\x64.obj : fatal error LNK1136: \u65e0\u6548\u6216\u635f\u574f\u7684\u6587\u4ef6 [E:\\SOFTWARE\\pyt\r\norch\\build\\caffe2\\caffe2_gpu.vcxproj]\r\n\r\n    0 \u4e2a\u8b66\u544a\r\n    1 \u4e2a\u9519\u8bef\r\n\r\n\u5df2\u7528\u65f6\u95f4 00:00:16.23\r\n\r\nE:\\SOFTWARE\\pytorch\\build>IF ERRORLEVEL 1 exit 1\r\nFailed to run 'tools\\build_pytorch_libs.bat --use-cuda --use-fbgemm --use-nnpack --use-mkldnn --use-qnnpack caffe2'\r\n\r\npytorch version\uff1astable\uff081.0\uff09\r\nos\uff1aWindows 10 Pro Version 1803\r\npython version:3.7\r\nCUDA:10\r\ncudnn:7.4.1\r\nVisual Studio version: Microsoft Visual Studio 2017 Community Edition\r\n\r\nCan anyone help me solve this problem? Thanks !\n<issue_comment>username_1: Please provide the full log.\n<issue_comment>username_0: the full log\uff1a\r\n\r\nBuilding wheel torch-1.0.0a0+df61437\r\nrunning install\r\nsetup.py::run()\r\nrunning build_deps\r\nsetup.py::build_deps::run()\r\n\r\nE:\\SOFTWARE\\pytorch>cd \"E:\\SOFTWARE\\pytorch\\tools\\\\..\"\r\n\r\nE:\\SOFTWARE\\pytorch>set PATH=\\bin;E:\\SOFTWARE\\Anaconda\\DLLs;E:\\SOFTWARE\\Visual Studio\\VC\\Tools\\MSVC\\14.11.25503\\bin\\HostX64\\x64;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\VC\\VCPackages;C:\\Program Files (x86)\\Microsoft SDKs\\TypeScript\\3.1;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\TestWindow;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\TeamFoundation\\Team Explorer;E:\\SOFTWARE\\Visual Studio\\MSBuild\\15.0\\bin\\Roslyn;E:\\SOFTWARE\\Visual Studio\\Team Tools\\Performance Tools\\x64;E:\\SOFTWARE\\Visual Studio\\Team Tools\\Performance Tools;E:\\SOFTWARE\\Visual Studio\\share\\Common\\VSPerfCollectionTools\\\\x64;E:\\SOFTWARE\\Visual Studio\\share\\Common\\VSPerfCollectionTools\\;C:\\Program Files (x86)\\Microsoft SDKs\\Windows\\v10.0A\\bin\\NETFX 4.6.1 Tools\\x64\\;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\10.0.17763.0\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x64;E:\\SOFTWARE\\Visual Studio\\\\MSBuild\\15.0\\bin;C:\\WINDOWS\\Microsoft.NET\\Framework64\\v4.0.30319;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\;E:\\SOFTWARE\\Visual Studio\\Common7\\Tools\\;E:\\SOFTWARE\\Anaconda;E:\\SOFTWARE\\Anaconda\\Library\\mingw-w64\\bin;E:\\SOFTWARE\\Anaconda\\Library\\usr\\bin;E:\\SOFTWARE\\Anaconda\\Library\\bin;E:\\SOFTWARE\\Anaconda\\Scripts;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\bin;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\libnvvp;C:\\WINDOWS\\system32;C:\\WINDOWS;C:\\WINDOWS\\System32\\Wbem;C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0\\;C:\\WINDOWS\\System32\\OpenSSH\\;E:\\\u8f6f\u4ef6\\Git\\cmd;E:\\SOFTWARE\\cmake\\bin;C:\\Program Files (x86)\\Windows Kits\\8.1\\Windows Performance Toolkit\\;C:\\Program Files (x86)\\NVIDIA Corporation\\PhysX\\Common;C:\\Program Files\\dotnet\\;C:\\Program Files\\NVIDIA Corporation\\NVIDIA NvDLISR;C:\\Windows\\Microsoft.NET\\Framework\\v4.0.30319;C:\\Windows\\Microsoft.NET\\Framework64\\v4.0.30319;C:\\Users\\11838\\AppData\\Local\\Microsoft\\WindowsApps;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\CMake\\bin;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\Ninja\r\n\r\nE:\\SOFTWARE\\pytorch>set BASE_DIR=E:/SOFTWARE/pytorch\r\n\r\nE:\\SOFTWARE\\pytorch>set TORCH_LIB_DIR=E:/SOFTWARE/pytorch/torch/lib\r\n\r\nE:\\SOFTWARE\\pytorch>set THIRD_PARTY_DIR=E:/SOFTWARE/pytorch/third_party\r\n\r\nE:\\SOFTWARE\\pytorch>set BASIC_C_FLAGS=\r\n\r\nE:\\SOFTWARE\\pytorch>set BASIC_CUDA_FLAGS=\r\n\r\nE:\\SOFTWARE\\pytorch>IF NOT DEFINED INSTALL_DIR (set \"INSTALL_DIR=E:/SOFTWARE/pytorch/torch/lib/tmp_install\" )  ELSE (set \"INSTALL_DIR=\\=/\" )\r\n\r\nE:\\SOFTWARE\\pytorch>set LDFLAGS=/LIBPATH:E:/SOFTWARE/pytorch/torch/lib/tmp_install/lib\r\n\r\nE:\\SOFTWARE\\pytorch>set C_FLAGS= /D_WIN32 /Z7 /EHa /DNOMINMAX\r\n\r\nE:\\SOFTWARE\\pytorch>set LINK_FLAGS=/DEBUG:FULL\r\n\r\nE:\\SOFTWARE\\pytorch>if not exist torch\\lib\\tmp_install mkdir torch\\lib\\tmp_install\r\n\r\nE:\\SOFTWARE\\pytorch>set /a USE_CUDA=0\r\n\r\nE:\\SOFTWARE\\pytorch>set /a USE_FBGEMM=1\r\n\r\nE:\\SOFTWARE\\pytorch>set /a USE_ROCM=0\r\n\r\nE:\\SOFTWARE\\pytorch>set /a USE_NNPACK=0\r\n\r\nE:\\SOFTWARE\\pytorch>set /a USE_QNNPACK=0\r\n\r\nE:\\SOFTWARE\\pytorch>set /a USE_GLOO_IBVERBS=0\r\n\r\nE:\\SOFTWARE\\pytorch>set /a USE_MKLDNN=0\r\n\r\nE:\\SOFTWARE\\pytorch>set _BUILD_ARGS=\r\n\r\nE:\\SOFTWARE\\pytorch>if \"--use-cuda\" == \"\" (goto :process_args_exit )\r\n\r\nE:\\SOFTWARE\\pytorch>if \"--use-cuda\" == \"--use-cuda\" (\r\nset /a USE_CUDA=1\r\n goto :process_args_processed\r\n)\r\n\r\nE:\\SOFTWARE\\pytorch>shift\r\n\r\nE:\\SOFTWARE\\pytorch>goto :process_args\r\n\r\nE:\\SOFTWARE\\pytorch>if \"--use-fbgemm\" == \"\" (goto :process_args_exit )\r\n\r\nE:\\SOFTWARE\\pytorch>if \"--use-fbgemm\" == \"--use-cuda\" (\r\nset /a USE_CUDA=1\r\n goto :process_args_processed\r\n)\r\n\r\nE:\\SOFTWARE\\pytorch>if \"--use-fbgemm\" == \"--use-fbgemm\" (\r\nset /a USE_FBGEMM=1\r\n goto :process_args_processed\r\n)\r\n\r\nE:\\SOFTWARE\\pytorch>shift\r\n\r\nE:\\SOFTWARE\\pytorch>goto :process_args\r\n\r\nE:\\SOFTWARE\\pytorch>if \"--use-nnpack\" == \"\" (goto :process_args_exit )\r\n\r\nE:\\SOFTWARE\\pytorch>if \"--use-nnpack\" == \"--use-cuda\" (\r\nset /a USE_CUDA=1\r\n goto :process_args_processed\r\n[Truncated]\nDone Building Project \"E:\\SOFTWARE\\pytorch\\build\\INSTALL.vcxproj\" (default targets) -- FAILED.\r\n\r\n\r\nBuild FAILED.\r\n\r\n\"E:\\SOFTWARE\\pytorch\\build\\INSTALL.vcxproj\" (default target) (1) ->\r\n\"E:\\SOFTWARE\\pytorch\\build\\ALL_BUILD.vcxproj\" (default target) (3) ->\r\n\"E:\\SOFTWARE\\pytorch\\build\\caffe2\\AlgorithmsTest.vcxproj\" (default target) (4) ->\r\n\"E:\\SOFTWARE\\pytorch\\build\\caffe2\\caffe2_gpu.vcxproj\" (default target) (22) ->\r\n(Link target) ->\r\n  LINK : fatal error LNK1181: \u65e0\u6cd5\u6253\u5f00\u8f93\u5165\u6587\u4ef6\u201cC:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\lib\\x64.obj\u201d [E:\\SOFTWA\r\nRE\\pytorch\\build\\caffe2\\caffe2_gpu.vcxproj]\r\n\r\n    0 Warning(s)\r\n    1 Error(s)\r\n\r\nTime Elapsed 00:00:22.22\r\n\r\nE:\\SOFTWARE\\pytorch\\build>IF ERRORLEVEL 1 exit 1\r\nFailed to run 'tools\\build_pytorch_libs.bat --use-cuda --use-fbgemm --use-nnpack --use-mkldnn --use-qnnpack caffe2'\n<issue_comment>username_1: The error is caused by the misconfiguration of the flag `CUDNN_LIBRARY`. Could you please run `tools/setup_helpers/cudnn.py` and see which code path it goes through?\n<issue_comment>username_0: My environmental variables is :\r\nCUDA_PATH=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\r\nCUDA_PATH_V10_0=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\r\nCUDNN_INCLUDE_DIR=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\include\r\nCUDNN_LIBRARY=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\lib\\x64\r\nNVCUDASAMPLES10_0_ROOT=C:\\ProgramData\\NVIDIA Corporation\\CUDA Samples\\v10.0\r\nNVCUDASAMPLES_ROOT=C:\\ProgramData\\NVIDIA Corporation\\CUDA Samples\\v10.0\r\nNVTOOLSEXT_PATH=C:\\Program Files\\NVIDIA Corporation\\NvToolsExt\\\r\nPath=E:\\SOFTWARE\\Visual Studio\\VC\\Tools\\MSVC\\14.11.25503\\bin\\HostX64\\x64;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\VC\\VCPackages;C:\\Program Files (x86)\\Microsoft SDKs\\TypeScript\\3.1;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\TestWindow;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\TeamFoundation\\Team Explorer;E:\\SOFTWARE\\Visual Studio\\MSBuild\\15.0\\bin\\Roslyn;E:\\SOFTWARE\\Visual Studio\\Team Tools\\Performance Tools\\x64;E:\\SOFTWARE\\Visual Studio\\Team Tools\\Performance Tools;E:\\SOFTWARE\\Visual Studio\\share\\Common\\VSPerfCollectionTools\\\\x64;E:\\SOFTWARE\\Visual Studio\\share\\Common\\VSPerfCollectionTools\\;C:\\Program Files (x86)\\Microsoft SDKs\\Windows\\v10.0A\\bin\\NETFX 4.6.1 Tools\\x64\\;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\10.0.17763.0\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x64;E:\\SOFTWARE\\Visual Studio\\\\MSBuild\\15.0\\bin;C:\\WINDOWS\\Microsoft.NET\\Framework64\\v4.0.30319;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\;E:\\SOFTWARE\\Visual Studio\\Common7\\Tools\\;E:\\SOFTWARE\\Anaconda;E:\\SOFTWARE\\Anaconda\\Library\\mingw-w64\\bin;E:\\SOFTWARE\\Anaconda\\Library\\usr\\bin;E:\\SOFTWARE\\Anaconda\\Library\\bin;E:\\SOFTWARE\\Anaconda\\Scripts;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\bin;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\libnvvp;C:\\WINDOWS\\system32;C:\\WINDOWS;C:\\WINDOWS\\System32\\Wbem;C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0\\;C:\\WINDOWS\\System32\\OpenSSH\\;E:\\\u8f6f\u4ef6\\Git\\cmd;E:\\SOFTWARE\\cmake\\bin;C:\\Program Files (x86)\\Windows Kits\\8.1\\Windows Performance Toolkit\\;C:\\Program Files (x86)\\NVIDIA Corporation\\PhysX\\Common;C:\\Program Files\\dotnet\\;C:\\Program Files\\NVIDIA Corporation\\NVIDIA NvDLISR;C:\\Windows\\Microsoft.NET\\Framework\\v4.0.30319;C:\\Windows\\Microsoft.NET\\Framework64\\v4.0.30319;C:\\Users\\11838\\AppData\\Local\\Microsoft\\WindowsApps;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\CMake\\bin;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\Ninja\r\n\r\nAND the cudnn.py shows:\r\nimport os\r\nimport glob\r\n\r\nfrom .env import IS_WINDOWS, IS_CONDA, CONDA_DIR, check_negative_env_flag, gather_paths, lib_paths_from_base\r\nfrom .cuda import USE_CUDA, CUDA_HOME\r\n\r\n\r\nUSE_CUDNN = False\r\nCUDNN_LIB_DIR = None\r\nCUDNN_INCLUDE_DIR = None\r\nCUDNN_LIBRARY = None\r\nWITH_STATIC_CUDNN = os.getenv(\"USE_STATIC_CUDNN\")\r\n\r\nif USE_CUDA and not check_negative_env_flag('USE_CUDNN'):\r\n    lib_paths = list(filter(bool, [\r\n        os.getenv('CUDNN_LIB_DIR')\r\n    ] + lib_paths_from_base(CUDA_HOME) + [\r\n        '/usr/lib/x86_64-linux-gnu/',\r\n        '/usr/lib/powerpc64le-linux-gnu/',\r\n        '/usr/lib/aarch64-linux-gnu/',\r\n    ] + gather_paths([\r\n        'LIBRARY_PATH',\r\n    ]) + gather_paths([\r\n        'LD_LIBRARY_PATH',\r\n    ])))\r\n    include_paths = list(filter(bool, [\r\n        os.getenv('CUDNN_INCLUDE_DIR'),\r\n        os.path.join(CUDA_HOME, 'include'),\r\n        '/usr/include/',\r\n    ] + gather_paths([\r\n        'CPATH',\r\n        'C_INCLUDE_PATH',\r\n        'CPLUS_INCLUDE_PATH',\r\n    ])))\r\n    # Add CUDA related dirs to candidate list\r\n    if IS_CONDA:\r\n        lib_paths.append(os.path.join(CONDA_DIR, 'lib'))\r\n        include_paths.append(os.path.join(CONDA_DIR, 'include'))\r\n    for path in include_paths:\r\n        if path is None or not os.path.exists(path):\r\n            continue\r\n        include_file_path = os.path.join(path, 'cudnn.h')\r\n        CUDNN_INCLUDE_VERSION = None\r\n        if os.path.exists(include_file_path):\r\n            CUDNN_INCLUDE_DIR = path\r\n            with open(include_file_path) as f:\r\n                for line in f:\r\n                    if \"#define CUDNN_MAJOR\" in line:\r\n                        CUDNN_INCLUDE_VERSION = int(line.split()[-1])\r\n                        break\r\n            if CUDNN_INCLUDE_VERSION is None:\r\n                raise AssertionError(\"Could not find #define CUDNN_MAJOR in \" + include_file_path)\r\n            break\r\n\r\n    if CUDNN_INCLUDE_VERSION is None:\r\n        pass\r\n\r\n    # Check for standalone cuDNN libraries\r\n    if CUDNN_INCLUDE_DIR is not None:\r\n        cudnn_path = os.path.join(os.path.dirname(CUDNN_INCLUDE_DIR))\r\n        cudnn_lib_paths = lib_paths_from_base(cudnn_path)\r\n        lib_paths.extend(cudnn_lib_paths)\r\n\r\n    for path in lib_paths:\r\n        if path is None or not os.path.exists(path):\r\n            continue\r\n        if IS_WINDOWS:\r\n            library = os.path.join(path, 'cudnn.lib')\r\n            if os.path.exists(library):\r\n[Truncated]\n                search_name = 'libcudnn*' + str(CUDNN_INCLUDE_VERSION) + \"*\"\r\n            libraries = sorted(glob.glob(os.path.join(path, search_name)))\r\n            if libraries:\r\n                CUDNN_LIBRARY = libraries[0]\r\n                CUDNN_LIB_DIR = path\r\n                break\r\n    # Specifying the library directly will overwrite the lib directory\r\n    library = os.getenv('CUDNN_LIBRARY')\r\n    if library is not None and os.path.exists(library):\r\n        CUDNN_LIBRARY = library\r\n        CUDNN_LIB_DIR = os.path.dirname(CUDNN_LIBRARY)\r\n\r\n    if not all([CUDNN_LIBRARY, CUDNN_LIB_DIR, CUDNN_INCLUDE_DIR]):\r\n        CUDNN_LIBRARY = CUDNN_LIB_DIR = CUDNN_INCLUDE_DIR = None\r\n    else:\r\n        real_cudnn_library = os.path.realpath(CUDNN_LIBRARY)\r\n        real_cudnn_lib_dir = os.path.realpath(CUDNN_LIB_DIR)\r\n        assert os.path.dirname(real_cudnn_library) == real_cudnn_lib_dir, (\r\n            'cudnn library and lib_dir must agree')\r\n        USE_CUDNN = True\n<issue_comment>username_1: Please remove `CUDNN_INCLUDE_DIR` and `CUDNN_LIBRARY` by using the following commands.\r\n```cmd\r\nset CUDNN_INCLUDE_DIR=\r\nset CUDNN_LIBRARY=\r\n```\n<issue_comment>username_0: It works! Thanks a lot!<issue_closed>"}
{"repo": "pytorch/pytorch", "issue_id": 391131022, "issue_number": 15222, "timestamp": "2018-12-20 14:49:12+00:00", "text": "", "context": "<issue_start><issue_comment>Title: Can't open x64.obj while installing pytorch with win10 \nusername_0: \u5df2\u5b8c\u6210\u751f\u6210\u9879\u76ee\u201cE:\\SOFTWARE\\pytorch\\build\\caffe2\\torch\\lib\\libshm_windows\\shm.vcxproj\u201d(\u9ed8\u8ba4\u76ee\u6807)\u7684\u64cd\u4f5c\u3002\r\n\r\n\u5df2\u5b8c\u6210\u751f\u6210\u9879\u76ee\u201cE:\\SOFTWARE\\pytorch\\build\\ALL_BUILD.vcxproj\u201d(\u9ed8\u8ba4\u76ee\u6807)\u7684\u64cd\u4f5c - \u5931\u8d25\u3002\r\n\r\n\u5df2\u5b8c\u6210\u751f\u6210\u9879\u76ee\u201cE:\\SOFTWARE\\pytorch\\build\\INSTALL.vcxproj\u201d(\u9ed8\u8ba4\u76ee\u6807)\u7684\u64cd\u4f5c - \u5931\u8d25\u3002\r\n\r\n\r\n\u751f\u6210\u5931\u8d25\u3002\r\n\r\n\u201cE:\\SOFTWARE\\pytorch\\build\\INSTALL.vcxproj\u201d(\u9ed8\u8ba4\u76ee\u6807) (1) ->\r\n\u201cE:\\SOFTWARE\\pytorch\\build\\ALL_BUILD.vcxproj\u201d(\u9ed8\u8ba4\u76ee\u6807) (3) ->\r\n\u201cE:\\SOFTWARE\\pytorch\\build\\caffe2\\AlgorithmsTest.vcxproj\u201d(\u9ed8\u8ba4\u76ee\u6807) (4) ->\r\n\u201cE:\\SOFTWARE\\pytorch\\build\\caffe2\\caffe2_gpu.vcxproj\u201d(\u9ed8\u8ba4\u76ee\u6807) (22) ->\r\n(Link \u76ee\u6807) ->\r\n  C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\lib\\x64.obj : fatal error LNK1136: \u65e0\u6548\u6216\u635f\u574f\u7684\u6587\u4ef6 [E:\\SOFTWARE\\pyt\r\norch\\build\\caffe2\\caffe2_gpu.vcxproj]\r\n\r\n    0 \u4e2a\u8b66\u544a\r\n    1 \u4e2a\u9519\u8bef\r\n\r\n\u5df2\u7528\u65f6\u95f4 00:00:16.23\r\n\r\nE:\\SOFTWARE\\pytorch\\build>IF ERRORLEVEL 1 exit 1\r\nFailed to run 'tools\\build_pytorch_libs.bat --use-cuda --use-fbgemm --use-nnpack --use-mkldnn --use-qnnpack caffe2'\r\n\r\npytorch version\uff1astable\uff081.0\uff09\r\nos\uff1aWindows 10 Pro Version 1803\r\npython version:3.7\r\nCUDA:10\r\ncudnn:7.4.1\r\nVisual Studio version: Microsoft Visual Studio 2017 Community Edition\r\n\r\nCan anyone help me solve this problem? Thanks !\n<issue_comment>username_1: Please provide the full log.\n<issue_comment>username_0: the full log\uff1a\r\n\r\nBuilding wheel torch-1.0.0a0+df61437\r\nrunning install\r\nsetup.py::run()\r\nrunning build_deps\r\nsetup.py::build_deps::run()\r\n\r\nE:\\SOFTWARE\\pytorch>cd \"E:\\SOFTWARE\\pytorch\\tools\\\\..\"\r\n\r\nE:\\SOFTWARE\\pytorch>set PATH=\\bin;E:\\SOFTWARE\\Anaconda\\DLLs;E:\\SOFTWARE\\Visual Studio\\VC\\Tools\\MSVC\\14.11.25503\\bin\\HostX64\\x64;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\VC\\VCPackages;C:\\Program Files (x86)\\Microsoft SDKs\\TypeScript\\3.1;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\TestWindow;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\TeamFoundation\\Team Explorer;E:\\SOFTWARE\\Visual Studio\\MSBuild\\15.0\\bin\\Roslyn;E:\\SOFTWARE\\Visual Studio\\Team Tools\\Performance Tools\\x64;E:\\SOFTWARE\\Visual Studio\\Team Tools\\Performance Tools;E:\\SOFTWARE\\Visual Studio\\share\\Common\\VSPerfCollectionTools\\\\x64;E:\\SOFTWARE\\Visual Studio\\share\\Common\\VSPerfCollectionTools\\;C:\\Program Files (x86)\\Microsoft SDKs\\Windows\\v10.0A\\bin\\NETFX 4.6.1 Tools\\x64\\;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\10.0.17763.0\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x64;E:\\SOFTWARE\\Visual Studio\\\\MSBuild\\15.0\\bin;C:\\WINDOWS\\Microsoft.NET\\Framework64\\v4.0.30319;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\;E:\\SOFTWARE\\Visual Studio\\Common7\\Tools\\;E:\\SOFTWARE\\Anaconda;E:\\SOFTWARE\\Anaconda\\Library\\mingw-w64\\bin;E:\\SOFTWARE\\Anaconda\\Library\\usr\\bin;E:\\SOFTWARE\\Anaconda\\Library\\bin;E:\\SOFTWARE\\Anaconda\\Scripts;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\bin;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\libnvvp;C:\\WINDOWS\\system32;C:\\WINDOWS;C:\\WINDOWS\\System32\\Wbem;C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0\\;C:\\WINDOWS\\System32\\OpenSSH\\;E:\\\u8f6f\u4ef6\\Git\\cmd;E:\\SOFTWARE\\cmake\\bin;C:\\Program Files (x86)\\Windows Kits\\8.1\\Windows Performance Toolkit\\;C:\\Program Files (x86)\\NVIDIA Corporation\\PhysX\\Common;C:\\Program Files\\dotnet\\;C:\\Program Files\\NVIDIA Corporation\\NVIDIA NvDLISR;C:\\Windows\\Microsoft.NET\\Framework\\v4.0.30319;C:\\Windows\\Microsoft.NET\\Framework64\\v4.0.30319;C:\\Users\\11838\\AppData\\Local\\Microsoft\\WindowsApps;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\CMake\\bin;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\Ninja\r\n\r\nE:\\SOFTWARE\\pytorch>set BASE_DIR=E:/SOFTWARE/pytorch\r\n\r\nE:\\SOFTWARE\\pytorch>set TORCH_LIB_DIR=E:/SOFTWARE/pytorch/torch/lib\r\n\r\nE:\\SOFTWARE\\pytorch>set THIRD_PARTY_DIR=E:/SOFTWARE/pytorch/third_party\r\n\r\nE:\\SOFTWARE\\pytorch>set BASIC_C_FLAGS=\r\n\r\nE:\\SOFTWARE\\pytorch>set BASIC_CUDA_FLAGS=\r\n\r\nE:\\SOFTWARE\\pytorch>IF NOT DEFINED INSTALL_DIR (set \"INSTALL_DIR=E:/SOFTWARE/pytorch/torch/lib/tmp_install\" )  ELSE (set \"INSTALL_DIR=\\=/\" )\r\n\r\nE:\\SOFTWARE\\pytorch>set LDFLAGS=/LIBPATH:E:/SOFTWARE/pytorch/torch/lib/tmp_install/lib\r\n\r\nE:\\SOFTWARE\\pytorch>set C_FLAGS= /D_WIN32 /Z7 /EHa /DNOMINMAX\r\n\r\nE:\\SOFTWARE\\pytorch>set LINK_FLAGS=/DEBUG:FULL\r\n\r\nE:\\SOFTWARE\\pytorch>if not exist torch\\lib\\tmp_install mkdir torch\\lib\\tmp_install\r\n\r\nE:\\SOFTWARE\\pytorch>set /a USE_CUDA=0\r\n\r\nE:\\SOFTWARE\\pytorch>set /a USE_FBGEMM=1\r\n\r\nE:\\SOFTWARE\\pytorch>set /a USE_ROCM=0\r\n\r\nE:\\SOFTWARE\\pytorch>set /a USE_NNPACK=0\r\n\r\nE:\\SOFTWARE\\pytorch>set /a USE_QNNPACK=0\r\n\r\nE:\\SOFTWARE\\pytorch>set /a USE_GLOO_IBVERBS=0\r\n\r\nE:\\SOFTWARE\\pytorch>set /a USE_MKLDNN=0\r\n\r\nE:\\SOFTWARE\\pytorch>set _BUILD_ARGS=\r\n\r\nE:\\SOFTWARE\\pytorch>if \"--use-cuda\" == \"\" (goto :process_args_exit )\r\n\r\nE:\\SOFTWARE\\pytorch>if \"--use-cuda\" == \"--use-cuda\" (\r\nset /a USE_CUDA=1\r\n goto :process_args_processed\r\n)\r\n\r\nE:\\SOFTWARE\\pytorch>shift\r\n\r\nE:\\SOFTWARE\\pytorch>goto :process_args\r\n\r\nE:\\SOFTWARE\\pytorch>if \"--use-fbgemm\" == \"\" (goto :process_args_exit )\r\n\r\nE:\\SOFTWARE\\pytorch>if \"--use-fbgemm\" == \"--use-cuda\" (\r\nset /a USE_CUDA=1\r\n goto :process_args_processed\r\n)\r\n\r\nE:\\SOFTWARE\\pytorch>if \"--use-fbgemm\" == \"--use-fbgemm\" (\r\nset /a USE_FBGEMM=1\r\n goto :process_args_processed\r\n)\r\n\r\nE:\\SOFTWARE\\pytorch>shift\r\n\r\nE:\\SOFTWARE\\pytorch>goto :process_args\r\n\r\nE:\\SOFTWARE\\pytorch>if \"--use-nnpack\" == \"\" (goto :process_args_exit )\r\n\r\nE:\\SOFTWARE\\pytorch>if \"--use-nnpack\" == \"--use-cuda\" (\r\nset /a USE_CUDA=1\r\n goto :process_args_processed\r\n[Truncated]\nDone Building Project \"E:\\SOFTWARE\\pytorch\\build\\INSTALL.vcxproj\" (default targets) -- FAILED.\r\n\r\n\r\nBuild FAILED.\r\n\r\n\"E:\\SOFTWARE\\pytorch\\build\\INSTALL.vcxproj\" (default target) (1) ->\r\n\"E:\\SOFTWARE\\pytorch\\build\\ALL_BUILD.vcxproj\" (default target) (3) ->\r\n\"E:\\SOFTWARE\\pytorch\\build\\caffe2\\AlgorithmsTest.vcxproj\" (default target) (4) ->\r\n\"E:\\SOFTWARE\\pytorch\\build\\caffe2\\caffe2_gpu.vcxproj\" (default target) (22) ->\r\n(Link target) ->\r\n  LINK : fatal error LNK1181: \u65e0\u6cd5\u6253\u5f00\u8f93\u5165\u6587\u4ef6\u201cC:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\lib\\x64.obj\u201d [E:\\SOFTWA\r\nRE\\pytorch\\build\\caffe2\\caffe2_gpu.vcxproj]\r\n\r\n    0 Warning(s)\r\n    1 Error(s)\r\n\r\nTime Elapsed 00:00:22.22\r\n\r\nE:\\SOFTWARE\\pytorch\\build>IF ERRORLEVEL 1 exit 1\r\nFailed to run 'tools\\build_pytorch_libs.bat --use-cuda --use-fbgemm --use-nnpack --use-mkldnn --use-qnnpack caffe2'\n<issue_comment>username_1: The error is caused by the misconfiguration of the flag `CUDNN_LIBRARY`. Could you please run `tools/setup_helpers/cudnn.py` and see which code path it goes through?\n<issue_comment>username_0: My environmental variables is :\r\nCUDA_PATH=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\r\nCUDA_PATH_V10_0=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\r\nCUDNN_INCLUDE_DIR=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\include\r\nCUDNN_LIBRARY=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\lib\\x64\r\nNVCUDASAMPLES10_0_ROOT=C:\\ProgramData\\NVIDIA Corporation\\CUDA Samples\\v10.0\r\nNVCUDASAMPLES_ROOT=C:\\ProgramData\\NVIDIA Corporation\\CUDA Samples\\v10.0\r\nNVTOOLSEXT_PATH=C:\\Program Files\\NVIDIA Corporation\\NvToolsExt\\\r\nPath=E:\\SOFTWARE\\Visual Studio\\VC\\Tools\\MSVC\\14.11.25503\\bin\\HostX64\\x64;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\VC\\VCPackages;C:\\Program Files (x86)\\Microsoft SDKs\\TypeScript\\3.1;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\TestWindow;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\TeamFoundation\\Team Explorer;E:\\SOFTWARE\\Visual Studio\\MSBuild\\15.0\\bin\\Roslyn;E:\\SOFTWARE\\Visual Studio\\Team Tools\\Performance Tools\\x64;E:\\SOFTWARE\\Visual Studio\\Team Tools\\Performance Tools;E:\\SOFTWARE\\Visual Studio\\share\\Common\\VSPerfCollectionTools\\\\x64;E:\\SOFTWARE\\Visual Studio\\share\\Common\\VSPerfCollectionTools\\;C:\\Program Files (x86)\\Microsoft SDKs\\Windows\\v10.0A\\bin\\NETFX 4.6.1 Tools\\x64\\;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\10.0.17763.0\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x64;E:\\SOFTWARE\\Visual Studio\\\\MSBuild\\15.0\\bin;C:\\WINDOWS\\Microsoft.NET\\Framework64\\v4.0.30319;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\;E:\\SOFTWARE\\Visual Studio\\Common7\\Tools\\;E:\\SOFTWARE\\Anaconda;E:\\SOFTWARE\\Anaconda\\Library\\mingw-w64\\bin;E:\\SOFTWARE\\Anaconda\\Library\\usr\\bin;E:\\SOFTWARE\\Anaconda\\Library\\bin;E:\\SOFTWARE\\Anaconda\\Scripts;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\bin;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\libnvvp;C:\\WINDOWS\\system32;C:\\WINDOWS;C:\\WINDOWS\\System32\\Wbem;C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0\\;C:\\WINDOWS\\System32\\OpenSSH\\;E:\\\u8f6f\u4ef6\\Git\\cmd;E:\\SOFTWARE\\cmake\\bin;C:\\Program Files (x86)\\Windows Kits\\8.1\\Windows Performance Toolkit\\;C:\\Program Files (x86)\\NVIDIA Corporation\\PhysX\\Common;C:\\Program Files\\dotnet\\;C:\\Program Files\\NVIDIA Corporation\\NVIDIA NvDLISR;C:\\Windows\\Microsoft.NET\\Framework\\v4.0.30319;C:\\Windows\\Microsoft.NET\\Framework64\\v4.0.30319;C:\\Users\\11838\\AppData\\Local\\Microsoft\\WindowsApps;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\CMake\\bin;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\Ninja\r\n\r\nAND the cudnn.py shows:\r\nimport os\r\nimport glob\r\n\r\nfrom .env import IS_WINDOWS, IS_CONDA, CONDA_DIR, check_negative_env_flag, gather_paths, lib_paths_from_base\r\nfrom .cuda import USE_CUDA, CUDA_HOME\r\n\r\n\r\nUSE_CUDNN = False\r\nCUDNN_LIB_DIR = None\r\nCUDNN_INCLUDE_DIR = None\r\nCUDNN_LIBRARY = None\r\nWITH_STATIC_CUDNN = os.getenv(\"USE_STATIC_CUDNN\")\r\n\r\nif USE_CUDA and not check_negative_env_flag('USE_CUDNN'):\r\n    lib_paths = list(filter(bool, [\r\n        os.getenv('CUDNN_LIB_DIR')\r\n    ] + lib_paths_from_base(CUDA_HOME) + [\r\n        '/usr/lib/x86_64-linux-gnu/',\r\n        '/usr/lib/powerpc64le-linux-gnu/',\r\n        '/usr/lib/aarch64-linux-gnu/',\r\n    ] + gather_paths([\r\n        'LIBRARY_PATH',\r\n    ]) + gather_paths([\r\n        'LD_LIBRARY_PATH',\r\n    ])))\r\n    include_paths = list(filter(bool, [\r\n        os.getenv('CUDNN_INCLUDE_DIR'),\r\n        os.path.join(CUDA_HOME, 'include'),\r\n        '/usr/include/',\r\n    ] + gather_paths([\r\n        'CPATH',\r\n        'C_INCLUDE_PATH',\r\n        'CPLUS_INCLUDE_PATH',\r\n    ])))\r\n    # Add CUDA related dirs to candidate list\r\n    if IS_CONDA:\r\n        lib_paths.append(os.path.join(CONDA_DIR, 'lib'))\r\n        include_paths.append(os.path.join(CONDA_DIR, 'include'))\r\n    for path in include_paths:\r\n        if path is None or not os.path.exists(path):\r\n            continue\r\n        include_file_path = os.path.join(path, 'cudnn.h')\r\n        CUDNN_INCLUDE_VERSION = None\r\n        if os.path.exists(include_file_path):\r\n            CUDNN_INCLUDE_DIR = path\r\n            with open(include_file_path) as f:\r\n                for line in f:\r\n                    if \"#define CUDNN_MAJOR\" in line:\r\n                        CUDNN_INCLUDE_VERSION = int(line.split()[-1])\r\n                        break\r\n            if CUDNN_INCLUDE_VERSION is None:\r\n                raise AssertionError(\"Could not find #define CUDNN_MAJOR in \" + include_file_path)\r\n            break\r\n\r\n    if CUDNN_INCLUDE_VERSION is None:\r\n        pass\r\n\r\n    # Check for standalone cuDNN libraries\r\n    if CUDNN_INCLUDE_DIR is not None:\r\n        cudnn_path = os.path.join(os.path.dirname(CUDNN_INCLUDE_DIR))\r\n        cudnn_lib_paths = lib_paths_from_base(cudnn_path)\r\n        lib_paths.extend(cudnn_lib_paths)\r\n\r\n    for path in lib_paths:\r\n        if path is None or not os.path.exists(path):\r\n            continue\r\n        if IS_WINDOWS:\r\n            library = os.path.join(path, 'cudnn.lib')\r\n            if os.path.exists(library):\r\n[Truncated]\n                search_name = 'libcudnn*' + str(CUDNN_INCLUDE_VERSION) + \"*\"\r\n            libraries = sorted(glob.glob(os.path.join(path, search_name)))\r\n            if libraries:\r\n                CUDNN_LIBRARY = libraries[0]\r\n                CUDNN_LIB_DIR = path\r\n                break\r\n    # Specifying the library directly will overwrite the lib directory\r\n    library = os.getenv('CUDNN_LIBRARY')\r\n    if library is not None and os.path.exists(library):\r\n        CUDNN_LIBRARY = library\r\n        CUDNN_LIB_DIR = os.path.dirname(CUDNN_LIBRARY)\r\n\r\n    if not all([CUDNN_LIBRARY, CUDNN_LIB_DIR, CUDNN_INCLUDE_DIR]):\r\n        CUDNN_LIBRARY = CUDNN_LIB_DIR = CUDNN_INCLUDE_DIR = None\r\n    else:\r\n        real_cudnn_library = os.path.realpath(CUDNN_LIBRARY)\r\n        real_cudnn_lib_dir = os.path.realpath(CUDNN_LIB_DIR)\r\n        assert os.path.dirname(real_cudnn_library) == real_cudnn_lib_dir, (\r\n            'cudnn library and lib_dir must agree')\r\n        USE_CUDNN = True\n<issue_comment>username_1: Please remove `CUDNN_INCLUDE_DIR` and `CUDNN_LIBRARY` by using the following commands.\r\n```cmd\r\nset CUDNN_INCLUDE_DIR=\r\nset CUDNN_LIBRARY=\r\n```\n<issue_comment>username_0: It works! Thanks a lot!<issue_closed>"}
{"repo": "pytorch/pytorch", "issue_id": 436372857, "issue_number": 19630, "timestamp": "2019-04-23T20:22:22Z", "text": "resolves #16158", "context": "<issue_start><issue_comment>Title: [WIP] UpSample GPU Porting \nusername_0: resolves #16158\n<issue_comment>username_1: Fewer errors than before, 4 instead of 8 with the same message:\r\n```\r\nRuntimeError: CUDA error: too many resources requested for launch\r\n```\r\nThis seems to be an existing problem: gh-8103.\r\n\r\n@username_2 could you have a look at this and tell us what you think?\n<issue_comment>username_2: Looking at it very briefly, the `Jetson TX` referenced in the issue only has 256 cores.\r\n\r\nSo while the issue looks the same, I'd probably not expect it on the CI platforms.  FWIW, some recent CI tests in other issues are green.\n<issue_comment>username_2: @pytorchbot retest this please.\n<issue_comment>username_1: same failures.\n<issue_comment>username_2: Is this rebased on the latest master (you can also ask the bot to rebase)?\n<issue_comment>username_0: I rebased that yesterday ... also needed to resolve conflicts because 7 days ago upsample files were changed.\n<issue_comment>username_0: @username_2 do you have any idea about what could be this problem? or a way to debug that?\r\nalso it seems some jobs is taking a lot of time to build, for example, for some job the estimate time is 19h .. not sure if it the regular estimate ..\n<issue_comment>username_2: @username_0 If you can't reproduce it at home it seems hard to debug other than reading at the diffs again.  I can take a look on Monday, it's getting a bit late here.\n<issue_comment>username_2: Also, has anyone found a way to show the actual hardware used on the CI in detail?\n<issue_comment>username_0: @username_2 I am working in parallel on a paperspace environment .. it tooks a lot of time it is running with 8 cpu cores.\n<issue_comment>username_2: But have you ever been able to reproduce this issue on paperspace?\n<issue_comment>username_1: I can reproduce it locally: Arch Linux, CUDA 10.0, RTX2070 GPU. Given the CI failures, I think it should be reproducible for multiple CUDA and GPU versions. I just haven't worked on any CUDA code before, and am short on time, so I'd rather not dig too deep.\n<issue_comment>username_0: not yet .. my last building was with a my previous commit ... I will work again in this task in some minutes :)\n<issue_comment>username_1: @username_0 your previous commit had the same failure though (except for the less clear exception message), and it seems quite reproducible. So I think you'll see it now.\n<issue_comment>username_2: Ah, thanks.  @username_0, then I guess just compiling with \"DEBUG=1\" and stepping through the offending code with gdb may be the fastest way.\n<issue_comment>username_0: @username_2 thanks! I will try that!\n<issue_comment>username_0: it seems just UpSampleBicubic2d is using upsample_get_value_bounded (https://github.com/pytorch/pytorch/pull/19630/files#diff-5092da792c30694ee4adf0d0ae2a37c6R171) and upsample_increment_value_bounded (https://github.com/pytorch/pytorch/pull/19630/files#diff-5092da792c30694ee4adf0d0ae2a37c6R191)\r\n\r\nmaybe the problem could be inside one of these functions ... maybe related to the order of indexes (x, y) ...\r\n\r\nbut the problem seems to be related to cuda block/threads ... so not sure if it is really related to these functions.\n<issue_comment>username_2: The new code uses far more registers than the existing one. I verified that the both versions actually call the offending test case with `1024` in `blockDim`.  So it's very likely a register issue.\r\n\r\n`Existing` uses `64` registers, which seems to be optimal or my card:\r\n\r\n```\r\nptxas info    : 77696 bytes gmem, 72 bytes cmem[3]\r\nptxas info    : Compiling entry function '_Z23bicubic_interp2d_kerneliddb15THCDeviceTensorIdLi4Ei16DefaultPtrTraitsES1_' for 'sm_61'\r\nptxas info    : Function properties for _Z23bicubic_interp2d_kerneliddb15THCDeviceTensorIdLi4Ei16DefaultPtrTraitsES1_\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 64 registers, 432 bytes cmem[0], 24 bytes cmem[2]\r\n```\r\n\r\n`New` uses `124` registers, which is too much for my card:\r\n\r\n```\r\nptxas info    : 77696 bytes gmem, 72 bytes cmem[3]\r\nptxas info    : Compiling entry function '_ZN2at6native76_GLOBAL__N__52_tmpxft_0000626d_00000000_6_UpSampleBicubic2d_cpp1_ii_b4c1e1f328upsample_bicubic2d_out_frameElddbNS_20PackedTensorAccessorIdLm4ENS_16DefaultPtrTraitsElEES4_' for 'sm_61'\r\nptxas info    : Function properties for _ZN2at6native76_GLOBAL__N__52_tmpxft_0000626d_00000000_6_UpSampleBicubic2d_cpp1_ii_b4c1e1f328upsample_bicubic2d_out_frameElddbNS_20PackedTensorAccessorIdLm4ENS_16DefaultPtrTraitsElEES4_\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 124 registers, 496 bytes cmem[0], 24 bytes cmem[2]\r\n```\r\n\r\nWith `__launch_bounds__(1024)`, the code again uses `64` registers.\r\n\r\nIf you use `C10_LAUNCH_BOUNDS_1(1024)` **for both kernels**, the tests pass here.\r\n\r\n\r\nNow *why* is the regcount higher in the new code? It could be `PackedTensorAccessor`, it could be the fact that many instances of `int` have been changed to `int64_t`. :)\r\n\r\nYou could experiment or just use the launch bounds.  Other code in `native/cuda` seems to use lower bounds for `blockDim`, too.  1024 seems to be an outlier.\n<issue_comment>username_0: @username_2 \r\n\r\nit seems it worked locally! thank you so much!  I really appreciate that!\n<issue_comment>username_0: thanks @username_1 and @username_2 for all the help!\r\n\r\n@username_3 it is done for review!\n<issue_comment>username_3: CircleCI runs on AWS, and the I'm fairly sure we've got one of the g3 types, either g3.4xlarge or g3.8xlarge\n<issue_comment>username_3: cc @ngimel, if you want to take a look at this (I'm planning to do a review too)\n<issue_comment>username_0: @username_4 could it be addressed in another PR?\n<issue_comment>username_4: @username_0 Sure, it's definitely better to rename both cpu & gpu impls together in a separate PR. Just want to make sure we have the correct one after porting.\n<issue_comment>username_3: Just as a clarification: I'm OK with having changes which are improvements over the old kernels come in latter patches.\n<issue_comment>username_3: I finished reviewing one of the kernels, and did a cursory scan of the others. In general the patch looks like it's in good shape. I'd suggest that we fix up the major correctness issues (PackedTensorAccessor ref, and int32 versus int64 indexing) and anything small that is easy to fix (for example, double zeroing the array), and leave other improvements for follow ups. Should be ready to land soon!\n<issue_comment>username_0: thanks so much @username_3!\r\nI will let you know when it is ready again for a new review! thanks!\n<issue_comment>username_0: @username_2 @username_3 \r\nnot sure but the errors on CI seems to be related to jenkins ... it seems all these jobs that failed (for building) ran by 01h 01min ... not sure if it is a coincidence ...\n<issue_comment>username_1: @username_0 indeed it looks like all jobs were aborted at the same time. no obvious issues in the build log related to your code. Comparing e.g. the `cuda9-cudnn7` build with a successful one from another PR, it takes 1hr 14min there and your build is about at the place where the other one is after an hour.\r\n\r\nI suggest to just push a new commit to rebuild. Probably a temporary CI hiccup.\n<issue_comment>username_3: I accidentally rebooted Jenkins yesterday which is the likely cause, my apologies.\r\n\r\n@pytorchbot retest this please\n<issue_comment>username_0: @username_3 @username_2 @username_1 \r\n\r\nall tests passed except `pr/caffe2-py2-cuda9.0-cudnn7-windows-build`:\r\n \r\n```\r\n14:43:49 Build timed out (after 180 minutes). Marking the build as failed.\r\n14:43:49 Build was aborted\r\n14:43:49 [BFA] Scanning build for known causes...\r\n14:43:49 [BFA] No failure causes found\r\n14:43:49 [BFA] Done. 0s\r\n14:43:49 Finished: FAILURE\r\n```\r\n\r\nnot sure if this timeout means that the code now is slower.\r\n\r\nwhat do you think? do you have any suggestion?\n<issue_comment>username_3: Sometimes the Windows build flakes out like that. It didn't timeout while running a relevant test, so I judge it to be not your problem.\n<issue_comment>username_3: The launch bounds logic is wrong, but I acknowledge that this is a big patch already; just fix it in a follow up. I am going to go ahead and land this.\n<issue_comment>username_0: sounds good @username_3 thanks!"}
{"repo": "pytorch/pytorch", "issue_id": 436372857, "issue_number": 19630, "timestamp": "2019-05-03 14:55:40+00:00", "text": "Fewer errors than before, 4 instead of 8 with the same message:\r\n```\r\nRuntimeError: CUDA error: too many resources requested for launch\r\n```\r\nThis seems to be an existing problem: gh-8103.\r\n\r\n@username_2 could you have a look at this and tell us what you think?", "context": "<issue_start><issue_comment>Title: [WIP] UpSample GPU Porting \nusername_0: resolves #16158\n<issue_comment>username_1: Fewer errors than before, 4 instead of 8 with the same message:\r\n```\r\nRuntimeError: CUDA error: too many resources requested for launch\r\n```\r\nThis seems to be an existing problem: gh-8103.\r\n\r\n@username_2 could you have a look at this and tell us what you think?\n<issue_comment>username_2: Looking at it very briefly, the `Jetson TX` referenced in the issue only has 256 cores.\r\n\r\nSo while the issue looks the same, I'd probably not expect it on the CI platforms.  FWIW, some recent CI tests in other issues are green.\n<issue_comment>username_2: @pytorchbot retest this please.\n<issue_comment>username_1: same failures.\n<issue_comment>username_2: Is this rebased on the latest master (you can also ask the bot to rebase)?\n<issue_comment>username_0: I rebased that yesterday ... also needed to resolve conflicts because 7 days ago upsample files were changed.\n<issue_comment>username_0: @username_2 do you have any idea about what could be this problem? or a way to debug that?\r\nalso it seems some jobs is taking a lot of time to build, for example, for some job the estimate time is 19h .. not sure if it the regular estimate ..\n<issue_comment>username_2: @username_0 If you can't reproduce it at home it seems hard to debug other than reading at the diffs again.  I can take a look on Monday, it's getting a bit late here.\n<issue_comment>username_2: Also, has anyone found a way to show the actual hardware used on the CI in detail?\n<issue_comment>username_0: @username_2 I am working in parallel on a paperspace environment .. it tooks a lot of time it is running with 8 cpu cores.\n<issue_comment>username_2: But have you ever been able to reproduce this issue on paperspace?\n<issue_comment>username_1: I can reproduce it locally: Arch Linux, CUDA 10.0, RTX2070 GPU. Given the CI failures, I think it should be reproducible for multiple CUDA and GPU versions. I just haven't worked on any CUDA code before, and am short on time, so I'd rather not dig too deep.\n<issue_comment>username_0: not yet .. my last building was with a my previous commit ... I will work again in this task in some minutes :)\n<issue_comment>username_1: @username_0 your previous commit had the same failure though (except for the less clear exception message), and it seems quite reproducible. So I think you'll see it now.\n<issue_comment>username_2: Ah, thanks.  @username_0, then I guess just compiling with \"DEBUG=1\" and stepping through the offending code with gdb may be the fastest way.\n<issue_comment>username_0: @username_2 thanks! I will try that!\n<issue_comment>username_0: it seems just UpSampleBicubic2d is using upsample_get_value_bounded (https://github.com/pytorch/pytorch/pull/19630/files#diff-5092da792c30694ee4adf0d0ae2a37c6R171) and upsample_increment_value_bounded (https://github.com/pytorch/pytorch/pull/19630/files#diff-5092da792c30694ee4adf0d0ae2a37c6R191)\r\n\r\nmaybe the problem could be inside one of these functions ... maybe related to the order of indexes (x, y) ...\r\n\r\nbut the problem seems to be related to cuda block/threads ... so not sure if it is really related to these functions.\n<issue_comment>username_2: The new code uses far more registers than the existing one. I verified that the both versions actually call the offending test case with `1024` in `blockDim`.  So it's very likely a register issue.\r\n\r\n`Existing` uses `64` registers, which seems to be optimal or my card:\r\n\r\n```\r\nptxas info    : 77696 bytes gmem, 72 bytes cmem[3]\r\nptxas info    : Compiling entry function '_Z23bicubic_interp2d_kerneliddb15THCDeviceTensorIdLi4Ei16DefaultPtrTraitsES1_' for 'sm_61'\r\nptxas info    : Function properties for _Z23bicubic_interp2d_kerneliddb15THCDeviceTensorIdLi4Ei16DefaultPtrTraitsES1_\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 64 registers, 432 bytes cmem[0], 24 bytes cmem[2]\r\n```\r\n\r\n`New` uses `124` registers, which is too much for my card:\r\n\r\n```\r\nptxas info    : 77696 bytes gmem, 72 bytes cmem[3]\r\nptxas info    : Compiling entry function '_ZN2at6native76_GLOBAL__N__52_tmpxft_0000626d_00000000_6_UpSampleBicubic2d_cpp1_ii_b4c1e1f328upsample_bicubic2d_out_frameElddbNS_20PackedTensorAccessorIdLm4ENS_16DefaultPtrTraitsElEES4_' for 'sm_61'\r\nptxas info    : Function properties for _ZN2at6native76_GLOBAL__N__52_tmpxft_0000626d_00000000_6_UpSampleBicubic2d_cpp1_ii_b4c1e1f328upsample_bicubic2d_out_frameElddbNS_20PackedTensorAccessorIdLm4ENS_16DefaultPtrTraitsElEES4_\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 124 registers, 496 bytes cmem[0], 24 bytes cmem[2]\r\n```\r\n\r\nWith `__launch_bounds__(1024)`, the code again uses `64` registers.\r\n\r\nIf you use `C10_LAUNCH_BOUNDS_1(1024)` **for both kernels**, the tests pass here.\r\n\r\n\r\nNow *why* is the regcount higher in the new code? It could be `PackedTensorAccessor`, it could be the fact that many instances of `int` have been changed to `int64_t`. :)\r\n\r\nYou could experiment or just use the launch bounds.  Other code in `native/cuda` seems to use lower bounds for `blockDim`, too.  1024 seems to be an outlier.\n<issue_comment>username_0: @username_2 \r\n\r\nit seems it worked locally! thank you so much!  I really appreciate that!\n<issue_comment>username_0: thanks @username_1 and @username_2 for all the help!\r\n\r\n@username_3 it is done for review!\n<issue_comment>username_3: CircleCI runs on AWS, and the I'm fairly sure we've got one of the g3 types, either g3.4xlarge or g3.8xlarge\n<issue_comment>username_3: cc @ngimel, if you want to take a look at this (I'm planning to do a review too)\n<issue_comment>username_0: @username_4 could it be addressed in another PR?\n<issue_comment>username_4: @username_0 Sure, it's definitely better to rename both cpu & gpu impls together in a separate PR. Just want to make sure we have the correct one after porting.\n<issue_comment>username_3: Just as a clarification: I'm OK with having changes which are improvements over the old kernels come in latter patches.\n<issue_comment>username_3: I finished reviewing one of the kernels, and did a cursory scan of the others. In general the patch looks like it's in good shape. I'd suggest that we fix up the major correctness issues (PackedTensorAccessor ref, and int32 versus int64 indexing) and anything small that is easy to fix (for example, double zeroing the array), and leave other improvements for follow ups. Should be ready to land soon!\n<issue_comment>username_0: thanks so much @username_3!\r\nI will let you know when it is ready again for a new review! thanks!\n<issue_comment>username_0: @username_2 @username_3 \r\nnot sure but the errors on CI seems to be related to jenkins ... it seems all these jobs that failed (for building) ran by 01h 01min ... not sure if it is a coincidence ...\n<issue_comment>username_1: @username_0 indeed it looks like all jobs were aborted at the same time. no obvious issues in the build log related to your code. Comparing e.g. the `cuda9-cudnn7` build with a successful one from another PR, it takes 1hr 14min there and your build is about at the place where the other one is after an hour.\r\n\r\nI suggest to just push a new commit to rebuild. Probably a temporary CI hiccup.\n<issue_comment>username_3: I accidentally rebooted Jenkins yesterday which is the likely cause, my apologies.\r\n\r\n@pytorchbot retest this please\n<issue_comment>username_0: @username_3 @username_2 @username_1 \r\n\r\nall tests passed except `pr/caffe2-py2-cuda9.0-cudnn7-windows-build`:\r\n \r\n```\r\n14:43:49 Build timed out (after 180 minutes). Marking the build as failed.\r\n14:43:49 Build was aborted\r\n14:43:49 [BFA] Scanning build for known causes...\r\n14:43:49 [BFA] No failure causes found\r\n14:43:49 [BFA] Done. 0s\r\n14:43:49 Finished: FAILURE\r\n```\r\n\r\nnot sure if this timeout means that the code now is slower.\r\n\r\nwhat do you think? do you have any suggestion?\n<issue_comment>username_3: Sometimes the Windows build flakes out like that. It didn't timeout while running a relevant test, so I judge it to be not your problem.\n<issue_comment>username_3: The launch bounds logic is wrong, but I acknowledge that this is a big patch already; just fix it in a follow up. I am going to go ahead and land this.\n<issue_comment>username_0: sounds good @username_3 thanks!"}
{"repo": "pytorch/pytorch", "issue_id": 436372857, "issue_number": 19630, "timestamp": "2019-05-03 17:23:19+00:00", "text": "Looking at it very briefly, the `Jetson TX` referenced in the issue only has 256 cores.\r\n\r\nSo while the issue looks the same, I'd probably not expect it on the CI platforms.  FWIW, some recent CI tests in other issues are green.", "context": "<issue_start><issue_comment>Title: [WIP] UpSample GPU Porting \nusername_0: resolves #16158\n<issue_comment>username_1: Fewer errors than before, 4 instead of 8 with the same message:\r\n```\r\nRuntimeError: CUDA error: too many resources requested for launch\r\n```\r\nThis seems to be an existing problem: gh-8103.\r\n\r\n@username_2 could you have a look at this and tell us what you think?\n<issue_comment>username_2: Looking at it very briefly, the `Jetson TX` referenced in the issue only has 256 cores.\r\n\r\nSo while the issue looks the same, I'd probably not expect it on the CI platforms.  FWIW, some recent CI tests in other issues are green.\n<issue_comment>username_2: @pytorchbot retest this please.\n<issue_comment>username_1: same failures.\n<issue_comment>username_2: Is this rebased on the latest master (you can also ask the bot to rebase)?\n<issue_comment>username_0: I rebased that yesterday ... also needed to resolve conflicts because 7 days ago upsample files were changed.\n<issue_comment>username_0: @username_2 do you have any idea about what could be this problem? or a way to debug that?\r\nalso it seems some jobs is taking a lot of time to build, for example, for some job the estimate time is 19h .. not sure if it the regular estimate ..\n<issue_comment>username_2: @username_0 If you can't reproduce it at home it seems hard to debug other than reading at the diffs again.  I can take a look on Monday, it's getting a bit late here.\n<issue_comment>username_2: Also, has anyone found a way to show the actual hardware used on the CI in detail?\n<issue_comment>username_0: @username_2 I am working in parallel on a paperspace environment .. it tooks a lot of time it is running with 8 cpu cores.\n<issue_comment>username_2: But have you ever been able to reproduce this issue on paperspace?\n<issue_comment>username_1: I can reproduce it locally: Arch Linux, CUDA 10.0, RTX2070 GPU. Given the CI failures, I think it should be reproducible for multiple CUDA and GPU versions. I just haven't worked on any CUDA code before, and am short on time, so I'd rather not dig too deep.\n<issue_comment>username_0: not yet .. my last building was with a my previous commit ... I will work again in this task in some minutes :)\n<issue_comment>username_1: @username_0 your previous commit had the same failure though (except for the less clear exception message), and it seems quite reproducible. So I think you'll see it now.\n<issue_comment>username_2: Ah, thanks.  @username_0, then I guess just compiling with \"DEBUG=1\" and stepping through the offending code with gdb may be the fastest way.\n<issue_comment>username_0: @username_2 thanks! I will try that!\n<issue_comment>username_0: it seems just UpSampleBicubic2d is using upsample_get_value_bounded (https://github.com/pytorch/pytorch/pull/19630/files#diff-5092da792c30694ee4adf0d0ae2a37c6R171) and upsample_increment_value_bounded (https://github.com/pytorch/pytorch/pull/19630/files#diff-5092da792c30694ee4adf0d0ae2a37c6R191)\r\n\r\nmaybe the problem could be inside one of these functions ... maybe related to the order of indexes (x, y) ...\r\n\r\nbut the problem seems to be related to cuda block/threads ... so not sure if it is really related to these functions.\n<issue_comment>username_2: The new code uses far more registers than the existing one. I verified that the both versions actually call the offending test case with `1024` in `blockDim`.  So it's very likely a register issue.\r\n\r\n`Existing` uses `64` registers, which seems to be optimal or my card:\r\n\r\n```\r\nptxas info    : 77696 bytes gmem, 72 bytes cmem[3]\r\nptxas info    : Compiling entry function '_Z23bicubic_interp2d_kerneliddb15THCDeviceTensorIdLi4Ei16DefaultPtrTraitsES1_' for 'sm_61'\r\nptxas info    : Function properties for _Z23bicubic_interp2d_kerneliddb15THCDeviceTensorIdLi4Ei16DefaultPtrTraitsES1_\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 64 registers, 432 bytes cmem[0], 24 bytes cmem[2]\r\n```\r\n\r\n`New` uses `124` registers, which is too much for my card:\r\n\r\n```\r\nptxas info    : 77696 bytes gmem, 72 bytes cmem[3]\r\nptxas info    : Compiling entry function '_ZN2at6native76_GLOBAL__N__52_tmpxft_0000626d_00000000_6_UpSampleBicubic2d_cpp1_ii_b4c1e1f328upsample_bicubic2d_out_frameElddbNS_20PackedTensorAccessorIdLm4ENS_16DefaultPtrTraitsElEES4_' for 'sm_61'\r\nptxas info    : Function properties for _ZN2at6native76_GLOBAL__N__52_tmpxft_0000626d_00000000_6_UpSampleBicubic2d_cpp1_ii_b4c1e1f328upsample_bicubic2d_out_frameElddbNS_20PackedTensorAccessorIdLm4ENS_16DefaultPtrTraitsElEES4_\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 124 registers, 496 bytes cmem[0], 24 bytes cmem[2]\r\n```\r\n\r\nWith `__launch_bounds__(1024)`, the code again uses `64` registers.\r\n\r\nIf you use `C10_LAUNCH_BOUNDS_1(1024)` **for both kernels**, the tests pass here.\r\n\r\n\r\nNow *why* is the regcount higher in the new code? It could be `PackedTensorAccessor`, it could be the fact that many instances of `int` have been changed to `int64_t`. :)\r\n\r\nYou could experiment or just use the launch bounds.  Other code in `native/cuda` seems to use lower bounds for `blockDim`, too.  1024 seems to be an outlier.\n<issue_comment>username_0: @username_2 \r\n\r\nit seems it worked locally! thank you so much!  I really appreciate that!\n<issue_comment>username_0: thanks @username_1 and @username_2 for all the help!\r\n\r\n@username_3 it is done for review!\n<issue_comment>username_3: CircleCI runs on AWS, and the I'm fairly sure we've got one of the g3 types, either g3.4xlarge or g3.8xlarge\n<issue_comment>username_3: cc @ngimel, if you want to take a look at this (I'm planning to do a review too)\n<issue_comment>username_0: @username_4 could it be addressed in another PR?\n<issue_comment>username_4: @username_0 Sure, it's definitely better to rename both cpu & gpu impls together in a separate PR. Just want to make sure we have the correct one after porting.\n<issue_comment>username_3: Just as a clarification: I'm OK with having changes which are improvements over the old kernels come in latter patches.\n<issue_comment>username_3: I finished reviewing one of the kernels, and did a cursory scan of the others. In general the patch looks like it's in good shape. I'd suggest that we fix up the major correctness issues (PackedTensorAccessor ref, and int32 versus int64 indexing) and anything small that is easy to fix (for example, double zeroing the array), and leave other improvements for follow ups. Should be ready to land soon!\n<issue_comment>username_0: thanks so much @username_3!\r\nI will let you know when it is ready again for a new review! thanks!\n<issue_comment>username_0: @username_2 @username_3 \r\nnot sure but the errors on CI seems to be related to jenkins ... it seems all these jobs that failed (for building) ran by 01h 01min ... not sure if it is a coincidence ...\n<issue_comment>username_1: @username_0 indeed it looks like all jobs were aborted at the same time. no obvious issues in the build log related to your code. Comparing e.g. the `cuda9-cudnn7` build with a successful one from another PR, it takes 1hr 14min there and your build is about at the place where the other one is after an hour.\r\n\r\nI suggest to just push a new commit to rebuild. Probably a temporary CI hiccup.\n<issue_comment>username_3: I accidentally rebooted Jenkins yesterday which is the likely cause, my apologies.\r\n\r\n@pytorchbot retest this please\n<issue_comment>username_0: @username_3 @username_2 @username_1 \r\n\r\nall tests passed except `pr/caffe2-py2-cuda9.0-cudnn7-windows-build`:\r\n \r\n```\r\n14:43:49 Build timed out (after 180 minutes). Marking the build as failed.\r\n14:43:49 Build was aborted\r\n14:43:49 [BFA] Scanning build for known causes...\r\n14:43:49 [BFA] No failure causes found\r\n14:43:49 [BFA] Done. 0s\r\n14:43:49 Finished: FAILURE\r\n```\r\n\r\nnot sure if this timeout means that the code now is slower.\r\n\r\nwhat do you think? do you have any suggestion?\n<issue_comment>username_3: Sometimes the Windows build flakes out like that. It didn't timeout while running a relevant test, so I judge it to be not your problem.\n<issue_comment>username_3: The launch bounds logic is wrong, but I acknowledge that this is a big patch already; just fix it in a follow up. I am going to go ahead and land this.\n<issue_comment>username_0: sounds good @username_3 thanks!"}
{"repo": "pytorch/pytorch", "issue_id": 436372857, "issue_number": 19630, "timestamp": "2019-05-03 17:24:05+00:00", "text": "@pytorchbot retest this please.", "context": "<issue_start><issue_comment>Title: [WIP] UpSample GPU Porting \nusername_0: resolves #16158\n<issue_comment>username_1: Fewer errors than before, 4 instead of 8 with the same message:\r\n```\r\nRuntimeError: CUDA error: too many resources requested for launch\r\n```\r\nThis seems to be an existing problem: gh-8103.\r\n\r\n@username_2 could you have a look at this and tell us what you think?\n<issue_comment>username_2: Looking at it very briefly, the `Jetson TX` referenced in the issue only has 256 cores.\r\n\r\nSo while the issue looks the same, I'd probably not expect it on the CI platforms.  FWIW, some recent CI tests in other issues are green.\n<issue_comment>username_2: @pytorchbot retest this please.\n<issue_comment>username_1: same failures.\n<issue_comment>username_2: Is this rebased on the latest master (you can also ask the bot to rebase)?\n<issue_comment>username_0: I rebased that yesterday ... also needed to resolve conflicts because 7 days ago upsample files were changed.\n<issue_comment>username_0: @username_2 do you have any idea about what could be this problem? or a way to debug that?\r\nalso it seems some jobs is taking a lot of time to build, for example, for some job the estimate time is 19h .. not sure if it the regular estimate ..\n<issue_comment>username_2: @username_0 If you can't reproduce it at home it seems hard to debug other than reading at the diffs again.  I can take a look on Monday, it's getting a bit late here.\n<issue_comment>username_2: Also, has anyone found a way to show the actual hardware used on the CI in detail?\n<issue_comment>username_0: @username_2 I am working in parallel on a paperspace environment .. it tooks a lot of time it is running with 8 cpu cores.\n<issue_comment>username_2: But have you ever been able to reproduce this issue on paperspace?\n<issue_comment>username_1: I can reproduce it locally: Arch Linux, CUDA 10.0, RTX2070 GPU. Given the CI failures, I think it should be reproducible for multiple CUDA and GPU versions. I just haven't worked on any CUDA code before, and am short on time, so I'd rather not dig too deep.\n<issue_comment>username_0: not yet .. my last building was with a my previous commit ... I will work again in this task in some minutes :)\n<issue_comment>username_1: @username_0 your previous commit had the same failure though (except for the less clear exception message), and it seems quite reproducible. So I think you'll see it now.\n<issue_comment>username_2: Ah, thanks.  @username_0, then I guess just compiling with \"DEBUG=1\" and stepping through the offending code with gdb may be the fastest way.\n<issue_comment>username_0: @username_2 thanks! I will try that!\n<issue_comment>username_0: it seems just UpSampleBicubic2d is using upsample_get_value_bounded (https://github.com/pytorch/pytorch/pull/19630/files#diff-5092da792c30694ee4adf0d0ae2a37c6R171) and upsample_increment_value_bounded (https://github.com/pytorch/pytorch/pull/19630/files#diff-5092da792c30694ee4adf0d0ae2a37c6R191)\r\n\r\nmaybe the problem could be inside one of these functions ... maybe related to the order of indexes (x, y) ...\r\n\r\nbut the problem seems to be related to cuda block/threads ... so not sure if it is really related to these functions.\n<issue_comment>username_2: The new code uses far more registers than the existing one. I verified that the both versions actually call the offending test case with `1024` in `blockDim`.  So it's very likely a register issue.\r\n\r\n`Existing` uses `64` registers, which seems to be optimal or my card:\r\n\r\n```\r\nptxas info    : 77696 bytes gmem, 72 bytes cmem[3]\r\nptxas info    : Compiling entry function '_Z23bicubic_interp2d_kerneliddb15THCDeviceTensorIdLi4Ei16DefaultPtrTraitsES1_' for 'sm_61'\r\nptxas info    : Function properties for _Z23bicubic_interp2d_kerneliddb15THCDeviceTensorIdLi4Ei16DefaultPtrTraitsES1_\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 64 registers, 432 bytes cmem[0], 24 bytes cmem[2]\r\n```\r\n\r\n`New` uses `124` registers, which is too much for my card:\r\n\r\n```\r\nptxas info    : 77696 bytes gmem, 72 bytes cmem[3]\r\nptxas info    : Compiling entry function '_ZN2at6native76_GLOBAL__N__52_tmpxft_0000626d_00000000_6_UpSampleBicubic2d_cpp1_ii_b4c1e1f328upsample_bicubic2d_out_frameElddbNS_20PackedTensorAccessorIdLm4ENS_16DefaultPtrTraitsElEES4_' for 'sm_61'\r\nptxas info    : Function properties for _ZN2at6native76_GLOBAL__N__52_tmpxft_0000626d_00000000_6_UpSampleBicubic2d_cpp1_ii_b4c1e1f328upsample_bicubic2d_out_frameElddbNS_20PackedTensorAccessorIdLm4ENS_16DefaultPtrTraitsElEES4_\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 124 registers, 496 bytes cmem[0], 24 bytes cmem[2]\r\n```\r\n\r\nWith `__launch_bounds__(1024)`, the code again uses `64` registers.\r\n\r\nIf you use `C10_LAUNCH_BOUNDS_1(1024)` **for both kernels**, the tests pass here.\r\n\r\n\r\nNow *why* is the regcount higher in the new code? It could be `PackedTensorAccessor`, it could be the fact that many instances of `int` have been changed to `int64_t`. :)\r\n\r\nYou could experiment or just use the launch bounds.  Other code in `native/cuda` seems to use lower bounds for `blockDim`, too.  1024 seems to be an outlier.\n<issue_comment>username_0: @username_2 \r\n\r\nit seems it worked locally! thank you so much!  I really appreciate that!\n<issue_comment>username_0: thanks @username_1 and @username_2 for all the help!\r\n\r\n@username_3 it is done for review!\n<issue_comment>username_3: CircleCI runs on AWS, and the I'm fairly sure we've got one of the g3 types, either g3.4xlarge or g3.8xlarge\n<issue_comment>username_3: cc @ngimel, if you want to take a look at this (I'm planning to do a review too)\n<issue_comment>username_0: @username_4 could it be addressed in another PR?\n<issue_comment>username_4: @username_0 Sure, it's definitely better to rename both cpu & gpu impls together in a separate PR. Just want to make sure we have the correct one after porting.\n<issue_comment>username_3: Just as a clarification: I'm OK with having changes which are improvements over the old kernels come in latter patches.\n<issue_comment>username_3: I finished reviewing one of the kernels, and did a cursory scan of the others. In general the patch looks like it's in good shape. I'd suggest that we fix up the major correctness issues (PackedTensorAccessor ref, and int32 versus int64 indexing) and anything small that is easy to fix (for example, double zeroing the array), and leave other improvements for follow ups. Should be ready to land soon!\n<issue_comment>username_0: thanks so much @username_3!\r\nI will let you know when it is ready again for a new review! thanks!\n<issue_comment>username_0: @username_2 @username_3 \r\nnot sure but the errors on CI seems to be related to jenkins ... it seems all these jobs that failed (for building) ran by 01h 01min ... not sure if it is a coincidence ...\n<issue_comment>username_1: @username_0 indeed it looks like all jobs were aborted at the same time. no obvious issues in the build log related to your code. Comparing e.g. the `cuda9-cudnn7` build with a successful one from another PR, it takes 1hr 14min there and your build is about at the place where the other one is after an hour.\r\n\r\nI suggest to just push a new commit to rebuild. Probably a temporary CI hiccup.\n<issue_comment>username_3: I accidentally rebooted Jenkins yesterday which is the likely cause, my apologies.\r\n\r\n@pytorchbot retest this please\n<issue_comment>username_0: @username_3 @username_2 @username_1 \r\n\r\nall tests passed except `pr/caffe2-py2-cuda9.0-cudnn7-windows-build`:\r\n \r\n```\r\n14:43:49 Build timed out (after 180 minutes). Marking the build as failed.\r\n14:43:49 Build was aborted\r\n14:43:49 [BFA] Scanning build for known causes...\r\n14:43:49 [BFA] No failure causes found\r\n14:43:49 [BFA] Done. 0s\r\n14:43:49 Finished: FAILURE\r\n```\r\n\r\nnot sure if this timeout means that the code now is slower.\r\n\r\nwhat do you think? do you have any suggestion?\n<issue_comment>username_3: Sometimes the Windows build flakes out like that. It didn't timeout while running a relevant test, so I judge it to be not your problem.\n<issue_comment>username_3: The launch bounds logic is wrong, but I acknowledge that this is a big patch already; just fix it in a follow up. I am going to go ahead and land this.\n<issue_comment>username_0: sounds good @username_3 thanks!"}
{"repo": "pytorch/pytorch", "issue_id": 436372857, "issue_number": 19630, "timestamp": "2019-05-03 17:56:30+00:00", "text": "same failures.", "context": "<issue_start><issue_comment>Title: [WIP] UpSample GPU Porting \nusername_0: resolves #16158\n<issue_comment>username_1: Fewer errors than before, 4 instead of 8 with the same message:\r\n```\r\nRuntimeError: CUDA error: too many resources requested for launch\r\n```\r\nThis seems to be an existing problem: gh-8103.\r\n\r\n@username_2 could you have a look at this and tell us what you think?\n<issue_comment>username_2: Looking at it very briefly, the `Jetson TX` referenced in the issue only has 256 cores.\r\n\r\nSo while the issue looks the same, I'd probably not expect it on the CI platforms.  FWIW, some recent CI tests in other issues are green.\n<issue_comment>username_2: @pytorchbot retest this please.\n<issue_comment>username_1: same failures.\n<issue_comment>username_2: Is this rebased on the latest master (you can also ask the bot to rebase)?\n<issue_comment>username_0: I rebased that yesterday ... also needed to resolve conflicts because 7 days ago upsample files were changed.\n<issue_comment>username_0: @username_2 do you have any idea about what could be this problem? or a way to debug that?\r\nalso it seems some jobs is taking a lot of time to build, for example, for some job the estimate time is 19h .. not sure if it the regular estimate ..\n<issue_comment>username_2: @username_0 If you can't reproduce it at home it seems hard to debug other than reading at the diffs again.  I can take a look on Monday, it's getting a bit late here.\n<issue_comment>username_2: Also, has anyone found a way to show the actual hardware used on the CI in detail?\n<issue_comment>username_0: @username_2 I am working in parallel on a paperspace environment .. it tooks a lot of time it is running with 8 cpu cores.\n<issue_comment>username_2: But have you ever been able to reproduce this issue on paperspace?\n<issue_comment>username_1: I can reproduce it locally: Arch Linux, CUDA 10.0, RTX2070 GPU. Given the CI failures, I think it should be reproducible for multiple CUDA and GPU versions. I just haven't worked on any CUDA code before, and am short on time, so I'd rather not dig too deep.\n<issue_comment>username_0: not yet .. my last building was with a my previous commit ... I will work again in this task in some minutes :)\n<issue_comment>username_1: @username_0 your previous commit had the same failure though (except for the less clear exception message), and it seems quite reproducible. So I think you'll see it now.\n<issue_comment>username_2: Ah, thanks.  @username_0, then I guess just compiling with \"DEBUG=1\" and stepping through the offending code with gdb may be the fastest way.\n<issue_comment>username_0: @username_2 thanks! I will try that!\n<issue_comment>username_0: it seems just UpSampleBicubic2d is using upsample_get_value_bounded (https://github.com/pytorch/pytorch/pull/19630/files#diff-5092da792c30694ee4adf0d0ae2a37c6R171) and upsample_increment_value_bounded (https://github.com/pytorch/pytorch/pull/19630/files#diff-5092da792c30694ee4adf0d0ae2a37c6R191)\r\n\r\nmaybe the problem could be inside one of these functions ... maybe related to the order of indexes (x, y) ...\r\n\r\nbut the problem seems to be related to cuda block/threads ... so not sure if it is really related to these functions.\n<issue_comment>username_2: The new code uses far more registers than the existing one. I verified that the both versions actually call the offending test case with `1024` in `blockDim`.  So it's very likely a register issue.\r\n\r\n`Existing` uses `64` registers, which seems to be optimal or my card:\r\n\r\n```\r\nptxas info    : 77696 bytes gmem, 72 bytes cmem[3]\r\nptxas info    : Compiling entry function '_Z23bicubic_interp2d_kerneliddb15THCDeviceTensorIdLi4Ei16DefaultPtrTraitsES1_' for 'sm_61'\r\nptxas info    : Function properties for _Z23bicubic_interp2d_kerneliddb15THCDeviceTensorIdLi4Ei16DefaultPtrTraitsES1_\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 64 registers, 432 bytes cmem[0], 24 bytes cmem[2]\r\n```\r\n\r\n`New` uses `124` registers, which is too much for my card:\r\n\r\n```\r\nptxas info    : 77696 bytes gmem, 72 bytes cmem[3]\r\nptxas info    : Compiling entry function '_ZN2at6native76_GLOBAL__N__52_tmpxft_0000626d_00000000_6_UpSampleBicubic2d_cpp1_ii_b4c1e1f328upsample_bicubic2d_out_frameElddbNS_20PackedTensorAccessorIdLm4ENS_16DefaultPtrTraitsElEES4_' for 'sm_61'\r\nptxas info    : Function properties for _ZN2at6native76_GLOBAL__N__52_tmpxft_0000626d_00000000_6_UpSampleBicubic2d_cpp1_ii_b4c1e1f328upsample_bicubic2d_out_frameElddbNS_20PackedTensorAccessorIdLm4ENS_16DefaultPtrTraitsElEES4_\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 124 registers, 496 bytes cmem[0], 24 bytes cmem[2]\r\n```\r\n\r\nWith `__launch_bounds__(1024)`, the code again uses `64` registers.\r\n\r\nIf you use `C10_LAUNCH_BOUNDS_1(1024)` **for both kernels**, the tests pass here.\r\n\r\n\r\nNow *why* is the regcount higher in the new code? It could be `PackedTensorAccessor`, it could be the fact that many instances of `int` have been changed to `int64_t`. :)\r\n\r\nYou could experiment or just use the launch bounds.  Other code in `native/cuda` seems to use lower bounds for `blockDim`, too.  1024 seems to be an outlier.\n<issue_comment>username_0: @username_2 \r\n\r\nit seems it worked locally! thank you so much!  I really appreciate that!\n<issue_comment>username_0: thanks @username_1 and @username_2 for all the help!\r\n\r\n@username_3 it is done for review!\n<issue_comment>username_3: CircleCI runs on AWS, and the I'm fairly sure we've got one of the g3 types, either g3.4xlarge or g3.8xlarge\n<issue_comment>username_3: cc @ngimel, if you want to take a look at this (I'm planning to do a review too)\n<issue_comment>username_0: @username_4 could it be addressed in another PR?\n<issue_comment>username_4: @username_0 Sure, it's definitely better to rename both cpu & gpu impls together in a separate PR. Just want to make sure we have the correct one after porting.\n<issue_comment>username_3: Just as a clarification: I'm OK with having changes which are improvements over the old kernels come in latter patches.\n<issue_comment>username_3: I finished reviewing one of the kernels, and did a cursory scan of the others. In general the patch looks like it's in good shape. I'd suggest that we fix up the major correctness issues (PackedTensorAccessor ref, and int32 versus int64 indexing) and anything small that is easy to fix (for example, double zeroing the array), and leave other improvements for follow ups. Should be ready to land soon!\n<issue_comment>username_0: thanks so much @username_3!\r\nI will let you know when it is ready again for a new review! thanks!\n<issue_comment>username_0: @username_2 @username_3 \r\nnot sure but the errors on CI seems to be related to jenkins ... it seems all these jobs that failed (for building) ran by 01h 01min ... not sure if it is a coincidence ...\n<issue_comment>username_1: @username_0 indeed it looks like all jobs were aborted at the same time. no obvious issues in the build log related to your code. Comparing e.g. the `cuda9-cudnn7` build with a successful one from another PR, it takes 1hr 14min there and your build is about at the place where the other one is after an hour.\r\n\r\nI suggest to just push a new commit to rebuild. Probably a temporary CI hiccup.\n<issue_comment>username_3: I accidentally rebooted Jenkins yesterday which is the likely cause, my apologies.\r\n\r\n@pytorchbot retest this please\n<issue_comment>username_0: @username_3 @username_2 @username_1 \r\n\r\nall tests passed except `pr/caffe2-py2-cuda9.0-cudnn7-windows-build`:\r\n \r\n```\r\n14:43:49 Build timed out (after 180 minutes). Marking the build as failed.\r\n14:43:49 Build was aborted\r\n14:43:49 [BFA] Scanning build for known causes...\r\n14:43:49 [BFA] No failure causes found\r\n14:43:49 [BFA] Done. 0s\r\n14:43:49 Finished: FAILURE\r\n```\r\n\r\nnot sure if this timeout means that the code now is slower.\r\n\r\nwhat do you think? do you have any suggestion?\n<issue_comment>username_3: Sometimes the Windows build flakes out like that. It didn't timeout while running a relevant test, so I judge it to be not your problem.\n<issue_comment>username_3: The launch bounds logic is wrong, but I acknowledge that this is a big patch already; just fix it in a follow up. I am going to go ahead and land this.\n<issue_comment>username_0: sounds good @username_3 thanks!"}
{"repo": "pytorch/pytorch", "issue_id": 436372857, "issue_number": 19630, "timestamp": "2019-05-03 17:58:09+00:00", "text": "Is this rebased on the latest master (you can also ask the bot to rebase)?", "context": "<issue_start><issue_comment>Title: [WIP] UpSample GPU Porting \nusername_0: resolves #16158\n<issue_comment>username_1: Fewer errors than before, 4 instead of 8 with the same message:\r\n```\r\nRuntimeError: CUDA error: too many resources requested for launch\r\n```\r\nThis seems to be an existing problem: gh-8103.\r\n\r\n@username_2 could you have a look at this and tell us what you think?\n<issue_comment>username_2: Looking at it very briefly, the `Jetson TX` referenced in the issue only has 256 cores.\r\n\r\nSo while the issue looks the same, I'd probably not expect it on the CI platforms.  FWIW, some recent CI tests in other issues are green.\n<issue_comment>username_2: @pytorchbot retest this please.\n<issue_comment>username_1: same failures.\n<issue_comment>username_2: Is this rebased on the latest master (you can also ask the bot to rebase)?\n<issue_comment>username_0: I rebased that yesterday ... also needed to resolve conflicts because 7 days ago upsample files were changed.\n<issue_comment>username_0: @username_2 do you have any idea about what could be this problem? or a way to debug that?\r\nalso it seems some jobs is taking a lot of time to build, for example, for some job the estimate time is 19h .. not sure if it the regular estimate ..\n<issue_comment>username_2: @username_0 If you can't reproduce it at home it seems hard to debug other than reading at the diffs again.  I can take a look on Monday, it's getting a bit late here.\n<issue_comment>username_2: Also, has anyone found a way to show the actual hardware used on the CI in detail?\n<issue_comment>username_0: @username_2 I am working in parallel on a paperspace environment .. it tooks a lot of time it is running with 8 cpu cores.\n<issue_comment>username_2: But have you ever been able to reproduce this issue on paperspace?\n<issue_comment>username_1: I can reproduce it locally: Arch Linux, CUDA 10.0, RTX2070 GPU. Given the CI failures, I think it should be reproducible for multiple CUDA and GPU versions. I just haven't worked on any CUDA code before, and am short on time, so I'd rather not dig too deep.\n<issue_comment>username_0: not yet .. my last building was with a my previous commit ... I will work again in this task in some minutes :)\n<issue_comment>username_1: @username_0 your previous commit had the same failure though (except for the less clear exception message), and it seems quite reproducible. So I think you'll see it now.\n<issue_comment>username_2: Ah, thanks.  @username_0, then I guess just compiling with \"DEBUG=1\" and stepping through the offending code with gdb may be the fastest way.\n<issue_comment>username_0: @username_2 thanks! I will try that!\n<issue_comment>username_0: it seems just UpSampleBicubic2d is using upsample_get_value_bounded (https://github.com/pytorch/pytorch/pull/19630/files#diff-5092da792c30694ee4adf0d0ae2a37c6R171) and upsample_increment_value_bounded (https://github.com/pytorch/pytorch/pull/19630/files#diff-5092da792c30694ee4adf0d0ae2a37c6R191)\r\n\r\nmaybe the problem could be inside one of these functions ... maybe related to the order of indexes (x, y) ...\r\n\r\nbut the problem seems to be related to cuda block/threads ... so not sure if it is really related to these functions.\n<issue_comment>username_2: The new code uses far more registers than the existing one. I verified that the both versions actually call the offending test case with `1024` in `blockDim`.  So it's very likely a register issue.\r\n\r\n`Existing` uses `64` registers, which seems to be optimal or my card:\r\n\r\n```\r\nptxas info    : 77696 bytes gmem, 72 bytes cmem[3]\r\nptxas info    : Compiling entry function '_Z23bicubic_interp2d_kerneliddb15THCDeviceTensorIdLi4Ei16DefaultPtrTraitsES1_' for 'sm_61'\r\nptxas info    : Function properties for _Z23bicubic_interp2d_kerneliddb15THCDeviceTensorIdLi4Ei16DefaultPtrTraitsES1_\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 64 registers, 432 bytes cmem[0], 24 bytes cmem[2]\r\n```\r\n\r\n`New` uses `124` registers, which is too much for my card:\r\n\r\n```\r\nptxas info    : 77696 bytes gmem, 72 bytes cmem[3]\r\nptxas info    : Compiling entry function '_ZN2at6native76_GLOBAL__N__52_tmpxft_0000626d_00000000_6_UpSampleBicubic2d_cpp1_ii_b4c1e1f328upsample_bicubic2d_out_frameElddbNS_20PackedTensorAccessorIdLm4ENS_16DefaultPtrTraitsElEES4_' for 'sm_61'\r\nptxas info    : Function properties for _ZN2at6native76_GLOBAL__N__52_tmpxft_0000626d_00000000_6_UpSampleBicubic2d_cpp1_ii_b4c1e1f328upsample_bicubic2d_out_frameElddbNS_20PackedTensorAccessorIdLm4ENS_16DefaultPtrTraitsElEES4_\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 124 registers, 496 bytes cmem[0], 24 bytes cmem[2]\r\n```\r\n\r\nWith `__launch_bounds__(1024)`, the code again uses `64` registers.\r\n\r\nIf you use `C10_LAUNCH_BOUNDS_1(1024)` **for both kernels**, the tests pass here.\r\n\r\n\r\nNow *why* is the regcount higher in the new code? It could be `PackedTensorAccessor`, it could be the fact that many instances of `int` have been changed to `int64_t`. :)\r\n\r\nYou could experiment or just use the launch bounds.  Other code in `native/cuda` seems to use lower bounds for `blockDim`, too.  1024 seems to be an outlier.\n<issue_comment>username_0: @username_2 \r\n\r\nit seems it worked locally! thank you so much!  I really appreciate that!\n<issue_comment>username_0: thanks @username_1 and @username_2 for all the help!\r\n\r\n@username_3 it is done for review!\n<issue_comment>username_3: CircleCI runs on AWS, and the I'm fairly sure we've got one of the g3 types, either g3.4xlarge or g3.8xlarge\n<issue_comment>username_3: cc @ngimel, if you want to take a look at this (I'm planning to do a review too)\n<issue_comment>username_0: @username_4 could it be addressed in another PR?\n<issue_comment>username_4: @username_0 Sure, it's definitely better to rename both cpu & gpu impls together in a separate PR. Just want to make sure we have the correct one after porting.\n<issue_comment>username_3: Just as a clarification: I'm OK with having changes which are improvements over the old kernels come in latter patches.\n<issue_comment>username_3: I finished reviewing one of the kernels, and did a cursory scan of the others. In general the patch looks like it's in good shape. I'd suggest that we fix up the major correctness issues (PackedTensorAccessor ref, and int32 versus int64 indexing) and anything small that is easy to fix (for example, double zeroing the array), and leave other improvements for follow ups. Should be ready to land soon!\n<issue_comment>username_0: thanks so much @username_3!\r\nI will let you know when it is ready again for a new review! thanks!\n<issue_comment>username_0: @username_2 @username_3 \r\nnot sure but the errors on CI seems to be related to jenkins ... it seems all these jobs that failed (for building) ran by 01h 01min ... not sure if it is a coincidence ...\n<issue_comment>username_1: @username_0 indeed it looks like all jobs were aborted at the same time. no obvious issues in the build log related to your code. Comparing e.g. the `cuda9-cudnn7` build with a successful one from another PR, it takes 1hr 14min there and your build is about at the place where the other one is after an hour.\r\n\r\nI suggest to just push a new commit to rebuild. Probably a temporary CI hiccup.\n<issue_comment>username_3: I accidentally rebooted Jenkins yesterday which is the likely cause, my apologies.\r\n\r\n@pytorchbot retest this please\n<issue_comment>username_0: @username_3 @username_2 @username_1 \r\n\r\nall tests passed except `pr/caffe2-py2-cuda9.0-cudnn7-windows-build`:\r\n \r\n```\r\n14:43:49 Build timed out (after 180 minutes). Marking the build as failed.\r\n14:43:49 Build was aborted\r\n14:43:49 [BFA] Scanning build for known causes...\r\n14:43:49 [BFA] No failure causes found\r\n14:43:49 [BFA] Done. 0s\r\n14:43:49 Finished: FAILURE\r\n```\r\n\r\nnot sure if this timeout means that the code now is slower.\r\n\r\nwhat do you think? do you have any suggestion?\n<issue_comment>username_3: Sometimes the Windows build flakes out like that. It didn't timeout while running a relevant test, so I judge it to be not your problem.\n<issue_comment>username_3: The launch bounds logic is wrong, but I acknowledge that this is a big patch already; just fix it in a follow up. I am going to go ahead and land this.\n<issue_comment>username_0: sounds good @username_3 thanks!"}
{"repo": "pytorch/pytorch", "issue_id": 436372857, "issue_number": 19630, "timestamp": "2019-05-03 17:59:41+00:00", "text": "I rebased that yesterday ... also needed to resolve conflicts because 7 days ago upsample files were changed.", "context": "<issue_start><issue_comment>Title: [WIP] UpSample GPU Porting \nusername_0: resolves #16158\n<issue_comment>username_1: Fewer errors than before, 4 instead of 8 with the same message:\r\n```\r\nRuntimeError: CUDA error: too many resources requested for launch\r\n```\r\nThis seems to be an existing problem: gh-8103.\r\n\r\n@username_2 could you have a look at this and tell us what you think?\n<issue_comment>username_2: Looking at it very briefly, the `Jetson TX` referenced in the issue only has 256 cores.\r\n\r\nSo while the issue looks the same, I'd probably not expect it on the CI platforms.  FWIW, some recent CI tests in other issues are green.\n<issue_comment>username_2: @pytorchbot retest this please.\n<issue_comment>username_1: same failures.\n<issue_comment>username_2: Is this rebased on the latest master (you can also ask the bot to rebase)?\n<issue_comment>username_0: I rebased that yesterday ... also needed to resolve conflicts because 7 days ago upsample files were changed.\n<issue_comment>username_0: @username_2 do you have any idea about what could be this problem? or a way to debug that?\r\nalso it seems some jobs is taking a lot of time to build, for example, for some job the estimate time is 19h .. not sure if it the regular estimate ..\n<issue_comment>username_2: @username_0 If you can't reproduce it at home it seems hard to debug other than reading at the diffs again.  I can take a look on Monday, it's getting a bit late here.\n<issue_comment>username_2: Also, has anyone found a way to show the actual hardware used on the CI in detail?\n<issue_comment>username_0: @username_2 I am working in parallel on a paperspace environment .. it tooks a lot of time it is running with 8 cpu cores.\n<issue_comment>username_2: But have you ever been able to reproduce this issue on paperspace?\n<issue_comment>username_1: I can reproduce it locally: Arch Linux, CUDA 10.0, RTX2070 GPU. Given the CI failures, I think it should be reproducible for multiple CUDA and GPU versions. I just haven't worked on any CUDA code before, and am short on time, so I'd rather not dig too deep.\n<issue_comment>username_0: not yet .. my last building was with a my previous commit ... I will work again in this task in some minutes :)\n<issue_comment>username_1: @username_0 your previous commit had the same failure though (except for the less clear exception message), and it seems quite reproducible. So I think you'll see it now.\n<issue_comment>username_2: Ah, thanks.  @username_0, then I guess just compiling with \"DEBUG=1\" and stepping through the offending code with gdb may be the fastest way.\n<issue_comment>username_0: @username_2 thanks! I will try that!\n<issue_comment>username_0: it seems just UpSampleBicubic2d is using upsample_get_value_bounded (https://github.com/pytorch/pytorch/pull/19630/files#diff-5092da792c30694ee4adf0d0ae2a37c6R171) and upsample_increment_value_bounded (https://github.com/pytorch/pytorch/pull/19630/files#diff-5092da792c30694ee4adf0d0ae2a37c6R191)\r\n\r\nmaybe the problem could be inside one of these functions ... maybe related to the order of indexes (x, y) ...\r\n\r\nbut the problem seems to be related to cuda block/threads ... so not sure if it is really related to these functions.\n<issue_comment>username_2: The new code uses far more registers than the existing one. I verified that the both versions actually call the offending test case with `1024` in `blockDim`.  So it's very likely a register issue.\r\n\r\n`Existing` uses `64` registers, which seems to be optimal or my card:\r\n\r\n```\r\nptxas info    : 77696 bytes gmem, 72 bytes cmem[3]\r\nptxas info    : Compiling entry function '_Z23bicubic_interp2d_kerneliddb15THCDeviceTensorIdLi4Ei16DefaultPtrTraitsES1_' for 'sm_61'\r\nptxas info    : Function properties for _Z23bicubic_interp2d_kerneliddb15THCDeviceTensorIdLi4Ei16DefaultPtrTraitsES1_\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 64 registers, 432 bytes cmem[0], 24 bytes cmem[2]\r\n```\r\n\r\n`New` uses `124` registers, which is too much for my card:\r\n\r\n```\r\nptxas info    : 77696 bytes gmem, 72 bytes cmem[3]\r\nptxas info    : Compiling entry function '_ZN2at6native76_GLOBAL__N__52_tmpxft_0000626d_00000000_6_UpSampleBicubic2d_cpp1_ii_b4c1e1f328upsample_bicubic2d_out_frameElddbNS_20PackedTensorAccessorIdLm4ENS_16DefaultPtrTraitsElEES4_' for 'sm_61'\r\nptxas info    : Function properties for _ZN2at6native76_GLOBAL__N__52_tmpxft_0000626d_00000000_6_UpSampleBicubic2d_cpp1_ii_b4c1e1f328upsample_bicubic2d_out_frameElddbNS_20PackedTensorAccessorIdLm4ENS_16DefaultPtrTraitsElEES4_\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 124 registers, 496 bytes cmem[0], 24 bytes cmem[2]\r\n```\r\n\r\nWith `__launch_bounds__(1024)`, the code again uses `64` registers.\r\n\r\nIf you use `C10_LAUNCH_BOUNDS_1(1024)` **for both kernels**, the tests pass here.\r\n\r\n\r\nNow *why* is the regcount higher in the new code? It could be `PackedTensorAccessor`, it could be the fact that many instances of `int` have been changed to `int64_t`. :)\r\n\r\nYou could experiment or just use the launch bounds.  Other code in `native/cuda` seems to use lower bounds for `blockDim`, too.  1024 seems to be an outlier.\n<issue_comment>username_0: @username_2 \r\n\r\nit seems it worked locally! thank you so much!  I really appreciate that!\n<issue_comment>username_0: thanks @username_1 and @username_2 for all the help!\r\n\r\n@username_3 it is done for review!\n<issue_comment>username_3: CircleCI runs on AWS, and the I'm fairly sure we've got one of the g3 types, either g3.4xlarge or g3.8xlarge\n<issue_comment>username_3: cc @ngimel, if you want to take a look at this (I'm planning to do a review too)\n<issue_comment>username_0: @username_4 could it be addressed in another PR?\n<issue_comment>username_4: @username_0 Sure, it's definitely better to rename both cpu & gpu impls together in a separate PR. Just want to make sure we have the correct one after porting.\n<issue_comment>username_3: Just as a clarification: I'm OK with having changes which are improvements over the old kernels come in latter patches.\n<issue_comment>username_3: I finished reviewing one of the kernels, and did a cursory scan of the others. In general the patch looks like it's in good shape. I'd suggest that we fix up the major correctness issues (PackedTensorAccessor ref, and int32 versus int64 indexing) and anything small that is easy to fix (for example, double zeroing the array), and leave other improvements for follow ups. Should be ready to land soon!\n<issue_comment>username_0: thanks so much @username_3!\r\nI will let you know when it is ready again for a new review! thanks!\n<issue_comment>username_0: @username_2 @username_3 \r\nnot sure but the errors on CI seems to be related to jenkins ... it seems all these jobs that failed (for building) ran by 01h 01min ... not sure if it is a coincidence ...\n<issue_comment>username_1: @username_0 indeed it looks like all jobs were aborted at the same time. no obvious issues in the build log related to your code. Comparing e.g. the `cuda9-cudnn7` build with a successful one from another PR, it takes 1hr 14min there and your build is about at the place where the other one is after an hour.\r\n\r\nI suggest to just push a new commit to rebuild. Probably a temporary CI hiccup.\n<issue_comment>username_3: I accidentally rebooted Jenkins yesterday which is the likely cause, my apologies.\r\n\r\n@pytorchbot retest this please\n<issue_comment>username_0: @username_3 @username_2 @username_1 \r\n\r\nall tests passed except `pr/caffe2-py2-cuda9.0-cudnn7-windows-build`:\r\n \r\n```\r\n14:43:49 Build timed out (after 180 minutes). Marking the build as failed.\r\n14:43:49 Build was aborted\r\n14:43:49 [BFA] Scanning build for known causes...\r\n14:43:49 [BFA] No failure causes found\r\n14:43:49 [BFA] Done. 0s\r\n14:43:49 Finished: FAILURE\r\n```\r\n\r\nnot sure if this timeout means that the code now is slower.\r\n\r\nwhat do you think? do you have any suggestion?\n<issue_comment>username_3: Sometimes the Windows build flakes out like that. It didn't timeout while running a relevant test, so I judge it to be not your problem.\n<issue_comment>username_3: The launch bounds logic is wrong, but I acknowledge that this is a big patch already; just fix it in a follow up. I am going to go ahead and land this.\n<issue_comment>username_0: sounds good @username_3 thanks!"}
{"repo": "pytorch/pytorch", "issue_id": 436372857, "issue_number": 19630, "timestamp": "2019-05-03 18:22:47+00:00", "text": "@username_2 do you have any idea about what could be this problem? or a way to debug that?\r\nalso it seems some jobs is taking a lot of time to build, for example, for some job the estimate time is 19h .. not sure if it the regular estimate ..", "context": "<issue_start><issue_comment>Title: [WIP] UpSample GPU Porting \nusername_0: resolves #16158\n<issue_comment>username_1: Fewer errors than before, 4 instead of 8 with the same message:\r\n```\r\nRuntimeError: CUDA error: too many resources requested for launch\r\n```\r\nThis seems to be an existing problem: gh-8103.\r\n\r\n@username_2 could you have a look at this and tell us what you think?\n<issue_comment>username_2: Looking at it very briefly, the `Jetson TX` referenced in the issue only has 256 cores.\r\n\r\nSo while the issue looks the same, I'd probably not expect it on the CI platforms.  FWIW, some recent CI tests in other issues are green.\n<issue_comment>username_2: @pytorchbot retest this please.\n<issue_comment>username_1: same failures.\n<issue_comment>username_2: Is this rebased on the latest master (you can also ask the bot to rebase)?\n<issue_comment>username_0: I rebased that yesterday ... also needed to resolve conflicts because 7 days ago upsample files were changed.\n<issue_comment>username_0: @username_2 do you have any idea about what could be this problem? or a way to debug that?\r\nalso it seems some jobs is taking a lot of time to build, for example, for some job the estimate time is 19h .. not sure if it the regular estimate ..\n<issue_comment>username_2: @username_0 If you can't reproduce it at home it seems hard to debug other than reading at the diffs again.  I can take a look on Monday, it's getting a bit late here.\n<issue_comment>username_2: Also, has anyone found a way to show the actual hardware used on the CI in detail?\n<issue_comment>username_0: @username_2 I am working in parallel on a paperspace environment .. it tooks a lot of time it is running with 8 cpu cores.\n<issue_comment>username_2: But have you ever been able to reproduce this issue on paperspace?\n<issue_comment>username_1: I can reproduce it locally: Arch Linux, CUDA 10.0, RTX2070 GPU. Given the CI failures, I think it should be reproducible for multiple CUDA and GPU versions. I just haven't worked on any CUDA code before, and am short on time, so I'd rather not dig too deep.\n<issue_comment>username_0: not yet .. my last building was with a my previous commit ... I will work again in this task in some minutes :)\n<issue_comment>username_1: @username_0 your previous commit had the same failure though (except for the less clear exception message), and it seems quite reproducible. So I think you'll see it now.\n<issue_comment>username_2: Ah, thanks.  @username_0, then I guess just compiling with \"DEBUG=1\" and stepping through the offending code with gdb may be the fastest way.\n<issue_comment>username_0: @username_2 thanks! I will try that!\n<issue_comment>username_0: it seems just UpSampleBicubic2d is using upsample_get_value_bounded (https://github.com/pytorch/pytorch/pull/19630/files#diff-5092da792c30694ee4adf0d0ae2a37c6R171) and upsample_increment_value_bounded (https://github.com/pytorch/pytorch/pull/19630/files#diff-5092da792c30694ee4adf0d0ae2a37c6R191)\r\n\r\nmaybe the problem could be inside one of these functions ... maybe related to the order of indexes (x, y) ...\r\n\r\nbut the problem seems to be related to cuda block/threads ... so not sure if it is really related to these functions.\n<issue_comment>username_2: The new code uses far more registers than the existing one. I verified that the both versions actually call the offending test case with `1024` in `blockDim`.  So it's very likely a register issue.\r\n\r\n`Existing` uses `64` registers, which seems to be optimal or my card:\r\n\r\n```\r\nptxas info    : 77696 bytes gmem, 72 bytes cmem[3]\r\nptxas info    : Compiling entry function '_Z23bicubic_interp2d_kerneliddb15THCDeviceTensorIdLi4Ei16DefaultPtrTraitsES1_' for 'sm_61'\r\nptxas info    : Function properties for _Z23bicubic_interp2d_kerneliddb15THCDeviceTensorIdLi4Ei16DefaultPtrTraitsES1_\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 64 registers, 432 bytes cmem[0], 24 bytes cmem[2]\r\n```\r\n\r\n`New` uses `124` registers, which is too much for my card:\r\n\r\n```\r\nptxas info    : 77696 bytes gmem, 72 bytes cmem[3]\r\nptxas info    : Compiling entry function '_ZN2at6native76_GLOBAL__N__52_tmpxft_0000626d_00000000_6_UpSampleBicubic2d_cpp1_ii_b4c1e1f328upsample_bicubic2d_out_frameElddbNS_20PackedTensorAccessorIdLm4ENS_16DefaultPtrTraitsElEES4_' for 'sm_61'\r\nptxas info    : Function properties for _ZN2at6native76_GLOBAL__N__52_tmpxft_0000626d_00000000_6_UpSampleBicubic2d_cpp1_ii_b4c1e1f328upsample_bicubic2d_out_frameElddbNS_20PackedTensorAccessorIdLm4ENS_16DefaultPtrTraitsElEES4_\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 124 registers, 496 bytes cmem[0], 24 bytes cmem[2]\r\n```\r\n\r\nWith `__launch_bounds__(1024)`, the code again uses `64` registers.\r\n\r\nIf you use `C10_LAUNCH_BOUNDS_1(1024)` **for both kernels**, the tests pass here.\r\n\r\n\r\nNow *why* is the regcount higher in the new code? It could be `PackedTensorAccessor`, it could be the fact that many instances of `int` have been changed to `int64_t`. :)\r\n\r\nYou could experiment or just use the launch bounds.  Other code in `native/cuda` seems to use lower bounds for `blockDim`, too.  1024 seems to be an outlier.\n<issue_comment>username_0: @username_2 \r\n\r\nit seems it worked locally! thank you so much!  I really appreciate that!\n<issue_comment>username_0: thanks @username_1 and @username_2 for all the help!\r\n\r\n@username_3 it is done for review!\n<issue_comment>username_3: CircleCI runs on AWS, and the I'm fairly sure we've got one of the g3 types, either g3.4xlarge or g3.8xlarge\n<issue_comment>username_3: cc @ngimel, if you want to take a look at this (I'm planning to do a review too)\n<issue_comment>username_0: @username_4 could it be addressed in another PR?\n<issue_comment>username_4: @username_0 Sure, it's definitely better to rename both cpu & gpu impls together in a separate PR. Just want to make sure we have the correct one after porting.\n<issue_comment>username_3: Just as a clarification: I'm OK with having changes which are improvements over the old kernels come in latter patches.\n<issue_comment>username_3: I finished reviewing one of the kernels, and did a cursory scan of the others. In general the patch looks like it's in good shape. I'd suggest that we fix up the major correctness issues (PackedTensorAccessor ref, and int32 versus int64 indexing) and anything small that is easy to fix (for example, double zeroing the array), and leave other improvements for follow ups. Should be ready to land soon!\n<issue_comment>username_0: thanks so much @username_3!\r\nI will let you know when it is ready again for a new review! thanks!\n<issue_comment>username_0: @username_2 @username_3 \r\nnot sure but the errors on CI seems to be related to jenkins ... it seems all these jobs that failed (for building) ran by 01h 01min ... not sure if it is a coincidence ...\n<issue_comment>username_1: @username_0 indeed it looks like all jobs were aborted at the same time. no obvious issues in the build log related to your code. Comparing e.g. the `cuda9-cudnn7` build with a successful one from another PR, it takes 1hr 14min there and your build is about at the place where the other one is after an hour.\r\n\r\nI suggest to just push a new commit to rebuild. Probably a temporary CI hiccup.\n<issue_comment>username_3: I accidentally rebooted Jenkins yesterday which is the likely cause, my apologies.\r\n\r\n@pytorchbot retest this please\n<issue_comment>username_0: @username_3 @username_2 @username_1 \r\n\r\nall tests passed except `pr/caffe2-py2-cuda9.0-cudnn7-windows-build`:\r\n \r\n```\r\n14:43:49 Build timed out (after 180 minutes). Marking the build as failed.\r\n14:43:49 Build was aborted\r\n14:43:49 [BFA] Scanning build for known causes...\r\n14:43:49 [BFA] No failure causes found\r\n14:43:49 [BFA] Done. 0s\r\n14:43:49 Finished: FAILURE\r\n```\r\n\r\nnot sure if this timeout means that the code now is slower.\r\n\r\nwhat do you think? do you have any suggestion?\n<issue_comment>username_3: Sometimes the Windows build flakes out like that. It didn't timeout while running a relevant test, so I judge it to be not your problem.\n<issue_comment>username_3: The launch bounds logic is wrong, but I acknowledge that this is a big patch already; just fix it in a follow up. I am going to go ahead and land this.\n<issue_comment>username_0: sounds good @username_3 thanks!"}
{"repo": "pytorch/pytorch", "issue_id": 436372857, "issue_number": 19630, "timestamp": "2019-05-03 18:57:24+00:00", "text": "@username_0 If you can't reproduce it at home it seems hard to debug other than reading at the diffs again.  I can take a look on Monday, it's getting a bit late here.", "context": "<issue_start><issue_comment>Title: [WIP] UpSample GPU Porting \nusername_0: resolves #16158\n<issue_comment>username_1: Fewer errors than before, 4 instead of 8 with the same message:\r\n```\r\nRuntimeError: CUDA error: too many resources requested for launch\r\n```\r\nThis seems to be an existing problem: gh-8103.\r\n\r\n@username_2 could you have a look at this and tell us what you think?\n<issue_comment>username_2: Looking at it very briefly, the `Jetson TX` referenced in the issue only has 256 cores.\r\n\r\nSo while the issue looks the same, I'd probably not expect it on the CI platforms.  FWIW, some recent CI tests in other issues are green.\n<issue_comment>username_2: @pytorchbot retest this please.\n<issue_comment>username_1: same failures.\n<issue_comment>username_2: Is this rebased on the latest master (you can also ask the bot to rebase)?\n<issue_comment>username_0: I rebased that yesterday ... also needed to resolve conflicts because 7 days ago upsample files were changed.\n<issue_comment>username_0: @username_2 do you have any idea about what could be this problem? or a way to debug that?\r\nalso it seems some jobs is taking a lot of time to build, for example, for some job the estimate time is 19h .. not sure if it the regular estimate ..\n<issue_comment>username_2: @username_0 If you can't reproduce it at home it seems hard to debug other than reading at the diffs again.  I can take a look on Monday, it's getting a bit late here.\n<issue_comment>username_2: Also, has anyone found a way to show the actual hardware used on the CI in detail?\n<issue_comment>username_0: @username_2 I am working in parallel on a paperspace environment .. it tooks a lot of time it is running with 8 cpu cores.\n<issue_comment>username_2: But have you ever been able to reproduce this issue on paperspace?\n<issue_comment>username_1: I can reproduce it locally: Arch Linux, CUDA 10.0, RTX2070 GPU. Given the CI failures, I think it should be reproducible for multiple CUDA and GPU versions. I just haven't worked on any CUDA code before, and am short on time, so I'd rather not dig too deep.\n<issue_comment>username_0: not yet .. my last building was with a my previous commit ... I will work again in this task in some minutes :)\n<issue_comment>username_1: @username_0 your previous commit had the same failure though (except for the less clear exception message), and it seems quite reproducible. So I think you'll see it now.\n<issue_comment>username_2: Ah, thanks.  @username_0, then I guess just compiling with \"DEBUG=1\" and stepping through the offending code with gdb may be the fastest way.\n<issue_comment>username_0: @username_2 thanks! I will try that!\n<issue_comment>username_0: it seems just UpSampleBicubic2d is using upsample_get_value_bounded (https://github.com/pytorch/pytorch/pull/19630/files#diff-5092da792c30694ee4adf0d0ae2a37c6R171) and upsample_increment_value_bounded (https://github.com/pytorch/pytorch/pull/19630/files#diff-5092da792c30694ee4adf0d0ae2a37c6R191)\r\n\r\nmaybe the problem could be inside one of these functions ... maybe related to the order of indexes (x, y) ...\r\n\r\nbut the problem seems to be related to cuda block/threads ... so not sure if it is really related to these functions.\n<issue_comment>username_2: The new code uses far more registers than the existing one. I verified that the both versions actually call the offending test case with `1024` in `blockDim`.  So it's very likely a register issue.\r\n\r\n`Existing` uses `64` registers, which seems to be optimal or my card:\r\n\r\n```\r\nptxas info    : 77696 bytes gmem, 72 bytes cmem[3]\r\nptxas info    : Compiling entry function '_Z23bicubic_interp2d_kerneliddb15THCDeviceTensorIdLi4Ei16DefaultPtrTraitsES1_' for 'sm_61'\r\nptxas info    : Function properties for _Z23bicubic_interp2d_kerneliddb15THCDeviceTensorIdLi4Ei16DefaultPtrTraitsES1_\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 64 registers, 432 bytes cmem[0], 24 bytes cmem[2]\r\n```\r\n\r\n`New` uses `124` registers, which is too much for my card:\r\n\r\n```\r\nptxas info    : 77696 bytes gmem, 72 bytes cmem[3]\r\nptxas info    : Compiling entry function '_ZN2at6native76_GLOBAL__N__52_tmpxft_0000626d_00000000_6_UpSampleBicubic2d_cpp1_ii_b4c1e1f328upsample_bicubic2d_out_frameElddbNS_20PackedTensorAccessorIdLm4ENS_16DefaultPtrTraitsElEES4_' for 'sm_61'\r\nptxas info    : Function properties for _ZN2at6native76_GLOBAL__N__52_tmpxft_0000626d_00000000_6_UpSampleBicubic2d_cpp1_ii_b4c1e1f328upsample_bicubic2d_out_frameElddbNS_20PackedTensorAccessorIdLm4ENS_16DefaultPtrTraitsElEES4_\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 124 registers, 496 bytes cmem[0], 24 bytes cmem[2]\r\n```\r\n\r\nWith `__launch_bounds__(1024)`, the code again uses `64` registers.\r\n\r\nIf you use `C10_LAUNCH_BOUNDS_1(1024)` **for both kernels**, the tests pass here.\r\n\r\n\r\nNow *why* is the regcount higher in the new code? It could be `PackedTensorAccessor`, it could be the fact that many instances of `int` have been changed to `int64_t`. :)\r\n\r\nYou could experiment or just use the launch bounds.  Other code in `native/cuda` seems to use lower bounds for `blockDim`, too.  1024 seems to be an outlier.\n<issue_comment>username_0: @username_2 \r\n\r\nit seems it worked locally! thank you so much!  I really appreciate that!\n<issue_comment>username_0: thanks @username_1 and @username_2 for all the help!\r\n\r\n@username_3 it is done for review!\n<issue_comment>username_3: CircleCI runs on AWS, and the I'm fairly sure we've got one of the g3 types, either g3.4xlarge or g3.8xlarge\n<issue_comment>username_3: cc @ngimel, if you want to take a look at this (I'm planning to do a review too)\n<issue_comment>username_0: @username_4 could it be addressed in another PR?\n<issue_comment>username_4: @username_0 Sure, it's definitely better to rename both cpu & gpu impls together in a separate PR. Just want to make sure we have the correct one after porting.\n<issue_comment>username_3: Just as a clarification: I'm OK with having changes which are improvements over the old kernels come in latter patches.\n<issue_comment>username_3: I finished reviewing one of the kernels, and did a cursory scan of the others. In general the patch looks like it's in good shape. I'd suggest that we fix up the major correctness issues (PackedTensorAccessor ref, and int32 versus int64 indexing) and anything small that is easy to fix (for example, double zeroing the array), and leave other improvements for follow ups. Should be ready to land soon!\n<issue_comment>username_0: thanks so much @username_3!\r\nI will let you know when it is ready again for a new review! thanks!\n<issue_comment>username_0: @username_2 @username_3 \r\nnot sure but the errors on CI seems to be related to jenkins ... it seems all these jobs that failed (for building) ran by 01h 01min ... not sure if it is a coincidence ...\n<issue_comment>username_1: @username_0 indeed it looks like all jobs were aborted at the same time. no obvious issues in the build log related to your code. Comparing e.g. the `cuda9-cudnn7` build with a successful one from another PR, it takes 1hr 14min there and your build is about at the place where the other one is after an hour.\r\n\r\nI suggest to just push a new commit to rebuild. Probably a temporary CI hiccup.\n<issue_comment>username_3: I accidentally rebooted Jenkins yesterday which is the likely cause, my apologies.\r\n\r\n@pytorchbot retest this please\n<issue_comment>username_0: @username_3 @username_2 @username_1 \r\n\r\nall tests passed except `pr/caffe2-py2-cuda9.0-cudnn7-windows-build`:\r\n \r\n```\r\n14:43:49 Build timed out (after 180 minutes). Marking the build as failed.\r\n14:43:49 Build was aborted\r\n14:43:49 [BFA] Scanning build for known causes...\r\n14:43:49 [BFA] No failure causes found\r\n14:43:49 [BFA] Done. 0s\r\n14:43:49 Finished: FAILURE\r\n```\r\n\r\nnot sure if this timeout means that the code now is slower.\r\n\r\nwhat do you think? do you have any suggestion?\n<issue_comment>username_3: Sometimes the Windows build flakes out like that. It didn't timeout while running a relevant test, so I judge it to be not your problem.\n<issue_comment>username_3: The launch bounds logic is wrong, but I acknowledge that this is a big patch already; just fix it in a follow up. I am going to go ahead and land this.\n<issue_comment>username_0: sounds good @username_3 thanks!"}
{"repo": "pytorch/pytorch", "issue_id": 436372857, "issue_number": 19630, "timestamp": "2019-05-03 18:58:19+00:00", "text": "Also, has anyone found a way to show the actual hardware used on the CI in detail?", "context": "<issue_start><issue_comment>Title: [WIP] UpSample GPU Porting \nusername_0: resolves #16158\n<issue_comment>username_1: Fewer errors than before, 4 instead of 8 with the same message:\r\n```\r\nRuntimeError: CUDA error: too many resources requested for launch\r\n```\r\nThis seems to be an existing problem: gh-8103.\r\n\r\n@username_2 could you have a look at this and tell us what you think?\n<issue_comment>username_2: Looking at it very briefly, the `Jetson TX` referenced in the issue only has 256 cores.\r\n\r\nSo while the issue looks the same, I'd probably not expect it on the CI platforms.  FWIW, some recent CI tests in other issues are green.\n<issue_comment>username_2: @pytorchbot retest this please.\n<issue_comment>username_1: same failures.\n<issue_comment>username_2: Is this rebased on the latest master (you can also ask the bot to rebase)?\n<issue_comment>username_0: I rebased that yesterday ... also needed to resolve conflicts because 7 days ago upsample files were changed.\n<issue_comment>username_0: @username_2 do you have any idea about what could be this problem? or a way to debug that?\r\nalso it seems some jobs is taking a lot of time to build, for example, for some job the estimate time is 19h .. not sure if it the regular estimate ..\n<issue_comment>username_2: @username_0 If you can't reproduce it at home it seems hard to debug other than reading at the diffs again.  I can take a look on Monday, it's getting a bit late here.\n<issue_comment>username_2: Also, has anyone found a way to show the actual hardware used on the CI in detail?\n<issue_comment>username_0: @username_2 I am working in parallel on a paperspace environment .. it tooks a lot of time it is running with 8 cpu cores.\n<issue_comment>username_2: But have you ever been able to reproduce this issue on paperspace?\n<issue_comment>username_1: I can reproduce it locally: Arch Linux, CUDA 10.0, RTX2070 GPU. Given the CI failures, I think it should be reproducible for multiple CUDA and GPU versions. I just haven't worked on any CUDA code before, and am short on time, so I'd rather not dig too deep.\n<issue_comment>username_0: not yet .. my last building was with a my previous commit ... I will work again in this task in some minutes :)\n<issue_comment>username_1: @username_0 your previous commit had the same failure though (except for the less clear exception message), and it seems quite reproducible. So I think you'll see it now.\n<issue_comment>username_2: Ah, thanks.  @username_0, then I guess just compiling with \"DEBUG=1\" and stepping through the offending code with gdb may be the fastest way.\n<issue_comment>username_0: @username_2 thanks! I will try that!\n<issue_comment>username_0: it seems just UpSampleBicubic2d is using upsample_get_value_bounded (https://github.com/pytorch/pytorch/pull/19630/files#diff-5092da792c30694ee4adf0d0ae2a37c6R171) and upsample_increment_value_bounded (https://github.com/pytorch/pytorch/pull/19630/files#diff-5092da792c30694ee4adf0d0ae2a37c6R191)\r\n\r\nmaybe the problem could be inside one of these functions ... maybe related to the order of indexes (x, y) ...\r\n\r\nbut the problem seems to be related to cuda block/threads ... so not sure if it is really related to these functions.\n<issue_comment>username_2: The new code uses far more registers than the existing one. I verified that the both versions actually call the offending test case with `1024` in `blockDim`.  So it's very likely a register issue.\r\n\r\n`Existing` uses `64` registers, which seems to be optimal or my card:\r\n\r\n```\r\nptxas info    : 77696 bytes gmem, 72 bytes cmem[3]\r\nptxas info    : Compiling entry function '_Z23bicubic_interp2d_kerneliddb15THCDeviceTensorIdLi4Ei16DefaultPtrTraitsES1_' for 'sm_61'\r\nptxas info    : Function properties for _Z23bicubic_interp2d_kerneliddb15THCDeviceTensorIdLi4Ei16DefaultPtrTraitsES1_\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 64 registers, 432 bytes cmem[0], 24 bytes cmem[2]\r\n```\r\n\r\n`New` uses `124` registers, which is too much for my card:\r\n\r\n```\r\nptxas info    : 77696 bytes gmem, 72 bytes cmem[3]\r\nptxas info    : Compiling entry function '_ZN2at6native76_GLOBAL__N__52_tmpxft_0000626d_00000000_6_UpSampleBicubic2d_cpp1_ii_b4c1e1f328upsample_bicubic2d_out_frameElddbNS_20PackedTensorAccessorIdLm4ENS_16DefaultPtrTraitsElEES4_' for 'sm_61'\r\nptxas info    : Function properties for _ZN2at6native76_GLOBAL__N__52_tmpxft_0000626d_00000000_6_UpSampleBicubic2d_cpp1_ii_b4c1e1f328upsample_bicubic2d_out_frameElddbNS_20PackedTensorAccessorIdLm4ENS_16DefaultPtrTraitsElEES4_\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 124 registers, 496 bytes cmem[0], 24 bytes cmem[2]\r\n```\r\n\r\nWith `__launch_bounds__(1024)`, the code again uses `64` registers.\r\n\r\nIf you use `C10_LAUNCH_BOUNDS_1(1024)` **for both kernels**, the tests pass here.\r\n\r\n\r\nNow *why* is the regcount higher in the new code? It could be `PackedTensorAccessor`, it could be the fact that many instances of `int` have been changed to `int64_t`. :)\r\n\r\nYou could experiment or just use the launch bounds.  Other code in `native/cuda` seems to use lower bounds for `blockDim`, too.  1024 seems to be an outlier.\n<issue_comment>username_0: @username_2 \r\n\r\nit seems it worked locally! thank you so much!  I really appreciate that!\n<issue_comment>username_0: thanks @username_1 and @username_2 for all the help!\r\n\r\n@username_3 it is done for review!\n<issue_comment>username_3: CircleCI runs on AWS, and the I'm fairly sure we've got one of the g3 types, either g3.4xlarge or g3.8xlarge\n<issue_comment>username_3: cc @ngimel, if you want to take a look at this (I'm planning to do a review too)\n<issue_comment>username_0: @username_4 could it be addressed in another PR?\n<issue_comment>username_4: @username_0 Sure, it's definitely better to rename both cpu & gpu impls together in a separate PR. Just want to make sure we have the correct one after porting.\n<issue_comment>username_3: Just as a clarification: I'm OK with having changes which are improvements over the old kernels come in latter patches.\n<issue_comment>username_3: I finished reviewing one of the kernels, and did a cursory scan of the others. In general the patch looks like it's in good shape. I'd suggest that we fix up the major correctness issues (PackedTensorAccessor ref, and int32 versus int64 indexing) and anything small that is easy to fix (for example, double zeroing the array), and leave other improvements for follow ups. Should be ready to land soon!\n<issue_comment>username_0: thanks so much @username_3!\r\nI will let you know when it is ready again for a new review! thanks!\n<issue_comment>username_0: @username_2 @username_3 \r\nnot sure but the errors on CI seems to be related to jenkins ... it seems all these jobs that failed (for building) ran by 01h 01min ... not sure if it is a coincidence ...\n<issue_comment>username_1: @username_0 indeed it looks like all jobs were aborted at the same time. no obvious issues in the build log related to your code. Comparing e.g. the `cuda9-cudnn7` build with a successful one from another PR, it takes 1hr 14min there and your build is about at the place where the other one is after an hour.\r\n\r\nI suggest to just push a new commit to rebuild. Probably a temporary CI hiccup.\n<issue_comment>username_3: I accidentally rebooted Jenkins yesterday which is the likely cause, my apologies.\r\n\r\n@pytorchbot retest this please\n<issue_comment>username_0: @username_3 @username_2 @username_1 \r\n\r\nall tests passed except `pr/caffe2-py2-cuda9.0-cudnn7-windows-build`:\r\n \r\n```\r\n14:43:49 Build timed out (after 180 minutes). Marking the build as failed.\r\n14:43:49 Build was aborted\r\n14:43:49 [BFA] Scanning build for known causes...\r\n14:43:49 [BFA] No failure causes found\r\n14:43:49 [BFA] Done. 0s\r\n14:43:49 Finished: FAILURE\r\n```\r\n\r\nnot sure if this timeout means that the code now is slower.\r\n\r\nwhat do you think? do you have any suggestion?\n<issue_comment>username_3: Sometimes the Windows build flakes out like that. It didn't timeout while running a relevant test, so I judge it to be not your problem.\n<issue_comment>username_3: The launch bounds logic is wrong, but I acknowledge that this is a big patch already; just fix it in a follow up. I am going to go ahead and land this.\n<issue_comment>username_0: sounds good @username_3 thanks!"}
{"repo": "pytorch/pytorch", "issue_id": 436372857, "issue_number": 19630, "timestamp": "2019-05-03 19:11:40+00:00", "text": "@username_2 I am working in parallel on a paperspace environment .. it tooks a lot of time it is running with 8 cpu cores.", "context": "<issue_start><issue_comment>Title: [WIP] UpSample GPU Porting \nusername_0: resolves #16158\n<issue_comment>username_1: Fewer errors than before, 4 instead of 8 with the same message:\r\n```\r\nRuntimeError: CUDA error: too many resources requested for launch\r\n```\r\nThis seems to be an existing problem: gh-8103.\r\n\r\n@username_2 could you have a look at this and tell us what you think?\n<issue_comment>username_2: Looking at it very briefly, the `Jetson TX` referenced in the issue only has 256 cores.\r\n\r\nSo while the issue looks the same, I'd probably not expect it on the CI platforms.  FWIW, some recent CI tests in other issues are green.\n<issue_comment>username_2: @pytorchbot retest this please.\n<issue_comment>username_1: same failures.\n<issue_comment>username_2: Is this rebased on the latest master (you can also ask the bot to rebase)?\n<issue_comment>username_0: I rebased that yesterday ... also needed to resolve conflicts because 7 days ago upsample files were changed.\n<issue_comment>username_0: @username_2 do you have any idea about what could be this problem? or a way to debug that?\r\nalso it seems some jobs is taking a lot of time to build, for example, for some job the estimate time is 19h .. not sure if it the regular estimate ..\n<issue_comment>username_2: @username_0 If you can't reproduce it at home it seems hard to debug other than reading at the diffs again.  I can take a look on Monday, it's getting a bit late here.\n<issue_comment>username_2: Also, has anyone found a way to show the actual hardware used on the CI in detail?\n<issue_comment>username_0: @username_2 I am working in parallel on a paperspace environment .. it tooks a lot of time it is running with 8 cpu cores.\n<issue_comment>username_2: But have you ever been able to reproduce this issue on paperspace?\n<issue_comment>username_1: I can reproduce it locally: Arch Linux, CUDA 10.0, RTX2070 GPU. Given the CI failures, I think it should be reproducible for multiple CUDA and GPU versions. I just haven't worked on any CUDA code before, and am short on time, so I'd rather not dig too deep.\n<issue_comment>username_0: not yet .. my last building was with a my previous commit ... I will work again in this task in some minutes :)\n<issue_comment>username_1: @username_0 your previous commit had the same failure though (except for the less clear exception message), and it seems quite reproducible. So I think you'll see it now.\n<issue_comment>username_2: Ah, thanks.  @username_0, then I guess just compiling with \"DEBUG=1\" and stepping through the offending code with gdb may be the fastest way.\n<issue_comment>username_0: @username_2 thanks! I will try that!\n<issue_comment>username_0: it seems just UpSampleBicubic2d is using upsample_get_value_bounded (https://github.com/pytorch/pytorch/pull/19630/files#diff-5092da792c30694ee4adf0d0ae2a37c6R171) and upsample_increment_value_bounded (https://github.com/pytorch/pytorch/pull/19630/files#diff-5092da792c30694ee4adf0d0ae2a37c6R191)\r\n\r\nmaybe the problem could be inside one of these functions ... maybe related to the order of indexes (x, y) ...\r\n\r\nbut the problem seems to be related to cuda block/threads ... so not sure if it is really related to these functions.\n<issue_comment>username_2: The new code uses far more registers than the existing one. I verified that the both versions actually call the offending test case with `1024` in `blockDim`.  So it's very likely a register issue.\r\n\r\n`Existing` uses `64` registers, which seems to be optimal or my card:\r\n\r\n```\r\nptxas info    : 77696 bytes gmem, 72 bytes cmem[3]\r\nptxas info    : Compiling entry function '_Z23bicubic_interp2d_kerneliddb15THCDeviceTensorIdLi4Ei16DefaultPtrTraitsES1_' for 'sm_61'\r\nptxas info    : Function properties for _Z23bicubic_interp2d_kerneliddb15THCDeviceTensorIdLi4Ei16DefaultPtrTraitsES1_\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 64 registers, 432 bytes cmem[0], 24 bytes cmem[2]\r\n```\r\n\r\n`New` uses `124` registers, which is too much for my card:\r\n\r\n```\r\nptxas info    : 77696 bytes gmem, 72 bytes cmem[3]\r\nptxas info    : Compiling entry function '_ZN2at6native76_GLOBAL__N__52_tmpxft_0000626d_00000000_6_UpSampleBicubic2d_cpp1_ii_b4c1e1f328upsample_bicubic2d_out_frameElddbNS_20PackedTensorAccessorIdLm4ENS_16DefaultPtrTraitsElEES4_' for 'sm_61'\r\nptxas info    : Function properties for _ZN2at6native76_GLOBAL__N__52_tmpxft_0000626d_00000000_6_UpSampleBicubic2d_cpp1_ii_b4c1e1f328upsample_bicubic2d_out_frameElddbNS_20PackedTensorAccessorIdLm4ENS_16DefaultPtrTraitsElEES4_\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 124 registers, 496 bytes cmem[0], 24 bytes cmem[2]\r\n```\r\n\r\nWith `__launch_bounds__(1024)`, the code again uses `64` registers.\r\n\r\nIf you use `C10_LAUNCH_BOUNDS_1(1024)` **for both kernels**, the tests pass here.\r\n\r\n\r\nNow *why* is the regcount higher in the new code? It could be `PackedTensorAccessor`, it could be the fact that many instances of `int` have been changed to `int64_t`. :)\r\n\r\nYou could experiment or just use the launch bounds.  Other code in `native/cuda` seems to use lower bounds for `blockDim`, too.  1024 seems to be an outlier.\n<issue_comment>username_0: @username_2 \r\n\r\nit seems it worked locally! thank you so much!  I really appreciate that!\n<issue_comment>username_0: thanks @username_1 and @username_2 for all the help!\r\n\r\n@username_3 it is done for review!\n<issue_comment>username_3: CircleCI runs on AWS, and the I'm fairly sure we've got one of the g3 types, either g3.4xlarge or g3.8xlarge\n<issue_comment>username_3: cc @ngimel, if you want to take a look at this (I'm planning to do a review too)\n<issue_comment>username_0: @username_4 could it be addressed in another PR?\n<issue_comment>username_4: @username_0 Sure, it's definitely better to rename both cpu & gpu impls together in a separate PR. Just want to make sure we have the correct one after porting.\n<issue_comment>username_3: Just as a clarification: I'm OK with having changes which are improvements over the old kernels come in latter patches.\n<issue_comment>username_3: I finished reviewing one of the kernels, and did a cursory scan of the others. In general the patch looks like it's in good shape. I'd suggest that we fix up the major correctness issues (PackedTensorAccessor ref, and int32 versus int64 indexing) and anything small that is easy to fix (for example, double zeroing the array), and leave other improvements for follow ups. Should be ready to land soon!\n<issue_comment>username_0: thanks so much @username_3!\r\nI will let you know when it is ready again for a new review! thanks!\n<issue_comment>username_0: @username_2 @username_3 \r\nnot sure but the errors on CI seems to be related to jenkins ... it seems all these jobs that failed (for building) ran by 01h 01min ... not sure if it is a coincidence ...\n<issue_comment>username_1: @username_0 indeed it looks like all jobs were aborted at the same time. no obvious issues in the build log related to your code. Comparing e.g. the `cuda9-cudnn7` build with a successful one from another PR, it takes 1hr 14min there and your build is about at the place where the other one is after an hour.\r\n\r\nI suggest to just push a new commit to rebuild. Probably a temporary CI hiccup.\n<issue_comment>username_3: I accidentally rebooted Jenkins yesterday which is the likely cause, my apologies.\r\n\r\n@pytorchbot retest this please\n<issue_comment>username_0: @username_3 @username_2 @username_1 \r\n\r\nall tests passed except `pr/caffe2-py2-cuda9.0-cudnn7-windows-build`:\r\n \r\n```\r\n14:43:49 Build timed out (after 180 minutes). Marking the build as failed.\r\n14:43:49 Build was aborted\r\n14:43:49 [BFA] Scanning build for known causes...\r\n14:43:49 [BFA] No failure causes found\r\n14:43:49 [BFA] Done. 0s\r\n14:43:49 Finished: FAILURE\r\n```\r\n\r\nnot sure if this timeout means that the code now is slower.\r\n\r\nwhat do you think? do you have any suggestion?\n<issue_comment>username_3: Sometimes the Windows build flakes out like that. It didn't timeout while running a relevant test, so I judge it to be not your problem.\n<issue_comment>username_3: The launch bounds logic is wrong, but I acknowledge that this is a big patch already; just fix it in a follow up. I am going to go ahead and land this.\n<issue_comment>username_0: sounds good @username_3 thanks!"}
{"repo": "pytorch/pytorch", "issue_id": 436372857, "issue_number": 19630, "timestamp": "2019-05-03 19:16:06+00:00", "text": "But have you ever been able to reproduce this issue on paperspace?", "context": "<issue_start><issue_comment>Title: [WIP] UpSample GPU Porting \nusername_0: resolves #16158\n<issue_comment>username_1: Fewer errors than before, 4 instead of 8 with the same message:\r\n```\r\nRuntimeError: CUDA error: too many resources requested for launch\r\n```\r\nThis seems to be an existing problem: gh-8103.\r\n\r\n@username_2 could you have a look at this and tell us what you think?\n<issue_comment>username_2: Looking at it very briefly, the `Jetson TX` referenced in the issue only has 256 cores.\r\n\r\nSo while the issue looks the same, I'd probably not expect it on the CI platforms.  FWIW, some recent CI tests in other issues are green.\n<issue_comment>username_2: @pytorchbot retest this please.\n<issue_comment>username_1: same failures.\n<issue_comment>username_2: Is this rebased on the latest master (you can also ask the bot to rebase)?\n<issue_comment>username_0: I rebased that yesterday ... also needed to resolve conflicts because 7 days ago upsample files were changed.\n<issue_comment>username_0: @username_2 do you have any idea about what could be this problem? or a way to debug that?\r\nalso it seems some jobs is taking a lot of time to build, for example, for some job the estimate time is 19h .. not sure if it the regular estimate ..\n<issue_comment>username_2: @username_0 If you can't reproduce it at home it seems hard to debug other than reading at the diffs again.  I can take a look on Monday, it's getting a bit late here.\n<issue_comment>username_2: Also, has anyone found a way to show the actual hardware used on the CI in detail?\n<issue_comment>username_0: @username_2 I am working in parallel on a paperspace environment .. it tooks a lot of time it is running with 8 cpu cores.\n<issue_comment>username_2: But have you ever been able to reproduce this issue on paperspace?\n<issue_comment>username_1: I can reproduce it locally: Arch Linux, CUDA 10.0, RTX2070 GPU. Given the CI failures, I think it should be reproducible for multiple CUDA and GPU versions. I just haven't worked on any CUDA code before, and am short on time, so I'd rather not dig too deep.\n<issue_comment>username_0: not yet .. my last building was with a my previous commit ... I will work again in this task in some minutes :)\n<issue_comment>username_1: @username_0 your previous commit had the same failure though (except for the less clear exception message), and it seems quite reproducible. So I think you'll see it now.\n<issue_comment>username_2: Ah, thanks.  @username_0, then I guess just compiling with \"DEBUG=1\" and stepping through the offending code with gdb may be the fastest way.\n<issue_comment>username_0: @username_2 thanks! I will try that!\n<issue_comment>username_0: it seems just UpSampleBicubic2d is using upsample_get_value_bounded (https://github.com/pytorch/pytorch/pull/19630/files#diff-5092da792c30694ee4adf0d0ae2a37c6R171) and upsample_increment_value_bounded (https://github.com/pytorch/pytorch/pull/19630/files#diff-5092da792c30694ee4adf0d0ae2a37c6R191)\r\n\r\nmaybe the problem could be inside one of these functions ... maybe related to the order of indexes (x, y) ...\r\n\r\nbut the problem seems to be related to cuda block/threads ... so not sure if it is really related to these functions.\n<issue_comment>username_2: The new code uses far more registers than the existing one. I verified that the both versions actually call the offending test case with `1024` in `blockDim`.  So it's very likely a register issue.\r\n\r\n`Existing` uses `64` registers, which seems to be optimal or my card:\r\n\r\n```\r\nptxas info    : 77696 bytes gmem, 72 bytes cmem[3]\r\nptxas info    : Compiling entry function '_Z23bicubic_interp2d_kerneliddb15THCDeviceTensorIdLi4Ei16DefaultPtrTraitsES1_' for 'sm_61'\r\nptxas info    : Function properties for _Z23bicubic_interp2d_kerneliddb15THCDeviceTensorIdLi4Ei16DefaultPtrTraitsES1_\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 64 registers, 432 bytes cmem[0], 24 bytes cmem[2]\r\n```\r\n\r\n`New` uses `124` registers, which is too much for my card:\r\n\r\n```\r\nptxas info    : 77696 bytes gmem, 72 bytes cmem[3]\r\nptxas info    : Compiling entry function '_ZN2at6native76_GLOBAL__N__52_tmpxft_0000626d_00000000_6_UpSampleBicubic2d_cpp1_ii_b4c1e1f328upsample_bicubic2d_out_frameElddbNS_20PackedTensorAccessorIdLm4ENS_16DefaultPtrTraitsElEES4_' for 'sm_61'\r\nptxas info    : Function properties for _ZN2at6native76_GLOBAL__N__52_tmpxft_0000626d_00000000_6_UpSampleBicubic2d_cpp1_ii_b4c1e1f328upsample_bicubic2d_out_frameElddbNS_20PackedTensorAccessorIdLm4ENS_16DefaultPtrTraitsElEES4_\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 124 registers, 496 bytes cmem[0], 24 bytes cmem[2]\r\n```\r\n\r\nWith `__launch_bounds__(1024)`, the code again uses `64` registers.\r\n\r\nIf you use `C10_LAUNCH_BOUNDS_1(1024)` **for both kernels**, the tests pass here.\r\n\r\n\r\nNow *why* is the regcount higher in the new code? It could be `PackedTensorAccessor`, it could be the fact that many instances of `int` have been changed to `int64_t`. :)\r\n\r\nYou could experiment or just use the launch bounds.  Other code in `native/cuda` seems to use lower bounds for `blockDim`, too.  1024 seems to be an outlier.\n<issue_comment>username_0: @username_2 \r\n\r\nit seems it worked locally! thank you so much!  I really appreciate that!\n<issue_comment>username_0: thanks @username_1 and @username_2 for all the help!\r\n\r\n@username_3 it is done for review!\n<issue_comment>username_3: CircleCI runs on AWS, and the I'm fairly sure we've got one of the g3 types, either g3.4xlarge or g3.8xlarge\n<issue_comment>username_3: cc @ngimel, if you want to take a look at this (I'm planning to do a review too)\n<issue_comment>username_0: @username_4 could it be addressed in another PR?\n<issue_comment>username_4: @username_0 Sure, it's definitely better to rename both cpu & gpu impls together in a separate PR. Just want to make sure we have the correct one after porting.\n<issue_comment>username_3: Just as a clarification: I'm OK with having changes which are improvements over the old kernels come in latter patches.\n<issue_comment>username_3: I finished reviewing one of the kernels, and did a cursory scan of the others. In general the patch looks like it's in good shape. I'd suggest that we fix up the major correctness issues (PackedTensorAccessor ref, and int32 versus int64 indexing) and anything small that is easy to fix (for example, double zeroing the array), and leave other improvements for follow ups. Should be ready to land soon!\n<issue_comment>username_0: thanks so much @username_3!\r\nI will let you know when it is ready again for a new review! thanks!\n<issue_comment>username_0: @username_2 @username_3 \r\nnot sure but the errors on CI seems to be related to jenkins ... it seems all these jobs that failed (for building) ran by 01h 01min ... not sure if it is a coincidence ...\n<issue_comment>username_1: @username_0 indeed it looks like all jobs were aborted at the same time. no obvious issues in the build log related to your code. Comparing e.g. the `cuda9-cudnn7` build with a successful one from another PR, it takes 1hr 14min there and your build is about at the place where the other one is after an hour.\r\n\r\nI suggest to just push a new commit to rebuild. Probably a temporary CI hiccup.\n<issue_comment>username_3: I accidentally rebooted Jenkins yesterday which is the likely cause, my apologies.\r\n\r\n@pytorchbot retest this please\n<issue_comment>username_0: @username_3 @username_2 @username_1 \r\n\r\nall tests passed except `pr/caffe2-py2-cuda9.0-cudnn7-windows-build`:\r\n \r\n```\r\n14:43:49 Build timed out (after 180 minutes). Marking the build as failed.\r\n14:43:49 Build was aborted\r\n14:43:49 [BFA] Scanning build for known causes...\r\n14:43:49 [BFA] No failure causes found\r\n14:43:49 [BFA] Done. 0s\r\n14:43:49 Finished: FAILURE\r\n```\r\n\r\nnot sure if this timeout means that the code now is slower.\r\n\r\nwhat do you think? do you have any suggestion?\n<issue_comment>username_3: Sometimes the Windows build flakes out like that. It didn't timeout while running a relevant test, so I judge it to be not your problem.\n<issue_comment>username_3: The launch bounds logic is wrong, but I acknowledge that this is a big patch already; just fix it in a follow up. I am going to go ahead and land this.\n<issue_comment>username_0: sounds good @username_3 thanks!"}
{"repo": "pytorch/pytorch", "issue_id": 436372857, "issue_number": 19630, "timestamp": "2019-05-03 19:18:45+00:00", "text": "I can reproduce it locally: Arch Linux, CUDA 10.0, RTX2070 GPU. Given the CI failures, I think it should be reproducible for multiple CUDA and GPU versions. I just haven't worked on any CUDA code before, and am short on time, so I'd rather not dig too deep.", "context": "<issue_start><issue_comment>Title: [WIP] UpSample GPU Porting \nusername_0: resolves #16158\n<issue_comment>username_1: Fewer errors than before, 4 instead of 8 with the same message:\r\n```\r\nRuntimeError: CUDA error: too many resources requested for launch\r\n```\r\nThis seems to be an existing problem: gh-8103.\r\n\r\n@username_2 could you have a look at this and tell us what you think?\n<issue_comment>username_2: Looking at it very briefly, the `Jetson TX` referenced in the issue only has 256 cores.\r\n\r\nSo while the issue looks the same, I'd probably not expect it on the CI platforms.  FWIW, some recent CI tests in other issues are green.\n<issue_comment>username_2: @pytorchbot retest this please.\n<issue_comment>username_1: same failures.\n<issue_comment>username_2: Is this rebased on the latest master (you can also ask the bot to rebase)?\n<issue_comment>username_0: I rebased that yesterday ... also needed to resolve conflicts because 7 days ago upsample files were changed.\n<issue_comment>username_0: @username_2 do you have any idea about what could be this problem? or a way to debug that?\r\nalso it seems some jobs is taking a lot of time to build, for example, for some job the estimate time is 19h .. not sure if it the regular estimate ..\n<issue_comment>username_2: @username_0 If you can't reproduce it at home it seems hard to debug other than reading at the diffs again.  I can take a look on Monday, it's getting a bit late here.\n<issue_comment>username_2: Also, has anyone found a way to show the actual hardware used on the CI in detail?\n<issue_comment>username_0: @username_2 I am working in parallel on a paperspace environment .. it tooks a lot of time it is running with 8 cpu cores.\n<issue_comment>username_2: But have you ever been able to reproduce this issue on paperspace?\n<issue_comment>username_1: I can reproduce it locally: Arch Linux, CUDA 10.0, RTX2070 GPU. Given the CI failures, I think it should be reproducible for multiple CUDA and GPU versions. I just haven't worked on any CUDA code before, and am short on time, so I'd rather not dig too deep.\n<issue_comment>username_0: not yet .. my last building was with a my previous commit ... I will work again in this task in some minutes :)\n<issue_comment>username_1: @username_0 your previous commit had the same failure though (except for the less clear exception message), and it seems quite reproducible. So I think you'll see it now.\n<issue_comment>username_2: Ah, thanks.  @username_0, then I guess just compiling with \"DEBUG=1\" and stepping through the offending code with gdb may be the fastest way.\n<issue_comment>username_0: @username_2 thanks! I will try that!\n<issue_comment>username_0: it seems just UpSampleBicubic2d is using upsample_get_value_bounded (https://github.com/pytorch/pytorch/pull/19630/files#diff-5092da792c30694ee4adf0d0ae2a37c6R171) and upsample_increment_value_bounded (https://github.com/pytorch/pytorch/pull/19630/files#diff-5092da792c30694ee4adf0d0ae2a37c6R191)\r\n\r\nmaybe the problem could be inside one of these functions ... maybe related to the order of indexes (x, y) ...\r\n\r\nbut the problem seems to be related to cuda block/threads ... so not sure if it is really related to these functions.\n<issue_comment>username_2: The new code uses far more registers than the existing one. I verified that the both versions actually call the offending test case with `1024` in `blockDim`.  So it's very likely a register issue.\r\n\r\n`Existing` uses `64` registers, which seems to be optimal or my card:\r\n\r\n```\r\nptxas info    : 77696 bytes gmem, 72 bytes cmem[3]\r\nptxas info    : Compiling entry function '_Z23bicubic_interp2d_kerneliddb15THCDeviceTensorIdLi4Ei16DefaultPtrTraitsES1_' for 'sm_61'\r\nptxas info    : Function properties for _Z23bicubic_interp2d_kerneliddb15THCDeviceTensorIdLi4Ei16DefaultPtrTraitsES1_\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 64 registers, 432 bytes cmem[0], 24 bytes cmem[2]\r\n```\r\n\r\n`New` uses `124` registers, which is too much for my card:\r\n\r\n```\r\nptxas info    : 77696 bytes gmem, 72 bytes cmem[3]\r\nptxas info    : Compiling entry function '_ZN2at6native76_GLOBAL__N__52_tmpxft_0000626d_00000000_6_UpSampleBicubic2d_cpp1_ii_b4c1e1f328upsample_bicubic2d_out_frameElddbNS_20PackedTensorAccessorIdLm4ENS_16DefaultPtrTraitsElEES4_' for 'sm_61'\r\nptxas info    : Function properties for _ZN2at6native76_GLOBAL__N__52_tmpxft_0000626d_00000000_6_UpSampleBicubic2d_cpp1_ii_b4c1e1f328upsample_bicubic2d_out_frameElddbNS_20PackedTensorAccessorIdLm4ENS_16DefaultPtrTraitsElEES4_\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 124 registers, 496 bytes cmem[0], 24 bytes cmem[2]\r\n```\r\n\r\nWith `__launch_bounds__(1024)`, the code again uses `64` registers.\r\n\r\nIf you use `C10_LAUNCH_BOUNDS_1(1024)` **for both kernels**, the tests pass here.\r\n\r\n\r\nNow *why* is the regcount higher in the new code? It could be `PackedTensorAccessor`, it could be the fact that many instances of `int` have been changed to `int64_t`. :)\r\n\r\nYou could experiment or just use the launch bounds.  Other code in `native/cuda` seems to use lower bounds for `blockDim`, too.  1024 seems to be an outlier.\n<issue_comment>username_0: @username_2 \r\n\r\nit seems it worked locally! thank you so much!  I really appreciate that!\n<issue_comment>username_0: thanks @username_1 and @username_2 for all the help!\r\n\r\n@username_3 it is done for review!\n<issue_comment>username_3: CircleCI runs on AWS, and the I'm fairly sure we've got one of the g3 types, either g3.4xlarge or g3.8xlarge\n<issue_comment>username_3: cc @ngimel, if you want to take a look at this (I'm planning to do a review too)\n<issue_comment>username_0: @username_4 could it be addressed in another PR?\n<issue_comment>username_4: @username_0 Sure, it's definitely better to rename both cpu & gpu impls together in a separate PR. Just want to make sure we have the correct one after porting.\n<issue_comment>username_3: Just as a clarification: I'm OK with having changes which are improvements over the old kernels come in latter patches.\n<issue_comment>username_3: I finished reviewing one of the kernels, and did a cursory scan of the others. In general the patch looks like it's in good shape. I'd suggest that we fix up the major correctness issues (PackedTensorAccessor ref, and int32 versus int64 indexing) and anything small that is easy to fix (for example, double zeroing the array), and leave other improvements for follow ups. Should be ready to land soon!\n<issue_comment>username_0: thanks so much @username_3!\r\nI will let you know when it is ready again for a new review! thanks!\n<issue_comment>username_0: @username_2 @username_3 \r\nnot sure but the errors on CI seems to be related to jenkins ... it seems all these jobs that failed (for building) ran by 01h 01min ... not sure if it is a coincidence ...\n<issue_comment>username_1: @username_0 indeed it looks like all jobs were aborted at the same time. no obvious issues in the build log related to your code. Comparing e.g. the `cuda9-cudnn7` build with a successful one from another PR, it takes 1hr 14min there and your build is about at the place where the other one is after an hour.\r\n\r\nI suggest to just push a new commit to rebuild. Probably a temporary CI hiccup.\n<issue_comment>username_3: I accidentally rebooted Jenkins yesterday which is the likely cause, my apologies.\r\n\r\n@pytorchbot retest this please\n<issue_comment>username_0: @username_3 @username_2 @username_1 \r\n\r\nall tests passed except `pr/caffe2-py2-cuda9.0-cudnn7-windows-build`:\r\n \r\n```\r\n14:43:49 Build timed out (after 180 minutes). Marking the build as failed.\r\n14:43:49 Build was aborted\r\n14:43:49 [BFA] Scanning build for known causes...\r\n14:43:49 [BFA] No failure causes found\r\n14:43:49 [BFA] Done. 0s\r\n14:43:49 Finished: FAILURE\r\n```\r\n\r\nnot sure if this timeout means that the code now is slower.\r\n\r\nwhat do you think? do you have any suggestion?\n<issue_comment>username_3: Sometimes the Windows build flakes out like that. It didn't timeout while running a relevant test, so I judge it to be not your problem.\n<issue_comment>username_3: The launch bounds logic is wrong, but I acknowledge that this is a big patch already; just fix it in a follow up. I am going to go ahead and land this.\n<issue_comment>username_0: sounds good @username_3 thanks!"}
{"repo": "pytorch/pytorch", "issue_id": 436372857, "issue_number": 19630, "timestamp": "2019-05-03 19:19:51+00:00", "text": "not yet .. my last building was with a my previous commit ... I will work again in this task in some minutes :)", "context": "<issue_start><issue_comment>Title: [WIP] UpSample GPU Porting \nusername_0: resolves #16158\n<issue_comment>username_1: Fewer errors than before, 4 instead of 8 with the same message:\r\n```\r\nRuntimeError: CUDA error: too many resources requested for launch\r\n```\r\nThis seems to be an existing problem: gh-8103.\r\n\r\n@username_2 could you have a look at this and tell us what you think?\n<issue_comment>username_2: Looking at it very briefly, the `Jetson TX` referenced in the issue only has 256 cores.\r\n\r\nSo while the issue looks the same, I'd probably not expect it on the CI platforms.  FWIW, some recent CI tests in other issues are green.\n<issue_comment>username_2: @pytorchbot retest this please.\n<issue_comment>username_1: same failures.\n<issue_comment>username_2: Is this rebased on the latest master (you can also ask the bot to rebase)?\n<issue_comment>username_0: I rebased that yesterday ... also needed to resolve conflicts because 7 days ago upsample files were changed.\n<issue_comment>username_0: @username_2 do you have any idea about what could be this problem? or a way to debug that?\r\nalso it seems some jobs is taking a lot of time to build, for example, for some job the estimate time is 19h .. not sure if it the regular estimate ..\n<issue_comment>username_2: @username_0 If you can't reproduce it at home it seems hard to debug other than reading at the diffs again.  I can take a look on Monday, it's getting a bit late here.\n<issue_comment>username_2: Also, has anyone found a way to show the actual hardware used on the CI in detail?\n<issue_comment>username_0: @username_2 I am working in parallel on a paperspace environment .. it tooks a lot of time it is running with 8 cpu cores.\n<issue_comment>username_2: But have you ever been able to reproduce this issue on paperspace?\n<issue_comment>username_1: I can reproduce it locally: Arch Linux, CUDA 10.0, RTX2070 GPU. Given the CI failures, I think it should be reproducible for multiple CUDA and GPU versions. I just haven't worked on any CUDA code before, and am short on time, so I'd rather not dig too deep.\n<issue_comment>username_0: not yet .. my last building was with a my previous commit ... I will work again in this task in some minutes :)\n<issue_comment>username_1: @username_0 your previous commit had the same failure though (except for the less clear exception message), and it seems quite reproducible. So I think you'll see it now.\n<issue_comment>username_2: Ah, thanks.  @username_0, then I guess just compiling with \"DEBUG=1\" and stepping through the offending code with gdb may be the fastest way.\n<issue_comment>username_0: @username_2 thanks! I will try that!\n<issue_comment>username_0: it seems just UpSampleBicubic2d is using upsample_get_value_bounded (https://github.com/pytorch/pytorch/pull/19630/files#diff-5092da792c30694ee4adf0d0ae2a37c6R171) and upsample_increment_value_bounded (https://github.com/pytorch/pytorch/pull/19630/files#diff-5092da792c30694ee4adf0d0ae2a37c6R191)\r\n\r\nmaybe the problem could be inside one of these functions ... maybe related to the order of indexes (x, y) ...\r\n\r\nbut the problem seems to be related to cuda block/threads ... so not sure if it is really related to these functions.\n<issue_comment>username_2: The new code uses far more registers than the existing one. I verified that the both versions actually call the offending test case with `1024` in `blockDim`.  So it's very likely a register issue.\r\n\r\n`Existing` uses `64` registers, which seems to be optimal or my card:\r\n\r\n```\r\nptxas info    : 77696 bytes gmem, 72 bytes cmem[3]\r\nptxas info    : Compiling entry function '_Z23bicubic_interp2d_kerneliddb15THCDeviceTensorIdLi4Ei16DefaultPtrTraitsES1_' for 'sm_61'\r\nptxas info    : Function properties for _Z23bicubic_interp2d_kerneliddb15THCDeviceTensorIdLi4Ei16DefaultPtrTraitsES1_\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 64 registers, 432 bytes cmem[0], 24 bytes cmem[2]\r\n```\r\n\r\n`New` uses `124` registers, which is too much for my card:\r\n\r\n```\r\nptxas info    : 77696 bytes gmem, 72 bytes cmem[3]\r\nptxas info    : Compiling entry function '_ZN2at6native76_GLOBAL__N__52_tmpxft_0000626d_00000000_6_UpSampleBicubic2d_cpp1_ii_b4c1e1f328upsample_bicubic2d_out_frameElddbNS_20PackedTensorAccessorIdLm4ENS_16DefaultPtrTraitsElEES4_' for 'sm_61'\r\nptxas info    : Function properties for _ZN2at6native76_GLOBAL__N__52_tmpxft_0000626d_00000000_6_UpSampleBicubic2d_cpp1_ii_b4c1e1f328upsample_bicubic2d_out_frameElddbNS_20PackedTensorAccessorIdLm4ENS_16DefaultPtrTraitsElEES4_\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 124 registers, 496 bytes cmem[0], 24 bytes cmem[2]\r\n```\r\n\r\nWith `__launch_bounds__(1024)`, the code again uses `64` registers.\r\n\r\nIf you use `C10_LAUNCH_BOUNDS_1(1024)` **for both kernels**, the tests pass here.\r\n\r\n\r\nNow *why* is the regcount higher in the new code? It could be `PackedTensorAccessor`, it could be the fact that many instances of `int` have been changed to `int64_t`. :)\r\n\r\nYou could experiment or just use the launch bounds.  Other code in `native/cuda` seems to use lower bounds for `blockDim`, too.  1024 seems to be an outlier.\n<issue_comment>username_0: @username_2 \r\n\r\nit seems it worked locally! thank you so much!  I really appreciate that!\n<issue_comment>username_0: thanks @username_1 and @username_2 for all the help!\r\n\r\n@username_3 it is done for review!\n<issue_comment>username_3: CircleCI runs on AWS, and the I'm fairly sure we've got one of the g3 types, either g3.4xlarge or g3.8xlarge\n<issue_comment>username_3: cc @ngimel, if you want to take a look at this (I'm planning to do a review too)\n<issue_comment>username_0: @username_4 could it be addressed in another PR?\n<issue_comment>username_4: @username_0 Sure, it's definitely better to rename both cpu & gpu impls together in a separate PR. Just want to make sure we have the correct one after porting.\n<issue_comment>username_3: Just as a clarification: I'm OK with having changes which are improvements over the old kernels come in latter patches.\n<issue_comment>username_3: I finished reviewing one of the kernels, and did a cursory scan of the others. In general the patch looks like it's in good shape. I'd suggest that we fix up the major correctness issues (PackedTensorAccessor ref, and int32 versus int64 indexing) and anything small that is easy to fix (for example, double zeroing the array), and leave other improvements for follow ups. Should be ready to land soon!\n<issue_comment>username_0: thanks so much @username_3!\r\nI will let you know when it is ready again for a new review! thanks!\n<issue_comment>username_0: @username_2 @username_3 \r\nnot sure but the errors on CI seems to be related to jenkins ... it seems all these jobs that failed (for building) ran by 01h 01min ... not sure if it is a coincidence ...\n<issue_comment>username_1: @username_0 indeed it looks like all jobs were aborted at the same time. no obvious issues in the build log related to your code. Comparing e.g. the `cuda9-cudnn7` build with a successful one from another PR, it takes 1hr 14min there and your build is about at the place where the other one is after an hour.\r\n\r\nI suggest to just push a new commit to rebuild. Probably a temporary CI hiccup.\n<issue_comment>username_3: I accidentally rebooted Jenkins yesterday which is the likely cause, my apologies.\r\n\r\n@pytorchbot retest this please\n<issue_comment>username_0: @username_3 @username_2 @username_1 \r\n\r\nall tests passed except `pr/caffe2-py2-cuda9.0-cudnn7-windows-build`:\r\n \r\n```\r\n14:43:49 Build timed out (after 180 minutes). Marking the build as failed.\r\n14:43:49 Build was aborted\r\n14:43:49 [BFA] Scanning build for known causes...\r\n14:43:49 [BFA] No failure causes found\r\n14:43:49 [BFA] Done. 0s\r\n14:43:49 Finished: FAILURE\r\n```\r\n\r\nnot sure if this timeout means that the code now is slower.\r\n\r\nwhat do you think? do you have any suggestion?\n<issue_comment>username_3: Sometimes the Windows build flakes out like that. It didn't timeout while running a relevant test, so I judge it to be not your problem.\n<issue_comment>username_3: The launch bounds logic is wrong, but I acknowledge that this is a big patch already; just fix it in a follow up. I am going to go ahead and land this.\n<issue_comment>username_0: sounds good @username_3 thanks!"}
{"repo": "pytorch/pytorch", "issue_id": 436372857, "issue_number": 19630, "timestamp": "2019-05-03 19:21:56+00:00", "text": "@username_0 your previous commit had the same failure though (except for the less clear exception message), and it seems quite reproducible. So I think you'll see it now.", "context": "<issue_start><issue_comment>Title: [WIP] UpSample GPU Porting \nusername_0: resolves #16158\n<issue_comment>username_1: Fewer errors than before, 4 instead of 8 with the same message:\r\n```\r\nRuntimeError: CUDA error: too many resources requested for launch\r\n```\r\nThis seems to be an existing problem: gh-8103.\r\n\r\n@username_2 could you have a look at this and tell us what you think?\n<issue_comment>username_2: Looking at it very briefly, the `Jetson TX` referenced in the issue only has 256 cores.\r\n\r\nSo while the issue looks the same, I'd probably not expect it on the CI platforms.  FWIW, some recent CI tests in other issues are green.\n<issue_comment>username_2: @pytorchbot retest this please.\n<issue_comment>username_1: same failures.\n<issue_comment>username_2: Is this rebased on the latest master (you can also ask the bot to rebase)?\n<issue_comment>username_0: I rebased that yesterday ... also needed to resolve conflicts because 7 days ago upsample files were changed.\n<issue_comment>username_0: @username_2 do you have any idea about what could be this problem? or a way to debug that?\r\nalso it seems some jobs is taking a lot of time to build, for example, for some job the estimate time is 19h .. not sure if it the regular estimate ..\n<issue_comment>username_2: @username_0 If you can't reproduce it at home it seems hard to debug other than reading at the diffs again.  I can take a look on Monday, it's getting a bit late here.\n<issue_comment>username_2: Also, has anyone found a way to show the actual hardware used on the CI in detail?\n<issue_comment>username_0: @username_2 I am working in parallel on a paperspace environment .. it tooks a lot of time it is running with 8 cpu cores.\n<issue_comment>username_2: But have you ever been able to reproduce this issue on paperspace?\n<issue_comment>username_1: I can reproduce it locally: Arch Linux, CUDA 10.0, RTX2070 GPU. Given the CI failures, I think it should be reproducible for multiple CUDA and GPU versions. I just haven't worked on any CUDA code before, and am short on time, so I'd rather not dig too deep.\n<issue_comment>username_0: not yet .. my last building was with a my previous commit ... I will work again in this task in some minutes :)\n<issue_comment>username_1: @username_0 your previous commit had the same failure though (except for the less clear exception message), and it seems quite reproducible. So I think you'll see it now.\n<issue_comment>username_2: Ah, thanks.  @username_0, then I guess just compiling with \"DEBUG=1\" and stepping through the offending code with gdb may be the fastest way.\n<issue_comment>username_0: @username_2 thanks! I will try that!\n<issue_comment>username_0: it seems just UpSampleBicubic2d is using upsample_get_value_bounded (https://github.com/pytorch/pytorch/pull/19630/files#diff-5092da792c30694ee4adf0d0ae2a37c6R171) and upsample_increment_value_bounded (https://github.com/pytorch/pytorch/pull/19630/files#diff-5092da792c30694ee4adf0d0ae2a37c6R191)\r\n\r\nmaybe the problem could be inside one of these functions ... maybe related to the order of indexes (x, y) ...\r\n\r\nbut the problem seems to be related to cuda block/threads ... so not sure if it is really related to these functions.\n<issue_comment>username_2: The new code uses far more registers than the existing one. I verified that the both versions actually call the offending test case with `1024` in `blockDim`.  So it's very likely a register issue.\r\n\r\n`Existing` uses `64` registers, which seems to be optimal or my card:\r\n\r\n```\r\nptxas info    : 77696 bytes gmem, 72 bytes cmem[3]\r\nptxas info    : Compiling entry function '_Z23bicubic_interp2d_kerneliddb15THCDeviceTensorIdLi4Ei16DefaultPtrTraitsES1_' for 'sm_61'\r\nptxas info    : Function properties for _Z23bicubic_interp2d_kerneliddb15THCDeviceTensorIdLi4Ei16DefaultPtrTraitsES1_\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 64 registers, 432 bytes cmem[0], 24 bytes cmem[2]\r\n```\r\n\r\n`New` uses `124` registers, which is too much for my card:\r\n\r\n```\r\nptxas info    : 77696 bytes gmem, 72 bytes cmem[3]\r\nptxas info    : Compiling entry function '_ZN2at6native76_GLOBAL__N__52_tmpxft_0000626d_00000000_6_UpSampleBicubic2d_cpp1_ii_b4c1e1f328upsample_bicubic2d_out_frameElddbNS_20PackedTensorAccessorIdLm4ENS_16DefaultPtrTraitsElEES4_' for 'sm_61'\r\nptxas info    : Function properties for _ZN2at6native76_GLOBAL__N__52_tmpxft_0000626d_00000000_6_UpSampleBicubic2d_cpp1_ii_b4c1e1f328upsample_bicubic2d_out_frameElddbNS_20PackedTensorAccessorIdLm4ENS_16DefaultPtrTraitsElEES4_\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 124 registers, 496 bytes cmem[0], 24 bytes cmem[2]\r\n```\r\n\r\nWith `__launch_bounds__(1024)`, the code again uses `64` registers.\r\n\r\nIf you use `C10_LAUNCH_BOUNDS_1(1024)` **for both kernels**, the tests pass here.\r\n\r\n\r\nNow *why* is the regcount higher in the new code? It could be `PackedTensorAccessor`, it could be the fact that many instances of `int` have been changed to `int64_t`. :)\r\n\r\nYou could experiment or just use the launch bounds.  Other code in `native/cuda` seems to use lower bounds for `blockDim`, too.  1024 seems to be an outlier.\n<issue_comment>username_0: @username_2 \r\n\r\nit seems it worked locally! thank you so much!  I really appreciate that!\n<issue_comment>username_0: thanks @username_1 and @username_2 for all the help!\r\n\r\n@username_3 it is done for review!\n<issue_comment>username_3: CircleCI runs on AWS, and the I'm fairly sure we've got one of the g3 types, either g3.4xlarge or g3.8xlarge\n<issue_comment>username_3: cc @ngimel, if you want to take a look at this (I'm planning to do a review too)\n<issue_comment>username_0: @username_4 could it be addressed in another PR?\n<issue_comment>username_4: @username_0 Sure, it's definitely better to rename both cpu & gpu impls together in a separate PR. Just want to make sure we have the correct one after porting.\n<issue_comment>username_3: Just as a clarification: I'm OK with having changes which are improvements over the old kernels come in latter patches.\n<issue_comment>username_3: I finished reviewing one of the kernels, and did a cursory scan of the others. In general the patch looks like it's in good shape. I'd suggest that we fix up the major correctness issues (PackedTensorAccessor ref, and int32 versus int64 indexing) and anything small that is easy to fix (for example, double zeroing the array), and leave other improvements for follow ups. Should be ready to land soon!\n<issue_comment>username_0: thanks so much @username_3!\r\nI will let you know when it is ready again for a new review! thanks!\n<issue_comment>username_0: @username_2 @username_3 \r\nnot sure but the errors on CI seems to be related to jenkins ... it seems all these jobs that failed (for building) ran by 01h 01min ... not sure if it is a coincidence ...\n<issue_comment>username_1: @username_0 indeed it looks like all jobs were aborted at the same time. no obvious issues in the build log related to your code. Comparing e.g. the `cuda9-cudnn7` build with a successful one from another PR, it takes 1hr 14min there and your build is about at the place where the other one is after an hour.\r\n\r\nI suggest to just push a new commit to rebuild. Probably a temporary CI hiccup.\n<issue_comment>username_3: I accidentally rebooted Jenkins yesterday which is the likely cause, my apologies.\r\n\r\n@pytorchbot retest this please\n<issue_comment>username_0: @username_3 @username_2 @username_1 \r\n\r\nall tests passed except `pr/caffe2-py2-cuda9.0-cudnn7-windows-build`:\r\n \r\n```\r\n14:43:49 Build timed out (after 180 minutes). Marking the build as failed.\r\n14:43:49 Build was aborted\r\n14:43:49 [BFA] Scanning build for known causes...\r\n14:43:49 [BFA] No failure causes found\r\n14:43:49 [BFA] Done. 0s\r\n14:43:49 Finished: FAILURE\r\n```\r\n\r\nnot sure if this timeout means that the code now is slower.\r\n\r\nwhat do you think? do you have any suggestion?\n<issue_comment>username_3: Sometimes the Windows build flakes out like that. It didn't timeout while running a relevant test, so I judge it to be not your problem.\n<issue_comment>username_3: The launch bounds logic is wrong, but I acknowledge that this is a big patch already; just fix it in a follow up. I am going to go ahead and land this.\n<issue_comment>username_0: sounds good @username_3 thanks!"}
{"repo": "pytorch/pytorch", "issue_id": 436372857, "issue_number": 19630, "timestamp": "2019-05-03 19:26:12+00:00", "text": "Ah, thanks.  @username_0, then I guess just compiling with \"DEBUG=1\" and stepping through the offending code with gdb may be the fastest way.", "context": "<issue_start><issue_comment>Title: [WIP] UpSample GPU Porting \nusername_0: resolves #16158\n<issue_comment>username_1: Fewer errors than before, 4 instead of 8 with the same message:\r\n```\r\nRuntimeError: CUDA error: too many resources requested for launch\r\n```\r\nThis seems to be an existing problem: gh-8103.\r\n\r\n@username_2 could you have a look at this and tell us what you think?\n<issue_comment>username_2: Looking at it very briefly, the `Jetson TX` referenced in the issue only has 256 cores.\r\n\r\nSo while the issue looks the same, I'd probably not expect it on the CI platforms.  FWIW, some recent CI tests in other issues are green.\n<issue_comment>username_2: @pytorchbot retest this please.\n<issue_comment>username_1: same failures.\n<issue_comment>username_2: Is this rebased on the latest master (you can also ask the bot to rebase)?\n<issue_comment>username_0: I rebased that yesterday ... also needed to resolve conflicts because 7 days ago upsample files were changed.\n<issue_comment>username_0: @username_2 do you have any idea about what could be this problem? or a way to debug that?\r\nalso it seems some jobs is taking a lot of time to build, for example, for some job the estimate time is 19h .. not sure if it the regular estimate ..\n<issue_comment>username_2: @username_0 If you can't reproduce it at home it seems hard to debug other than reading at the diffs again.  I can take a look on Monday, it's getting a bit late here.\n<issue_comment>username_2: Also, has anyone found a way to show the actual hardware used on the CI in detail?\n<issue_comment>username_0: @username_2 I am working in parallel on a paperspace environment .. it tooks a lot of time it is running with 8 cpu cores.\n<issue_comment>username_2: But have you ever been able to reproduce this issue on paperspace?\n<issue_comment>username_1: I can reproduce it locally: Arch Linux, CUDA 10.0, RTX2070 GPU. Given the CI failures, I think it should be reproducible for multiple CUDA and GPU versions. I just haven't worked on any CUDA code before, and am short on time, so I'd rather not dig too deep.\n<issue_comment>username_0: not yet .. my last building was with a my previous commit ... I will work again in this task in some minutes :)\n<issue_comment>username_1: @username_0 your previous commit had the same failure though (except for the less clear exception message), and it seems quite reproducible. So I think you'll see it now.\n<issue_comment>username_2: Ah, thanks.  @username_0, then I guess just compiling with \"DEBUG=1\" and stepping through the offending code with gdb may be the fastest way.\n<issue_comment>username_0: @username_2 thanks! I will try that!\n<issue_comment>username_0: it seems just UpSampleBicubic2d is using upsample_get_value_bounded (https://github.com/pytorch/pytorch/pull/19630/files#diff-5092da792c30694ee4adf0d0ae2a37c6R171) and upsample_increment_value_bounded (https://github.com/pytorch/pytorch/pull/19630/files#diff-5092da792c30694ee4adf0d0ae2a37c6R191)\r\n\r\nmaybe the problem could be inside one of these functions ... maybe related to the order of indexes (x, y) ...\r\n\r\nbut the problem seems to be related to cuda block/threads ... so not sure if it is really related to these functions.\n<issue_comment>username_2: The new code uses far more registers than the existing one. I verified that the both versions actually call the offending test case with `1024` in `blockDim`.  So it's very likely a register issue.\r\n\r\n`Existing` uses `64` registers, which seems to be optimal or my card:\r\n\r\n```\r\nptxas info    : 77696 bytes gmem, 72 bytes cmem[3]\r\nptxas info    : Compiling entry function '_Z23bicubic_interp2d_kerneliddb15THCDeviceTensorIdLi4Ei16DefaultPtrTraitsES1_' for 'sm_61'\r\nptxas info    : Function properties for _Z23bicubic_interp2d_kerneliddb15THCDeviceTensorIdLi4Ei16DefaultPtrTraitsES1_\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 64 registers, 432 bytes cmem[0], 24 bytes cmem[2]\r\n```\r\n\r\n`New` uses `124` registers, which is too much for my card:\r\n\r\n```\r\nptxas info    : 77696 bytes gmem, 72 bytes cmem[3]\r\nptxas info    : Compiling entry function '_ZN2at6native76_GLOBAL__N__52_tmpxft_0000626d_00000000_6_UpSampleBicubic2d_cpp1_ii_b4c1e1f328upsample_bicubic2d_out_frameElddbNS_20PackedTensorAccessorIdLm4ENS_16DefaultPtrTraitsElEES4_' for 'sm_61'\r\nptxas info    : Function properties for _ZN2at6native76_GLOBAL__N__52_tmpxft_0000626d_00000000_6_UpSampleBicubic2d_cpp1_ii_b4c1e1f328upsample_bicubic2d_out_frameElddbNS_20PackedTensorAccessorIdLm4ENS_16DefaultPtrTraitsElEES4_\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 124 registers, 496 bytes cmem[0], 24 bytes cmem[2]\r\n```\r\n\r\nWith `__launch_bounds__(1024)`, the code again uses `64` registers.\r\n\r\nIf you use `C10_LAUNCH_BOUNDS_1(1024)` **for both kernels**, the tests pass here.\r\n\r\n\r\nNow *why* is the regcount higher in the new code? It could be `PackedTensorAccessor`, it could be the fact that many instances of `int` have been changed to `int64_t`. :)\r\n\r\nYou could experiment or just use the launch bounds.  Other code in `native/cuda` seems to use lower bounds for `blockDim`, too.  1024 seems to be an outlier.\n<issue_comment>username_0: @username_2 \r\n\r\nit seems it worked locally! thank you so much!  I really appreciate that!\n<issue_comment>username_0: thanks @username_1 and @username_2 for all the help!\r\n\r\n@username_3 it is done for review!\n<issue_comment>username_3: CircleCI runs on AWS, and the I'm fairly sure we've got one of the g3 types, either g3.4xlarge or g3.8xlarge\n<issue_comment>username_3: cc @ngimel, if you want to take a look at this (I'm planning to do a review too)\n<issue_comment>username_0: @username_4 could it be addressed in another PR?\n<issue_comment>username_4: @username_0 Sure, it's definitely better to rename both cpu & gpu impls together in a separate PR. Just want to make sure we have the correct one after porting.\n<issue_comment>username_3: Just as a clarification: I'm OK with having changes which are improvements over the old kernels come in latter patches.\n<issue_comment>username_3: I finished reviewing one of the kernels, and did a cursory scan of the others. In general the patch looks like it's in good shape. I'd suggest that we fix up the major correctness issues (PackedTensorAccessor ref, and int32 versus int64 indexing) and anything small that is easy to fix (for example, double zeroing the array), and leave other improvements for follow ups. Should be ready to land soon!\n<issue_comment>username_0: thanks so much @username_3!\r\nI will let you know when it is ready again for a new review! thanks!\n<issue_comment>username_0: @username_2 @username_3 \r\nnot sure but the errors on CI seems to be related to jenkins ... it seems all these jobs that failed (for building) ran by 01h 01min ... not sure if it is a coincidence ...\n<issue_comment>username_1: @username_0 indeed it looks like all jobs were aborted at the same time. no obvious issues in the build log related to your code. Comparing e.g. the `cuda9-cudnn7` build with a successful one from another PR, it takes 1hr 14min there and your build is about at the place where the other one is after an hour.\r\n\r\nI suggest to just push a new commit to rebuild. Probably a temporary CI hiccup.\n<issue_comment>username_3: I accidentally rebooted Jenkins yesterday which is the likely cause, my apologies.\r\n\r\n@pytorchbot retest this please\n<issue_comment>username_0: @username_3 @username_2 @username_1 \r\n\r\nall tests passed except `pr/caffe2-py2-cuda9.0-cudnn7-windows-build`:\r\n \r\n```\r\n14:43:49 Build timed out (after 180 minutes). Marking the build as failed.\r\n14:43:49 Build was aborted\r\n14:43:49 [BFA] Scanning build for known causes...\r\n14:43:49 [BFA] No failure causes found\r\n14:43:49 [BFA] Done. 0s\r\n14:43:49 Finished: FAILURE\r\n```\r\n\r\nnot sure if this timeout means that the code now is slower.\r\n\r\nwhat do you think? do you have any suggestion?\n<issue_comment>username_3: Sometimes the Windows build flakes out like that. It didn't timeout while running a relevant test, so I judge it to be not your problem.\n<issue_comment>username_3: The launch bounds logic is wrong, but I acknowledge that this is a big patch already; just fix it in a follow up. I am going to go ahead and land this.\n<issue_comment>username_0: sounds good @username_3 thanks!"}
{"repo": "pytorch/pytorch", "issue_id": 436372857, "issue_number": 19630, "timestamp": "2019-05-03 20:05:48+00:00", "text": "@username_2 thanks! I will try that!", "context": "<issue_start><issue_comment>Title: [WIP] UpSample GPU Porting \nusername_0: resolves #16158\n<issue_comment>username_1: Fewer errors than before, 4 instead of 8 with the same message:\r\n```\r\nRuntimeError: CUDA error: too many resources requested for launch\r\n```\r\nThis seems to be an existing problem: gh-8103.\r\n\r\n@username_2 could you have a look at this and tell us what you think?\n<issue_comment>username_2: Looking at it very briefly, the `Jetson TX` referenced in the issue only has 256 cores.\r\n\r\nSo while the issue looks the same, I'd probably not expect it on the CI platforms.  FWIW, some recent CI tests in other issues are green.\n<issue_comment>username_2: @pytorchbot retest this please.\n<issue_comment>username_1: same failures.\n<issue_comment>username_2: Is this rebased on the latest master (you can also ask the bot to rebase)?\n<issue_comment>username_0: I rebased that yesterday ... also needed to resolve conflicts because 7 days ago upsample files were changed.\n<issue_comment>username_0: @username_2 do you have any idea about what could be this problem? or a way to debug that?\r\nalso it seems some jobs is taking a lot of time to build, for example, for some job the estimate time is 19h .. not sure if it the regular estimate ..\n<issue_comment>username_2: @username_0 If you can't reproduce it at home it seems hard to debug other than reading at the diffs again.  I can take a look on Monday, it's getting a bit late here.\n<issue_comment>username_2: Also, has anyone found a way to show the actual hardware used on the CI in detail?\n<issue_comment>username_0: @username_2 I am working in parallel on a paperspace environment .. it tooks a lot of time it is running with 8 cpu cores.\n<issue_comment>username_2: But have you ever been able to reproduce this issue on paperspace?\n<issue_comment>username_1: I can reproduce it locally: Arch Linux, CUDA 10.0, RTX2070 GPU. Given the CI failures, I think it should be reproducible for multiple CUDA and GPU versions. I just haven't worked on any CUDA code before, and am short on time, so I'd rather not dig too deep.\n<issue_comment>username_0: not yet .. my last building was with a my previous commit ... I will work again in this task in some minutes :)\n<issue_comment>username_1: @username_0 your previous commit had the same failure though (except for the less clear exception message), and it seems quite reproducible. So I think you'll see it now.\n<issue_comment>username_2: Ah, thanks.  @username_0, then I guess just compiling with \"DEBUG=1\" and stepping through the offending code with gdb may be the fastest way.\n<issue_comment>username_0: @username_2 thanks! I will try that!\n<issue_comment>username_0: it seems just UpSampleBicubic2d is using upsample_get_value_bounded (https://github.com/pytorch/pytorch/pull/19630/files#diff-5092da792c30694ee4adf0d0ae2a37c6R171) and upsample_increment_value_bounded (https://github.com/pytorch/pytorch/pull/19630/files#diff-5092da792c30694ee4adf0d0ae2a37c6R191)\r\n\r\nmaybe the problem could be inside one of these functions ... maybe related to the order of indexes (x, y) ...\r\n\r\nbut the problem seems to be related to cuda block/threads ... so not sure if it is really related to these functions.\n<issue_comment>username_2: The new code uses far more registers than the existing one. I verified that the both versions actually call the offending test case with `1024` in `blockDim`.  So it's very likely a register issue.\r\n\r\n`Existing` uses `64` registers, which seems to be optimal or my card:\r\n\r\n```\r\nptxas info    : 77696 bytes gmem, 72 bytes cmem[3]\r\nptxas info    : Compiling entry function '_Z23bicubic_interp2d_kerneliddb15THCDeviceTensorIdLi4Ei16DefaultPtrTraitsES1_' for 'sm_61'\r\nptxas info    : Function properties for _Z23bicubic_interp2d_kerneliddb15THCDeviceTensorIdLi4Ei16DefaultPtrTraitsES1_\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 64 registers, 432 bytes cmem[0], 24 bytes cmem[2]\r\n```\r\n\r\n`New` uses `124` registers, which is too much for my card:\r\n\r\n```\r\nptxas info    : 77696 bytes gmem, 72 bytes cmem[3]\r\nptxas info    : Compiling entry function '_ZN2at6native76_GLOBAL__N__52_tmpxft_0000626d_00000000_6_UpSampleBicubic2d_cpp1_ii_b4c1e1f328upsample_bicubic2d_out_frameElddbNS_20PackedTensorAccessorIdLm4ENS_16DefaultPtrTraitsElEES4_' for 'sm_61'\r\nptxas info    : Function properties for _ZN2at6native76_GLOBAL__N__52_tmpxft_0000626d_00000000_6_UpSampleBicubic2d_cpp1_ii_b4c1e1f328upsample_bicubic2d_out_frameElddbNS_20PackedTensorAccessorIdLm4ENS_16DefaultPtrTraitsElEES4_\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 124 registers, 496 bytes cmem[0], 24 bytes cmem[2]\r\n```\r\n\r\nWith `__launch_bounds__(1024)`, the code again uses `64` registers.\r\n\r\nIf you use `C10_LAUNCH_BOUNDS_1(1024)` **for both kernels**, the tests pass here.\r\n\r\n\r\nNow *why* is the regcount higher in the new code? It could be `PackedTensorAccessor`, it could be the fact that many instances of `int` have been changed to `int64_t`. :)\r\n\r\nYou could experiment or just use the launch bounds.  Other code in `native/cuda` seems to use lower bounds for `blockDim`, too.  1024 seems to be an outlier.\n<issue_comment>username_0: @username_2 \r\n\r\nit seems it worked locally! thank you so much!  I really appreciate that!\n<issue_comment>username_0: thanks @username_1 and @username_2 for all the help!\r\n\r\n@username_3 it is done for review!\n<issue_comment>username_3: CircleCI runs on AWS, and the I'm fairly sure we've got one of the g3 types, either g3.4xlarge or g3.8xlarge\n<issue_comment>username_3: cc @ngimel, if you want to take a look at this (I'm planning to do a review too)\n<issue_comment>username_0: @username_4 could it be addressed in another PR?\n<issue_comment>username_4: @username_0 Sure, it's definitely better to rename both cpu & gpu impls together in a separate PR. Just want to make sure we have the correct one after porting.\n<issue_comment>username_3: Just as a clarification: I'm OK with having changes which are improvements over the old kernels come in latter patches.\n<issue_comment>username_3: I finished reviewing one of the kernels, and did a cursory scan of the others. In general the patch looks like it's in good shape. I'd suggest that we fix up the major correctness issues (PackedTensorAccessor ref, and int32 versus int64 indexing) and anything small that is easy to fix (for example, double zeroing the array), and leave other improvements for follow ups. Should be ready to land soon!\n<issue_comment>username_0: thanks so much @username_3!\r\nI will let you know when it is ready again for a new review! thanks!\n<issue_comment>username_0: @username_2 @username_3 \r\nnot sure but the errors on CI seems to be related to jenkins ... it seems all these jobs that failed (for building) ran by 01h 01min ... not sure if it is a coincidence ...\n<issue_comment>username_1: @username_0 indeed it looks like all jobs were aborted at the same time. no obvious issues in the build log related to your code. Comparing e.g. the `cuda9-cudnn7` build with a successful one from another PR, it takes 1hr 14min there and your build is about at the place where the other one is after an hour.\r\n\r\nI suggest to just push a new commit to rebuild. Probably a temporary CI hiccup.\n<issue_comment>username_3: I accidentally rebooted Jenkins yesterday which is the likely cause, my apologies.\r\n\r\n@pytorchbot retest this please\n<issue_comment>username_0: @username_3 @username_2 @username_1 \r\n\r\nall tests passed except `pr/caffe2-py2-cuda9.0-cudnn7-windows-build`:\r\n \r\n```\r\n14:43:49 Build timed out (after 180 minutes). Marking the build as failed.\r\n14:43:49 Build was aborted\r\n14:43:49 [BFA] Scanning build for known causes...\r\n14:43:49 [BFA] No failure causes found\r\n14:43:49 [BFA] Done. 0s\r\n14:43:49 Finished: FAILURE\r\n```\r\n\r\nnot sure if this timeout means that the code now is slower.\r\n\r\nwhat do you think? do you have any suggestion?\n<issue_comment>username_3: Sometimes the Windows build flakes out like that. It didn't timeout while running a relevant test, so I judge it to be not your problem.\n<issue_comment>username_3: The launch bounds logic is wrong, but I acknowledge that this is a big patch already; just fix it in a follow up. I am going to go ahead and land this.\n<issue_comment>username_0: sounds good @username_3 thanks!"}
{"repo": "pytorch/pytorch", "issue_id": 436372857, "issue_number": 19630, "timestamp": "2019-05-04 17:25:38+00:00", "text": "it seems just UpSampleBicubic2d is using upsample_get_value_bounded (https://github.com/pytorch/pytorch/pull/19630/files#diff-5092da792c30694ee4adf0d0ae2a37c6R171) and upsample_increment_value_bounded (https://github.com/pytorch/pytorch/pull/19630/files#diff-5092da792c30694ee4adf0d0ae2a37c6R191)\r\n\r\nmaybe the problem could be inside one of these functions ... maybe related to the order of indexes (x, y) ...\r\n\r\nbut the problem seems to be related to cuda block/threads ... so not sure if it is really related to these functions.", "context": "<issue_start><issue_comment>Title: [WIP] UpSample GPU Porting \nusername_0: resolves #16158\n<issue_comment>username_1: Fewer errors than before, 4 instead of 8 with the same message:\r\n```\r\nRuntimeError: CUDA error: too many resources requested for launch\r\n```\r\nThis seems to be an existing problem: gh-8103.\r\n\r\n@username_2 could you have a look at this and tell us what you think?\n<issue_comment>username_2: Looking at it very briefly, the `Jetson TX` referenced in the issue only has 256 cores.\r\n\r\nSo while the issue looks the same, I'd probably not expect it on the CI platforms.  FWIW, some recent CI tests in other issues are green.\n<issue_comment>username_2: @pytorchbot retest this please.\n<issue_comment>username_1: same failures.\n<issue_comment>username_2: Is this rebased on the latest master (you can also ask the bot to rebase)?\n<issue_comment>username_0: I rebased that yesterday ... also needed to resolve conflicts because 7 days ago upsample files were changed.\n<issue_comment>username_0: @username_2 do you have any idea about what could be this problem? or a way to debug that?\r\nalso it seems some jobs is taking a lot of time to build, for example, for some job the estimate time is 19h .. not sure if it the regular estimate ..\n<issue_comment>username_2: @username_0 If you can't reproduce it at home it seems hard to debug other than reading at the diffs again.  I can take a look on Monday, it's getting a bit late here.\n<issue_comment>username_2: Also, has anyone found a way to show the actual hardware used on the CI in detail?\n<issue_comment>username_0: @username_2 I am working in parallel on a paperspace environment .. it tooks a lot of time it is running with 8 cpu cores.\n<issue_comment>username_2: But have you ever been able to reproduce this issue on paperspace?\n<issue_comment>username_1: I can reproduce it locally: Arch Linux, CUDA 10.0, RTX2070 GPU. Given the CI failures, I think it should be reproducible for multiple CUDA and GPU versions. I just haven't worked on any CUDA code before, and am short on time, so I'd rather not dig too deep.\n<issue_comment>username_0: not yet .. my last building was with a my previous commit ... I will work again in this task in some minutes :)\n<issue_comment>username_1: @username_0 your previous commit had the same failure though (except for the less clear exception message), and it seems quite reproducible. So I think you'll see it now.\n<issue_comment>username_2: Ah, thanks.  @username_0, then I guess just compiling with \"DEBUG=1\" and stepping through the offending code with gdb may be the fastest way.\n<issue_comment>username_0: @username_2 thanks! I will try that!\n<issue_comment>username_0: it seems just UpSampleBicubic2d is using upsample_get_value_bounded (https://github.com/pytorch/pytorch/pull/19630/files#diff-5092da792c30694ee4adf0d0ae2a37c6R171) and upsample_increment_value_bounded (https://github.com/pytorch/pytorch/pull/19630/files#diff-5092da792c30694ee4adf0d0ae2a37c6R191)\r\n\r\nmaybe the problem could be inside one of these functions ... maybe related to the order of indexes (x, y) ...\r\n\r\nbut the problem seems to be related to cuda block/threads ... so not sure if it is really related to these functions.\n<issue_comment>username_2: The new code uses far more registers than the existing one. I verified that the both versions actually call the offending test case with `1024` in `blockDim`.  So it's very likely a register issue.\r\n\r\n`Existing` uses `64` registers, which seems to be optimal or my card:\r\n\r\n```\r\nptxas info    : 77696 bytes gmem, 72 bytes cmem[3]\r\nptxas info    : Compiling entry function '_Z23bicubic_interp2d_kerneliddb15THCDeviceTensorIdLi4Ei16DefaultPtrTraitsES1_' for 'sm_61'\r\nptxas info    : Function properties for _Z23bicubic_interp2d_kerneliddb15THCDeviceTensorIdLi4Ei16DefaultPtrTraitsES1_\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 64 registers, 432 bytes cmem[0], 24 bytes cmem[2]\r\n```\r\n\r\n`New` uses `124` registers, which is too much for my card:\r\n\r\n```\r\nptxas info    : 77696 bytes gmem, 72 bytes cmem[3]\r\nptxas info    : Compiling entry function '_ZN2at6native76_GLOBAL__N__52_tmpxft_0000626d_00000000_6_UpSampleBicubic2d_cpp1_ii_b4c1e1f328upsample_bicubic2d_out_frameElddbNS_20PackedTensorAccessorIdLm4ENS_16DefaultPtrTraitsElEES4_' for 'sm_61'\r\nptxas info    : Function properties for _ZN2at6native76_GLOBAL__N__52_tmpxft_0000626d_00000000_6_UpSampleBicubic2d_cpp1_ii_b4c1e1f328upsample_bicubic2d_out_frameElddbNS_20PackedTensorAccessorIdLm4ENS_16DefaultPtrTraitsElEES4_\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 124 registers, 496 bytes cmem[0], 24 bytes cmem[2]\r\n```\r\n\r\nWith `__launch_bounds__(1024)`, the code again uses `64` registers.\r\n\r\nIf you use `C10_LAUNCH_BOUNDS_1(1024)` **for both kernels**, the tests pass here.\r\n\r\n\r\nNow *why* is the regcount higher in the new code? It could be `PackedTensorAccessor`, it could be the fact that many instances of `int` have been changed to `int64_t`. :)\r\n\r\nYou could experiment or just use the launch bounds.  Other code in `native/cuda` seems to use lower bounds for `blockDim`, too.  1024 seems to be an outlier.\n<issue_comment>username_0: @username_2 \r\n\r\nit seems it worked locally! thank you so much!  I really appreciate that!\n<issue_comment>username_0: thanks @username_1 and @username_2 for all the help!\r\n\r\n@username_3 it is done for review!\n<issue_comment>username_3: CircleCI runs on AWS, and the I'm fairly sure we've got one of the g3 types, either g3.4xlarge or g3.8xlarge\n<issue_comment>username_3: cc @ngimel, if you want to take a look at this (I'm planning to do a review too)\n<issue_comment>username_0: @username_4 could it be addressed in another PR?\n<issue_comment>username_4: @username_0 Sure, it's definitely better to rename both cpu & gpu impls together in a separate PR. Just want to make sure we have the correct one after porting.\n<issue_comment>username_3: Just as a clarification: I'm OK with having changes which are improvements over the old kernels come in latter patches.\n<issue_comment>username_3: I finished reviewing one of the kernels, and did a cursory scan of the others. In general the patch looks like it's in good shape. I'd suggest that we fix up the major correctness issues (PackedTensorAccessor ref, and int32 versus int64 indexing) and anything small that is easy to fix (for example, double zeroing the array), and leave other improvements for follow ups. Should be ready to land soon!\n<issue_comment>username_0: thanks so much @username_3!\r\nI will let you know when it is ready again for a new review! thanks!\n<issue_comment>username_0: @username_2 @username_3 \r\nnot sure but the errors on CI seems to be related to jenkins ... it seems all these jobs that failed (for building) ran by 01h 01min ... not sure if it is a coincidence ...\n<issue_comment>username_1: @username_0 indeed it looks like all jobs were aborted at the same time. no obvious issues in the build log related to your code. Comparing e.g. the `cuda9-cudnn7` build with a successful one from another PR, it takes 1hr 14min there and your build is about at the place where the other one is after an hour.\r\n\r\nI suggest to just push a new commit to rebuild. Probably a temporary CI hiccup.\n<issue_comment>username_3: I accidentally rebooted Jenkins yesterday which is the likely cause, my apologies.\r\n\r\n@pytorchbot retest this please\n<issue_comment>username_0: @username_3 @username_2 @username_1 \r\n\r\nall tests passed except `pr/caffe2-py2-cuda9.0-cudnn7-windows-build`:\r\n \r\n```\r\n14:43:49 Build timed out (after 180 minutes). Marking the build as failed.\r\n14:43:49 Build was aborted\r\n14:43:49 [BFA] Scanning build for known causes...\r\n14:43:49 [BFA] No failure causes found\r\n14:43:49 [BFA] Done. 0s\r\n14:43:49 Finished: FAILURE\r\n```\r\n\r\nnot sure if this timeout means that the code now is slower.\r\n\r\nwhat do you think? do you have any suggestion?\n<issue_comment>username_3: Sometimes the Windows build flakes out like that. It didn't timeout while running a relevant test, so I judge it to be not your problem.\n<issue_comment>username_3: The launch bounds logic is wrong, but I acknowledge that this is a big patch already; just fix it in a follow up. I am going to go ahead and land this.\n<issue_comment>username_0: sounds good @username_3 thanks!"}
{"repo": "pytorch/pytorch", "issue_id": 436372857, "issue_number": 19630, "timestamp": "2019-05-04 19:52:42+00:00", "text": "The new code uses far more registers than the existing one. I verified that the both versions actually call the offending test case with `1024` in `blockDim`.  So it's very likely a register issue.\r\n\r\n`Existing` uses `64` registers, which seems to be optimal or my card:\r\n\r\n```\r\nptxas info    : 77696 bytes gmem, 72 bytes cmem[3]\r\nptxas info    : Compiling entry function '_Z23bicubic_interp2d_kerneliddb15THCDeviceTensorIdLi4Ei16DefaultPtrTraitsES1_' for 'sm_61'\r\nptxas info    : Function properties for _Z23bicubic_interp2d_kerneliddb15THCDeviceTensorIdLi4Ei16DefaultPtrTraitsES1_\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 64 registers, 432 bytes cmem[0], 24 bytes cmem[2]\r\n```\r\n\r\n`New` uses `124` registers, which is too much for my card:\r\n\r\n```\r\nptxas info    : 77696 bytes gmem, 72 bytes cmem[3]\r\nptxas info    : Compiling entry function '_ZN2at6native76_GLOBAL__N__52_tmpxft_0000626d_00000000_6_UpSampleBicubic2d_cpp1_ii_b4c1e1f328upsample_bicubic2d_out_frameElddbNS_20PackedTensorAccessorIdLm4ENS_16DefaultPtrTraitsElEES4_' for 'sm_61'\r\nptxas info    : Function properties for _ZN2at6native76_GLOBAL__N__52_tmpxft_0000626d_00000000_6_UpSampleBicubic2d_cpp1_ii_b4c1e1f328upsample_bicubic2d_out_frameElddbNS_20PackedTensorAccessorIdLm4ENS_16DefaultPtrTraitsElEES4_\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 124 registers, 496 bytes cmem[0], 24 bytes cmem[2]\r\n```\r\n\r\nWith `__launch_bounds__(1024)`, the code again uses `64` registers.\r\n\r\nIf you use `C10_LAUNCH_BOUNDS_1(1024)` **for both kernels**, the tests pass here.\r\n\r\n\r\nNow *why* is the regcount higher in the new code? It could be `PackedTensorAccessor`, it could be the fact that many instances of `int` have been changed to `int64_t`. :)\r\n\r\nYou could experiment or just use the launch bounds.  Other code in `native/cuda` seems to use lower bounds for `blockDim`, too.  1024 seems to be an outlier.", "context": "<issue_start><issue_comment>Title: [WIP] UpSample GPU Porting \nusername_0: resolves #16158\n<issue_comment>username_1: Fewer errors than before, 4 instead of 8 with the same message:\r\n```\r\nRuntimeError: CUDA error: too many resources requested for launch\r\n```\r\nThis seems to be an existing problem: gh-8103.\r\n\r\n@username_2 could you have a look at this and tell us what you think?\n<issue_comment>username_2: Looking at it very briefly, the `Jetson TX` referenced in the issue only has 256 cores.\r\n\r\nSo while the issue looks the same, I'd probably not expect it on the CI platforms.  FWIW, some recent CI tests in other issues are green.\n<issue_comment>username_2: @pytorchbot retest this please.\n<issue_comment>username_1: same failures.\n<issue_comment>username_2: Is this rebased on the latest master (you can also ask the bot to rebase)?\n<issue_comment>username_0: I rebased that yesterday ... also needed to resolve conflicts because 7 days ago upsample files were changed.\n<issue_comment>username_0: @username_2 do you have any idea about what could be this problem? or a way to debug that?\r\nalso it seems some jobs is taking a lot of time to build, for example, for some job the estimate time is 19h .. not sure if it the regular estimate ..\n<issue_comment>username_2: @username_0 If you can't reproduce it at home it seems hard to debug other than reading at the diffs again.  I can take a look on Monday, it's getting a bit late here.\n<issue_comment>username_2: Also, has anyone found a way to show the actual hardware used on the CI in detail?\n<issue_comment>username_0: @username_2 I am working in parallel on a paperspace environment .. it tooks a lot of time it is running with 8 cpu cores.\n<issue_comment>username_2: But have you ever been able to reproduce this issue on paperspace?\n<issue_comment>username_1: I can reproduce it locally: Arch Linux, CUDA 10.0, RTX2070 GPU. Given the CI failures, I think it should be reproducible for multiple CUDA and GPU versions. I just haven't worked on any CUDA code before, and am short on time, so I'd rather not dig too deep.\n<issue_comment>username_0: not yet .. my last building was with a my previous commit ... I will work again in this task in some minutes :)\n<issue_comment>username_1: @username_0 your previous commit had the same failure though (except for the less clear exception message), and it seems quite reproducible. So I think you'll see it now.\n<issue_comment>username_2: Ah, thanks.  @username_0, then I guess just compiling with \"DEBUG=1\" and stepping through the offending code with gdb may be the fastest way.\n<issue_comment>username_0: @username_2 thanks! I will try that!\n<issue_comment>username_0: it seems just UpSampleBicubic2d is using upsample_get_value_bounded (https://github.com/pytorch/pytorch/pull/19630/files#diff-5092da792c30694ee4adf0d0ae2a37c6R171) and upsample_increment_value_bounded (https://github.com/pytorch/pytorch/pull/19630/files#diff-5092da792c30694ee4adf0d0ae2a37c6R191)\r\n\r\nmaybe the problem could be inside one of these functions ... maybe related to the order of indexes (x, y) ...\r\n\r\nbut the problem seems to be related to cuda block/threads ... so not sure if it is really related to these functions.\n<issue_comment>username_2: The new code uses far more registers than the existing one. I verified that the both versions actually call the offending test case with `1024` in `blockDim`.  So it's very likely a register issue.\r\n\r\n`Existing` uses `64` registers, which seems to be optimal or my card:\r\n\r\n```\r\nptxas info    : 77696 bytes gmem, 72 bytes cmem[3]\r\nptxas info    : Compiling entry function '_Z23bicubic_interp2d_kerneliddb15THCDeviceTensorIdLi4Ei16DefaultPtrTraitsES1_' for 'sm_61'\r\nptxas info    : Function properties for _Z23bicubic_interp2d_kerneliddb15THCDeviceTensorIdLi4Ei16DefaultPtrTraitsES1_\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 64 registers, 432 bytes cmem[0], 24 bytes cmem[2]\r\n```\r\n\r\n`New` uses `124` registers, which is too much for my card:\r\n\r\n```\r\nptxas info    : 77696 bytes gmem, 72 bytes cmem[3]\r\nptxas info    : Compiling entry function '_ZN2at6native76_GLOBAL__N__52_tmpxft_0000626d_00000000_6_UpSampleBicubic2d_cpp1_ii_b4c1e1f328upsample_bicubic2d_out_frameElddbNS_20PackedTensorAccessorIdLm4ENS_16DefaultPtrTraitsElEES4_' for 'sm_61'\r\nptxas info    : Function properties for _ZN2at6native76_GLOBAL__N__52_tmpxft_0000626d_00000000_6_UpSampleBicubic2d_cpp1_ii_b4c1e1f328upsample_bicubic2d_out_frameElddbNS_20PackedTensorAccessorIdLm4ENS_16DefaultPtrTraitsElEES4_\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 124 registers, 496 bytes cmem[0], 24 bytes cmem[2]\r\n```\r\n\r\nWith `__launch_bounds__(1024)`, the code again uses `64` registers.\r\n\r\nIf you use `C10_LAUNCH_BOUNDS_1(1024)` **for both kernels**, the tests pass here.\r\n\r\n\r\nNow *why* is the regcount higher in the new code? It could be `PackedTensorAccessor`, it could be the fact that many instances of `int` have been changed to `int64_t`. :)\r\n\r\nYou could experiment or just use the launch bounds.  Other code in `native/cuda` seems to use lower bounds for `blockDim`, too.  1024 seems to be an outlier.\n<issue_comment>username_0: @username_2 \r\n\r\nit seems it worked locally! thank you so much!  I really appreciate that!\n<issue_comment>username_0: thanks @username_1 and @username_2 for all the help!\r\n\r\n@username_3 it is done for review!\n<issue_comment>username_3: CircleCI runs on AWS, and the I'm fairly sure we've got one of the g3 types, either g3.4xlarge or g3.8xlarge\n<issue_comment>username_3: cc @ngimel, if you want to take a look at this (I'm planning to do a review too)\n<issue_comment>username_0: @username_4 could it be addressed in another PR?\n<issue_comment>username_4: @username_0 Sure, it's definitely better to rename both cpu & gpu impls together in a separate PR. Just want to make sure we have the correct one after porting.\n<issue_comment>username_3: Just as a clarification: I'm OK with having changes which are improvements over the old kernels come in latter patches.\n<issue_comment>username_3: I finished reviewing one of the kernels, and did a cursory scan of the others. In general the patch looks like it's in good shape. I'd suggest that we fix up the major correctness issues (PackedTensorAccessor ref, and int32 versus int64 indexing) and anything small that is easy to fix (for example, double zeroing the array), and leave other improvements for follow ups. Should be ready to land soon!\n<issue_comment>username_0: thanks so much @username_3!\r\nI will let you know when it is ready again for a new review! thanks!\n<issue_comment>username_0: @username_2 @username_3 \r\nnot sure but the errors on CI seems to be related to jenkins ... it seems all these jobs that failed (for building) ran by 01h 01min ... not sure if it is a coincidence ...\n<issue_comment>username_1: @username_0 indeed it looks like all jobs were aborted at the same time. no obvious issues in the build log related to your code. Comparing e.g. the `cuda9-cudnn7` build with a successful one from another PR, it takes 1hr 14min there and your build is about at the place where the other one is after an hour.\r\n\r\nI suggest to just push a new commit to rebuild. Probably a temporary CI hiccup.\n<issue_comment>username_3: I accidentally rebooted Jenkins yesterday which is the likely cause, my apologies.\r\n\r\n@pytorchbot retest this please\n<issue_comment>username_0: @username_3 @username_2 @username_1 \r\n\r\nall tests passed except `pr/caffe2-py2-cuda9.0-cudnn7-windows-build`:\r\n \r\n```\r\n14:43:49 Build timed out (after 180 minutes). Marking the build as failed.\r\n14:43:49 Build was aborted\r\n14:43:49 [BFA] Scanning build for known causes...\r\n14:43:49 [BFA] No failure causes found\r\n14:43:49 [BFA] Done. 0s\r\n14:43:49 Finished: FAILURE\r\n```\r\n\r\nnot sure if this timeout means that the code now is slower.\r\n\r\nwhat do you think? do you have any suggestion?\n<issue_comment>username_3: Sometimes the Windows build flakes out like that. It didn't timeout while running a relevant test, so I judge it to be not your problem.\n<issue_comment>username_3: The launch bounds logic is wrong, but I acknowledge that this is a big patch already; just fix it in a follow up. I am going to go ahead and land this.\n<issue_comment>username_0: sounds good @username_3 thanks!"}
{"repo": "pytorch/pytorch", "issue_id": 436372857, "issue_number": 19630, "timestamp": "2019-05-04 20:18:22+00:00", "text": "@username_2 \r\n\r\nit seems it worked locally! thank you so much!  I really appreciate that!", "context": "<issue_start><issue_comment>Title: [WIP] UpSample GPU Porting \nusername_0: resolves #16158\n<issue_comment>username_1: Fewer errors than before, 4 instead of 8 with the same message:\r\n```\r\nRuntimeError: CUDA error: too many resources requested for launch\r\n```\r\nThis seems to be an existing problem: gh-8103.\r\n\r\n@username_2 could you have a look at this and tell us what you think?\n<issue_comment>username_2: Looking at it very briefly, the `Jetson TX` referenced in the issue only has 256 cores.\r\n\r\nSo while the issue looks the same, I'd probably not expect it on the CI platforms.  FWIW, some recent CI tests in other issues are green.\n<issue_comment>username_2: @pytorchbot retest this please.\n<issue_comment>username_1: same failures.\n<issue_comment>username_2: Is this rebased on the latest master (you can also ask the bot to rebase)?\n<issue_comment>username_0: I rebased that yesterday ... also needed to resolve conflicts because 7 days ago upsample files were changed.\n<issue_comment>username_0: @username_2 do you have any idea about what could be this problem? or a way to debug that?\r\nalso it seems some jobs is taking a lot of time to build, for example, for some job the estimate time is 19h .. not sure if it the regular estimate ..\n<issue_comment>username_2: @username_0 If you can't reproduce it at home it seems hard to debug other than reading at the diffs again.  I can take a look on Monday, it's getting a bit late here.\n<issue_comment>username_2: Also, has anyone found a way to show the actual hardware used on the CI in detail?\n<issue_comment>username_0: @username_2 I am working in parallel on a paperspace environment .. it tooks a lot of time it is running with 8 cpu cores.\n<issue_comment>username_2: But have you ever been able to reproduce this issue on paperspace?\n<issue_comment>username_1: I can reproduce it locally: Arch Linux, CUDA 10.0, RTX2070 GPU. Given the CI failures, I think it should be reproducible for multiple CUDA and GPU versions. I just haven't worked on any CUDA code before, and am short on time, so I'd rather not dig too deep.\n<issue_comment>username_0: not yet .. my last building was with a my previous commit ... I will work again in this task in some minutes :)\n<issue_comment>username_1: @username_0 your previous commit had the same failure though (except for the less clear exception message), and it seems quite reproducible. So I think you'll see it now.\n<issue_comment>username_2: Ah, thanks.  @username_0, then I guess just compiling with \"DEBUG=1\" and stepping through the offending code with gdb may be the fastest way.\n<issue_comment>username_0: @username_2 thanks! I will try that!\n<issue_comment>username_0: it seems just UpSampleBicubic2d is using upsample_get_value_bounded (https://github.com/pytorch/pytorch/pull/19630/files#diff-5092da792c30694ee4adf0d0ae2a37c6R171) and upsample_increment_value_bounded (https://github.com/pytorch/pytorch/pull/19630/files#diff-5092da792c30694ee4adf0d0ae2a37c6R191)\r\n\r\nmaybe the problem could be inside one of these functions ... maybe related to the order of indexes (x, y) ...\r\n\r\nbut the problem seems to be related to cuda block/threads ... so not sure if it is really related to these functions.\n<issue_comment>username_2: The new code uses far more registers than the existing one. I verified that the both versions actually call the offending test case with `1024` in `blockDim`.  So it's very likely a register issue.\r\n\r\n`Existing` uses `64` registers, which seems to be optimal or my card:\r\n\r\n```\r\nptxas info    : 77696 bytes gmem, 72 bytes cmem[3]\r\nptxas info    : Compiling entry function '_Z23bicubic_interp2d_kerneliddb15THCDeviceTensorIdLi4Ei16DefaultPtrTraitsES1_' for 'sm_61'\r\nptxas info    : Function properties for _Z23bicubic_interp2d_kerneliddb15THCDeviceTensorIdLi4Ei16DefaultPtrTraitsES1_\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 64 registers, 432 bytes cmem[0], 24 bytes cmem[2]\r\n```\r\n\r\n`New` uses `124` registers, which is too much for my card:\r\n\r\n```\r\nptxas info    : 77696 bytes gmem, 72 bytes cmem[3]\r\nptxas info    : Compiling entry function '_ZN2at6native76_GLOBAL__N__52_tmpxft_0000626d_00000000_6_UpSampleBicubic2d_cpp1_ii_b4c1e1f328upsample_bicubic2d_out_frameElddbNS_20PackedTensorAccessorIdLm4ENS_16DefaultPtrTraitsElEES4_' for 'sm_61'\r\nptxas info    : Function properties for _ZN2at6native76_GLOBAL__N__52_tmpxft_0000626d_00000000_6_UpSampleBicubic2d_cpp1_ii_b4c1e1f328upsample_bicubic2d_out_frameElddbNS_20PackedTensorAccessorIdLm4ENS_16DefaultPtrTraitsElEES4_\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 124 registers, 496 bytes cmem[0], 24 bytes cmem[2]\r\n```\r\n\r\nWith `__launch_bounds__(1024)`, the code again uses `64` registers.\r\n\r\nIf you use `C10_LAUNCH_BOUNDS_1(1024)` **for both kernels**, the tests pass here.\r\n\r\n\r\nNow *why* is the regcount higher in the new code? It could be `PackedTensorAccessor`, it could be the fact that many instances of `int` have been changed to `int64_t`. :)\r\n\r\nYou could experiment or just use the launch bounds.  Other code in `native/cuda` seems to use lower bounds for `blockDim`, too.  1024 seems to be an outlier.\n<issue_comment>username_0: @username_2 \r\n\r\nit seems it worked locally! thank you so much!  I really appreciate that!\n<issue_comment>username_0: thanks @username_1 and @username_2 for all the help!\r\n\r\n@username_3 it is done for review!\n<issue_comment>username_3: CircleCI runs on AWS, and the I'm fairly sure we've got one of the g3 types, either g3.4xlarge or g3.8xlarge\n<issue_comment>username_3: cc @ngimel, if you want to take a look at this (I'm planning to do a review too)\n<issue_comment>username_0: @username_4 could it be addressed in another PR?\n<issue_comment>username_4: @username_0 Sure, it's definitely better to rename both cpu & gpu impls together in a separate PR. Just want to make sure we have the correct one after porting.\n<issue_comment>username_3: Just as a clarification: I'm OK with having changes which are improvements over the old kernels come in latter patches.\n<issue_comment>username_3: I finished reviewing one of the kernels, and did a cursory scan of the others. In general the patch looks like it's in good shape. I'd suggest that we fix up the major correctness issues (PackedTensorAccessor ref, and int32 versus int64 indexing) and anything small that is easy to fix (for example, double zeroing the array), and leave other improvements for follow ups. Should be ready to land soon!\n<issue_comment>username_0: thanks so much @username_3!\r\nI will let you know when it is ready again for a new review! thanks!\n<issue_comment>username_0: @username_2 @username_3 \r\nnot sure but the errors on CI seems to be related to jenkins ... it seems all these jobs that failed (for building) ran by 01h 01min ... not sure if it is a coincidence ...\n<issue_comment>username_1: @username_0 indeed it looks like all jobs were aborted at the same time. no obvious issues in the build log related to your code. Comparing e.g. the `cuda9-cudnn7` build with a successful one from another PR, it takes 1hr 14min there and your build is about at the place where the other one is after an hour.\r\n\r\nI suggest to just push a new commit to rebuild. Probably a temporary CI hiccup.\n<issue_comment>username_3: I accidentally rebooted Jenkins yesterday which is the likely cause, my apologies.\r\n\r\n@pytorchbot retest this please\n<issue_comment>username_0: @username_3 @username_2 @username_1 \r\n\r\nall tests passed except `pr/caffe2-py2-cuda9.0-cudnn7-windows-build`:\r\n \r\n```\r\n14:43:49 Build timed out (after 180 minutes). Marking the build as failed.\r\n14:43:49 Build was aborted\r\n14:43:49 [BFA] Scanning build for known causes...\r\n14:43:49 [BFA] No failure causes found\r\n14:43:49 [BFA] Done. 0s\r\n14:43:49 Finished: FAILURE\r\n```\r\n\r\nnot sure if this timeout means that the code now is slower.\r\n\r\nwhat do you think? do you have any suggestion?\n<issue_comment>username_3: Sometimes the Windows build flakes out like that. It didn't timeout while running a relevant test, so I judge it to be not your problem.\n<issue_comment>username_3: The launch bounds logic is wrong, but I acknowledge that this is a big patch already; just fix it in a follow up. I am going to go ahead and land this.\n<issue_comment>username_0: sounds good @username_3 thanks!"}
{"repo": "pytorch/pytorch", "issue_id": 436372857, "issue_number": 19630, "timestamp": "2019-05-04 23:08:04+00:00", "text": "thanks @username_1 and @username_2 for all the help!\r\n\r\n@username_3 it is done for review!", "context": "<issue_start><issue_comment>Title: [WIP] UpSample GPU Porting \nusername_0: resolves #16158\n<issue_comment>username_1: Fewer errors than before, 4 instead of 8 with the same message:\r\n```\r\nRuntimeError: CUDA error: too many resources requested for launch\r\n```\r\nThis seems to be an existing problem: gh-8103.\r\n\r\n@username_2 could you have a look at this and tell us what you think?\n<issue_comment>username_2: Looking at it very briefly, the `Jetson TX` referenced in the issue only has 256 cores.\r\n\r\nSo while the issue looks the same, I'd probably not expect it on the CI platforms.  FWIW, some recent CI tests in other issues are green.\n<issue_comment>username_2: @pytorchbot retest this please.\n<issue_comment>username_1: same failures.\n<issue_comment>username_2: Is this rebased on the latest master (you can also ask the bot to rebase)?\n<issue_comment>username_0: I rebased that yesterday ... also needed to resolve conflicts because 7 days ago upsample files were changed.\n<issue_comment>username_0: @username_2 do you have any idea about what could be this problem? or a way to debug that?\r\nalso it seems some jobs is taking a lot of time to build, for example, for some job the estimate time is 19h .. not sure if it the regular estimate ..\n<issue_comment>username_2: @username_0 If you can't reproduce it at home it seems hard to debug other than reading at the diffs again.  I can take a look on Monday, it's getting a bit late here.\n<issue_comment>username_2: Also, has anyone found a way to show the actual hardware used on the CI in detail?\n<issue_comment>username_0: @username_2 I am working in parallel on a paperspace environment .. it tooks a lot of time it is running with 8 cpu cores.\n<issue_comment>username_2: But have you ever been able to reproduce this issue on paperspace?\n<issue_comment>username_1: I can reproduce it locally: Arch Linux, CUDA 10.0, RTX2070 GPU. Given the CI failures, I think it should be reproducible for multiple CUDA and GPU versions. I just haven't worked on any CUDA code before, and am short on time, so I'd rather not dig too deep.\n<issue_comment>username_0: not yet .. my last building was with a my previous commit ... I will work again in this task in some minutes :)\n<issue_comment>username_1: @username_0 your previous commit had the same failure though (except for the less clear exception message), and it seems quite reproducible. So I think you'll see it now.\n<issue_comment>username_2: Ah, thanks.  @username_0, then I guess just compiling with \"DEBUG=1\" and stepping through the offending code with gdb may be the fastest way.\n<issue_comment>username_0: @username_2 thanks! I will try that!\n<issue_comment>username_0: it seems just UpSampleBicubic2d is using upsample_get_value_bounded (https://github.com/pytorch/pytorch/pull/19630/files#diff-5092da792c30694ee4adf0d0ae2a37c6R171) and upsample_increment_value_bounded (https://github.com/pytorch/pytorch/pull/19630/files#diff-5092da792c30694ee4adf0d0ae2a37c6R191)\r\n\r\nmaybe the problem could be inside one of these functions ... maybe related to the order of indexes (x, y) ...\r\n\r\nbut the problem seems to be related to cuda block/threads ... so not sure if it is really related to these functions.\n<issue_comment>username_2: The new code uses far more registers than the existing one. I verified that the both versions actually call the offending test case with `1024` in `blockDim`.  So it's very likely a register issue.\r\n\r\n`Existing` uses `64` registers, which seems to be optimal or my card:\r\n\r\n```\r\nptxas info    : 77696 bytes gmem, 72 bytes cmem[3]\r\nptxas info    : Compiling entry function '_Z23bicubic_interp2d_kerneliddb15THCDeviceTensorIdLi4Ei16DefaultPtrTraitsES1_' for 'sm_61'\r\nptxas info    : Function properties for _Z23bicubic_interp2d_kerneliddb15THCDeviceTensorIdLi4Ei16DefaultPtrTraitsES1_\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 64 registers, 432 bytes cmem[0], 24 bytes cmem[2]\r\n```\r\n\r\n`New` uses `124` registers, which is too much for my card:\r\n\r\n```\r\nptxas info    : 77696 bytes gmem, 72 bytes cmem[3]\r\nptxas info    : Compiling entry function '_ZN2at6native76_GLOBAL__N__52_tmpxft_0000626d_00000000_6_UpSampleBicubic2d_cpp1_ii_b4c1e1f328upsample_bicubic2d_out_frameElddbNS_20PackedTensorAccessorIdLm4ENS_16DefaultPtrTraitsElEES4_' for 'sm_61'\r\nptxas info    : Function properties for _ZN2at6native76_GLOBAL__N__52_tmpxft_0000626d_00000000_6_UpSampleBicubic2d_cpp1_ii_b4c1e1f328upsample_bicubic2d_out_frameElddbNS_20PackedTensorAccessorIdLm4ENS_16DefaultPtrTraitsElEES4_\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 124 registers, 496 bytes cmem[0], 24 bytes cmem[2]\r\n```\r\n\r\nWith `__launch_bounds__(1024)`, the code again uses `64` registers.\r\n\r\nIf you use `C10_LAUNCH_BOUNDS_1(1024)` **for both kernels**, the tests pass here.\r\n\r\n\r\nNow *why* is the regcount higher in the new code? It could be `PackedTensorAccessor`, it could be the fact that many instances of `int` have been changed to `int64_t`. :)\r\n\r\nYou could experiment or just use the launch bounds.  Other code in `native/cuda` seems to use lower bounds for `blockDim`, too.  1024 seems to be an outlier.\n<issue_comment>username_0: @username_2 \r\n\r\nit seems it worked locally! thank you so much!  I really appreciate that!\n<issue_comment>username_0: thanks @username_1 and @username_2 for all the help!\r\n\r\n@username_3 it is done for review!\n<issue_comment>username_3: CircleCI runs on AWS, and the I'm fairly sure we've got one of the g3 types, either g3.4xlarge or g3.8xlarge\n<issue_comment>username_3: cc @ngimel, if you want to take a look at this (I'm planning to do a review too)\n<issue_comment>username_0: @username_4 could it be addressed in another PR?\n<issue_comment>username_4: @username_0 Sure, it's definitely better to rename both cpu & gpu impls together in a separate PR. Just want to make sure we have the correct one after porting.\n<issue_comment>username_3: Just as a clarification: I'm OK with having changes which are improvements over the old kernels come in latter patches.\n<issue_comment>username_3: I finished reviewing one of the kernels, and did a cursory scan of the others. In general the patch looks like it's in good shape. I'd suggest that we fix up the major correctness issues (PackedTensorAccessor ref, and int32 versus int64 indexing) and anything small that is easy to fix (for example, double zeroing the array), and leave other improvements for follow ups. Should be ready to land soon!\n<issue_comment>username_0: thanks so much @username_3!\r\nI will let you know when it is ready again for a new review! thanks!\n<issue_comment>username_0: @username_2 @username_3 \r\nnot sure but the errors on CI seems to be related to jenkins ... it seems all these jobs that failed (for building) ran by 01h 01min ... not sure if it is a coincidence ...\n<issue_comment>username_1: @username_0 indeed it looks like all jobs were aborted at the same time. no obvious issues in the build log related to your code. Comparing e.g. the `cuda9-cudnn7` build with a successful one from another PR, it takes 1hr 14min there and your build is about at the place where the other one is after an hour.\r\n\r\nI suggest to just push a new commit to rebuild. Probably a temporary CI hiccup.\n<issue_comment>username_3: I accidentally rebooted Jenkins yesterday which is the likely cause, my apologies.\r\n\r\n@pytorchbot retest this please\n<issue_comment>username_0: @username_3 @username_2 @username_1 \r\n\r\nall tests passed except `pr/caffe2-py2-cuda9.0-cudnn7-windows-build`:\r\n \r\n```\r\n14:43:49 Build timed out (after 180 minutes). Marking the build as failed.\r\n14:43:49 Build was aborted\r\n14:43:49 [BFA] Scanning build for known causes...\r\n14:43:49 [BFA] No failure causes found\r\n14:43:49 [BFA] Done. 0s\r\n14:43:49 Finished: FAILURE\r\n```\r\n\r\nnot sure if this timeout means that the code now is slower.\r\n\r\nwhat do you think? do you have any suggestion?\n<issue_comment>username_3: Sometimes the Windows build flakes out like that. It didn't timeout while running a relevant test, so I judge it to be not your problem.\n<issue_comment>username_3: The launch bounds logic is wrong, but I acknowledge that this is a big patch already; just fix it in a follow up. I am going to go ahead and land this.\n<issue_comment>username_0: sounds good @username_3 thanks!"}
{"repo": "pytorch/pytorch", "issue_id": 436372857, "issue_number": 19630, "timestamp": "2019-05-06 14:50:44+00:00", "text": "CircleCI runs on AWS, and the I'm fairly sure we've got one of the g3 types, either g3.4xlarge or g3.8xlarge", "context": "<issue_start><issue_comment>Title: [WIP] UpSample GPU Porting \nusername_0: resolves #16158\n<issue_comment>username_1: Fewer errors than before, 4 instead of 8 with the same message:\r\n```\r\nRuntimeError: CUDA error: too many resources requested for launch\r\n```\r\nThis seems to be an existing problem: gh-8103.\r\n\r\n@username_2 could you have a look at this and tell us what you think?\n<issue_comment>username_2: Looking at it very briefly, the `Jetson TX` referenced in the issue only has 256 cores.\r\n\r\nSo while the issue looks the same, I'd probably not expect it on the CI platforms.  FWIW, some recent CI tests in other issues are green.\n<issue_comment>username_2: @pytorchbot retest this please.\n<issue_comment>username_1: same failures.\n<issue_comment>username_2: Is this rebased on the latest master (you can also ask the bot to rebase)?\n<issue_comment>username_0: I rebased that yesterday ... also needed to resolve conflicts because 7 days ago upsample files were changed.\n<issue_comment>username_0: @username_2 do you have any idea about what could be this problem? or a way to debug that?\r\nalso it seems some jobs is taking a lot of time to build, for example, for some job the estimate time is 19h .. not sure if it the regular estimate ..\n<issue_comment>username_2: @username_0 If you can't reproduce it at home it seems hard to debug other than reading at the diffs again.  I can take a look on Monday, it's getting a bit late here.\n<issue_comment>username_2: Also, has anyone found a way to show the actual hardware used on the CI in detail?\n<issue_comment>username_0: @username_2 I am working in parallel on a paperspace environment .. it tooks a lot of time it is running with 8 cpu cores.\n<issue_comment>username_2: But have you ever been able to reproduce this issue on paperspace?\n<issue_comment>username_1: I can reproduce it locally: Arch Linux, CUDA 10.0, RTX2070 GPU. Given the CI failures, I think it should be reproducible for multiple CUDA and GPU versions. I just haven't worked on any CUDA code before, and am short on time, so I'd rather not dig too deep.\n<issue_comment>username_0: not yet .. my last building was with a my previous commit ... I will work again in this task in some minutes :)\n<issue_comment>username_1: @username_0 your previous commit had the same failure though (except for the less clear exception message), and it seems quite reproducible. So I think you'll see it now.\n<issue_comment>username_2: Ah, thanks.  @username_0, then I guess just compiling with \"DEBUG=1\" and stepping through the offending code with gdb may be the fastest way.\n<issue_comment>username_0: @username_2 thanks! I will try that!\n<issue_comment>username_0: it seems just UpSampleBicubic2d is using upsample_get_value_bounded (https://github.com/pytorch/pytorch/pull/19630/files#diff-5092da792c30694ee4adf0d0ae2a37c6R171) and upsample_increment_value_bounded (https://github.com/pytorch/pytorch/pull/19630/files#diff-5092da792c30694ee4adf0d0ae2a37c6R191)\r\n\r\nmaybe the problem could be inside one of these functions ... maybe related to the order of indexes (x, y) ...\r\n\r\nbut the problem seems to be related to cuda block/threads ... so not sure if it is really related to these functions.\n<issue_comment>username_2: The new code uses far more registers than the existing one. I verified that the both versions actually call the offending test case with `1024` in `blockDim`.  So it's very likely a register issue.\r\n\r\n`Existing` uses `64` registers, which seems to be optimal or my card:\r\n\r\n```\r\nptxas info    : 77696 bytes gmem, 72 bytes cmem[3]\r\nptxas info    : Compiling entry function '_Z23bicubic_interp2d_kerneliddb15THCDeviceTensorIdLi4Ei16DefaultPtrTraitsES1_' for 'sm_61'\r\nptxas info    : Function properties for _Z23bicubic_interp2d_kerneliddb15THCDeviceTensorIdLi4Ei16DefaultPtrTraitsES1_\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 64 registers, 432 bytes cmem[0], 24 bytes cmem[2]\r\n```\r\n\r\n`New` uses `124` registers, which is too much for my card:\r\n\r\n```\r\nptxas info    : 77696 bytes gmem, 72 bytes cmem[3]\r\nptxas info    : Compiling entry function '_ZN2at6native76_GLOBAL__N__52_tmpxft_0000626d_00000000_6_UpSampleBicubic2d_cpp1_ii_b4c1e1f328upsample_bicubic2d_out_frameElddbNS_20PackedTensorAccessorIdLm4ENS_16DefaultPtrTraitsElEES4_' for 'sm_61'\r\nptxas info    : Function properties for _ZN2at6native76_GLOBAL__N__52_tmpxft_0000626d_00000000_6_UpSampleBicubic2d_cpp1_ii_b4c1e1f328upsample_bicubic2d_out_frameElddbNS_20PackedTensorAccessorIdLm4ENS_16DefaultPtrTraitsElEES4_\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 124 registers, 496 bytes cmem[0], 24 bytes cmem[2]\r\n```\r\n\r\nWith `__launch_bounds__(1024)`, the code again uses `64` registers.\r\n\r\nIf you use `C10_LAUNCH_BOUNDS_1(1024)` **for both kernels**, the tests pass here.\r\n\r\n\r\nNow *why* is the regcount higher in the new code? It could be `PackedTensorAccessor`, it could be the fact that many instances of `int` have been changed to `int64_t`. :)\r\n\r\nYou could experiment or just use the launch bounds.  Other code in `native/cuda` seems to use lower bounds for `blockDim`, too.  1024 seems to be an outlier.\n<issue_comment>username_0: @username_2 \r\n\r\nit seems it worked locally! thank you so much!  I really appreciate that!\n<issue_comment>username_0: thanks @username_1 and @username_2 for all the help!\r\n\r\n@username_3 it is done for review!\n<issue_comment>username_3: CircleCI runs on AWS, and the I'm fairly sure we've got one of the g3 types, either g3.4xlarge or g3.8xlarge\n<issue_comment>username_3: cc @ngimel, if you want to take a look at this (I'm planning to do a review too)\n<issue_comment>username_0: @username_4 could it be addressed in another PR?\n<issue_comment>username_4: @username_0 Sure, it's definitely better to rename both cpu & gpu impls together in a separate PR. Just want to make sure we have the correct one after porting.\n<issue_comment>username_3: Just as a clarification: I'm OK with having changes which are improvements over the old kernels come in latter patches.\n<issue_comment>username_3: I finished reviewing one of the kernels, and did a cursory scan of the others. In general the patch looks like it's in good shape. I'd suggest that we fix up the major correctness issues (PackedTensorAccessor ref, and int32 versus int64 indexing) and anything small that is easy to fix (for example, double zeroing the array), and leave other improvements for follow ups. Should be ready to land soon!\n<issue_comment>username_0: thanks so much @username_3!\r\nI will let you know when it is ready again for a new review! thanks!\n<issue_comment>username_0: @username_2 @username_3 \r\nnot sure but the errors on CI seems to be related to jenkins ... it seems all these jobs that failed (for building) ran by 01h 01min ... not sure if it is a coincidence ...\n<issue_comment>username_1: @username_0 indeed it looks like all jobs were aborted at the same time. no obvious issues in the build log related to your code. Comparing e.g. the `cuda9-cudnn7` build with a successful one from another PR, it takes 1hr 14min there and your build is about at the place where the other one is after an hour.\r\n\r\nI suggest to just push a new commit to rebuild. Probably a temporary CI hiccup.\n<issue_comment>username_3: I accidentally rebooted Jenkins yesterday which is the likely cause, my apologies.\r\n\r\n@pytorchbot retest this please\n<issue_comment>username_0: @username_3 @username_2 @username_1 \r\n\r\nall tests passed except `pr/caffe2-py2-cuda9.0-cudnn7-windows-build`:\r\n \r\n```\r\n14:43:49 Build timed out (after 180 minutes). Marking the build as failed.\r\n14:43:49 Build was aborted\r\n14:43:49 [BFA] Scanning build for known causes...\r\n14:43:49 [BFA] No failure causes found\r\n14:43:49 [BFA] Done. 0s\r\n14:43:49 Finished: FAILURE\r\n```\r\n\r\nnot sure if this timeout means that the code now is slower.\r\n\r\nwhat do you think? do you have any suggestion?\n<issue_comment>username_3: Sometimes the Windows build flakes out like that. It didn't timeout while running a relevant test, so I judge it to be not your problem.\n<issue_comment>username_3: The launch bounds logic is wrong, but I acknowledge that this is a big patch already; just fix it in a follow up. I am going to go ahead and land this.\n<issue_comment>username_0: sounds good @username_3 thanks!"}
{"repo": "pytorch/pytorch", "issue_id": 436372857, "issue_number": 19630, "timestamp": "2019-05-06 14:53:32+00:00", "text": "cc @ngimel, if you want to take a look at this (I'm planning to do a review too)", "context": "<issue_start><issue_comment>Title: [WIP] UpSample GPU Porting \nusername_0: resolves #16158\n<issue_comment>username_1: Fewer errors than before, 4 instead of 8 with the same message:\r\n```\r\nRuntimeError: CUDA error: too many resources requested for launch\r\n```\r\nThis seems to be an existing problem: gh-8103.\r\n\r\n@username_2 could you have a look at this and tell us what you think?\n<issue_comment>username_2: Looking at it very briefly, the `Jetson TX` referenced in the issue only has 256 cores.\r\n\r\nSo while the issue looks the same, I'd probably not expect it on the CI platforms.  FWIW, some recent CI tests in other issues are green.\n<issue_comment>username_2: @pytorchbot retest this please.\n<issue_comment>username_1: same failures.\n<issue_comment>username_2: Is this rebased on the latest master (you can also ask the bot to rebase)?\n<issue_comment>username_0: I rebased that yesterday ... also needed to resolve conflicts because 7 days ago upsample files were changed.\n<issue_comment>username_0: @username_2 do you have any idea about what could be this problem? or a way to debug that?\r\nalso it seems some jobs is taking a lot of time to build, for example, for some job the estimate time is 19h .. not sure if it the regular estimate ..\n<issue_comment>username_2: @username_0 If you can't reproduce it at home it seems hard to debug other than reading at the diffs again.  I can take a look on Monday, it's getting a bit late here.\n<issue_comment>username_2: Also, has anyone found a way to show the actual hardware used on the CI in detail?\n<issue_comment>username_0: @username_2 I am working in parallel on a paperspace environment .. it tooks a lot of time it is running with 8 cpu cores.\n<issue_comment>username_2: But have you ever been able to reproduce this issue on paperspace?\n<issue_comment>username_1: I can reproduce it locally: Arch Linux, CUDA 10.0, RTX2070 GPU. Given the CI failures, I think it should be reproducible for multiple CUDA and GPU versions. I just haven't worked on any CUDA code before, and am short on time, so I'd rather not dig too deep.\n<issue_comment>username_0: not yet .. my last building was with a my previous commit ... I will work again in this task in some minutes :)\n<issue_comment>username_1: @username_0 your previous commit had the same failure though (except for the less clear exception message), and it seems quite reproducible. So I think you'll see it now.\n<issue_comment>username_2: Ah, thanks.  @username_0, then I guess just compiling with \"DEBUG=1\" and stepping through the offending code with gdb may be the fastest way.\n<issue_comment>username_0: @username_2 thanks! I will try that!\n<issue_comment>username_0: it seems just UpSampleBicubic2d is using upsample_get_value_bounded (https://github.com/pytorch/pytorch/pull/19630/files#diff-5092da792c30694ee4adf0d0ae2a37c6R171) and upsample_increment_value_bounded (https://github.com/pytorch/pytorch/pull/19630/files#diff-5092da792c30694ee4adf0d0ae2a37c6R191)\r\n\r\nmaybe the problem could be inside one of these functions ... maybe related to the order of indexes (x, y) ...\r\n\r\nbut the problem seems to be related to cuda block/threads ... so not sure if it is really related to these functions.\n<issue_comment>username_2: The new code uses far more registers than the existing one. I verified that the both versions actually call the offending test case with `1024` in `blockDim`.  So it's very likely a register issue.\r\n\r\n`Existing` uses `64` registers, which seems to be optimal or my card:\r\n\r\n```\r\nptxas info    : 77696 bytes gmem, 72 bytes cmem[3]\r\nptxas info    : Compiling entry function '_Z23bicubic_interp2d_kerneliddb15THCDeviceTensorIdLi4Ei16DefaultPtrTraitsES1_' for 'sm_61'\r\nptxas info    : Function properties for _Z23bicubic_interp2d_kerneliddb15THCDeviceTensorIdLi4Ei16DefaultPtrTraitsES1_\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 64 registers, 432 bytes cmem[0], 24 bytes cmem[2]\r\n```\r\n\r\n`New` uses `124` registers, which is too much for my card:\r\n\r\n```\r\nptxas info    : 77696 bytes gmem, 72 bytes cmem[3]\r\nptxas info    : Compiling entry function '_ZN2at6native76_GLOBAL__N__52_tmpxft_0000626d_00000000_6_UpSampleBicubic2d_cpp1_ii_b4c1e1f328upsample_bicubic2d_out_frameElddbNS_20PackedTensorAccessorIdLm4ENS_16DefaultPtrTraitsElEES4_' for 'sm_61'\r\nptxas info    : Function properties for _ZN2at6native76_GLOBAL__N__52_tmpxft_0000626d_00000000_6_UpSampleBicubic2d_cpp1_ii_b4c1e1f328upsample_bicubic2d_out_frameElddbNS_20PackedTensorAccessorIdLm4ENS_16DefaultPtrTraitsElEES4_\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 124 registers, 496 bytes cmem[0], 24 bytes cmem[2]\r\n```\r\n\r\nWith `__launch_bounds__(1024)`, the code again uses `64` registers.\r\n\r\nIf you use `C10_LAUNCH_BOUNDS_1(1024)` **for both kernels**, the tests pass here.\r\n\r\n\r\nNow *why* is the regcount higher in the new code? It could be `PackedTensorAccessor`, it could be the fact that many instances of `int` have been changed to `int64_t`. :)\r\n\r\nYou could experiment or just use the launch bounds.  Other code in `native/cuda` seems to use lower bounds for `blockDim`, too.  1024 seems to be an outlier.\n<issue_comment>username_0: @username_2 \r\n\r\nit seems it worked locally! thank you so much!  I really appreciate that!\n<issue_comment>username_0: thanks @username_1 and @username_2 for all the help!\r\n\r\n@username_3 it is done for review!\n<issue_comment>username_3: CircleCI runs on AWS, and the I'm fairly sure we've got one of the g3 types, either g3.4xlarge or g3.8xlarge\n<issue_comment>username_3: cc @ngimel, if you want to take a look at this (I'm planning to do a review too)\n<issue_comment>username_0: @username_4 could it be addressed in another PR?\n<issue_comment>username_4: @username_0 Sure, it's definitely better to rename both cpu & gpu impls together in a separate PR. Just want to make sure we have the correct one after porting.\n<issue_comment>username_3: Just as a clarification: I'm OK with having changes which are improvements over the old kernels come in latter patches.\n<issue_comment>username_3: I finished reviewing one of the kernels, and did a cursory scan of the others. In general the patch looks like it's in good shape. I'd suggest that we fix up the major correctness issues (PackedTensorAccessor ref, and int32 versus int64 indexing) and anything small that is easy to fix (for example, double zeroing the array), and leave other improvements for follow ups. Should be ready to land soon!\n<issue_comment>username_0: thanks so much @username_3!\r\nI will let you know when it is ready again for a new review! thanks!\n<issue_comment>username_0: @username_2 @username_3 \r\nnot sure but the errors on CI seems to be related to jenkins ... it seems all these jobs that failed (for building) ran by 01h 01min ... not sure if it is a coincidence ...\n<issue_comment>username_1: @username_0 indeed it looks like all jobs were aborted at the same time. no obvious issues in the build log related to your code. Comparing e.g. the `cuda9-cudnn7` build with a successful one from another PR, it takes 1hr 14min there and your build is about at the place where the other one is after an hour.\r\n\r\nI suggest to just push a new commit to rebuild. Probably a temporary CI hiccup.\n<issue_comment>username_3: I accidentally rebooted Jenkins yesterday which is the likely cause, my apologies.\r\n\r\n@pytorchbot retest this please\n<issue_comment>username_0: @username_3 @username_2 @username_1 \r\n\r\nall tests passed except `pr/caffe2-py2-cuda9.0-cudnn7-windows-build`:\r\n \r\n```\r\n14:43:49 Build timed out (after 180 minutes). Marking the build as failed.\r\n14:43:49 Build was aborted\r\n14:43:49 [BFA] Scanning build for known causes...\r\n14:43:49 [BFA] No failure causes found\r\n14:43:49 [BFA] Done. 0s\r\n14:43:49 Finished: FAILURE\r\n```\r\n\r\nnot sure if this timeout means that the code now is slower.\r\n\r\nwhat do you think? do you have any suggestion?\n<issue_comment>username_3: Sometimes the Windows build flakes out like that. It didn't timeout while running a relevant test, so I judge it to be not your problem.\n<issue_comment>username_3: The launch bounds logic is wrong, but I acknowledge that this is a big patch already; just fix it in a follow up. I am going to go ahead and land this.\n<issue_comment>username_0: sounds good @username_3 thanks!"}
{"repo": "pytorch/pytorch", "issue_id": 436372857, "issue_number": 19630, "timestamp": "2019-05-06 20:38:43+00:00", "text": "@username_4 could it be addressed in another PR?", "context": "<issue_start><issue_comment>Title: [WIP] UpSample GPU Porting \nusername_0: resolves #16158\n<issue_comment>username_1: Fewer errors than before, 4 instead of 8 with the same message:\r\n```\r\nRuntimeError: CUDA error: too many resources requested for launch\r\n```\r\nThis seems to be an existing problem: gh-8103.\r\n\r\n@username_2 could you have a look at this and tell us what you think?\n<issue_comment>username_2: Looking at it very briefly, the `Jetson TX` referenced in the issue only has 256 cores.\r\n\r\nSo while the issue looks the same, I'd probably not expect it on the CI platforms.  FWIW, some recent CI tests in other issues are green.\n<issue_comment>username_2: @pytorchbot retest this please.\n<issue_comment>username_1: same failures.\n<issue_comment>username_2: Is this rebased on the latest master (you can also ask the bot to rebase)?\n<issue_comment>username_0: I rebased that yesterday ... also needed to resolve conflicts because 7 days ago upsample files were changed.\n<issue_comment>username_0: @username_2 do you have any idea about what could be this problem? or a way to debug that?\r\nalso it seems some jobs is taking a lot of time to build, for example, for some job the estimate time is 19h .. not sure if it the regular estimate ..\n<issue_comment>username_2: @username_0 If you can't reproduce it at home it seems hard to debug other than reading at the diffs again.  I can take a look on Monday, it's getting a bit late here.\n<issue_comment>username_2: Also, has anyone found a way to show the actual hardware used on the CI in detail?\n<issue_comment>username_0: @username_2 I am working in parallel on a paperspace environment .. it tooks a lot of time it is running with 8 cpu cores.\n<issue_comment>username_2: But have you ever been able to reproduce this issue on paperspace?\n<issue_comment>username_1: I can reproduce it locally: Arch Linux, CUDA 10.0, RTX2070 GPU. Given the CI failures, I think it should be reproducible for multiple CUDA and GPU versions. I just haven't worked on any CUDA code before, and am short on time, so I'd rather not dig too deep.\n<issue_comment>username_0: not yet .. my last building was with a my previous commit ... I will work again in this task in some minutes :)\n<issue_comment>username_1: @username_0 your previous commit had the same failure though (except for the less clear exception message), and it seems quite reproducible. So I think you'll see it now.\n<issue_comment>username_2: Ah, thanks.  @username_0, then I guess just compiling with \"DEBUG=1\" and stepping through the offending code with gdb may be the fastest way.\n<issue_comment>username_0: @username_2 thanks! I will try that!\n<issue_comment>username_0: it seems just UpSampleBicubic2d is using upsample_get_value_bounded (https://github.com/pytorch/pytorch/pull/19630/files#diff-5092da792c30694ee4adf0d0ae2a37c6R171) and upsample_increment_value_bounded (https://github.com/pytorch/pytorch/pull/19630/files#diff-5092da792c30694ee4adf0d0ae2a37c6R191)\r\n\r\nmaybe the problem could be inside one of these functions ... maybe related to the order of indexes (x, y) ...\r\n\r\nbut the problem seems to be related to cuda block/threads ... so not sure if it is really related to these functions.\n<issue_comment>username_2: The new code uses far more registers than the existing one. I verified that the both versions actually call the offending test case with `1024` in `blockDim`.  So it's very likely a register issue.\r\n\r\n`Existing` uses `64` registers, which seems to be optimal or my card:\r\n\r\n```\r\nptxas info    : 77696 bytes gmem, 72 bytes cmem[3]\r\nptxas info    : Compiling entry function '_Z23bicubic_interp2d_kerneliddb15THCDeviceTensorIdLi4Ei16DefaultPtrTraitsES1_' for 'sm_61'\r\nptxas info    : Function properties for _Z23bicubic_interp2d_kerneliddb15THCDeviceTensorIdLi4Ei16DefaultPtrTraitsES1_\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 64 registers, 432 bytes cmem[0], 24 bytes cmem[2]\r\n```\r\n\r\n`New` uses `124` registers, which is too much for my card:\r\n\r\n```\r\nptxas info    : 77696 bytes gmem, 72 bytes cmem[3]\r\nptxas info    : Compiling entry function '_ZN2at6native76_GLOBAL__N__52_tmpxft_0000626d_00000000_6_UpSampleBicubic2d_cpp1_ii_b4c1e1f328upsample_bicubic2d_out_frameElddbNS_20PackedTensorAccessorIdLm4ENS_16DefaultPtrTraitsElEES4_' for 'sm_61'\r\nptxas info    : Function properties for _ZN2at6native76_GLOBAL__N__52_tmpxft_0000626d_00000000_6_UpSampleBicubic2d_cpp1_ii_b4c1e1f328upsample_bicubic2d_out_frameElddbNS_20PackedTensorAccessorIdLm4ENS_16DefaultPtrTraitsElEES4_\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 124 registers, 496 bytes cmem[0], 24 bytes cmem[2]\r\n```\r\n\r\nWith `__launch_bounds__(1024)`, the code again uses `64` registers.\r\n\r\nIf you use `C10_LAUNCH_BOUNDS_1(1024)` **for both kernels**, the tests pass here.\r\n\r\n\r\nNow *why* is the regcount higher in the new code? It could be `PackedTensorAccessor`, it could be the fact that many instances of `int` have been changed to `int64_t`. :)\r\n\r\nYou could experiment or just use the launch bounds.  Other code in `native/cuda` seems to use lower bounds for `blockDim`, too.  1024 seems to be an outlier.\n<issue_comment>username_0: @username_2 \r\n\r\nit seems it worked locally! thank you so much!  I really appreciate that!\n<issue_comment>username_0: thanks @username_1 and @username_2 for all the help!\r\n\r\n@username_3 it is done for review!\n<issue_comment>username_3: CircleCI runs on AWS, and the I'm fairly sure we've got one of the g3 types, either g3.4xlarge or g3.8xlarge\n<issue_comment>username_3: cc @ngimel, if you want to take a look at this (I'm planning to do a review too)\n<issue_comment>username_0: @username_4 could it be addressed in another PR?\n<issue_comment>username_4: @username_0 Sure, it's definitely better to rename both cpu & gpu impls together in a separate PR. Just want to make sure we have the correct one after porting.\n<issue_comment>username_3: Just as a clarification: I'm OK with having changes which are improvements over the old kernels come in latter patches.\n<issue_comment>username_3: I finished reviewing one of the kernels, and did a cursory scan of the others. In general the patch looks like it's in good shape. I'd suggest that we fix up the major correctness issues (PackedTensorAccessor ref, and int32 versus int64 indexing) and anything small that is easy to fix (for example, double zeroing the array), and leave other improvements for follow ups. Should be ready to land soon!\n<issue_comment>username_0: thanks so much @username_3!\r\nI will let you know when it is ready again for a new review! thanks!\n<issue_comment>username_0: @username_2 @username_3 \r\nnot sure but the errors on CI seems to be related to jenkins ... it seems all these jobs that failed (for building) ran by 01h 01min ... not sure if it is a coincidence ...\n<issue_comment>username_1: @username_0 indeed it looks like all jobs were aborted at the same time. no obvious issues in the build log related to your code. Comparing e.g. the `cuda9-cudnn7` build with a successful one from another PR, it takes 1hr 14min there and your build is about at the place where the other one is after an hour.\r\n\r\nI suggest to just push a new commit to rebuild. Probably a temporary CI hiccup.\n<issue_comment>username_3: I accidentally rebooted Jenkins yesterday which is the likely cause, my apologies.\r\n\r\n@pytorchbot retest this please\n<issue_comment>username_0: @username_3 @username_2 @username_1 \r\n\r\nall tests passed except `pr/caffe2-py2-cuda9.0-cudnn7-windows-build`:\r\n \r\n```\r\n14:43:49 Build timed out (after 180 minutes). Marking the build as failed.\r\n14:43:49 Build was aborted\r\n14:43:49 [BFA] Scanning build for known causes...\r\n14:43:49 [BFA] No failure causes found\r\n14:43:49 [BFA] Done. 0s\r\n14:43:49 Finished: FAILURE\r\n```\r\n\r\nnot sure if this timeout means that the code now is slower.\r\n\r\nwhat do you think? do you have any suggestion?\n<issue_comment>username_3: Sometimes the Windows build flakes out like that. It didn't timeout while running a relevant test, so I judge it to be not your problem.\n<issue_comment>username_3: The launch bounds logic is wrong, but I acknowledge that this is a big patch already; just fix it in a follow up. I am going to go ahead and land this.\n<issue_comment>username_0: sounds good @username_3 thanks!"}
{"repo": "pytorch/pytorch", "issue_id": 436372857, "issue_number": 19630, "timestamp": "2019-05-06 20:41:43+00:00", "text": "@username_0 Sure, it's definitely better to rename both cpu & gpu impls together in a separate PR. Just want to make sure we have the correct one after porting.", "context": "<issue_start><issue_comment>Title: [WIP] UpSample GPU Porting \nusername_0: resolves #16158\n<issue_comment>username_1: Fewer errors than before, 4 instead of 8 with the same message:\r\n```\r\nRuntimeError: CUDA error: too many resources requested for launch\r\n```\r\nThis seems to be an existing problem: gh-8103.\r\n\r\n@username_2 could you have a look at this and tell us what you think?\n<issue_comment>username_2: Looking at it very briefly, the `Jetson TX` referenced in the issue only has 256 cores.\r\n\r\nSo while the issue looks the same, I'd probably not expect it on the CI platforms.  FWIW, some recent CI tests in other issues are green.\n<issue_comment>username_2: @pytorchbot retest this please.\n<issue_comment>username_1: same failures.\n<issue_comment>username_2: Is this rebased on the latest master (you can also ask the bot to rebase)?\n<issue_comment>username_0: I rebased that yesterday ... also needed to resolve conflicts because 7 days ago upsample files were changed.\n<issue_comment>username_0: @username_2 do you have any idea about what could be this problem? or a way to debug that?\r\nalso it seems some jobs is taking a lot of time to build, for example, for some job the estimate time is 19h .. not sure if it the regular estimate ..\n<issue_comment>username_2: @username_0 If you can't reproduce it at home it seems hard to debug other than reading at the diffs again.  I can take a look on Monday, it's getting a bit late here.\n<issue_comment>username_2: Also, has anyone found a way to show the actual hardware used on the CI in detail?\n<issue_comment>username_0: @username_2 I am working in parallel on a paperspace environment .. it tooks a lot of time it is running with 8 cpu cores.\n<issue_comment>username_2: But have you ever been able to reproduce this issue on paperspace?\n<issue_comment>username_1: I can reproduce it locally: Arch Linux, CUDA 10.0, RTX2070 GPU. Given the CI failures, I think it should be reproducible for multiple CUDA and GPU versions. I just haven't worked on any CUDA code before, and am short on time, so I'd rather not dig too deep.\n<issue_comment>username_0: not yet .. my last building was with a my previous commit ... I will work again in this task in some minutes :)\n<issue_comment>username_1: @username_0 your previous commit had the same failure though (except for the less clear exception message), and it seems quite reproducible. So I think you'll see it now.\n<issue_comment>username_2: Ah, thanks.  @username_0, then I guess just compiling with \"DEBUG=1\" and stepping through the offending code with gdb may be the fastest way.\n<issue_comment>username_0: @username_2 thanks! I will try that!\n<issue_comment>username_0: it seems just UpSampleBicubic2d is using upsample_get_value_bounded (https://github.com/pytorch/pytorch/pull/19630/files#diff-5092da792c30694ee4adf0d0ae2a37c6R171) and upsample_increment_value_bounded (https://github.com/pytorch/pytorch/pull/19630/files#diff-5092da792c30694ee4adf0d0ae2a37c6R191)\r\n\r\nmaybe the problem could be inside one of these functions ... maybe related to the order of indexes (x, y) ...\r\n\r\nbut the problem seems to be related to cuda block/threads ... so not sure if it is really related to these functions.\n<issue_comment>username_2: The new code uses far more registers than the existing one. I verified that the both versions actually call the offending test case with `1024` in `blockDim`.  So it's very likely a register issue.\r\n\r\n`Existing` uses `64` registers, which seems to be optimal or my card:\r\n\r\n```\r\nptxas info    : 77696 bytes gmem, 72 bytes cmem[3]\r\nptxas info    : Compiling entry function '_Z23bicubic_interp2d_kerneliddb15THCDeviceTensorIdLi4Ei16DefaultPtrTraitsES1_' for 'sm_61'\r\nptxas info    : Function properties for _Z23bicubic_interp2d_kerneliddb15THCDeviceTensorIdLi4Ei16DefaultPtrTraitsES1_\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 64 registers, 432 bytes cmem[0], 24 bytes cmem[2]\r\n```\r\n\r\n`New` uses `124` registers, which is too much for my card:\r\n\r\n```\r\nptxas info    : 77696 bytes gmem, 72 bytes cmem[3]\r\nptxas info    : Compiling entry function '_ZN2at6native76_GLOBAL__N__52_tmpxft_0000626d_00000000_6_UpSampleBicubic2d_cpp1_ii_b4c1e1f328upsample_bicubic2d_out_frameElddbNS_20PackedTensorAccessorIdLm4ENS_16DefaultPtrTraitsElEES4_' for 'sm_61'\r\nptxas info    : Function properties for _ZN2at6native76_GLOBAL__N__52_tmpxft_0000626d_00000000_6_UpSampleBicubic2d_cpp1_ii_b4c1e1f328upsample_bicubic2d_out_frameElddbNS_20PackedTensorAccessorIdLm4ENS_16DefaultPtrTraitsElEES4_\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 124 registers, 496 bytes cmem[0], 24 bytes cmem[2]\r\n```\r\n\r\nWith `__launch_bounds__(1024)`, the code again uses `64` registers.\r\n\r\nIf you use `C10_LAUNCH_BOUNDS_1(1024)` **for both kernels**, the tests pass here.\r\n\r\n\r\nNow *why* is the regcount higher in the new code? It could be `PackedTensorAccessor`, it could be the fact that many instances of `int` have been changed to `int64_t`. :)\r\n\r\nYou could experiment or just use the launch bounds.  Other code in `native/cuda` seems to use lower bounds for `blockDim`, too.  1024 seems to be an outlier.\n<issue_comment>username_0: @username_2 \r\n\r\nit seems it worked locally! thank you so much!  I really appreciate that!\n<issue_comment>username_0: thanks @username_1 and @username_2 for all the help!\r\n\r\n@username_3 it is done for review!\n<issue_comment>username_3: CircleCI runs on AWS, and the I'm fairly sure we've got one of the g3 types, either g3.4xlarge or g3.8xlarge\n<issue_comment>username_3: cc @ngimel, if you want to take a look at this (I'm planning to do a review too)\n<issue_comment>username_0: @username_4 could it be addressed in another PR?\n<issue_comment>username_4: @username_0 Sure, it's definitely better to rename both cpu & gpu impls together in a separate PR. Just want to make sure we have the correct one after porting.\n<issue_comment>username_3: Just as a clarification: I'm OK with having changes which are improvements over the old kernels come in latter patches.\n<issue_comment>username_3: I finished reviewing one of the kernels, and did a cursory scan of the others. In general the patch looks like it's in good shape. I'd suggest that we fix up the major correctness issues (PackedTensorAccessor ref, and int32 versus int64 indexing) and anything small that is easy to fix (for example, double zeroing the array), and leave other improvements for follow ups. Should be ready to land soon!\n<issue_comment>username_0: thanks so much @username_3!\r\nI will let you know when it is ready again for a new review! thanks!\n<issue_comment>username_0: @username_2 @username_3 \r\nnot sure but the errors on CI seems to be related to jenkins ... it seems all these jobs that failed (for building) ran by 01h 01min ... not sure if it is a coincidence ...\n<issue_comment>username_1: @username_0 indeed it looks like all jobs were aborted at the same time. no obvious issues in the build log related to your code. Comparing e.g. the `cuda9-cudnn7` build with a successful one from another PR, it takes 1hr 14min there and your build is about at the place where the other one is after an hour.\r\n\r\nI suggest to just push a new commit to rebuild. Probably a temporary CI hiccup.\n<issue_comment>username_3: I accidentally rebooted Jenkins yesterday which is the likely cause, my apologies.\r\n\r\n@pytorchbot retest this please\n<issue_comment>username_0: @username_3 @username_2 @username_1 \r\n\r\nall tests passed except `pr/caffe2-py2-cuda9.0-cudnn7-windows-build`:\r\n \r\n```\r\n14:43:49 Build timed out (after 180 minutes). Marking the build as failed.\r\n14:43:49 Build was aborted\r\n14:43:49 [BFA] Scanning build for known causes...\r\n14:43:49 [BFA] No failure causes found\r\n14:43:49 [BFA] Done. 0s\r\n14:43:49 Finished: FAILURE\r\n```\r\n\r\nnot sure if this timeout means that the code now is slower.\r\n\r\nwhat do you think? do you have any suggestion?\n<issue_comment>username_3: Sometimes the Windows build flakes out like that. It didn't timeout while running a relevant test, so I judge it to be not your problem.\n<issue_comment>username_3: The launch bounds logic is wrong, but I acknowledge that this is a big patch already; just fix it in a follow up. I am going to go ahead and land this.\n<issue_comment>username_0: sounds good @username_3 thanks!"}
{"repo": "pytorch/pytorch", "issue_id": 436372857, "issue_number": 19630, "timestamp": "2019-05-07 13:13:22+00:00", "text": "Just as a clarification: I'm OK with having changes which are improvements over the old kernels come in latter patches.", "context": "<issue_start><issue_comment>Title: [WIP] UpSample GPU Porting \nusername_0: resolves #16158\n<issue_comment>username_1: Fewer errors than before, 4 instead of 8 with the same message:\r\n```\r\nRuntimeError: CUDA error: too many resources requested for launch\r\n```\r\nThis seems to be an existing problem: gh-8103.\r\n\r\n@username_2 could you have a look at this and tell us what you think?\n<issue_comment>username_2: Looking at it very briefly, the `Jetson TX` referenced in the issue only has 256 cores.\r\n\r\nSo while the issue looks the same, I'd probably not expect it on the CI platforms.  FWIW, some recent CI tests in other issues are green.\n<issue_comment>username_2: @pytorchbot retest this please.\n<issue_comment>username_1: same failures.\n<issue_comment>username_2: Is this rebased on the latest master (you can also ask the bot to rebase)?\n<issue_comment>username_0: I rebased that yesterday ... also needed to resolve conflicts because 7 days ago upsample files were changed.\n<issue_comment>username_0: @username_2 do you have any idea about what could be this problem? or a way to debug that?\r\nalso it seems some jobs is taking a lot of time to build, for example, for some job the estimate time is 19h .. not sure if it the regular estimate ..\n<issue_comment>username_2: @username_0 If you can't reproduce it at home it seems hard to debug other than reading at the diffs again.  I can take a look on Monday, it's getting a bit late here.\n<issue_comment>username_2: Also, has anyone found a way to show the actual hardware used on the CI in detail?\n<issue_comment>username_0: @username_2 I am working in parallel on a paperspace environment .. it tooks a lot of time it is running with 8 cpu cores.\n<issue_comment>username_2: But have you ever been able to reproduce this issue on paperspace?\n<issue_comment>username_1: I can reproduce it locally: Arch Linux, CUDA 10.0, RTX2070 GPU. Given the CI failures, I think it should be reproducible for multiple CUDA and GPU versions. I just haven't worked on any CUDA code before, and am short on time, so I'd rather not dig too deep.\n<issue_comment>username_0: not yet .. my last building was with a my previous commit ... I will work again in this task in some minutes :)\n<issue_comment>username_1: @username_0 your previous commit had the same failure though (except for the less clear exception message), and it seems quite reproducible. So I think you'll see it now.\n<issue_comment>username_2: Ah, thanks.  @username_0, then I guess just compiling with \"DEBUG=1\" and stepping through the offending code with gdb may be the fastest way.\n<issue_comment>username_0: @username_2 thanks! I will try that!\n<issue_comment>username_0: it seems just UpSampleBicubic2d is using upsample_get_value_bounded (https://github.com/pytorch/pytorch/pull/19630/files#diff-5092da792c30694ee4adf0d0ae2a37c6R171) and upsample_increment_value_bounded (https://github.com/pytorch/pytorch/pull/19630/files#diff-5092da792c30694ee4adf0d0ae2a37c6R191)\r\n\r\nmaybe the problem could be inside one of these functions ... maybe related to the order of indexes (x, y) ...\r\n\r\nbut the problem seems to be related to cuda block/threads ... so not sure if it is really related to these functions.\n<issue_comment>username_2: The new code uses far more registers than the existing one. I verified that the both versions actually call the offending test case with `1024` in `blockDim`.  So it's very likely a register issue.\r\n\r\n`Existing` uses `64` registers, which seems to be optimal or my card:\r\n\r\n```\r\nptxas info    : 77696 bytes gmem, 72 bytes cmem[3]\r\nptxas info    : Compiling entry function '_Z23bicubic_interp2d_kerneliddb15THCDeviceTensorIdLi4Ei16DefaultPtrTraitsES1_' for 'sm_61'\r\nptxas info    : Function properties for _Z23bicubic_interp2d_kerneliddb15THCDeviceTensorIdLi4Ei16DefaultPtrTraitsES1_\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 64 registers, 432 bytes cmem[0], 24 bytes cmem[2]\r\n```\r\n\r\n`New` uses `124` registers, which is too much for my card:\r\n\r\n```\r\nptxas info    : 77696 bytes gmem, 72 bytes cmem[3]\r\nptxas info    : Compiling entry function '_ZN2at6native76_GLOBAL__N__52_tmpxft_0000626d_00000000_6_UpSampleBicubic2d_cpp1_ii_b4c1e1f328upsample_bicubic2d_out_frameElddbNS_20PackedTensorAccessorIdLm4ENS_16DefaultPtrTraitsElEES4_' for 'sm_61'\r\nptxas info    : Function properties for _ZN2at6native76_GLOBAL__N__52_tmpxft_0000626d_00000000_6_UpSampleBicubic2d_cpp1_ii_b4c1e1f328upsample_bicubic2d_out_frameElddbNS_20PackedTensorAccessorIdLm4ENS_16DefaultPtrTraitsElEES4_\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 124 registers, 496 bytes cmem[0], 24 bytes cmem[2]\r\n```\r\n\r\nWith `__launch_bounds__(1024)`, the code again uses `64` registers.\r\n\r\nIf you use `C10_LAUNCH_BOUNDS_1(1024)` **for both kernels**, the tests pass here.\r\n\r\n\r\nNow *why* is the regcount higher in the new code? It could be `PackedTensorAccessor`, it could be the fact that many instances of `int` have been changed to `int64_t`. :)\r\n\r\nYou could experiment or just use the launch bounds.  Other code in `native/cuda` seems to use lower bounds for `blockDim`, too.  1024 seems to be an outlier.\n<issue_comment>username_0: @username_2 \r\n\r\nit seems it worked locally! thank you so much!  I really appreciate that!\n<issue_comment>username_0: thanks @username_1 and @username_2 for all the help!\r\n\r\n@username_3 it is done for review!\n<issue_comment>username_3: CircleCI runs on AWS, and the I'm fairly sure we've got one of the g3 types, either g3.4xlarge or g3.8xlarge\n<issue_comment>username_3: cc @ngimel, if you want to take a look at this (I'm planning to do a review too)\n<issue_comment>username_0: @username_4 could it be addressed in another PR?\n<issue_comment>username_4: @username_0 Sure, it's definitely better to rename both cpu & gpu impls together in a separate PR. Just want to make sure we have the correct one after porting.\n<issue_comment>username_3: Just as a clarification: I'm OK with having changes which are improvements over the old kernels come in latter patches.\n<issue_comment>username_3: I finished reviewing one of the kernels, and did a cursory scan of the others. In general the patch looks like it's in good shape. I'd suggest that we fix up the major correctness issues (PackedTensorAccessor ref, and int32 versus int64 indexing) and anything small that is easy to fix (for example, double zeroing the array), and leave other improvements for follow ups. Should be ready to land soon!\n<issue_comment>username_0: thanks so much @username_3!\r\nI will let you know when it is ready again for a new review! thanks!\n<issue_comment>username_0: @username_2 @username_3 \r\nnot sure but the errors on CI seems to be related to jenkins ... it seems all these jobs that failed (for building) ran by 01h 01min ... not sure if it is a coincidence ...\n<issue_comment>username_1: @username_0 indeed it looks like all jobs were aborted at the same time. no obvious issues in the build log related to your code. Comparing e.g. the `cuda9-cudnn7` build with a successful one from another PR, it takes 1hr 14min there and your build is about at the place where the other one is after an hour.\r\n\r\nI suggest to just push a new commit to rebuild. Probably a temporary CI hiccup.\n<issue_comment>username_3: I accidentally rebooted Jenkins yesterday which is the likely cause, my apologies.\r\n\r\n@pytorchbot retest this please\n<issue_comment>username_0: @username_3 @username_2 @username_1 \r\n\r\nall tests passed except `pr/caffe2-py2-cuda9.0-cudnn7-windows-build`:\r\n \r\n```\r\n14:43:49 Build timed out (after 180 minutes). Marking the build as failed.\r\n14:43:49 Build was aborted\r\n14:43:49 [BFA] Scanning build for known causes...\r\n14:43:49 [BFA] No failure causes found\r\n14:43:49 [BFA] Done. 0s\r\n14:43:49 Finished: FAILURE\r\n```\r\n\r\nnot sure if this timeout means that the code now is slower.\r\n\r\nwhat do you think? do you have any suggestion?\n<issue_comment>username_3: Sometimes the Windows build flakes out like that. It didn't timeout while running a relevant test, so I judge it to be not your problem.\n<issue_comment>username_3: The launch bounds logic is wrong, but I acknowledge that this is a big patch already; just fix it in a follow up. I am going to go ahead and land this.\n<issue_comment>username_0: sounds good @username_3 thanks!"}
{"repo": "pytorch/pytorch", "issue_id": 436372857, "issue_number": 19630, "timestamp": "2019-05-07 13:18:09+00:00", "text": "I finished reviewing one of the kernels, and did a cursory scan of the others. In general the patch looks like it's in good shape. I'd suggest that we fix up the major correctness issues (PackedTensorAccessor ref, and int32 versus int64 indexing) and anything small that is easy to fix (for example, double zeroing the array), and leave other improvements for follow ups. Should be ready to land soon!", "context": "<issue_start><issue_comment>Title: [WIP] UpSample GPU Porting \nusername_0: resolves #16158\n<issue_comment>username_1: Fewer errors than before, 4 instead of 8 with the same message:\r\n```\r\nRuntimeError: CUDA error: too many resources requested for launch\r\n```\r\nThis seems to be an existing problem: gh-8103.\r\n\r\n@username_2 could you have a look at this and tell us what you think?\n<issue_comment>username_2: Looking at it very briefly, the `Jetson TX` referenced in the issue only has 256 cores.\r\n\r\nSo while the issue looks the same, I'd probably not expect it on the CI platforms.  FWIW, some recent CI tests in other issues are green.\n<issue_comment>username_2: @pytorchbot retest this please.\n<issue_comment>username_1: same failures.\n<issue_comment>username_2: Is this rebased on the latest master (you can also ask the bot to rebase)?\n<issue_comment>username_0: I rebased that yesterday ... also needed to resolve conflicts because 7 days ago upsample files were changed.\n<issue_comment>username_0: @username_2 do you have any idea about what could be this problem? or a way to debug that?\r\nalso it seems some jobs is taking a lot of time to build, for example, for some job the estimate time is 19h .. not sure if it the regular estimate ..\n<issue_comment>username_2: @username_0 If you can't reproduce it at home it seems hard to debug other than reading at the diffs again.  I can take a look on Monday, it's getting a bit late here.\n<issue_comment>username_2: Also, has anyone found a way to show the actual hardware used on the CI in detail?\n<issue_comment>username_0: @username_2 I am working in parallel on a paperspace environment .. it tooks a lot of time it is running with 8 cpu cores.\n<issue_comment>username_2: But have you ever been able to reproduce this issue on paperspace?\n<issue_comment>username_1: I can reproduce it locally: Arch Linux, CUDA 10.0, RTX2070 GPU. Given the CI failures, I think it should be reproducible for multiple CUDA and GPU versions. I just haven't worked on any CUDA code before, and am short on time, so I'd rather not dig too deep.\n<issue_comment>username_0: not yet .. my last building was with a my previous commit ... I will work again in this task in some minutes :)\n<issue_comment>username_1: @username_0 your previous commit had the same failure though (except for the less clear exception message), and it seems quite reproducible. So I think you'll see it now.\n<issue_comment>username_2: Ah, thanks.  @username_0, then I guess just compiling with \"DEBUG=1\" and stepping through the offending code with gdb may be the fastest way.\n<issue_comment>username_0: @username_2 thanks! I will try that!\n<issue_comment>username_0: it seems just UpSampleBicubic2d is using upsample_get_value_bounded (https://github.com/pytorch/pytorch/pull/19630/files#diff-5092da792c30694ee4adf0d0ae2a37c6R171) and upsample_increment_value_bounded (https://github.com/pytorch/pytorch/pull/19630/files#diff-5092da792c30694ee4adf0d0ae2a37c6R191)\r\n\r\nmaybe the problem could be inside one of these functions ... maybe related to the order of indexes (x, y) ...\r\n\r\nbut the problem seems to be related to cuda block/threads ... so not sure if it is really related to these functions.\n<issue_comment>username_2: The new code uses far more registers than the existing one. I verified that the both versions actually call the offending test case with `1024` in `blockDim`.  So it's very likely a register issue.\r\n\r\n`Existing` uses `64` registers, which seems to be optimal or my card:\r\n\r\n```\r\nptxas info    : 77696 bytes gmem, 72 bytes cmem[3]\r\nptxas info    : Compiling entry function '_Z23bicubic_interp2d_kerneliddb15THCDeviceTensorIdLi4Ei16DefaultPtrTraitsES1_' for 'sm_61'\r\nptxas info    : Function properties for _Z23bicubic_interp2d_kerneliddb15THCDeviceTensorIdLi4Ei16DefaultPtrTraitsES1_\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 64 registers, 432 bytes cmem[0], 24 bytes cmem[2]\r\n```\r\n\r\n`New` uses `124` registers, which is too much for my card:\r\n\r\n```\r\nptxas info    : 77696 bytes gmem, 72 bytes cmem[3]\r\nptxas info    : Compiling entry function '_ZN2at6native76_GLOBAL__N__52_tmpxft_0000626d_00000000_6_UpSampleBicubic2d_cpp1_ii_b4c1e1f328upsample_bicubic2d_out_frameElddbNS_20PackedTensorAccessorIdLm4ENS_16DefaultPtrTraitsElEES4_' for 'sm_61'\r\nptxas info    : Function properties for _ZN2at6native76_GLOBAL__N__52_tmpxft_0000626d_00000000_6_UpSampleBicubic2d_cpp1_ii_b4c1e1f328upsample_bicubic2d_out_frameElddbNS_20PackedTensorAccessorIdLm4ENS_16DefaultPtrTraitsElEES4_\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 124 registers, 496 bytes cmem[0], 24 bytes cmem[2]\r\n```\r\n\r\nWith `__launch_bounds__(1024)`, the code again uses `64` registers.\r\n\r\nIf you use `C10_LAUNCH_BOUNDS_1(1024)` **for both kernels**, the tests pass here.\r\n\r\n\r\nNow *why* is the regcount higher in the new code? It could be `PackedTensorAccessor`, it could be the fact that many instances of `int` have been changed to `int64_t`. :)\r\n\r\nYou could experiment or just use the launch bounds.  Other code in `native/cuda` seems to use lower bounds for `blockDim`, too.  1024 seems to be an outlier.\n<issue_comment>username_0: @username_2 \r\n\r\nit seems it worked locally! thank you so much!  I really appreciate that!\n<issue_comment>username_0: thanks @username_1 and @username_2 for all the help!\r\n\r\n@username_3 it is done for review!\n<issue_comment>username_3: CircleCI runs on AWS, and the I'm fairly sure we've got one of the g3 types, either g3.4xlarge or g3.8xlarge\n<issue_comment>username_3: cc @ngimel, if you want to take a look at this (I'm planning to do a review too)\n<issue_comment>username_0: @username_4 could it be addressed in another PR?\n<issue_comment>username_4: @username_0 Sure, it's definitely better to rename both cpu & gpu impls together in a separate PR. Just want to make sure we have the correct one after porting.\n<issue_comment>username_3: Just as a clarification: I'm OK with having changes which are improvements over the old kernels come in latter patches.\n<issue_comment>username_3: I finished reviewing one of the kernels, and did a cursory scan of the others. In general the patch looks like it's in good shape. I'd suggest that we fix up the major correctness issues (PackedTensorAccessor ref, and int32 versus int64 indexing) and anything small that is easy to fix (for example, double zeroing the array), and leave other improvements for follow ups. Should be ready to land soon!\n<issue_comment>username_0: thanks so much @username_3!\r\nI will let you know when it is ready again for a new review! thanks!\n<issue_comment>username_0: @username_2 @username_3 \r\nnot sure but the errors on CI seems to be related to jenkins ... it seems all these jobs that failed (for building) ran by 01h 01min ... not sure if it is a coincidence ...\n<issue_comment>username_1: @username_0 indeed it looks like all jobs were aborted at the same time. no obvious issues in the build log related to your code. Comparing e.g. the `cuda9-cudnn7` build with a successful one from another PR, it takes 1hr 14min there and your build is about at the place where the other one is after an hour.\r\n\r\nI suggest to just push a new commit to rebuild. Probably a temporary CI hiccup.\n<issue_comment>username_3: I accidentally rebooted Jenkins yesterday which is the likely cause, my apologies.\r\n\r\n@pytorchbot retest this please\n<issue_comment>username_0: @username_3 @username_2 @username_1 \r\n\r\nall tests passed except `pr/caffe2-py2-cuda9.0-cudnn7-windows-build`:\r\n \r\n```\r\n14:43:49 Build timed out (after 180 minutes). Marking the build as failed.\r\n14:43:49 Build was aborted\r\n14:43:49 [BFA] Scanning build for known causes...\r\n14:43:49 [BFA] No failure causes found\r\n14:43:49 [BFA] Done. 0s\r\n14:43:49 Finished: FAILURE\r\n```\r\n\r\nnot sure if this timeout means that the code now is slower.\r\n\r\nwhat do you think? do you have any suggestion?\n<issue_comment>username_3: Sometimes the Windows build flakes out like that. It didn't timeout while running a relevant test, so I judge it to be not your problem.\n<issue_comment>username_3: The launch bounds logic is wrong, but I acknowledge that this is a big patch already; just fix it in a follow up. I am going to go ahead and land this.\n<issue_comment>username_0: sounds good @username_3 thanks!"}
{"repo": "pytorch/pytorch", "issue_id": 436372857, "issue_number": 19630, "timestamp": "2019-05-07 14:07:17+00:00", "text": "thanks so much @username_3!\r\nI will let you know when it is ready again for a new review! thanks!", "context": "<issue_start><issue_comment>Title: [WIP] UpSample GPU Porting \nusername_0: resolves #16158\n<issue_comment>username_1: Fewer errors than before, 4 instead of 8 with the same message:\r\n```\r\nRuntimeError: CUDA error: too many resources requested for launch\r\n```\r\nThis seems to be an existing problem: gh-8103.\r\n\r\n@username_2 could you have a look at this and tell us what you think?\n<issue_comment>username_2: Looking at it very briefly, the `Jetson TX` referenced in the issue only has 256 cores.\r\n\r\nSo while the issue looks the same, I'd probably not expect it on the CI platforms.  FWIW, some recent CI tests in other issues are green.\n<issue_comment>username_2: @pytorchbot retest this please.\n<issue_comment>username_1: same failures.\n<issue_comment>username_2: Is this rebased on the latest master (you can also ask the bot to rebase)?\n<issue_comment>username_0: I rebased that yesterday ... also needed to resolve conflicts because 7 days ago upsample files were changed.\n<issue_comment>username_0: @username_2 do you have any idea about what could be this problem? or a way to debug that?\r\nalso it seems some jobs is taking a lot of time to build, for example, for some job the estimate time is 19h .. not sure if it the regular estimate ..\n<issue_comment>username_2: @username_0 If you can't reproduce it at home it seems hard to debug other than reading at the diffs again.  I can take a look on Monday, it's getting a bit late here.\n<issue_comment>username_2: Also, has anyone found a way to show the actual hardware used on the CI in detail?\n<issue_comment>username_0: @username_2 I am working in parallel on a paperspace environment .. it tooks a lot of time it is running with 8 cpu cores.\n<issue_comment>username_2: But have you ever been able to reproduce this issue on paperspace?\n<issue_comment>username_1: I can reproduce it locally: Arch Linux, CUDA 10.0, RTX2070 GPU. Given the CI failures, I think it should be reproducible for multiple CUDA and GPU versions. I just haven't worked on any CUDA code before, and am short on time, so I'd rather not dig too deep.\n<issue_comment>username_0: not yet .. my last building was with a my previous commit ... I will work again in this task in some minutes :)\n<issue_comment>username_1: @username_0 your previous commit had the same failure though (except for the less clear exception message), and it seems quite reproducible. So I think you'll see it now.\n<issue_comment>username_2: Ah, thanks.  @username_0, then I guess just compiling with \"DEBUG=1\" and stepping through the offending code with gdb may be the fastest way.\n<issue_comment>username_0: @username_2 thanks! I will try that!\n<issue_comment>username_0: it seems just UpSampleBicubic2d is using upsample_get_value_bounded (https://github.com/pytorch/pytorch/pull/19630/files#diff-5092da792c30694ee4adf0d0ae2a37c6R171) and upsample_increment_value_bounded (https://github.com/pytorch/pytorch/pull/19630/files#diff-5092da792c30694ee4adf0d0ae2a37c6R191)\r\n\r\nmaybe the problem could be inside one of these functions ... maybe related to the order of indexes (x, y) ...\r\n\r\nbut the problem seems to be related to cuda block/threads ... so not sure if it is really related to these functions.\n<issue_comment>username_2: The new code uses far more registers than the existing one. I verified that the both versions actually call the offending test case with `1024` in `blockDim`.  So it's very likely a register issue.\r\n\r\n`Existing` uses `64` registers, which seems to be optimal or my card:\r\n\r\n```\r\nptxas info    : 77696 bytes gmem, 72 bytes cmem[3]\r\nptxas info    : Compiling entry function '_Z23bicubic_interp2d_kerneliddb15THCDeviceTensorIdLi4Ei16DefaultPtrTraitsES1_' for 'sm_61'\r\nptxas info    : Function properties for _Z23bicubic_interp2d_kerneliddb15THCDeviceTensorIdLi4Ei16DefaultPtrTraitsES1_\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 64 registers, 432 bytes cmem[0], 24 bytes cmem[2]\r\n```\r\n\r\n`New` uses `124` registers, which is too much for my card:\r\n\r\n```\r\nptxas info    : 77696 bytes gmem, 72 bytes cmem[3]\r\nptxas info    : Compiling entry function '_ZN2at6native76_GLOBAL__N__52_tmpxft_0000626d_00000000_6_UpSampleBicubic2d_cpp1_ii_b4c1e1f328upsample_bicubic2d_out_frameElddbNS_20PackedTensorAccessorIdLm4ENS_16DefaultPtrTraitsElEES4_' for 'sm_61'\r\nptxas info    : Function properties for _ZN2at6native76_GLOBAL__N__52_tmpxft_0000626d_00000000_6_UpSampleBicubic2d_cpp1_ii_b4c1e1f328upsample_bicubic2d_out_frameElddbNS_20PackedTensorAccessorIdLm4ENS_16DefaultPtrTraitsElEES4_\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 124 registers, 496 bytes cmem[0], 24 bytes cmem[2]\r\n```\r\n\r\nWith `__launch_bounds__(1024)`, the code again uses `64` registers.\r\n\r\nIf you use `C10_LAUNCH_BOUNDS_1(1024)` **for both kernels**, the tests pass here.\r\n\r\n\r\nNow *why* is the regcount higher in the new code? It could be `PackedTensorAccessor`, it could be the fact that many instances of `int` have been changed to `int64_t`. :)\r\n\r\nYou could experiment or just use the launch bounds.  Other code in `native/cuda` seems to use lower bounds for `blockDim`, too.  1024 seems to be an outlier.\n<issue_comment>username_0: @username_2 \r\n\r\nit seems it worked locally! thank you so much!  I really appreciate that!\n<issue_comment>username_0: thanks @username_1 and @username_2 for all the help!\r\n\r\n@username_3 it is done for review!\n<issue_comment>username_3: CircleCI runs on AWS, and the I'm fairly sure we've got one of the g3 types, either g3.4xlarge or g3.8xlarge\n<issue_comment>username_3: cc @ngimel, if you want to take a look at this (I'm planning to do a review too)\n<issue_comment>username_0: @username_4 could it be addressed in another PR?\n<issue_comment>username_4: @username_0 Sure, it's definitely better to rename both cpu & gpu impls together in a separate PR. Just want to make sure we have the correct one after porting.\n<issue_comment>username_3: Just as a clarification: I'm OK with having changes which are improvements over the old kernels come in latter patches.\n<issue_comment>username_3: I finished reviewing one of the kernels, and did a cursory scan of the others. In general the patch looks like it's in good shape. I'd suggest that we fix up the major correctness issues (PackedTensorAccessor ref, and int32 versus int64 indexing) and anything small that is easy to fix (for example, double zeroing the array), and leave other improvements for follow ups. Should be ready to land soon!\n<issue_comment>username_0: thanks so much @username_3!\r\nI will let you know when it is ready again for a new review! thanks!\n<issue_comment>username_0: @username_2 @username_3 \r\nnot sure but the errors on CI seems to be related to jenkins ... it seems all these jobs that failed (for building) ran by 01h 01min ... not sure if it is a coincidence ...\n<issue_comment>username_1: @username_0 indeed it looks like all jobs were aborted at the same time. no obvious issues in the build log related to your code. Comparing e.g. the `cuda9-cudnn7` build with a successful one from another PR, it takes 1hr 14min there and your build is about at the place where the other one is after an hour.\r\n\r\nI suggest to just push a new commit to rebuild. Probably a temporary CI hiccup.\n<issue_comment>username_3: I accidentally rebooted Jenkins yesterday which is the likely cause, my apologies.\r\n\r\n@pytorchbot retest this please\n<issue_comment>username_0: @username_3 @username_2 @username_1 \r\n\r\nall tests passed except `pr/caffe2-py2-cuda9.0-cudnn7-windows-build`:\r\n \r\n```\r\n14:43:49 Build timed out (after 180 minutes). Marking the build as failed.\r\n14:43:49 Build was aborted\r\n14:43:49 [BFA] Scanning build for known causes...\r\n14:43:49 [BFA] No failure causes found\r\n14:43:49 [BFA] Done. 0s\r\n14:43:49 Finished: FAILURE\r\n```\r\n\r\nnot sure if this timeout means that the code now is slower.\r\n\r\nwhat do you think? do you have any suggestion?\n<issue_comment>username_3: Sometimes the Windows build flakes out like that. It didn't timeout while running a relevant test, so I judge it to be not your problem.\n<issue_comment>username_3: The launch bounds logic is wrong, but I acknowledge that this is a big patch already; just fix it in a follow up. I am going to go ahead and land this.\n<issue_comment>username_0: sounds good @username_3 thanks!"}
{"repo": "pytorch/pytorch", "issue_id": 436372857, "issue_number": 19630, "timestamp": "2019-05-14 03:22:19+00:00", "text": "@username_2 @username_3 \r\nnot sure but the errors on CI seems to be related to jenkins ... it seems all these jobs that failed (for building) ran by 01h 01min ... not sure if it is a coincidence ...", "context": "<issue_start><issue_comment>Title: [WIP] UpSample GPU Porting \nusername_0: resolves #16158\n<issue_comment>username_1: Fewer errors than before, 4 instead of 8 with the same message:\r\n```\r\nRuntimeError: CUDA error: too many resources requested for launch\r\n```\r\nThis seems to be an existing problem: gh-8103.\r\n\r\n@username_2 could you have a look at this and tell us what you think?\n<issue_comment>username_2: Looking at it very briefly, the `Jetson TX` referenced in the issue only has 256 cores.\r\n\r\nSo while the issue looks the same, I'd probably not expect it on the CI platforms.  FWIW, some recent CI tests in other issues are green.\n<issue_comment>username_2: @pytorchbot retest this please.\n<issue_comment>username_1: same failures.\n<issue_comment>username_2: Is this rebased on the latest master (you can also ask the bot to rebase)?\n<issue_comment>username_0: I rebased that yesterday ... also needed to resolve conflicts because 7 days ago upsample files were changed.\n<issue_comment>username_0: @username_2 do you have any idea about what could be this problem? or a way to debug that?\r\nalso it seems some jobs is taking a lot of time to build, for example, for some job the estimate time is 19h .. not sure if it the regular estimate ..\n<issue_comment>username_2: @username_0 If you can't reproduce it at home it seems hard to debug other than reading at the diffs again.  I can take a look on Monday, it's getting a bit late here.\n<issue_comment>username_2: Also, has anyone found a way to show the actual hardware used on the CI in detail?\n<issue_comment>username_0: @username_2 I am working in parallel on a paperspace environment .. it tooks a lot of time it is running with 8 cpu cores.\n<issue_comment>username_2: But have you ever been able to reproduce this issue on paperspace?\n<issue_comment>username_1: I can reproduce it locally: Arch Linux, CUDA 10.0, RTX2070 GPU. Given the CI failures, I think it should be reproducible for multiple CUDA and GPU versions. I just haven't worked on any CUDA code before, and am short on time, so I'd rather not dig too deep.\n<issue_comment>username_0: not yet .. my last building was with a my previous commit ... I will work again in this task in some minutes :)\n<issue_comment>username_1: @username_0 your previous commit had the same failure though (except for the less clear exception message), and it seems quite reproducible. So I think you'll see it now.\n<issue_comment>username_2: Ah, thanks.  @username_0, then I guess just compiling with \"DEBUG=1\" and stepping through the offending code with gdb may be the fastest way.\n<issue_comment>username_0: @username_2 thanks! I will try that!\n<issue_comment>username_0: it seems just UpSampleBicubic2d is using upsample_get_value_bounded (https://github.com/pytorch/pytorch/pull/19630/files#diff-5092da792c30694ee4adf0d0ae2a37c6R171) and upsample_increment_value_bounded (https://github.com/pytorch/pytorch/pull/19630/files#diff-5092da792c30694ee4adf0d0ae2a37c6R191)\r\n\r\nmaybe the problem could be inside one of these functions ... maybe related to the order of indexes (x, y) ...\r\n\r\nbut the problem seems to be related to cuda block/threads ... so not sure if it is really related to these functions.\n<issue_comment>username_2: The new code uses far more registers than the existing one. I verified that the both versions actually call the offending test case with `1024` in `blockDim`.  So it's very likely a register issue.\r\n\r\n`Existing` uses `64` registers, which seems to be optimal or my card:\r\n\r\n```\r\nptxas info    : 77696 bytes gmem, 72 bytes cmem[3]\r\nptxas info    : Compiling entry function '_Z23bicubic_interp2d_kerneliddb15THCDeviceTensorIdLi4Ei16DefaultPtrTraitsES1_' for 'sm_61'\r\nptxas info    : Function properties for _Z23bicubic_interp2d_kerneliddb15THCDeviceTensorIdLi4Ei16DefaultPtrTraitsES1_\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 64 registers, 432 bytes cmem[0], 24 bytes cmem[2]\r\n```\r\n\r\n`New` uses `124` registers, which is too much for my card:\r\n\r\n```\r\nptxas info    : 77696 bytes gmem, 72 bytes cmem[3]\r\nptxas info    : Compiling entry function '_ZN2at6native76_GLOBAL__N__52_tmpxft_0000626d_00000000_6_UpSampleBicubic2d_cpp1_ii_b4c1e1f328upsample_bicubic2d_out_frameElddbNS_20PackedTensorAccessorIdLm4ENS_16DefaultPtrTraitsElEES4_' for 'sm_61'\r\nptxas info    : Function properties for _ZN2at6native76_GLOBAL__N__52_tmpxft_0000626d_00000000_6_UpSampleBicubic2d_cpp1_ii_b4c1e1f328upsample_bicubic2d_out_frameElddbNS_20PackedTensorAccessorIdLm4ENS_16DefaultPtrTraitsElEES4_\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 124 registers, 496 bytes cmem[0], 24 bytes cmem[2]\r\n```\r\n\r\nWith `__launch_bounds__(1024)`, the code again uses `64` registers.\r\n\r\nIf you use `C10_LAUNCH_BOUNDS_1(1024)` **for both kernels**, the tests pass here.\r\n\r\n\r\nNow *why* is the regcount higher in the new code? It could be `PackedTensorAccessor`, it could be the fact that many instances of `int` have been changed to `int64_t`. :)\r\n\r\nYou could experiment or just use the launch bounds.  Other code in `native/cuda` seems to use lower bounds for `blockDim`, too.  1024 seems to be an outlier.\n<issue_comment>username_0: @username_2 \r\n\r\nit seems it worked locally! thank you so much!  I really appreciate that!\n<issue_comment>username_0: thanks @username_1 and @username_2 for all the help!\r\n\r\n@username_3 it is done for review!\n<issue_comment>username_3: CircleCI runs on AWS, and the I'm fairly sure we've got one of the g3 types, either g3.4xlarge or g3.8xlarge\n<issue_comment>username_3: cc @ngimel, if you want to take a look at this (I'm planning to do a review too)\n<issue_comment>username_0: @username_4 could it be addressed in another PR?\n<issue_comment>username_4: @username_0 Sure, it's definitely better to rename both cpu & gpu impls together in a separate PR. Just want to make sure we have the correct one after porting.\n<issue_comment>username_3: Just as a clarification: I'm OK with having changes which are improvements over the old kernels come in latter patches.\n<issue_comment>username_3: I finished reviewing one of the kernels, and did a cursory scan of the others. In general the patch looks like it's in good shape. I'd suggest that we fix up the major correctness issues (PackedTensorAccessor ref, and int32 versus int64 indexing) and anything small that is easy to fix (for example, double zeroing the array), and leave other improvements for follow ups. Should be ready to land soon!\n<issue_comment>username_0: thanks so much @username_3!\r\nI will let you know when it is ready again for a new review! thanks!\n<issue_comment>username_0: @username_2 @username_3 \r\nnot sure but the errors on CI seems to be related to jenkins ... it seems all these jobs that failed (for building) ran by 01h 01min ... not sure if it is a coincidence ...\n<issue_comment>username_1: @username_0 indeed it looks like all jobs were aborted at the same time. no obvious issues in the build log related to your code. Comparing e.g. the `cuda9-cudnn7` build with a successful one from another PR, it takes 1hr 14min there and your build is about at the place where the other one is after an hour.\r\n\r\nI suggest to just push a new commit to rebuild. Probably a temporary CI hiccup.\n<issue_comment>username_3: I accidentally rebooted Jenkins yesterday which is the likely cause, my apologies.\r\n\r\n@pytorchbot retest this please\n<issue_comment>username_0: @username_3 @username_2 @username_1 \r\n\r\nall tests passed except `pr/caffe2-py2-cuda9.0-cudnn7-windows-build`:\r\n \r\n```\r\n14:43:49 Build timed out (after 180 minutes). Marking the build as failed.\r\n14:43:49 Build was aborted\r\n14:43:49 [BFA] Scanning build for known causes...\r\n14:43:49 [BFA] No failure causes found\r\n14:43:49 [BFA] Done. 0s\r\n14:43:49 Finished: FAILURE\r\n```\r\n\r\nnot sure if this timeout means that the code now is slower.\r\n\r\nwhat do you think? do you have any suggestion?\n<issue_comment>username_3: Sometimes the Windows build flakes out like that. It didn't timeout while running a relevant test, so I judge it to be not your problem.\n<issue_comment>username_3: The launch bounds logic is wrong, but I acknowledge that this is a big patch already; just fix it in a follow up. I am going to go ahead and land this.\n<issue_comment>username_0: sounds good @username_3 thanks!"}
{"repo": "pytorch/pytorch", "issue_id": 436372857, "issue_number": 19630, "timestamp": "2019-05-14 09:15:07+00:00", "text": "@username_0 indeed it looks like all jobs were aborted at the same time. no obvious issues in the build log related to your code. Comparing e.g. the `cuda9-cudnn7` build with a successful one from another PR, it takes 1hr 14min there and your build is about at the place where the other one is after an hour.\r\n\r\nI suggest to just push a new commit to rebuild. Probably a temporary CI hiccup.", "context": "<issue_start><issue_comment>Title: [WIP] UpSample GPU Porting \nusername_0: resolves #16158\n<issue_comment>username_1: Fewer errors than before, 4 instead of 8 with the same message:\r\n```\r\nRuntimeError: CUDA error: too many resources requested for launch\r\n```\r\nThis seems to be an existing problem: gh-8103.\r\n\r\n@username_2 could you have a look at this and tell us what you think?\n<issue_comment>username_2: Looking at it very briefly, the `Jetson TX` referenced in the issue only has 256 cores.\r\n\r\nSo while the issue looks the same, I'd probably not expect it on the CI platforms.  FWIW, some recent CI tests in other issues are green.\n<issue_comment>username_2: @pytorchbot retest this please.\n<issue_comment>username_1: same failures.\n<issue_comment>username_2: Is this rebased on the latest master (you can also ask the bot to rebase)?\n<issue_comment>username_0: I rebased that yesterday ... also needed to resolve conflicts because 7 days ago upsample files were changed.\n<issue_comment>username_0: @username_2 do you have any idea about what could be this problem? or a way to debug that?\r\nalso it seems some jobs is taking a lot of time to build, for example, for some job the estimate time is 19h .. not sure if it the regular estimate ..\n<issue_comment>username_2: @username_0 If you can't reproduce it at home it seems hard to debug other than reading at the diffs again.  I can take a look on Monday, it's getting a bit late here.\n<issue_comment>username_2: Also, has anyone found a way to show the actual hardware used on the CI in detail?\n<issue_comment>username_0: @username_2 I am working in parallel on a paperspace environment .. it tooks a lot of time it is running with 8 cpu cores.\n<issue_comment>username_2: But have you ever been able to reproduce this issue on paperspace?\n<issue_comment>username_1: I can reproduce it locally: Arch Linux, CUDA 10.0, RTX2070 GPU. Given the CI failures, I think it should be reproducible for multiple CUDA and GPU versions. I just haven't worked on any CUDA code before, and am short on time, so I'd rather not dig too deep.\n<issue_comment>username_0: not yet .. my last building was with a my previous commit ... I will work again in this task in some minutes :)\n<issue_comment>username_1: @username_0 your previous commit had the same failure though (except for the less clear exception message), and it seems quite reproducible. So I think you'll see it now.\n<issue_comment>username_2: Ah, thanks.  @username_0, then I guess just compiling with \"DEBUG=1\" and stepping through the offending code with gdb may be the fastest way.\n<issue_comment>username_0: @username_2 thanks! I will try that!\n<issue_comment>username_0: it seems just UpSampleBicubic2d is using upsample_get_value_bounded (https://github.com/pytorch/pytorch/pull/19630/files#diff-5092da792c30694ee4adf0d0ae2a37c6R171) and upsample_increment_value_bounded (https://github.com/pytorch/pytorch/pull/19630/files#diff-5092da792c30694ee4adf0d0ae2a37c6R191)\r\n\r\nmaybe the problem could be inside one of these functions ... maybe related to the order of indexes (x, y) ...\r\n\r\nbut the problem seems to be related to cuda block/threads ... so not sure if it is really related to these functions.\n<issue_comment>username_2: The new code uses far more registers than the existing one. I verified that the both versions actually call the offending test case with `1024` in `blockDim`.  So it's very likely a register issue.\r\n\r\n`Existing` uses `64` registers, which seems to be optimal or my card:\r\n\r\n```\r\nptxas info    : 77696 bytes gmem, 72 bytes cmem[3]\r\nptxas info    : Compiling entry function '_Z23bicubic_interp2d_kerneliddb15THCDeviceTensorIdLi4Ei16DefaultPtrTraitsES1_' for 'sm_61'\r\nptxas info    : Function properties for _Z23bicubic_interp2d_kerneliddb15THCDeviceTensorIdLi4Ei16DefaultPtrTraitsES1_\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 64 registers, 432 bytes cmem[0], 24 bytes cmem[2]\r\n```\r\n\r\n`New` uses `124` registers, which is too much for my card:\r\n\r\n```\r\nptxas info    : 77696 bytes gmem, 72 bytes cmem[3]\r\nptxas info    : Compiling entry function '_ZN2at6native76_GLOBAL__N__52_tmpxft_0000626d_00000000_6_UpSampleBicubic2d_cpp1_ii_b4c1e1f328upsample_bicubic2d_out_frameElddbNS_20PackedTensorAccessorIdLm4ENS_16DefaultPtrTraitsElEES4_' for 'sm_61'\r\nptxas info    : Function properties for _ZN2at6native76_GLOBAL__N__52_tmpxft_0000626d_00000000_6_UpSampleBicubic2d_cpp1_ii_b4c1e1f328upsample_bicubic2d_out_frameElddbNS_20PackedTensorAccessorIdLm4ENS_16DefaultPtrTraitsElEES4_\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 124 registers, 496 bytes cmem[0], 24 bytes cmem[2]\r\n```\r\n\r\nWith `__launch_bounds__(1024)`, the code again uses `64` registers.\r\n\r\nIf you use `C10_LAUNCH_BOUNDS_1(1024)` **for both kernels**, the tests pass here.\r\n\r\n\r\nNow *why* is the regcount higher in the new code? It could be `PackedTensorAccessor`, it could be the fact that many instances of `int` have been changed to `int64_t`. :)\r\n\r\nYou could experiment or just use the launch bounds.  Other code in `native/cuda` seems to use lower bounds for `blockDim`, too.  1024 seems to be an outlier.\n<issue_comment>username_0: @username_2 \r\n\r\nit seems it worked locally! thank you so much!  I really appreciate that!\n<issue_comment>username_0: thanks @username_1 and @username_2 for all the help!\r\n\r\n@username_3 it is done for review!\n<issue_comment>username_3: CircleCI runs on AWS, and the I'm fairly sure we've got one of the g3 types, either g3.4xlarge or g3.8xlarge\n<issue_comment>username_3: cc @ngimel, if you want to take a look at this (I'm planning to do a review too)\n<issue_comment>username_0: @username_4 could it be addressed in another PR?\n<issue_comment>username_4: @username_0 Sure, it's definitely better to rename both cpu & gpu impls together in a separate PR. Just want to make sure we have the correct one after porting.\n<issue_comment>username_3: Just as a clarification: I'm OK with having changes which are improvements over the old kernels come in latter patches.\n<issue_comment>username_3: I finished reviewing one of the kernels, and did a cursory scan of the others. In general the patch looks like it's in good shape. I'd suggest that we fix up the major correctness issues (PackedTensorAccessor ref, and int32 versus int64 indexing) and anything small that is easy to fix (for example, double zeroing the array), and leave other improvements for follow ups. Should be ready to land soon!\n<issue_comment>username_0: thanks so much @username_3!\r\nI will let you know when it is ready again for a new review! thanks!\n<issue_comment>username_0: @username_2 @username_3 \r\nnot sure but the errors on CI seems to be related to jenkins ... it seems all these jobs that failed (for building) ran by 01h 01min ... not sure if it is a coincidence ...\n<issue_comment>username_1: @username_0 indeed it looks like all jobs were aborted at the same time. no obvious issues in the build log related to your code. Comparing e.g. the `cuda9-cudnn7` build with a successful one from another PR, it takes 1hr 14min there and your build is about at the place where the other one is after an hour.\r\n\r\nI suggest to just push a new commit to rebuild. Probably a temporary CI hiccup.\n<issue_comment>username_3: I accidentally rebooted Jenkins yesterday which is the likely cause, my apologies.\r\n\r\n@pytorchbot retest this please\n<issue_comment>username_0: @username_3 @username_2 @username_1 \r\n\r\nall tests passed except `pr/caffe2-py2-cuda9.0-cudnn7-windows-build`:\r\n \r\n```\r\n14:43:49 Build timed out (after 180 minutes). Marking the build as failed.\r\n14:43:49 Build was aborted\r\n14:43:49 [BFA] Scanning build for known causes...\r\n14:43:49 [BFA] No failure causes found\r\n14:43:49 [BFA] Done. 0s\r\n14:43:49 Finished: FAILURE\r\n```\r\n\r\nnot sure if this timeout means that the code now is slower.\r\n\r\nwhat do you think? do you have any suggestion?\n<issue_comment>username_3: Sometimes the Windows build flakes out like that. It didn't timeout while running a relevant test, so I judge it to be not your problem.\n<issue_comment>username_3: The launch bounds logic is wrong, but I acknowledge that this is a big patch already; just fix it in a follow up. I am going to go ahead and land this.\n<issue_comment>username_0: sounds good @username_3 thanks!"}
{"repo": "pytorch/pytorch", "issue_id": 436372857, "issue_number": 19630, "timestamp": "2019-05-14 11:06:08+00:00", "text": "I accidentally rebooted Jenkins yesterday which is the likely cause, my apologies.\r\n\r\n@pytorchbot retest this please", "context": "<issue_start><issue_comment>Title: [WIP] UpSample GPU Porting \nusername_0: resolves #16158\n<issue_comment>username_1: Fewer errors than before, 4 instead of 8 with the same message:\r\n```\r\nRuntimeError: CUDA error: too many resources requested for launch\r\n```\r\nThis seems to be an existing problem: gh-8103.\r\n\r\n@username_2 could you have a look at this and tell us what you think?\n<issue_comment>username_2: Looking at it very briefly, the `Jetson TX` referenced in the issue only has 256 cores.\r\n\r\nSo while the issue looks the same, I'd probably not expect it on the CI platforms.  FWIW, some recent CI tests in other issues are green.\n<issue_comment>username_2: @pytorchbot retest this please.\n<issue_comment>username_1: same failures.\n<issue_comment>username_2: Is this rebased on the latest master (you can also ask the bot to rebase)?\n<issue_comment>username_0: I rebased that yesterday ... also needed to resolve conflicts because 7 days ago upsample files were changed.\n<issue_comment>username_0: @username_2 do you have any idea about what could be this problem? or a way to debug that?\r\nalso it seems some jobs is taking a lot of time to build, for example, for some job the estimate time is 19h .. not sure if it the regular estimate ..\n<issue_comment>username_2: @username_0 If you can't reproduce it at home it seems hard to debug other than reading at the diffs again.  I can take a look on Monday, it's getting a bit late here.\n<issue_comment>username_2: Also, has anyone found a way to show the actual hardware used on the CI in detail?\n<issue_comment>username_0: @username_2 I am working in parallel on a paperspace environment .. it tooks a lot of time it is running with 8 cpu cores.\n<issue_comment>username_2: But have you ever been able to reproduce this issue on paperspace?\n<issue_comment>username_1: I can reproduce it locally: Arch Linux, CUDA 10.0, RTX2070 GPU. Given the CI failures, I think it should be reproducible for multiple CUDA and GPU versions. I just haven't worked on any CUDA code before, and am short on time, so I'd rather not dig too deep.\n<issue_comment>username_0: not yet .. my last building was with a my previous commit ... I will work again in this task in some minutes :)\n<issue_comment>username_1: @username_0 your previous commit had the same failure though (except for the less clear exception message), and it seems quite reproducible. So I think you'll see it now.\n<issue_comment>username_2: Ah, thanks.  @username_0, then I guess just compiling with \"DEBUG=1\" and stepping through the offending code with gdb may be the fastest way.\n<issue_comment>username_0: @username_2 thanks! I will try that!\n<issue_comment>username_0: it seems just UpSampleBicubic2d is using upsample_get_value_bounded (https://github.com/pytorch/pytorch/pull/19630/files#diff-5092da792c30694ee4adf0d0ae2a37c6R171) and upsample_increment_value_bounded (https://github.com/pytorch/pytorch/pull/19630/files#diff-5092da792c30694ee4adf0d0ae2a37c6R191)\r\n\r\nmaybe the problem could be inside one of these functions ... maybe related to the order of indexes (x, y) ...\r\n\r\nbut the problem seems to be related to cuda block/threads ... so not sure if it is really related to these functions.\n<issue_comment>username_2: The new code uses far more registers than the existing one. I verified that the both versions actually call the offending test case with `1024` in `blockDim`.  So it's very likely a register issue.\r\n\r\n`Existing` uses `64` registers, which seems to be optimal or my card:\r\n\r\n```\r\nptxas info    : 77696 bytes gmem, 72 bytes cmem[3]\r\nptxas info    : Compiling entry function '_Z23bicubic_interp2d_kerneliddb15THCDeviceTensorIdLi4Ei16DefaultPtrTraitsES1_' for 'sm_61'\r\nptxas info    : Function properties for _Z23bicubic_interp2d_kerneliddb15THCDeviceTensorIdLi4Ei16DefaultPtrTraitsES1_\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 64 registers, 432 bytes cmem[0], 24 bytes cmem[2]\r\n```\r\n\r\n`New` uses `124` registers, which is too much for my card:\r\n\r\n```\r\nptxas info    : 77696 bytes gmem, 72 bytes cmem[3]\r\nptxas info    : Compiling entry function '_ZN2at6native76_GLOBAL__N__52_tmpxft_0000626d_00000000_6_UpSampleBicubic2d_cpp1_ii_b4c1e1f328upsample_bicubic2d_out_frameElddbNS_20PackedTensorAccessorIdLm4ENS_16DefaultPtrTraitsElEES4_' for 'sm_61'\r\nptxas info    : Function properties for _ZN2at6native76_GLOBAL__N__52_tmpxft_0000626d_00000000_6_UpSampleBicubic2d_cpp1_ii_b4c1e1f328upsample_bicubic2d_out_frameElddbNS_20PackedTensorAccessorIdLm4ENS_16DefaultPtrTraitsElEES4_\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 124 registers, 496 bytes cmem[0], 24 bytes cmem[2]\r\n```\r\n\r\nWith `__launch_bounds__(1024)`, the code again uses `64` registers.\r\n\r\nIf you use `C10_LAUNCH_BOUNDS_1(1024)` **for both kernels**, the tests pass here.\r\n\r\n\r\nNow *why* is the regcount higher in the new code? It could be `PackedTensorAccessor`, it could be the fact that many instances of `int` have been changed to `int64_t`. :)\r\n\r\nYou could experiment or just use the launch bounds.  Other code in `native/cuda` seems to use lower bounds for `blockDim`, too.  1024 seems to be an outlier.\n<issue_comment>username_0: @username_2 \r\n\r\nit seems it worked locally! thank you so much!  I really appreciate that!\n<issue_comment>username_0: thanks @username_1 and @username_2 for all the help!\r\n\r\n@username_3 it is done for review!\n<issue_comment>username_3: CircleCI runs on AWS, and the I'm fairly sure we've got one of the g3 types, either g3.4xlarge or g3.8xlarge\n<issue_comment>username_3: cc @ngimel, if you want to take a look at this (I'm planning to do a review too)\n<issue_comment>username_0: @username_4 could it be addressed in another PR?\n<issue_comment>username_4: @username_0 Sure, it's definitely better to rename both cpu & gpu impls together in a separate PR. Just want to make sure we have the correct one after porting.\n<issue_comment>username_3: Just as a clarification: I'm OK with having changes which are improvements over the old kernels come in latter patches.\n<issue_comment>username_3: I finished reviewing one of the kernels, and did a cursory scan of the others. In general the patch looks like it's in good shape. I'd suggest that we fix up the major correctness issues (PackedTensorAccessor ref, and int32 versus int64 indexing) and anything small that is easy to fix (for example, double zeroing the array), and leave other improvements for follow ups. Should be ready to land soon!\n<issue_comment>username_0: thanks so much @username_3!\r\nI will let you know when it is ready again for a new review! thanks!\n<issue_comment>username_0: @username_2 @username_3 \r\nnot sure but the errors on CI seems to be related to jenkins ... it seems all these jobs that failed (for building) ran by 01h 01min ... not sure if it is a coincidence ...\n<issue_comment>username_1: @username_0 indeed it looks like all jobs were aborted at the same time. no obvious issues in the build log related to your code. Comparing e.g. the `cuda9-cudnn7` build with a successful one from another PR, it takes 1hr 14min there and your build is about at the place where the other one is after an hour.\r\n\r\nI suggest to just push a new commit to rebuild. Probably a temporary CI hiccup.\n<issue_comment>username_3: I accidentally rebooted Jenkins yesterday which is the likely cause, my apologies.\r\n\r\n@pytorchbot retest this please\n<issue_comment>username_0: @username_3 @username_2 @username_1 \r\n\r\nall tests passed except `pr/caffe2-py2-cuda9.0-cudnn7-windows-build`:\r\n \r\n```\r\n14:43:49 Build timed out (after 180 minutes). Marking the build as failed.\r\n14:43:49 Build was aborted\r\n14:43:49 [BFA] Scanning build for known causes...\r\n14:43:49 [BFA] No failure causes found\r\n14:43:49 [BFA] Done. 0s\r\n14:43:49 Finished: FAILURE\r\n```\r\n\r\nnot sure if this timeout means that the code now is slower.\r\n\r\nwhat do you think? do you have any suggestion?\n<issue_comment>username_3: Sometimes the Windows build flakes out like that. It didn't timeout while running a relevant test, so I judge it to be not your problem.\n<issue_comment>username_3: The launch bounds logic is wrong, but I acknowledge that this is a big patch already; just fix it in a follow up. I am going to go ahead and land this.\n<issue_comment>username_0: sounds good @username_3 thanks!"}
{"repo": "pytorch/pytorch", "issue_id": 436372857, "issue_number": 19630, "timestamp": "2019-05-14 14:48:22+00:00", "text": "@username_3 @username_2 @username_1 \r\n\r\nall tests passed except `pr/caffe2-py2-cuda9.0-cudnn7-windows-build`:\r\n \r\n```\r\n14:43:49 Build timed out (after 180 minutes). Marking the build as failed.\r\n14:43:49 Build was aborted\r\n14:43:49 [BFA] Scanning build for known causes...\r\n14:43:49 [BFA] No failure causes found\r\n14:43:49 [BFA] Done. 0s\r\n14:43:49 Finished: FAILURE\r\n```\r\n\r\nnot sure if this timeout means that the code now is slower.\r\n\r\nwhat do you think? do you have any suggestion?", "context": "<issue_start><issue_comment>Title: [WIP] UpSample GPU Porting \nusername_0: resolves #16158\n<issue_comment>username_1: Fewer errors than before, 4 instead of 8 with the same message:\r\n```\r\nRuntimeError: CUDA error: too many resources requested for launch\r\n```\r\nThis seems to be an existing problem: gh-8103.\r\n\r\n@username_2 could you have a look at this and tell us what you think?\n<issue_comment>username_2: Looking at it very briefly, the `Jetson TX` referenced in the issue only has 256 cores.\r\n\r\nSo while the issue looks the same, I'd probably not expect it on the CI platforms.  FWIW, some recent CI tests in other issues are green.\n<issue_comment>username_2: @pytorchbot retest this please.\n<issue_comment>username_1: same failures.\n<issue_comment>username_2: Is this rebased on the latest master (you can also ask the bot to rebase)?\n<issue_comment>username_0: I rebased that yesterday ... also needed to resolve conflicts because 7 days ago upsample files were changed.\n<issue_comment>username_0: @username_2 do you have any idea about what could be this problem? or a way to debug that?\r\nalso it seems some jobs is taking a lot of time to build, for example, for some job the estimate time is 19h .. not sure if it the regular estimate ..\n<issue_comment>username_2: @username_0 If you can't reproduce it at home it seems hard to debug other than reading at the diffs again.  I can take a look on Monday, it's getting a bit late here.\n<issue_comment>username_2: Also, has anyone found a way to show the actual hardware used on the CI in detail?\n<issue_comment>username_0: @username_2 I am working in parallel on a paperspace environment .. it tooks a lot of time it is running with 8 cpu cores.\n<issue_comment>username_2: But have you ever been able to reproduce this issue on paperspace?\n<issue_comment>username_1: I can reproduce it locally: Arch Linux, CUDA 10.0, RTX2070 GPU. Given the CI failures, I think it should be reproducible for multiple CUDA and GPU versions. I just haven't worked on any CUDA code before, and am short on time, so I'd rather not dig too deep.\n<issue_comment>username_0: not yet .. my last building was with a my previous commit ... I will work again in this task in some minutes :)\n<issue_comment>username_1: @username_0 your previous commit had the same failure though (except for the less clear exception message), and it seems quite reproducible. So I think you'll see it now.\n<issue_comment>username_2: Ah, thanks.  @username_0, then I guess just compiling with \"DEBUG=1\" and stepping through the offending code with gdb may be the fastest way.\n<issue_comment>username_0: @username_2 thanks! I will try that!\n<issue_comment>username_0: it seems just UpSampleBicubic2d is using upsample_get_value_bounded (https://github.com/pytorch/pytorch/pull/19630/files#diff-5092da792c30694ee4adf0d0ae2a37c6R171) and upsample_increment_value_bounded (https://github.com/pytorch/pytorch/pull/19630/files#diff-5092da792c30694ee4adf0d0ae2a37c6R191)\r\n\r\nmaybe the problem could be inside one of these functions ... maybe related to the order of indexes (x, y) ...\r\n\r\nbut the problem seems to be related to cuda block/threads ... so not sure if it is really related to these functions.\n<issue_comment>username_2: The new code uses far more registers than the existing one. I verified that the both versions actually call the offending test case with `1024` in `blockDim`.  So it's very likely a register issue.\r\n\r\n`Existing` uses `64` registers, which seems to be optimal or my card:\r\n\r\n```\r\nptxas info    : 77696 bytes gmem, 72 bytes cmem[3]\r\nptxas info    : Compiling entry function '_Z23bicubic_interp2d_kerneliddb15THCDeviceTensorIdLi4Ei16DefaultPtrTraitsES1_' for 'sm_61'\r\nptxas info    : Function properties for _Z23bicubic_interp2d_kerneliddb15THCDeviceTensorIdLi4Ei16DefaultPtrTraitsES1_\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 64 registers, 432 bytes cmem[0], 24 bytes cmem[2]\r\n```\r\n\r\n`New` uses `124` registers, which is too much for my card:\r\n\r\n```\r\nptxas info    : 77696 bytes gmem, 72 bytes cmem[3]\r\nptxas info    : Compiling entry function '_ZN2at6native76_GLOBAL__N__52_tmpxft_0000626d_00000000_6_UpSampleBicubic2d_cpp1_ii_b4c1e1f328upsample_bicubic2d_out_frameElddbNS_20PackedTensorAccessorIdLm4ENS_16DefaultPtrTraitsElEES4_' for 'sm_61'\r\nptxas info    : Function properties for _ZN2at6native76_GLOBAL__N__52_tmpxft_0000626d_00000000_6_UpSampleBicubic2d_cpp1_ii_b4c1e1f328upsample_bicubic2d_out_frameElddbNS_20PackedTensorAccessorIdLm4ENS_16DefaultPtrTraitsElEES4_\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 124 registers, 496 bytes cmem[0], 24 bytes cmem[2]\r\n```\r\n\r\nWith `__launch_bounds__(1024)`, the code again uses `64` registers.\r\n\r\nIf you use `C10_LAUNCH_BOUNDS_1(1024)` **for both kernels**, the tests pass here.\r\n\r\n\r\nNow *why* is the regcount higher in the new code? It could be `PackedTensorAccessor`, it could be the fact that many instances of `int` have been changed to `int64_t`. :)\r\n\r\nYou could experiment or just use the launch bounds.  Other code in `native/cuda` seems to use lower bounds for `blockDim`, too.  1024 seems to be an outlier.\n<issue_comment>username_0: @username_2 \r\n\r\nit seems it worked locally! thank you so much!  I really appreciate that!\n<issue_comment>username_0: thanks @username_1 and @username_2 for all the help!\r\n\r\n@username_3 it is done for review!\n<issue_comment>username_3: CircleCI runs on AWS, and the I'm fairly sure we've got one of the g3 types, either g3.4xlarge or g3.8xlarge\n<issue_comment>username_3: cc @ngimel, if you want to take a look at this (I'm planning to do a review too)\n<issue_comment>username_0: @username_4 could it be addressed in another PR?\n<issue_comment>username_4: @username_0 Sure, it's definitely better to rename both cpu & gpu impls together in a separate PR. Just want to make sure we have the correct one after porting.\n<issue_comment>username_3: Just as a clarification: I'm OK with having changes which are improvements over the old kernels come in latter patches.\n<issue_comment>username_3: I finished reviewing one of the kernels, and did a cursory scan of the others. In general the patch looks like it's in good shape. I'd suggest that we fix up the major correctness issues (PackedTensorAccessor ref, and int32 versus int64 indexing) and anything small that is easy to fix (for example, double zeroing the array), and leave other improvements for follow ups. Should be ready to land soon!\n<issue_comment>username_0: thanks so much @username_3!\r\nI will let you know when it is ready again for a new review! thanks!\n<issue_comment>username_0: @username_2 @username_3 \r\nnot sure but the errors on CI seems to be related to jenkins ... it seems all these jobs that failed (for building) ran by 01h 01min ... not sure if it is a coincidence ...\n<issue_comment>username_1: @username_0 indeed it looks like all jobs were aborted at the same time. no obvious issues in the build log related to your code. Comparing e.g. the `cuda9-cudnn7` build with a successful one from another PR, it takes 1hr 14min there and your build is about at the place where the other one is after an hour.\r\n\r\nI suggest to just push a new commit to rebuild. Probably a temporary CI hiccup.\n<issue_comment>username_3: I accidentally rebooted Jenkins yesterday which is the likely cause, my apologies.\r\n\r\n@pytorchbot retest this please\n<issue_comment>username_0: @username_3 @username_2 @username_1 \r\n\r\nall tests passed except `pr/caffe2-py2-cuda9.0-cudnn7-windows-build`:\r\n \r\n```\r\n14:43:49 Build timed out (after 180 minutes). Marking the build as failed.\r\n14:43:49 Build was aborted\r\n14:43:49 [BFA] Scanning build for known causes...\r\n14:43:49 [BFA] No failure causes found\r\n14:43:49 [BFA] Done. 0s\r\n14:43:49 Finished: FAILURE\r\n```\r\n\r\nnot sure if this timeout means that the code now is slower.\r\n\r\nwhat do you think? do you have any suggestion?\n<issue_comment>username_3: Sometimes the Windows build flakes out like that. It didn't timeout while running a relevant test, so I judge it to be not your problem.\n<issue_comment>username_3: The launch bounds logic is wrong, but I acknowledge that this is a big patch already; just fix it in a follow up. I am going to go ahead and land this.\n<issue_comment>username_0: sounds good @username_3 thanks!"}
{"repo": "pytorch/pytorch", "issue_id": 436372857, "issue_number": 19630, "timestamp": "2019-05-14 15:21:48+00:00", "text": "Sometimes the Windows build flakes out like that. It didn't timeout while running a relevant test, so I judge it to be not your problem.", "context": "<issue_start><issue_comment>Title: [WIP] UpSample GPU Porting \nusername_0: resolves #16158\n<issue_comment>username_1: Fewer errors than before, 4 instead of 8 with the same message:\r\n```\r\nRuntimeError: CUDA error: too many resources requested for launch\r\n```\r\nThis seems to be an existing problem: gh-8103.\r\n\r\n@username_2 could you have a look at this and tell us what you think?\n<issue_comment>username_2: Looking at it very briefly, the `Jetson TX` referenced in the issue only has 256 cores.\r\n\r\nSo while the issue looks the same, I'd probably not expect it on the CI platforms.  FWIW, some recent CI tests in other issues are green.\n<issue_comment>username_2: @pytorchbot retest this please.\n<issue_comment>username_1: same failures.\n<issue_comment>username_2: Is this rebased on the latest master (you can also ask the bot to rebase)?\n<issue_comment>username_0: I rebased that yesterday ... also needed to resolve conflicts because 7 days ago upsample files were changed.\n<issue_comment>username_0: @username_2 do you have any idea about what could be this problem? or a way to debug that?\r\nalso it seems some jobs is taking a lot of time to build, for example, for some job the estimate time is 19h .. not sure if it the regular estimate ..\n<issue_comment>username_2: @username_0 If you can't reproduce it at home it seems hard to debug other than reading at the diffs again.  I can take a look on Monday, it's getting a bit late here.\n<issue_comment>username_2: Also, has anyone found a way to show the actual hardware used on the CI in detail?\n<issue_comment>username_0: @username_2 I am working in parallel on a paperspace environment .. it tooks a lot of time it is running with 8 cpu cores.\n<issue_comment>username_2: But have you ever been able to reproduce this issue on paperspace?\n<issue_comment>username_1: I can reproduce it locally: Arch Linux, CUDA 10.0, RTX2070 GPU. Given the CI failures, I think it should be reproducible for multiple CUDA and GPU versions. I just haven't worked on any CUDA code before, and am short on time, so I'd rather not dig too deep.\n<issue_comment>username_0: not yet .. my last building was with a my previous commit ... I will work again in this task in some minutes :)\n<issue_comment>username_1: @username_0 your previous commit had the same failure though (except for the less clear exception message), and it seems quite reproducible. So I think you'll see it now.\n<issue_comment>username_2: Ah, thanks.  @username_0, then I guess just compiling with \"DEBUG=1\" and stepping through the offending code with gdb may be the fastest way.\n<issue_comment>username_0: @username_2 thanks! I will try that!\n<issue_comment>username_0: it seems just UpSampleBicubic2d is using upsample_get_value_bounded (https://github.com/pytorch/pytorch/pull/19630/files#diff-5092da792c30694ee4adf0d0ae2a37c6R171) and upsample_increment_value_bounded (https://github.com/pytorch/pytorch/pull/19630/files#diff-5092da792c30694ee4adf0d0ae2a37c6R191)\r\n\r\nmaybe the problem could be inside one of these functions ... maybe related to the order of indexes (x, y) ...\r\n\r\nbut the problem seems to be related to cuda block/threads ... so not sure if it is really related to these functions.\n<issue_comment>username_2: The new code uses far more registers than the existing one. I verified that the both versions actually call the offending test case with `1024` in `blockDim`.  So it's very likely a register issue.\r\n\r\n`Existing` uses `64` registers, which seems to be optimal or my card:\r\n\r\n```\r\nptxas info    : 77696 bytes gmem, 72 bytes cmem[3]\r\nptxas info    : Compiling entry function '_Z23bicubic_interp2d_kerneliddb15THCDeviceTensorIdLi4Ei16DefaultPtrTraitsES1_' for 'sm_61'\r\nptxas info    : Function properties for _Z23bicubic_interp2d_kerneliddb15THCDeviceTensorIdLi4Ei16DefaultPtrTraitsES1_\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 64 registers, 432 bytes cmem[0], 24 bytes cmem[2]\r\n```\r\n\r\n`New` uses `124` registers, which is too much for my card:\r\n\r\n```\r\nptxas info    : 77696 bytes gmem, 72 bytes cmem[3]\r\nptxas info    : Compiling entry function '_ZN2at6native76_GLOBAL__N__52_tmpxft_0000626d_00000000_6_UpSampleBicubic2d_cpp1_ii_b4c1e1f328upsample_bicubic2d_out_frameElddbNS_20PackedTensorAccessorIdLm4ENS_16DefaultPtrTraitsElEES4_' for 'sm_61'\r\nptxas info    : Function properties for _ZN2at6native76_GLOBAL__N__52_tmpxft_0000626d_00000000_6_UpSampleBicubic2d_cpp1_ii_b4c1e1f328upsample_bicubic2d_out_frameElddbNS_20PackedTensorAccessorIdLm4ENS_16DefaultPtrTraitsElEES4_\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 124 registers, 496 bytes cmem[0], 24 bytes cmem[2]\r\n```\r\n\r\nWith `__launch_bounds__(1024)`, the code again uses `64` registers.\r\n\r\nIf you use `C10_LAUNCH_BOUNDS_1(1024)` **for both kernels**, the tests pass here.\r\n\r\n\r\nNow *why* is the regcount higher in the new code? It could be `PackedTensorAccessor`, it could be the fact that many instances of `int` have been changed to `int64_t`. :)\r\n\r\nYou could experiment or just use the launch bounds.  Other code in `native/cuda` seems to use lower bounds for `blockDim`, too.  1024 seems to be an outlier.\n<issue_comment>username_0: @username_2 \r\n\r\nit seems it worked locally! thank you so much!  I really appreciate that!\n<issue_comment>username_0: thanks @username_1 and @username_2 for all the help!\r\n\r\n@username_3 it is done for review!\n<issue_comment>username_3: CircleCI runs on AWS, and the I'm fairly sure we've got one of the g3 types, either g3.4xlarge or g3.8xlarge\n<issue_comment>username_3: cc @ngimel, if you want to take a look at this (I'm planning to do a review too)\n<issue_comment>username_0: @username_4 could it be addressed in another PR?\n<issue_comment>username_4: @username_0 Sure, it's definitely better to rename both cpu & gpu impls together in a separate PR. Just want to make sure we have the correct one after porting.\n<issue_comment>username_3: Just as a clarification: I'm OK with having changes which are improvements over the old kernels come in latter patches.\n<issue_comment>username_3: I finished reviewing one of the kernels, and did a cursory scan of the others. In general the patch looks like it's in good shape. I'd suggest that we fix up the major correctness issues (PackedTensorAccessor ref, and int32 versus int64 indexing) and anything small that is easy to fix (for example, double zeroing the array), and leave other improvements for follow ups. Should be ready to land soon!\n<issue_comment>username_0: thanks so much @username_3!\r\nI will let you know when it is ready again for a new review! thanks!\n<issue_comment>username_0: @username_2 @username_3 \r\nnot sure but the errors on CI seems to be related to jenkins ... it seems all these jobs that failed (for building) ran by 01h 01min ... not sure if it is a coincidence ...\n<issue_comment>username_1: @username_0 indeed it looks like all jobs were aborted at the same time. no obvious issues in the build log related to your code. Comparing e.g. the `cuda9-cudnn7` build with a successful one from another PR, it takes 1hr 14min there and your build is about at the place where the other one is after an hour.\r\n\r\nI suggest to just push a new commit to rebuild. Probably a temporary CI hiccup.\n<issue_comment>username_3: I accidentally rebooted Jenkins yesterday which is the likely cause, my apologies.\r\n\r\n@pytorchbot retest this please\n<issue_comment>username_0: @username_3 @username_2 @username_1 \r\n\r\nall tests passed except `pr/caffe2-py2-cuda9.0-cudnn7-windows-build`:\r\n \r\n```\r\n14:43:49 Build timed out (after 180 minutes). Marking the build as failed.\r\n14:43:49 Build was aborted\r\n14:43:49 [BFA] Scanning build for known causes...\r\n14:43:49 [BFA] No failure causes found\r\n14:43:49 [BFA] Done. 0s\r\n14:43:49 Finished: FAILURE\r\n```\r\n\r\nnot sure if this timeout means that the code now is slower.\r\n\r\nwhat do you think? do you have any suggestion?\n<issue_comment>username_3: Sometimes the Windows build flakes out like that. It didn't timeout while running a relevant test, so I judge it to be not your problem.\n<issue_comment>username_3: The launch bounds logic is wrong, but I acknowledge that this is a big patch already; just fix it in a follow up. I am going to go ahead and land this.\n<issue_comment>username_0: sounds good @username_3 thanks!"}
{"repo": "pytorch/pytorch", "issue_id": 436372857, "issue_number": 19630, "timestamp": "2019-05-14 15:31:02+00:00", "text": "The launch bounds logic is wrong, but I acknowledge that this is a big patch already; just fix it in a follow up. I am going to go ahead and land this.", "context": "<issue_start><issue_comment>Title: [WIP] UpSample GPU Porting \nusername_0: resolves #16158\n<issue_comment>username_1: Fewer errors than before, 4 instead of 8 with the same message:\r\n```\r\nRuntimeError: CUDA error: too many resources requested for launch\r\n```\r\nThis seems to be an existing problem: gh-8103.\r\n\r\n@username_2 could you have a look at this and tell us what you think?\n<issue_comment>username_2: Looking at it very briefly, the `Jetson TX` referenced in the issue only has 256 cores.\r\n\r\nSo while the issue looks the same, I'd probably not expect it on the CI platforms.  FWIW, some recent CI tests in other issues are green.\n<issue_comment>username_2: @pytorchbot retest this please.\n<issue_comment>username_1: same failures.\n<issue_comment>username_2: Is this rebased on the latest master (you can also ask the bot to rebase)?\n<issue_comment>username_0: I rebased that yesterday ... also needed to resolve conflicts because 7 days ago upsample files were changed.\n<issue_comment>username_0: @username_2 do you have any idea about what could be this problem? or a way to debug that?\r\nalso it seems some jobs is taking a lot of time to build, for example, for some job the estimate time is 19h .. not sure if it the regular estimate ..\n<issue_comment>username_2: @username_0 If you can't reproduce it at home it seems hard to debug other than reading at the diffs again.  I can take a look on Monday, it's getting a bit late here.\n<issue_comment>username_2: Also, has anyone found a way to show the actual hardware used on the CI in detail?\n<issue_comment>username_0: @username_2 I am working in parallel on a paperspace environment .. it tooks a lot of time it is running with 8 cpu cores.\n<issue_comment>username_2: But have you ever been able to reproduce this issue on paperspace?\n<issue_comment>username_1: I can reproduce it locally: Arch Linux, CUDA 10.0, RTX2070 GPU. Given the CI failures, I think it should be reproducible for multiple CUDA and GPU versions. I just haven't worked on any CUDA code before, and am short on time, so I'd rather not dig too deep.\n<issue_comment>username_0: not yet .. my last building was with a my previous commit ... I will work again in this task in some minutes :)\n<issue_comment>username_1: @username_0 your previous commit had the same failure though (except for the less clear exception message), and it seems quite reproducible. So I think you'll see it now.\n<issue_comment>username_2: Ah, thanks.  @username_0, then I guess just compiling with \"DEBUG=1\" and stepping through the offending code with gdb may be the fastest way.\n<issue_comment>username_0: @username_2 thanks! I will try that!\n<issue_comment>username_0: it seems just UpSampleBicubic2d is using upsample_get_value_bounded (https://github.com/pytorch/pytorch/pull/19630/files#diff-5092da792c30694ee4adf0d0ae2a37c6R171) and upsample_increment_value_bounded (https://github.com/pytorch/pytorch/pull/19630/files#diff-5092da792c30694ee4adf0d0ae2a37c6R191)\r\n\r\nmaybe the problem could be inside one of these functions ... maybe related to the order of indexes (x, y) ...\r\n\r\nbut the problem seems to be related to cuda block/threads ... so not sure if it is really related to these functions.\n<issue_comment>username_2: The new code uses far more registers than the existing one. I verified that the both versions actually call the offending test case with `1024` in `blockDim`.  So it's very likely a register issue.\r\n\r\n`Existing` uses `64` registers, which seems to be optimal or my card:\r\n\r\n```\r\nptxas info    : 77696 bytes gmem, 72 bytes cmem[3]\r\nptxas info    : Compiling entry function '_Z23bicubic_interp2d_kerneliddb15THCDeviceTensorIdLi4Ei16DefaultPtrTraitsES1_' for 'sm_61'\r\nptxas info    : Function properties for _Z23bicubic_interp2d_kerneliddb15THCDeviceTensorIdLi4Ei16DefaultPtrTraitsES1_\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 64 registers, 432 bytes cmem[0], 24 bytes cmem[2]\r\n```\r\n\r\n`New` uses `124` registers, which is too much for my card:\r\n\r\n```\r\nptxas info    : 77696 bytes gmem, 72 bytes cmem[3]\r\nptxas info    : Compiling entry function '_ZN2at6native76_GLOBAL__N__52_tmpxft_0000626d_00000000_6_UpSampleBicubic2d_cpp1_ii_b4c1e1f328upsample_bicubic2d_out_frameElddbNS_20PackedTensorAccessorIdLm4ENS_16DefaultPtrTraitsElEES4_' for 'sm_61'\r\nptxas info    : Function properties for _ZN2at6native76_GLOBAL__N__52_tmpxft_0000626d_00000000_6_UpSampleBicubic2d_cpp1_ii_b4c1e1f328upsample_bicubic2d_out_frameElddbNS_20PackedTensorAccessorIdLm4ENS_16DefaultPtrTraitsElEES4_\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 124 registers, 496 bytes cmem[0], 24 bytes cmem[2]\r\n```\r\n\r\nWith `__launch_bounds__(1024)`, the code again uses `64` registers.\r\n\r\nIf you use `C10_LAUNCH_BOUNDS_1(1024)` **for both kernels**, the tests pass here.\r\n\r\n\r\nNow *why* is the regcount higher in the new code? It could be `PackedTensorAccessor`, it could be the fact that many instances of `int` have been changed to `int64_t`. :)\r\n\r\nYou could experiment or just use the launch bounds.  Other code in `native/cuda` seems to use lower bounds for `blockDim`, too.  1024 seems to be an outlier.\n<issue_comment>username_0: @username_2 \r\n\r\nit seems it worked locally! thank you so much!  I really appreciate that!\n<issue_comment>username_0: thanks @username_1 and @username_2 for all the help!\r\n\r\n@username_3 it is done for review!\n<issue_comment>username_3: CircleCI runs on AWS, and the I'm fairly sure we've got one of the g3 types, either g3.4xlarge or g3.8xlarge\n<issue_comment>username_3: cc @ngimel, if you want to take a look at this (I'm planning to do a review too)\n<issue_comment>username_0: @username_4 could it be addressed in another PR?\n<issue_comment>username_4: @username_0 Sure, it's definitely better to rename both cpu & gpu impls together in a separate PR. Just want to make sure we have the correct one after porting.\n<issue_comment>username_3: Just as a clarification: I'm OK with having changes which are improvements over the old kernels come in latter patches.\n<issue_comment>username_3: I finished reviewing one of the kernels, and did a cursory scan of the others. In general the patch looks like it's in good shape. I'd suggest that we fix up the major correctness issues (PackedTensorAccessor ref, and int32 versus int64 indexing) and anything small that is easy to fix (for example, double zeroing the array), and leave other improvements for follow ups. Should be ready to land soon!\n<issue_comment>username_0: thanks so much @username_3!\r\nI will let you know when it is ready again for a new review! thanks!\n<issue_comment>username_0: @username_2 @username_3 \r\nnot sure but the errors on CI seems to be related to jenkins ... it seems all these jobs that failed (for building) ran by 01h 01min ... not sure if it is a coincidence ...\n<issue_comment>username_1: @username_0 indeed it looks like all jobs were aborted at the same time. no obvious issues in the build log related to your code. Comparing e.g. the `cuda9-cudnn7` build with a successful one from another PR, it takes 1hr 14min there and your build is about at the place where the other one is after an hour.\r\n\r\nI suggest to just push a new commit to rebuild. Probably a temporary CI hiccup.\n<issue_comment>username_3: I accidentally rebooted Jenkins yesterday which is the likely cause, my apologies.\r\n\r\n@pytorchbot retest this please\n<issue_comment>username_0: @username_3 @username_2 @username_1 \r\n\r\nall tests passed except `pr/caffe2-py2-cuda9.0-cudnn7-windows-build`:\r\n \r\n```\r\n14:43:49 Build timed out (after 180 minutes). Marking the build as failed.\r\n14:43:49 Build was aborted\r\n14:43:49 [BFA] Scanning build for known causes...\r\n14:43:49 [BFA] No failure causes found\r\n14:43:49 [BFA] Done. 0s\r\n14:43:49 Finished: FAILURE\r\n```\r\n\r\nnot sure if this timeout means that the code now is slower.\r\n\r\nwhat do you think? do you have any suggestion?\n<issue_comment>username_3: Sometimes the Windows build flakes out like that. It didn't timeout while running a relevant test, so I judge it to be not your problem.\n<issue_comment>username_3: The launch bounds logic is wrong, but I acknowledge that this is a big patch already; just fix it in a follow up. I am going to go ahead and land this.\n<issue_comment>username_0: sounds good @username_3 thanks!"}
{"repo": "pytorch/pytorch", "issue_id": 436372857, "issue_number": 19630, "timestamp": "2019-05-14 15:55:58+00:00", "text": "sounds good @username_3 thanks!", "context": "<issue_start><issue_comment>Title: [WIP] UpSample GPU Porting \nusername_0: resolves #16158\n<issue_comment>username_1: Fewer errors than before, 4 instead of 8 with the same message:\r\n```\r\nRuntimeError: CUDA error: too many resources requested for launch\r\n```\r\nThis seems to be an existing problem: gh-8103.\r\n\r\n@username_2 could you have a look at this and tell us what you think?\n<issue_comment>username_2: Looking at it very briefly, the `Jetson TX` referenced in the issue only has 256 cores.\r\n\r\nSo while the issue looks the same, I'd probably not expect it on the CI platforms.  FWIW, some recent CI tests in other issues are green.\n<issue_comment>username_2: @pytorchbot retest this please.\n<issue_comment>username_1: same failures.\n<issue_comment>username_2: Is this rebased on the latest master (you can also ask the bot to rebase)?\n<issue_comment>username_0: I rebased that yesterday ... also needed to resolve conflicts because 7 days ago upsample files were changed.\n<issue_comment>username_0: @username_2 do you have any idea about what could be this problem? or a way to debug that?\r\nalso it seems some jobs is taking a lot of time to build, for example, for some job the estimate time is 19h .. not sure if it the regular estimate ..\n<issue_comment>username_2: @username_0 If you can't reproduce it at home it seems hard to debug other than reading at the diffs again.  I can take a look on Monday, it's getting a bit late here.\n<issue_comment>username_2: Also, has anyone found a way to show the actual hardware used on the CI in detail?\n<issue_comment>username_0: @username_2 I am working in parallel on a paperspace environment .. it tooks a lot of time it is running with 8 cpu cores.\n<issue_comment>username_2: But have you ever been able to reproduce this issue on paperspace?\n<issue_comment>username_1: I can reproduce it locally: Arch Linux, CUDA 10.0, RTX2070 GPU. Given the CI failures, I think it should be reproducible for multiple CUDA and GPU versions. I just haven't worked on any CUDA code before, and am short on time, so I'd rather not dig too deep.\n<issue_comment>username_0: not yet .. my last building was with a my previous commit ... I will work again in this task in some minutes :)\n<issue_comment>username_1: @username_0 your previous commit had the same failure though (except for the less clear exception message), and it seems quite reproducible. So I think you'll see it now.\n<issue_comment>username_2: Ah, thanks.  @username_0, then I guess just compiling with \"DEBUG=1\" and stepping through the offending code with gdb may be the fastest way.\n<issue_comment>username_0: @username_2 thanks! I will try that!\n<issue_comment>username_0: it seems just UpSampleBicubic2d is using upsample_get_value_bounded (https://github.com/pytorch/pytorch/pull/19630/files#diff-5092da792c30694ee4adf0d0ae2a37c6R171) and upsample_increment_value_bounded (https://github.com/pytorch/pytorch/pull/19630/files#diff-5092da792c30694ee4adf0d0ae2a37c6R191)\r\n\r\nmaybe the problem could be inside one of these functions ... maybe related to the order of indexes (x, y) ...\r\n\r\nbut the problem seems to be related to cuda block/threads ... so not sure if it is really related to these functions.\n<issue_comment>username_2: The new code uses far more registers than the existing one. I verified that the both versions actually call the offending test case with `1024` in `blockDim`.  So it's very likely a register issue.\r\n\r\n`Existing` uses `64` registers, which seems to be optimal or my card:\r\n\r\n```\r\nptxas info    : 77696 bytes gmem, 72 bytes cmem[3]\r\nptxas info    : Compiling entry function '_Z23bicubic_interp2d_kerneliddb15THCDeviceTensorIdLi4Ei16DefaultPtrTraitsES1_' for 'sm_61'\r\nptxas info    : Function properties for _Z23bicubic_interp2d_kerneliddb15THCDeviceTensorIdLi4Ei16DefaultPtrTraitsES1_\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 64 registers, 432 bytes cmem[0], 24 bytes cmem[2]\r\n```\r\n\r\n`New` uses `124` registers, which is too much for my card:\r\n\r\n```\r\nptxas info    : 77696 bytes gmem, 72 bytes cmem[3]\r\nptxas info    : Compiling entry function '_ZN2at6native76_GLOBAL__N__52_tmpxft_0000626d_00000000_6_UpSampleBicubic2d_cpp1_ii_b4c1e1f328upsample_bicubic2d_out_frameElddbNS_20PackedTensorAccessorIdLm4ENS_16DefaultPtrTraitsElEES4_' for 'sm_61'\r\nptxas info    : Function properties for _ZN2at6native76_GLOBAL__N__52_tmpxft_0000626d_00000000_6_UpSampleBicubic2d_cpp1_ii_b4c1e1f328upsample_bicubic2d_out_frameElddbNS_20PackedTensorAccessorIdLm4ENS_16DefaultPtrTraitsElEES4_\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 124 registers, 496 bytes cmem[0], 24 bytes cmem[2]\r\n```\r\n\r\nWith `__launch_bounds__(1024)`, the code again uses `64` registers.\r\n\r\nIf you use `C10_LAUNCH_BOUNDS_1(1024)` **for both kernels**, the tests pass here.\r\n\r\n\r\nNow *why* is the regcount higher in the new code? It could be `PackedTensorAccessor`, it could be the fact that many instances of `int` have been changed to `int64_t`. :)\r\n\r\nYou could experiment or just use the launch bounds.  Other code in `native/cuda` seems to use lower bounds for `blockDim`, too.  1024 seems to be an outlier.\n<issue_comment>username_0: @username_2 \r\n\r\nit seems it worked locally! thank you so much!  I really appreciate that!\n<issue_comment>username_0: thanks @username_1 and @username_2 for all the help!\r\n\r\n@username_3 it is done for review!\n<issue_comment>username_3: CircleCI runs on AWS, and the I'm fairly sure we've got one of the g3 types, either g3.4xlarge or g3.8xlarge\n<issue_comment>username_3: cc @ngimel, if you want to take a look at this (I'm planning to do a review too)\n<issue_comment>username_0: @username_4 could it be addressed in another PR?\n<issue_comment>username_4: @username_0 Sure, it's definitely better to rename both cpu & gpu impls together in a separate PR. Just want to make sure we have the correct one after porting.\n<issue_comment>username_3: Just as a clarification: I'm OK with having changes which are improvements over the old kernels come in latter patches.\n<issue_comment>username_3: I finished reviewing one of the kernels, and did a cursory scan of the others. In general the patch looks like it's in good shape. I'd suggest that we fix up the major correctness issues (PackedTensorAccessor ref, and int32 versus int64 indexing) and anything small that is easy to fix (for example, double zeroing the array), and leave other improvements for follow ups. Should be ready to land soon!\n<issue_comment>username_0: thanks so much @username_3!\r\nI will let you know when it is ready again for a new review! thanks!\n<issue_comment>username_0: @username_2 @username_3 \r\nnot sure but the errors on CI seems to be related to jenkins ... it seems all these jobs that failed (for building) ran by 01h 01min ... not sure if it is a coincidence ...\n<issue_comment>username_1: @username_0 indeed it looks like all jobs were aborted at the same time. no obvious issues in the build log related to your code. Comparing e.g. the `cuda9-cudnn7` build with a successful one from another PR, it takes 1hr 14min there and your build is about at the place where the other one is after an hour.\r\n\r\nI suggest to just push a new commit to rebuild. Probably a temporary CI hiccup.\n<issue_comment>username_3: I accidentally rebooted Jenkins yesterday which is the likely cause, my apologies.\r\n\r\n@pytorchbot retest this please\n<issue_comment>username_0: @username_3 @username_2 @username_1 \r\n\r\nall tests passed except `pr/caffe2-py2-cuda9.0-cudnn7-windows-build`:\r\n \r\n```\r\n14:43:49 Build timed out (after 180 minutes). Marking the build as failed.\r\n14:43:49 Build was aborted\r\n14:43:49 [BFA] Scanning build for known causes...\r\n14:43:49 [BFA] No failure causes found\r\n14:43:49 [BFA] Done. 0s\r\n14:43:49 Finished: FAILURE\r\n```\r\n\r\nnot sure if this timeout means that the code now is slower.\r\n\r\nwhat do you think? do you have any suggestion?\n<issue_comment>username_3: Sometimes the Windows build flakes out like that. It didn't timeout while running a relevant test, so I judge it to be not your problem.\n<issue_comment>username_3: The launch bounds logic is wrong, but I acknowledge that this is a big patch already; just fix it in a follow up. I am going to go ahead and land this.\n<issue_comment>username_0: sounds good @username_3 thanks!"}
{"repo": "pytorch/pytorch", "issue_id": 475728990, "issue_number": 23655, "timestamp": "2019-08-01 14:51:57+00:00", "text": "The performance of a TorchScript that handles the computation in the [Adam Optimizer](https://github.com/pytorch/pytorch/blob/master/torch/optim/adam.py) is worse than using the native PyTorch operations on CUDA. \r\n\r\n### Background\r\nI am trying to improve the GPU performance of the [Adam Optimizer](https://github.com/pytorch/pytorch/blob/master/torch/optim/adam.py) since it is the bottleneck in one of our workflows. \r\n\r\n### Results\r\nI have written a standalone script that compares PyTorch native tensor operations, TorchScript, and Numba (three runs each):\r\n```\r\nnative :  0.0506\r\nnative :  0.0322\r\nnative :  0.0322\r\nTScript:  0.1917\r\nTScript:  0.0442\r\nTScript:  0.0442\r\nnumba  :  0.1638\r\nnumba  :  0.0145\r\nnumba  :  0.0143\r\n```\r\nI suspect the problem is that TorchScript does not fuse all operations into a single CUDA. It generates two CUDA kernels (see the `PYTORCH_FUSION_DEBUG` output below)\r\n\r\n### Standalone Script\r\n```python\r\nimport math\r\nimport torch\r\nimport time\r\nimport random\r\nimport sys\r\n\r\ndef kernel_native(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n    exp_avg.mul_(beta1).add_(1 - beta1, grad)\r\n    exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\r\n    denom = exp_avg_sq.sqrt().add_(eps)\r\n    param.addcdiv_(-step_size, exp_avg, denom)   \r\n\r\n@torch.jit.script\r\ndef kernel_jit_script(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n    exp_avg.mul_(beta1)\r\n    exp_avg.add_((1 - beta1) + grad)\r\n    exp_avg_sq.mul_(beta2)\r\n    exp_avg_sq.add_((1 - beta2)*(grad*grad))\r\n    denom = exp_avg_sq.sqrt().add_(eps)\r\n    param += -step_size * (exp_avg / denom)\r\n\r\nfrom numba import cuda\r\n@cuda.jit\r\ndef kernel_numba_cuda(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    i = cuda.grid(1)\r\n    if i > grad.size:\r\n        return\r\n\r\n    exp_avg[i] = exp_avg[i] * beta1 + (1 - beta1) + grad[i]\r\n    exp_avg_sq[i] = exp_avg_sq[i] * beta2 + (1 - beta2) *grad[i]*grad[i]\r\n    denom = math.sqrt(exp_avg_sq[i]) + eps\r\n    param[i] += -step_size * (exp_avg[i] / denom)\r\n\r\ndef kernel_numba(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n\r\n    import ctypes\r\n    import numpy as np\r\n    def get_devicendarray(t):\r\n        assert t.type() == 'torch.cuda.FloatTensor'\r\n        numpy_dtype = np.float32\r\n        itemsize = 4\r\n\r\n        ctx = cuda.cudadrv.driver.driver.get_context()\r\n        mp = cuda.cudadrv.driver.MemoryPointer(ctx, ctypes.c_ulong(t.data_ptr()), t.numel()*itemsize)\r\n        return cuda.cudadrv.devicearray.DeviceNDArray(t.numel(), itemsize, numpy_dtype(), gpu_data=mp, stream=torch.cuda.current_stream().cuda_stream)\r\n\r\n    numba_param = get_devicendarray(param)\r\n    numba_grad = get_devicendarray(grad)\r\n    numba_exp_avg = get_devicendarray(exp_avg)\r\n    numba_exp_avg_sq = get_devicendarray(exp_avg_sq)\r\n    threadsperblock = 32\r\n    blockspergrid = (param.numel() + (threadsperblock - 1)) // threadsperblock\r\n    kernel_numba_cuda[blockspergrid, threadsperblock](numba_param, numba_grad, numba_exp_avg, numba_exp_avg_sq, beta1, beta2, step_size, eps)\r\n\r\nkernels = [(\"native \", kernel_native), (\"TScript\", kernel_jit_script), (\"numba  \", kernel_numba)]\r\n[Truncated]\n      size_t t1_dimIndex0 = t1_linearIndex ;\r\n      t1_offset += t1_dimIndex0 ;\r\n      IndexType t3_offset = 0;\r\n      IndexType t3_linearIndex = linearIndex;\r\n      \r\n      //printf(\"tensor t3 sizes[0] = %d, strides[0] = %d\\n\", t3.sizes[0],t3.strides[0]);\r\n      size_t t3_dimIndex0 = t3_linearIndex ;\r\n      t3_offset += t3_dimIndex0 ;\r\n      \r\n      // calculate the results\r\n      float n0 = __ldg(&t0.data[t0_offset]);\r\n      float n1 = __ldg(&t1.data[t1_offset]);\r\n      double n2 = s2;\r\n      float n3 = n0 / n1;\r\n      float n4 = n3 * ((float) n2);\r\n      t3.data[t3_offset] = n4;\r\n      \r\n    }\r\n}\r\n```", "context": "<issue_start><issue_comment>Title: TorchScript Poor CUDA Performance \nusername_0: The performance of a TorchScript that handles the computation in the [Adam Optimizer](https://github.com/pytorch/pytorch/blob/master/torch/optim/adam.py) is worse than using the native PyTorch operations on CUDA. \r\n\r\n### Background\r\nI am trying to improve the GPU performance of the [Adam Optimizer](https://github.com/pytorch/pytorch/blob/master/torch/optim/adam.py) since it is the bottleneck in one of our workflows. \r\n\r\n### Results\r\nI have written a standalone script that compares PyTorch native tensor operations, TorchScript, and Numba (three runs each):\r\n```\r\nnative :  0.0506\r\nnative :  0.0322\r\nnative :  0.0322\r\nTScript:  0.1917\r\nTScript:  0.0442\r\nTScript:  0.0442\r\nnumba  :  0.1638\r\nnumba  :  0.0145\r\nnumba  :  0.0143\r\n```\r\nI suspect the problem is that TorchScript does not fuse all operations into a single CUDA. It generates two CUDA kernels (see the `PYTORCH_FUSION_DEBUG` output below)\r\n\r\n### Standalone Script\r\n```python\r\nimport math\r\nimport torch\r\nimport time\r\nimport random\r\nimport sys\r\n\r\ndef kernel_native(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n    exp_avg.mul_(beta1).add_(1 - beta1, grad)\r\n    exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\r\n    denom = exp_avg_sq.sqrt().add_(eps)\r\n    param.addcdiv_(-step_size, exp_avg, denom)   \r\n\r\n@torch.jit.script\r\ndef kernel_jit_script(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n    exp_avg.mul_(beta1)\r\n    exp_avg.add_((1 - beta1) + grad)\r\n    exp_avg_sq.mul_(beta2)\r\n    exp_avg_sq.add_((1 - beta2)*(grad*grad))\r\n    denom = exp_avg_sq.sqrt().add_(eps)\r\n    param += -step_size * (exp_avg / denom)\r\n\r\nfrom numba import cuda\r\n@cuda.jit\r\ndef kernel_numba_cuda(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    i = cuda.grid(1)\r\n    if i > grad.size:\r\n        return\r\n\r\n    exp_avg[i] = exp_avg[i] * beta1 + (1 - beta1) + grad[i]\r\n    exp_avg_sq[i] = exp_avg_sq[i] * beta2 + (1 - beta2) *grad[i]*grad[i]\r\n    denom = math.sqrt(exp_avg_sq[i]) + eps\r\n    param[i] += -step_size * (exp_avg[i] / denom)\r\n\r\ndef kernel_numba(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n\r\n    import ctypes\r\n    import numpy as np\r\n    def get_devicendarray(t):\r\n        assert t.type() == 'torch.cuda.FloatTensor'\r\n        numpy_dtype = np.float32\r\n        itemsize = 4\r\n\r\n        ctx = cuda.cudadrv.driver.driver.get_context()\r\n        mp = cuda.cudadrv.driver.MemoryPointer(ctx, ctypes.c_ulong(t.data_ptr()), t.numel()*itemsize)\r\n        return cuda.cudadrv.devicearray.DeviceNDArray(t.numel(), itemsize, numpy_dtype(), gpu_data=mp, stream=torch.cuda.current_stream().cuda_stream)\r\n\r\n    numba_param = get_devicendarray(param)\r\n    numba_grad = get_devicendarray(grad)\r\n    numba_exp_avg = get_devicendarray(exp_avg)\r\n    numba_exp_avg_sq = get_devicendarray(exp_avg_sq)\r\n    threadsperblock = 32\r\n    blockspergrid = (param.numel() + (threadsperblock - 1)) // threadsperblock\r\n    kernel_numba_cuda[blockspergrid, threadsperblock](numba_param, numba_grad, numba_exp_avg, numba_exp_avg_sq, beta1, beta2, step_size, eps)\r\n\r\nkernels = [(\"native \", kernel_native), (\"TScript\", kernel_jit_script), (\"numba  \", kernel_numba)]\r\n[Truncated]\n      size_t t1_dimIndex0 = t1_linearIndex ;\r\n      t1_offset += t1_dimIndex0 ;\r\n      IndexType t3_offset = 0;\r\n      IndexType t3_linearIndex = linearIndex;\r\n      \r\n      //printf(\"tensor t3 sizes[0] = %d, strides[0] = %d\\n\", t3.sizes[0],t3.strides[0]);\r\n      size_t t3_dimIndex0 = t3_linearIndex ;\r\n      t3_offset += t3_dimIndex0 ;\r\n      \r\n      // calculate the results\r\n      float n0 = __ldg(&t0.data[t0_offset]);\r\n      float n1 = __ldg(&t1.data[t1_offset]);\r\n      double n2 = s2;\r\n      float n3 = n0 / n1;\r\n      float n4 = n3 * ((float) n2);\r\n      t3.data[t3_offset] = n4;\r\n      \r\n    }\r\n}\r\n```\n<issue_comment>username_1: @username_0 can you try removing all the in-place operations in your scripted version?\r\nIIRC in-place ops block fusion.\r\n\r\nI was btw able to fuse most of sparse adam by removing those in-place ops, see https://github.com/pytorch/pytorch/pull/23522#issuecomment-516952462\n<issue_comment>username_2: cc: @username_3 and also relevant WIP PR: [JIT version of Adagrad](https://github.com/pytorch/pytorch/pull/23640)\n<issue_comment>username_3: Yeah the CUDA fusion does not support fusing in-place operations yet, Can you try the suggestion by @username_1 to see if you get a better perf?\n<issue_comment>username_3: @username_0 Also, it seems like you are only timing it in 3 iterations, and the perf numbers seems including the compilation time. The first time JIT/TorchScript sees an input it will specialize and do optimizations against that input, so when comparing performance please allow a compilation time and runtime optimization warmup for the input.\n<issue_comment>username_4: @username_3 That made a big difference to my traced module too. Any other tricks like this? It slows down the non-JIT version, however.\n<issue_comment>username_3: @username_4 That's pretty much all, let me summarize:\r\n\r\n1. JIT fusion does not support in-place operation fusion and reduction operation yet.\r\n2. we did the fusion code generation at the first time we see an input that the shape we haven't seen before (like if the number of dimensions are different), this will help us generate faster cuda kernels. \r\n\r\nSo the TorchScript compilation time will be gone after compile and the first run :)\n<issue_comment>username_0: Thanks for the feedback!\r\n\r\nI have removed all in-place operations and added three kernel implementations:\r\n```python\r\n\r\nimport math\r\nimport torch\r\nimport time\r\nimport random\r\nimport sys\r\n\r\ndef kernel_native(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n    exp_avg.mul_(beta1).add_(1 - beta1, grad)\r\n    exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\r\n    denom = exp_avg_sq.sqrt().add_(eps)\r\n    param.addcdiv_(-step_size, exp_avg, denom)   \r\n\r\n@torch.jit.script\r\ndef kernel_jit_script1(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n    exp_avg *= beta1\r\n    exp_avg += (1 - beta1) + grad\r\n    exp_avg_sq *= beta2\r\n    exp_avg_sq += (1 - beta2)*(grad*grad)\r\n    param += -step_size * (exp_avg / exp_avg_sq.sqrt() + eps)\r\n\r\n@torch.jit.script\r\ndef kernel_jit_script2(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n    exp_avg[:] = exp_avg * beta1 + (1 - beta1) + grad\r\n    exp_avg_sq[:] = exp_avg_sq * beta2 + (1 - beta2)*(grad*grad)\r\n    param[:] = param -step_size * (exp_avg / exp_avg_sq.sqrt() + eps)\r\n\r\n@torch.jit.script\r\ndef kernel_jit_script3(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> Tuple[Tensor, Tensor, Tensor, Tensor]\r\n    exp_avg = exp_avg * beta1 + (1 - beta1) + grad\r\n    exp_avg_sq = exp_avg_sq * beta2 + (1 - beta2)*(grad*grad)\r\n    param = param -step_size * (exp_avg / exp_avg_sq.sqrt() + eps)\r\n    return (param, grad, exp_avg, exp_avg_sq)\r\n\r\n\r\nfrom numba import cuda\r\n@cuda.jit\r\ndef kernel_numba_cuda(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    i = cuda.grid(1)\r\n    if i > grad.size:\r\n        return\r\n\r\n    exp_avg[i] = exp_avg[i] * beta1 + (1 - beta1) + grad[i]\r\n    exp_avg_sq[i] = exp_avg_sq[i] * beta2 + (1 - beta2) *grad[i]*grad[i]\r\n    denom = math.sqrt(exp_avg_sq[i]) + eps\r\n    param[i] += -step_size * (exp_avg[i] / denom)\r\n\r\ndef kernel_numba(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n\r\n    import ctypes\r\n    import numpy as np\r\n    def get_devicendarray(t):\r\n        assert t.type() == 'torch.cuda.FloatTensor'\r\n        numpy_dtype = np.float32\r\n        itemsize = 4\r\n\r\n        ctx = cuda.cudadrv.driver.driver.get_context()\r\n        mp = cuda.cudadrv.driver.MemoryPointer(ctx, ctypes.c_ulong(t.data_ptr()), t.numel()*itemsize)\r\n        return cuda.cudadrv.devicearray.DeviceNDArray(t.numel(), itemsize, numpy_dtype(), gpu_data=mp, stream=torch.cuda.current_stream().cuda_stream)\r\n\r\n    numba_param = get_devicendarray(param)\r\n    numba_grad = get_devicendarray(grad)\r\n    numba_exp_avg = get_devicendarray(exp_avg)\r\n    numba_exp_avg_sq = get_devicendarray(exp_avg_sq)\r\n    threadsperblock = 32\r\n    blockspergrid = (param.numel() + (threadsperblock - 1)) // threadsperblock\r\n    kernel_numba_cuda[blockspergrid, threadsperblock](numba_param, numba_grad, numba_exp_avg, numba_exp_avg_sq, beta1, beta2, step_size, eps)\r\n\r\nkernels = [(\"native \", kernel_native), (\"numba  \", kernel_numba)]\r\nshape = (4194304*64,)\r\nparam, grad, exp_avg, exp_avg_sq = [torch.rand(shape).to(\"cuda\") for x in range(4)]\r\n[Truncated]\n        res = func(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps)\r\n        if res is not None:\r\n            param[:], _, exp_avg[:], exp_avg_sq[:] = res\r\n        torch.cuda.synchronize()\r\n        t2 = time.time()\r\n        print(\"%s: \" % name, t2-t1)\r\n```\r\nThe results are:\r\n```\r\nnative :  0.0323\r\nnumba  :  0.0147\r\nTScript1:  0.0369\r\nTScript2:  0.0352\r\nTScript3:  0.0280\r\n```\r\nTScript3 is now better than native but still far from numba. TScript3 is now generating a single CUDA kernel but the back-copying of the result now dominates the timing. If I remove the line `param[:], _, exp_avg[:], exp_avg_sq[:] = res` the performance is almost on pair with numba:\r\n```\r\nTScript3 (no back-copy):  0.01664\r\n```\r\nThe problem is I really do need to update the values :/\n<issue_comment>username_5: cc @mruberry"}
{"repo": "pytorch/pytorch", "issue_id": 475728990, "issue_number": 23655, "timestamp": "2019-08-01 14:55:11+00:00", "text": "@username_0 can you try removing all the in-place operations in your scripted version?\r\nIIRC in-place ops block fusion.\r\n\r\nI was btw able to fuse most of sparse adam by removing those in-place ops, see https://github.com/pytorch/pytorch/pull/23522#issuecomment-516952462", "context": "<issue_start><issue_comment>Title: TorchScript Poor CUDA Performance \nusername_0: The performance of a TorchScript that handles the computation in the [Adam Optimizer](https://github.com/pytorch/pytorch/blob/master/torch/optim/adam.py) is worse than using the native PyTorch operations on CUDA. \r\n\r\n### Background\r\nI am trying to improve the GPU performance of the [Adam Optimizer](https://github.com/pytorch/pytorch/blob/master/torch/optim/adam.py) since it is the bottleneck in one of our workflows. \r\n\r\n### Results\r\nI have written a standalone script that compares PyTorch native tensor operations, TorchScript, and Numba (three runs each):\r\n```\r\nnative :  0.0506\r\nnative :  0.0322\r\nnative :  0.0322\r\nTScript:  0.1917\r\nTScript:  0.0442\r\nTScript:  0.0442\r\nnumba  :  0.1638\r\nnumba  :  0.0145\r\nnumba  :  0.0143\r\n```\r\nI suspect the problem is that TorchScript does not fuse all operations into a single CUDA. It generates two CUDA kernels (see the `PYTORCH_FUSION_DEBUG` output below)\r\n\r\n### Standalone Script\r\n```python\r\nimport math\r\nimport torch\r\nimport time\r\nimport random\r\nimport sys\r\n\r\ndef kernel_native(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n    exp_avg.mul_(beta1).add_(1 - beta1, grad)\r\n    exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\r\n    denom = exp_avg_sq.sqrt().add_(eps)\r\n    param.addcdiv_(-step_size, exp_avg, denom)   \r\n\r\n@torch.jit.script\r\ndef kernel_jit_script(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n    exp_avg.mul_(beta1)\r\n    exp_avg.add_((1 - beta1) + grad)\r\n    exp_avg_sq.mul_(beta2)\r\n    exp_avg_sq.add_((1 - beta2)*(grad*grad))\r\n    denom = exp_avg_sq.sqrt().add_(eps)\r\n    param += -step_size * (exp_avg / denom)\r\n\r\nfrom numba import cuda\r\n@cuda.jit\r\ndef kernel_numba_cuda(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    i = cuda.grid(1)\r\n    if i > grad.size:\r\n        return\r\n\r\n    exp_avg[i] = exp_avg[i] * beta1 + (1 - beta1) + grad[i]\r\n    exp_avg_sq[i] = exp_avg_sq[i] * beta2 + (1 - beta2) *grad[i]*grad[i]\r\n    denom = math.sqrt(exp_avg_sq[i]) + eps\r\n    param[i] += -step_size * (exp_avg[i] / denom)\r\n\r\ndef kernel_numba(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n\r\n    import ctypes\r\n    import numpy as np\r\n    def get_devicendarray(t):\r\n        assert t.type() == 'torch.cuda.FloatTensor'\r\n        numpy_dtype = np.float32\r\n        itemsize = 4\r\n\r\n        ctx = cuda.cudadrv.driver.driver.get_context()\r\n        mp = cuda.cudadrv.driver.MemoryPointer(ctx, ctypes.c_ulong(t.data_ptr()), t.numel()*itemsize)\r\n        return cuda.cudadrv.devicearray.DeviceNDArray(t.numel(), itemsize, numpy_dtype(), gpu_data=mp, stream=torch.cuda.current_stream().cuda_stream)\r\n\r\n    numba_param = get_devicendarray(param)\r\n    numba_grad = get_devicendarray(grad)\r\n    numba_exp_avg = get_devicendarray(exp_avg)\r\n    numba_exp_avg_sq = get_devicendarray(exp_avg_sq)\r\n    threadsperblock = 32\r\n    blockspergrid = (param.numel() + (threadsperblock - 1)) // threadsperblock\r\n    kernel_numba_cuda[blockspergrid, threadsperblock](numba_param, numba_grad, numba_exp_avg, numba_exp_avg_sq, beta1, beta2, step_size, eps)\r\n\r\nkernels = [(\"native \", kernel_native), (\"TScript\", kernel_jit_script), (\"numba  \", kernel_numba)]\r\n[Truncated]\n      size_t t1_dimIndex0 = t1_linearIndex ;\r\n      t1_offset += t1_dimIndex0 ;\r\n      IndexType t3_offset = 0;\r\n      IndexType t3_linearIndex = linearIndex;\r\n      \r\n      //printf(\"tensor t3 sizes[0] = %d, strides[0] = %d\\n\", t3.sizes[0],t3.strides[0]);\r\n      size_t t3_dimIndex0 = t3_linearIndex ;\r\n      t3_offset += t3_dimIndex0 ;\r\n      \r\n      // calculate the results\r\n      float n0 = __ldg(&t0.data[t0_offset]);\r\n      float n1 = __ldg(&t1.data[t1_offset]);\r\n      double n2 = s2;\r\n      float n3 = n0 / n1;\r\n      float n4 = n3 * ((float) n2);\r\n      t3.data[t3_offset] = n4;\r\n      \r\n    }\r\n}\r\n```\n<issue_comment>username_1: @username_0 can you try removing all the in-place operations in your scripted version?\r\nIIRC in-place ops block fusion.\r\n\r\nI was btw able to fuse most of sparse adam by removing those in-place ops, see https://github.com/pytorch/pytorch/pull/23522#issuecomment-516952462\n<issue_comment>username_2: cc: @username_3 and also relevant WIP PR: [JIT version of Adagrad](https://github.com/pytorch/pytorch/pull/23640)\n<issue_comment>username_3: Yeah the CUDA fusion does not support fusing in-place operations yet, Can you try the suggestion by @username_1 to see if you get a better perf?\n<issue_comment>username_3: @username_0 Also, it seems like you are only timing it in 3 iterations, and the perf numbers seems including the compilation time. The first time JIT/TorchScript sees an input it will specialize and do optimizations against that input, so when comparing performance please allow a compilation time and runtime optimization warmup for the input.\n<issue_comment>username_4: @username_3 That made a big difference to my traced module too. Any other tricks like this? It slows down the non-JIT version, however.\n<issue_comment>username_3: @username_4 That's pretty much all, let me summarize:\r\n\r\n1. JIT fusion does not support in-place operation fusion and reduction operation yet.\r\n2. we did the fusion code generation at the first time we see an input that the shape we haven't seen before (like if the number of dimensions are different), this will help us generate faster cuda kernels. \r\n\r\nSo the TorchScript compilation time will be gone after compile and the first run :)\n<issue_comment>username_0: Thanks for the feedback!\r\n\r\nI have removed all in-place operations and added three kernel implementations:\r\n```python\r\n\r\nimport math\r\nimport torch\r\nimport time\r\nimport random\r\nimport sys\r\n\r\ndef kernel_native(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n    exp_avg.mul_(beta1).add_(1 - beta1, grad)\r\n    exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\r\n    denom = exp_avg_sq.sqrt().add_(eps)\r\n    param.addcdiv_(-step_size, exp_avg, denom)   \r\n\r\n@torch.jit.script\r\ndef kernel_jit_script1(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n    exp_avg *= beta1\r\n    exp_avg += (1 - beta1) + grad\r\n    exp_avg_sq *= beta2\r\n    exp_avg_sq += (1 - beta2)*(grad*grad)\r\n    param += -step_size * (exp_avg / exp_avg_sq.sqrt() + eps)\r\n\r\n@torch.jit.script\r\ndef kernel_jit_script2(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n    exp_avg[:] = exp_avg * beta1 + (1 - beta1) + grad\r\n    exp_avg_sq[:] = exp_avg_sq * beta2 + (1 - beta2)*(grad*grad)\r\n    param[:] = param -step_size * (exp_avg / exp_avg_sq.sqrt() + eps)\r\n\r\n@torch.jit.script\r\ndef kernel_jit_script3(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> Tuple[Tensor, Tensor, Tensor, Tensor]\r\n    exp_avg = exp_avg * beta1 + (1 - beta1) + grad\r\n    exp_avg_sq = exp_avg_sq * beta2 + (1 - beta2)*(grad*grad)\r\n    param = param -step_size * (exp_avg / exp_avg_sq.sqrt() + eps)\r\n    return (param, grad, exp_avg, exp_avg_sq)\r\n\r\n\r\nfrom numba import cuda\r\n@cuda.jit\r\ndef kernel_numba_cuda(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    i = cuda.grid(1)\r\n    if i > grad.size:\r\n        return\r\n\r\n    exp_avg[i] = exp_avg[i] * beta1 + (1 - beta1) + grad[i]\r\n    exp_avg_sq[i] = exp_avg_sq[i] * beta2 + (1 - beta2) *grad[i]*grad[i]\r\n    denom = math.sqrt(exp_avg_sq[i]) + eps\r\n    param[i] += -step_size * (exp_avg[i] / denom)\r\n\r\ndef kernel_numba(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n\r\n    import ctypes\r\n    import numpy as np\r\n    def get_devicendarray(t):\r\n        assert t.type() == 'torch.cuda.FloatTensor'\r\n        numpy_dtype = np.float32\r\n        itemsize = 4\r\n\r\n        ctx = cuda.cudadrv.driver.driver.get_context()\r\n        mp = cuda.cudadrv.driver.MemoryPointer(ctx, ctypes.c_ulong(t.data_ptr()), t.numel()*itemsize)\r\n        return cuda.cudadrv.devicearray.DeviceNDArray(t.numel(), itemsize, numpy_dtype(), gpu_data=mp, stream=torch.cuda.current_stream().cuda_stream)\r\n\r\n    numba_param = get_devicendarray(param)\r\n    numba_grad = get_devicendarray(grad)\r\n    numba_exp_avg = get_devicendarray(exp_avg)\r\n    numba_exp_avg_sq = get_devicendarray(exp_avg_sq)\r\n    threadsperblock = 32\r\n    blockspergrid = (param.numel() + (threadsperblock - 1)) // threadsperblock\r\n    kernel_numba_cuda[blockspergrid, threadsperblock](numba_param, numba_grad, numba_exp_avg, numba_exp_avg_sq, beta1, beta2, step_size, eps)\r\n\r\nkernels = [(\"native \", kernel_native), (\"numba  \", kernel_numba)]\r\nshape = (4194304*64,)\r\nparam, grad, exp_avg, exp_avg_sq = [torch.rand(shape).to(\"cuda\") for x in range(4)]\r\n[Truncated]\n        res = func(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps)\r\n        if res is not None:\r\n            param[:], _, exp_avg[:], exp_avg_sq[:] = res\r\n        torch.cuda.synchronize()\r\n        t2 = time.time()\r\n        print(\"%s: \" % name, t2-t1)\r\n```\r\nThe results are:\r\n```\r\nnative :  0.0323\r\nnumba  :  0.0147\r\nTScript1:  0.0369\r\nTScript2:  0.0352\r\nTScript3:  0.0280\r\n```\r\nTScript3 is now better than native but still far from numba. TScript3 is now generating a single CUDA kernel but the back-copying of the result now dominates the timing. If I remove the line `param[:], _, exp_avg[:], exp_avg_sq[:] = res` the performance is almost on pair with numba:\r\n```\r\nTScript3 (no back-copy):  0.01664\r\n```\r\nThe problem is I really do need to update the values :/\n<issue_comment>username_5: cc @mruberry"}
{"repo": "pytorch/pytorch", "issue_id": 475728990, "issue_number": 23655, "timestamp": "2019-08-01 14:57:00+00:00", "text": "cc: @username_3 and also relevant WIP PR: [JIT version of Adagrad](https://github.com/pytorch/pytorch/pull/23640)", "context": "<issue_start><issue_comment>Title: TorchScript Poor CUDA Performance \nusername_0: The performance of a TorchScript that handles the computation in the [Adam Optimizer](https://github.com/pytorch/pytorch/blob/master/torch/optim/adam.py) is worse than using the native PyTorch operations on CUDA. \r\n\r\n### Background\r\nI am trying to improve the GPU performance of the [Adam Optimizer](https://github.com/pytorch/pytorch/blob/master/torch/optim/adam.py) since it is the bottleneck in one of our workflows. \r\n\r\n### Results\r\nI have written a standalone script that compares PyTorch native tensor operations, TorchScript, and Numba (three runs each):\r\n```\r\nnative :  0.0506\r\nnative :  0.0322\r\nnative :  0.0322\r\nTScript:  0.1917\r\nTScript:  0.0442\r\nTScript:  0.0442\r\nnumba  :  0.1638\r\nnumba  :  0.0145\r\nnumba  :  0.0143\r\n```\r\nI suspect the problem is that TorchScript does not fuse all operations into a single CUDA. It generates two CUDA kernels (see the `PYTORCH_FUSION_DEBUG` output below)\r\n\r\n### Standalone Script\r\n```python\r\nimport math\r\nimport torch\r\nimport time\r\nimport random\r\nimport sys\r\n\r\ndef kernel_native(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n    exp_avg.mul_(beta1).add_(1 - beta1, grad)\r\n    exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\r\n    denom = exp_avg_sq.sqrt().add_(eps)\r\n    param.addcdiv_(-step_size, exp_avg, denom)   \r\n\r\n@torch.jit.script\r\ndef kernel_jit_script(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n    exp_avg.mul_(beta1)\r\n    exp_avg.add_((1 - beta1) + grad)\r\n    exp_avg_sq.mul_(beta2)\r\n    exp_avg_sq.add_((1 - beta2)*(grad*grad))\r\n    denom = exp_avg_sq.sqrt().add_(eps)\r\n    param += -step_size * (exp_avg / denom)\r\n\r\nfrom numba import cuda\r\n@cuda.jit\r\ndef kernel_numba_cuda(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    i = cuda.grid(1)\r\n    if i > grad.size:\r\n        return\r\n\r\n    exp_avg[i] = exp_avg[i] * beta1 + (1 - beta1) + grad[i]\r\n    exp_avg_sq[i] = exp_avg_sq[i] * beta2 + (1 - beta2) *grad[i]*grad[i]\r\n    denom = math.sqrt(exp_avg_sq[i]) + eps\r\n    param[i] += -step_size * (exp_avg[i] / denom)\r\n\r\ndef kernel_numba(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n\r\n    import ctypes\r\n    import numpy as np\r\n    def get_devicendarray(t):\r\n        assert t.type() == 'torch.cuda.FloatTensor'\r\n        numpy_dtype = np.float32\r\n        itemsize = 4\r\n\r\n        ctx = cuda.cudadrv.driver.driver.get_context()\r\n        mp = cuda.cudadrv.driver.MemoryPointer(ctx, ctypes.c_ulong(t.data_ptr()), t.numel()*itemsize)\r\n        return cuda.cudadrv.devicearray.DeviceNDArray(t.numel(), itemsize, numpy_dtype(), gpu_data=mp, stream=torch.cuda.current_stream().cuda_stream)\r\n\r\n    numba_param = get_devicendarray(param)\r\n    numba_grad = get_devicendarray(grad)\r\n    numba_exp_avg = get_devicendarray(exp_avg)\r\n    numba_exp_avg_sq = get_devicendarray(exp_avg_sq)\r\n    threadsperblock = 32\r\n    blockspergrid = (param.numel() + (threadsperblock - 1)) // threadsperblock\r\n    kernel_numba_cuda[blockspergrid, threadsperblock](numba_param, numba_grad, numba_exp_avg, numba_exp_avg_sq, beta1, beta2, step_size, eps)\r\n\r\nkernels = [(\"native \", kernel_native), (\"TScript\", kernel_jit_script), (\"numba  \", kernel_numba)]\r\n[Truncated]\n      size_t t1_dimIndex0 = t1_linearIndex ;\r\n      t1_offset += t1_dimIndex0 ;\r\n      IndexType t3_offset = 0;\r\n      IndexType t3_linearIndex = linearIndex;\r\n      \r\n      //printf(\"tensor t3 sizes[0] = %d, strides[0] = %d\\n\", t3.sizes[0],t3.strides[0]);\r\n      size_t t3_dimIndex0 = t3_linearIndex ;\r\n      t3_offset += t3_dimIndex0 ;\r\n      \r\n      // calculate the results\r\n      float n0 = __ldg(&t0.data[t0_offset]);\r\n      float n1 = __ldg(&t1.data[t1_offset]);\r\n      double n2 = s2;\r\n      float n3 = n0 / n1;\r\n      float n4 = n3 * ((float) n2);\r\n      t3.data[t3_offset] = n4;\r\n      \r\n    }\r\n}\r\n```\n<issue_comment>username_1: @username_0 can you try removing all the in-place operations in your scripted version?\r\nIIRC in-place ops block fusion.\r\n\r\nI was btw able to fuse most of sparse adam by removing those in-place ops, see https://github.com/pytorch/pytorch/pull/23522#issuecomment-516952462\n<issue_comment>username_2: cc: @username_3 and also relevant WIP PR: [JIT version of Adagrad](https://github.com/pytorch/pytorch/pull/23640)\n<issue_comment>username_3: Yeah the CUDA fusion does not support fusing in-place operations yet, Can you try the suggestion by @username_1 to see if you get a better perf?\n<issue_comment>username_3: @username_0 Also, it seems like you are only timing it in 3 iterations, and the perf numbers seems including the compilation time. The first time JIT/TorchScript sees an input it will specialize and do optimizations against that input, so when comparing performance please allow a compilation time and runtime optimization warmup for the input.\n<issue_comment>username_4: @username_3 That made a big difference to my traced module too. Any other tricks like this? It slows down the non-JIT version, however.\n<issue_comment>username_3: @username_4 That's pretty much all, let me summarize:\r\n\r\n1. JIT fusion does not support in-place operation fusion and reduction operation yet.\r\n2. we did the fusion code generation at the first time we see an input that the shape we haven't seen before (like if the number of dimensions are different), this will help us generate faster cuda kernels. \r\n\r\nSo the TorchScript compilation time will be gone after compile and the first run :)\n<issue_comment>username_0: Thanks for the feedback!\r\n\r\nI have removed all in-place operations and added three kernel implementations:\r\n```python\r\n\r\nimport math\r\nimport torch\r\nimport time\r\nimport random\r\nimport sys\r\n\r\ndef kernel_native(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n    exp_avg.mul_(beta1).add_(1 - beta1, grad)\r\n    exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\r\n    denom = exp_avg_sq.sqrt().add_(eps)\r\n    param.addcdiv_(-step_size, exp_avg, denom)   \r\n\r\n@torch.jit.script\r\ndef kernel_jit_script1(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n    exp_avg *= beta1\r\n    exp_avg += (1 - beta1) + grad\r\n    exp_avg_sq *= beta2\r\n    exp_avg_sq += (1 - beta2)*(grad*grad)\r\n    param += -step_size * (exp_avg / exp_avg_sq.sqrt() + eps)\r\n\r\n@torch.jit.script\r\ndef kernel_jit_script2(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n    exp_avg[:] = exp_avg * beta1 + (1 - beta1) + grad\r\n    exp_avg_sq[:] = exp_avg_sq * beta2 + (1 - beta2)*(grad*grad)\r\n    param[:] = param -step_size * (exp_avg / exp_avg_sq.sqrt() + eps)\r\n\r\n@torch.jit.script\r\ndef kernel_jit_script3(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> Tuple[Tensor, Tensor, Tensor, Tensor]\r\n    exp_avg = exp_avg * beta1 + (1 - beta1) + grad\r\n    exp_avg_sq = exp_avg_sq * beta2 + (1 - beta2)*(grad*grad)\r\n    param = param -step_size * (exp_avg / exp_avg_sq.sqrt() + eps)\r\n    return (param, grad, exp_avg, exp_avg_sq)\r\n\r\n\r\nfrom numba import cuda\r\n@cuda.jit\r\ndef kernel_numba_cuda(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    i = cuda.grid(1)\r\n    if i > grad.size:\r\n        return\r\n\r\n    exp_avg[i] = exp_avg[i] * beta1 + (1 - beta1) + grad[i]\r\n    exp_avg_sq[i] = exp_avg_sq[i] * beta2 + (1 - beta2) *grad[i]*grad[i]\r\n    denom = math.sqrt(exp_avg_sq[i]) + eps\r\n    param[i] += -step_size * (exp_avg[i] / denom)\r\n\r\ndef kernel_numba(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n\r\n    import ctypes\r\n    import numpy as np\r\n    def get_devicendarray(t):\r\n        assert t.type() == 'torch.cuda.FloatTensor'\r\n        numpy_dtype = np.float32\r\n        itemsize = 4\r\n\r\n        ctx = cuda.cudadrv.driver.driver.get_context()\r\n        mp = cuda.cudadrv.driver.MemoryPointer(ctx, ctypes.c_ulong(t.data_ptr()), t.numel()*itemsize)\r\n        return cuda.cudadrv.devicearray.DeviceNDArray(t.numel(), itemsize, numpy_dtype(), gpu_data=mp, stream=torch.cuda.current_stream().cuda_stream)\r\n\r\n    numba_param = get_devicendarray(param)\r\n    numba_grad = get_devicendarray(grad)\r\n    numba_exp_avg = get_devicendarray(exp_avg)\r\n    numba_exp_avg_sq = get_devicendarray(exp_avg_sq)\r\n    threadsperblock = 32\r\n    blockspergrid = (param.numel() + (threadsperblock - 1)) // threadsperblock\r\n    kernel_numba_cuda[blockspergrid, threadsperblock](numba_param, numba_grad, numba_exp_avg, numba_exp_avg_sq, beta1, beta2, step_size, eps)\r\n\r\nkernels = [(\"native \", kernel_native), (\"numba  \", kernel_numba)]\r\nshape = (4194304*64,)\r\nparam, grad, exp_avg, exp_avg_sq = [torch.rand(shape).to(\"cuda\") for x in range(4)]\r\n[Truncated]\n        res = func(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps)\r\n        if res is not None:\r\n            param[:], _, exp_avg[:], exp_avg_sq[:] = res\r\n        torch.cuda.synchronize()\r\n        t2 = time.time()\r\n        print(\"%s: \" % name, t2-t1)\r\n```\r\nThe results are:\r\n```\r\nnative :  0.0323\r\nnumba  :  0.0147\r\nTScript1:  0.0369\r\nTScript2:  0.0352\r\nTScript3:  0.0280\r\n```\r\nTScript3 is now better than native but still far from numba. TScript3 is now generating a single CUDA kernel but the back-copying of the result now dominates the timing. If I remove the line `param[:], _, exp_avg[:], exp_avg_sq[:] = res` the performance is almost on pair with numba:\r\n```\r\nTScript3 (no back-copy):  0.01664\r\n```\r\nThe problem is I really do need to update the values :/\n<issue_comment>username_5: cc @mruberry"}
{"repo": "pytorch/pytorch", "issue_id": 475728990, "issue_number": 23655, "timestamp": "2019-08-01 17:37:52+00:00", "text": "Yeah the CUDA fusion does not support fusing in-place operations yet, Can you try the suggestion by @username_1 to see if you get a better perf?", "context": "<issue_start><issue_comment>Title: TorchScript Poor CUDA Performance \nusername_0: The performance of a TorchScript that handles the computation in the [Adam Optimizer](https://github.com/pytorch/pytorch/blob/master/torch/optim/adam.py) is worse than using the native PyTorch operations on CUDA. \r\n\r\n### Background\r\nI am trying to improve the GPU performance of the [Adam Optimizer](https://github.com/pytorch/pytorch/blob/master/torch/optim/adam.py) since it is the bottleneck in one of our workflows. \r\n\r\n### Results\r\nI have written a standalone script that compares PyTorch native tensor operations, TorchScript, and Numba (three runs each):\r\n```\r\nnative :  0.0506\r\nnative :  0.0322\r\nnative :  0.0322\r\nTScript:  0.1917\r\nTScript:  0.0442\r\nTScript:  0.0442\r\nnumba  :  0.1638\r\nnumba  :  0.0145\r\nnumba  :  0.0143\r\n```\r\nI suspect the problem is that TorchScript does not fuse all operations into a single CUDA. It generates two CUDA kernels (see the `PYTORCH_FUSION_DEBUG` output below)\r\n\r\n### Standalone Script\r\n```python\r\nimport math\r\nimport torch\r\nimport time\r\nimport random\r\nimport sys\r\n\r\ndef kernel_native(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n    exp_avg.mul_(beta1).add_(1 - beta1, grad)\r\n    exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\r\n    denom = exp_avg_sq.sqrt().add_(eps)\r\n    param.addcdiv_(-step_size, exp_avg, denom)   \r\n\r\n@torch.jit.script\r\ndef kernel_jit_script(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n    exp_avg.mul_(beta1)\r\n    exp_avg.add_((1 - beta1) + grad)\r\n    exp_avg_sq.mul_(beta2)\r\n    exp_avg_sq.add_((1 - beta2)*(grad*grad))\r\n    denom = exp_avg_sq.sqrt().add_(eps)\r\n    param += -step_size * (exp_avg / denom)\r\n\r\nfrom numba import cuda\r\n@cuda.jit\r\ndef kernel_numba_cuda(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    i = cuda.grid(1)\r\n    if i > grad.size:\r\n        return\r\n\r\n    exp_avg[i] = exp_avg[i] * beta1 + (1 - beta1) + grad[i]\r\n    exp_avg_sq[i] = exp_avg_sq[i] * beta2 + (1 - beta2) *grad[i]*grad[i]\r\n    denom = math.sqrt(exp_avg_sq[i]) + eps\r\n    param[i] += -step_size * (exp_avg[i] / denom)\r\n\r\ndef kernel_numba(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n\r\n    import ctypes\r\n    import numpy as np\r\n    def get_devicendarray(t):\r\n        assert t.type() == 'torch.cuda.FloatTensor'\r\n        numpy_dtype = np.float32\r\n        itemsize = 4\r\n\r\n        ctx = cuda.cudadrv.driver.driver.get_context()\r\n        mp = cuda.cudadrv.driver.MemoryPointer(ctx, ctypes.c_ulong(t.data_ptr()), t.numel()*itemsize)\r\n        return cuda.cudadrv.devicearray.DeviceNDArray(t.numel(), itemsize, numpy_dtype(), gpu_data=mp, stream=torch.cuda.current_stream().cuda_stream)\r\n\r\n    numba_param = get_devicendarray(param)\r\n    numba_grad = get_devicendarray(grad)\r\n    numba_exp_avg = get_devicendarray(exp_avg)\r\n    numba_exp_avg_sq = get_devicendarray(exp_avg_sq)\r\n    threadsperblock = 32\r\n    blockspergrid = (param.numel() + (threadsperblock - 1)) // threadsperblock\r\n    kernel_numba_cuda[blockspergrid, threadsperblock](numba_param, numba_grad, numba_exp_avg, numba_exp_avg_sq, beta1, beta2, step_size, eps)\r\n\r\nkernels = [(\"native \", kernel_native), (\"TScript\", kernel_jit_script), (\"numba  \", kernel_numba)]\r\n[Truncated]\n      size_t t1_dimIndex0 = t1_linearIndex ;\r\n      t1_offset += t1_dimIndex0 ;\r\n      IndexType t3_offset = 0;\r\n      IndexType t3_linearIndex = linearIndex;\r\n      \r\n      //printf(\"tensor t3 sizes[0] = %d, strides[0] = %d\\n\", t3.sizes[0],t3.strides[0]);\r\n      size_t t3_dimIndex0 = t3_linearIndex ;\r\n      t3_offset += t3_dimIndex0 ;\r\n      \r\n      // calculate the results\r\n      float n0 = __ldg(&t0.data[t0_offset]);\r\n      float n1 = __ldg(&t1.data[t1_offset]);\r\n      double n2 = s2;\r\n      float n3 = n0 / n1;\r\n      float n4 = n3 * ((float) n2);\r\n      t3.data[t3_offset] = n4;\r\n      \r\n    }\r\n}\r\n```\n<issue_comment>username_1: @username_0 can you try removing all the in-place operations in your scripted version?\r\nIIRC in-place ops block fusion.\r\n\r\nI was btw able to fuse most of sparse adam by removing those in-place ops, see https://github.com/pytorch/pytorch/pull/23522#issuecomment-516952462\n<issue_comment>username_2: cc: @username_3 and also relevant WIP PR: [JIT version of Adagrad](https://github.com/pytorch/pytorch/pull/23640)\n<issue_comment>username_3: Yeah the CUDA fusion does not support fusing in-place operations yet, Can you try the suggestion by @username_1 to see if you get a better perf?\n<issue_comment>username_3: @username_0 Also, it seems like you are only timing it in 3 iterations, and the perf numbers seems including the compilation time. The first time JIT/TorchScript sees an input it will specialize and do optimizations against that input, so when comparing performance please allow a compilation time and runtime optimization warmup for the input.\n<issue_comment>username_4: @username_3 That made a big difference to my traced module too. Any other tricks like this? It slows down the non-JIT version, however.\n<issue_comment>username_3: @username_4 That's pretty much all, let me summarize:\r\n\r\n1. JIT fusion does not support in-place operation fusion and reduction operation yet.\r\n2. we did the fusion code generation at the first time we see an input that the shape we haven't seen before (like if the number of dimensions are different), this will help us generate faster cuda kernels. \r\n\r\nSo the TorchScript compilation time will be gone after compile and the first run :)\n<issue_comment>username_0: Thanks for the feedback!\r\n\r\nI have removed all in-place operations and added three kernel implementations:\r\n```python\r\n\r\nimport math\r\nimport torch\r\nimport time\r\nimport random\r\nimport sys\r\n\r\ndef kernel_native(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n    exp_avg.mul_(beta1).add_(1 - beta1, grad)\r\n    exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\r\n    denom = exp_avg_sq.sqrt().add_(eps)\r\n    param.addcdiv_(-step_size, exp_avg, denom)   \r\n\r\n@torch.jit.script\r\ndef kernel_jit_script1(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n    exp_avg *= beta1\r\n    exp_avg += (1 - beta1) + grad\r\n    exp_avg_sq *= beta2\r\n    exp_avg_sq += (1 - beta2)*(grad*grad)\r\n    param += -step_size * (exp_avg / exp_avg_sq.sqrt() + eps)\r\n\r\n@torch.jit.script\r\ndef kernel_jit_script2(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n    exp_avg[:] = exp_avg * beta1 + (1 - beta1) + grad\r\n    exp_avg_sq[:] = exp_avg_sq * beta2 + (1 - beta2)*(grad*grad)\r\n    param[:] = param -step_size * (exp_avg / exp_avg_sq.sqrt() + eps)\r\n\r\n@torch.jit.script\r\ndef kernel_jit_script3(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> Tuple[Tensor, Tensor, Tensor, Tensor]\r\n    exp_avg = exp_avg * beta1 + (1 - beta1) + grad\r\n    exp_avg_sq = exp_avg_sq * beta2 + (1 - beta2)*(grad*grad)\r\n    param = param -step_size * (exp_avg / exp_avg_sq.sqrt() + eps)\r\n    return (param, grad, exp_avg, exp_avg_sq)\r\n\r\n\r\nfrom numba import cuda\r\n@cuda.jit\r\ndef kernel_numba_cuda(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    i = cuda.grid(1)\r\n    if i > grad.size:\r\n        return\r\n\r\n    exp_avg[i] = exp_avg[i] * beta1 + (1 - beta1) + grad[i]\r\n    exp_avg_sq[i] = exp_avg_sq[i] * beta2 + (1 - beta2) *grad[i]*grad[i]\r\n    denom = math.sqrt(exp_avg_sq[i]) + eps\r\n    param[i] += -step_size * (exp_avg[i] / denom)\r\n\r\ndef kernel_numba(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n\r\n    import ctypes\r\n    import numpy as np\r\n    def get_devicendarray(t):\r\n        assert t.type() == 'torch.cuda.FloatTensor'\r\n        numpy_dtype = np.float32\r\n        itemsize = 4\r\n\r\n        ctx = cuda.cudadrv.driver.driver.get_context()\r\n        mp = cuda.cudadrv.driver.MemoryPointer(ctx, ctypes.c_ulong(t.data_ptr()), t.numel()*itemsize)\r\n        return cuda.cudadrv.devicearray.DeviceNDArray(t.numel(), itemsize, numpy_dtype(), gpu_data=mp, stream=torch.cuda.current_stream().cuda_stream)\r\n\r\n    numba_param = get_devicendarray(param)\r\n    numba_grad = get_devicendarray(grad)\r\n    numba_exp_avg = get_devicendarray(exp_avg)\r\n    numba_exp_avg_sq = get_devicendarray(exp_avg_sq)\r\n    threadsperblock = 32\r\n    blockspergrid = (param.numel() + (threadsperblock - 1)) // threadsperblock\r\n    kernel_numba_cuda[blockspergrid, threadsperblock](numba_param, numba_grad, numba_exp_avg, numba_exp_avg_sq, beta1, beta2, step_size, eps)\r\n\r\nkernels = [(\"native \", kernel_native), (\"numba  \", kernel_numba)]\r\nshape = (4194304*64,)\r\nparam, grad, exp_avg, exp_avg_sq = [torch.rand(shape).to(\"cuda\") for x in range(4)]\r\n[Truncated]\n        res = func(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps)\r\n        if res is not None:\r\n            param[:], _, exp_avg[:], exp_avg_sq[:] = res\r\n        torch.cuda.synchronize()\r\n        t2 = time.time()\r\n        print(\"%s: \" % name, t2-t1)\r\n```\r\nThe results are:\r\n```\r\nnative :  0.0323\r\nnumba  :  0.0147\r\nTScript1:  0.0369\r\nTScript2:  0.0352\r\nTScript3:  0.0280\r\n```\r\nTScript3 is now better than native but still far from numba. TScript3 is now generating a single CUDA kernel but the back-copying of the result now dominates the timing. If I remove the line `param[:], _, exp_avg[:], exp_avg_sq[:] = res` the performance is almost on pair with numba:\r\n```\r\nTScript3 (no back-copy):  0.01664\r\n```\r\nThe problem is I really do need to update the values :/\n<issue_comment>username_5: cc @mruberry"}
{"repo": "pytorch/pytorch", "issue_id": 475728990, "issue_number": 23655, "timestamp": "2019-08-01 22:20:09+00:00", "text": "@username_0 Also, it seems like you are only timing it in 3 iterations, and the perf numbers seems including the compilation time. The first time JIT/TorchScript sees an input it will specialize and do optimizations against that input, so when comparing performance please allow a compilation time and runtime optimization warmup for the input.", "context": "<issue_start><issue_comment>Title: TorchScript Poor CUDA Performance \nusername_0: The performance of a TorchScript that handles the computation in the [Adam Optimizer](https://github.com/pytorch/pytorch/blob/master/torch/optim/adam.py) is worse than using the native PyTorch operations on CUDA. \r\n\r\n### Background\r\nI am trying to improve the GPU performance of the [Adam Optimizer](https://github.com/pytorch/pytorch/blob/master/torch/optim/adam.py) since it is the bottleneck in one of our workflows. \r\n\r\n### Results\r\nI have written a standalone script that compares PyTorch native tensor operations, TorchScript, and Numba (three runs each):\r\n```\r\nnative :  0.0506\r\nnative :  0.0322\r\nnative :  0.0322\r\nTScript:  0.1917\r\nTScript:  0.0442\r\nTScript:  0.0442\r\nnumba  :  0.1638\r\nnumba  :  0.0145\r\nnumba  :  0.0143\r\n```\r\nI suspect the problem is that TorchScript does not fuse all operations into a single CUDA. It generates two CUDA kernels (see the `PYTORCH_FUSION_DEBUG` output below)\r\n\r\n### Standalone Script\r\n```python\r\nimport math\r\nimport torch\r\nimport time\r\nimport random\r\nimport sys\r\n\r\ndef kernel_native(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n    exp_avg.mul_(beta1).add_(1 - beta1, grad)\r\n    exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\r\n    denom = exp_avg_sq.sqrt().add_(eps)\r\n    param.addcdiv_(-step_size, exp_avg, denom)   \r\n\r\n@torch.jit.script\r\ndef kernel_jit_script(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n    exp_avg.mul_(beta1)\r\n    exp_avg.add_((1 - beta1) + grad)\r\n    exp_avg_sq.mul_(beta2)\r\n    exp_avg_sq.add_((1 - beta2)*(grad*grad))\r\n    denom = exp_avg_sq.sqrt().add_(eps)\r\n    param += -step_size * (exp_avg / denom)\r\n\r\nfrom numba import cuda\r\n@cuda.jit\r\ndef kernel_numba_cuda(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    i = cuda.grid(1)\r\n    if i > grad.size:\r\n        return\r\n\r\n    exp_avg[i] = exp_avg[i] * beta1 + (1 - beta1) + grad[i]\r\n    exp_avg_sq[i] = exp_avg_sq[i] * beta2 + (1 - beta2) *grad[i]*grad[i]\r\n    denom = math.sqrt(exp_avg_sq[i]) + eps\r\n    param[i] += -step_size * (exp_avg[i] / denom)\r\n\r\ndef kernel_numba(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n\r\n    import ctypes\r\n    import numpy as np\r\n    def get_devicendarray(t):\r\n        assert t.type() == 'torch.cuda.FloatTensor'\r\n        numpy_dtype = np.float32\r\n        itemsize = 4\r\n\r\n        ctx = cuda.cudadrv.driver.driver.get_context()\r\n        mp = cuda.cudadrv.driver.MemoryPointer(ctx, ctypes.c_ulong(t.data_ptr()), t.numel()*itemsize)\r\n        return cuda.cudadrv.devicearray.DeviceNDArray(t.numel(), itemsize, numpy_dtype(), gpu_data=mp, stream=torch.cuda.current_stream().cuda_stream)\r\n\r\n    numba_param = get_devicendarray(param)\r\n    numba_grad = get_devicendarray(grad)\r\n    numba_exp_avg = get_devicendarray(exp_avg)\r\n    numba_exp_avg_sq = get_devicendarray(exp_avg_sq)\r\n    threadsperblock = 32\r\n    blockspergrid = (param.numel() + (threadsperblock - 1)) // threadsperblock\r\n    kernel_numba_cuda[blockspergrid, threadsperblock](numba_param, numba_grad, numba_exp_avg, numba_exp_avg_sq, beta1, beta2, step_size, eps)\r\n\r\nkernels = [(\"native \", kernel_native), (\"TScript\", kernel_jit_script), (\"numba  \", kernel_numba)]\r\n[Truncated]\n      size_t t1_dimIndex0 = t1_linearIndex ;\r\n      t1_offset += t1_dimIndex0 ;\r\n      IndexType t3_offset = 0;\r\n      IndexType t3_linearIndex = linearIndex;\r\n      \r\n      //printf(\"tensor t3 sizes[0] = %d, strides[0] = %d\\n\", t3.sizes[0],t3.strides[0]);\r\n      size_t t3_dimIndex0 = t3_linearIndex ;\r\n      t3_offset += t3_dimIndex0 ;\r\n      \r\n      // calculate the results\r\n      float n0 = __ldg(&t0.data[t0_offset]);\r\n      float n1 = __ldg(&t1.data[t1_offset]);\r\n      double n2 = s2;\r\n      float n3 = n0 / n1;\r\n      float n4 = n3 * ((float) n2);\r\n      t3.data[t3_offset] = n4;\r\n      \r\n    }\r\n}\r\n```\n<issue_comment>username_1: @username_0 can you try removing all the in-place operations in your scripted version?\r\nIIRC in-place ops block fusion.\r\n\r\nI was btw able to fuse most of sparse adam by removing those in-place ops, see https://github.com/pytorch/pytorch/pull/23522#issuecomment-516952462\n<issue_comment>username_2: cc: @username_3 and also relevant WIP PR: [JIT version of Adagrad](https://github.com/pytorch/pytorch/pull/23640)\n<issue_comment>username_3: Yeah the CUDA fusion does not support fusing in-place operations yet, Can you try the suggestion by @username_1 to see if you get a better perf?\n<issue_comment>username_3: @username_0 Also, it seems like you are only timing it in 3 iterations, and the perf numbers seems including the compilation time. The first time JIT/TorchScript sees an input it will specialize and do optimizations against that input, so when comparing performance please allow a compilation time and runtime optimization warmup for the input.\n<issue_comment>username_4: @username_3 That made a big difference to my traced module too. Any other tricks like this? It slows down the non-JIT version, however.\n<issue_comment>username_3: @username_4 That's pretty much all, let me summarize:\r\n\r\n1. JIT fusion does not support in-place operation fusion and reduction operation yet.\r\n2. we did the fusion code generation at the first time we see an input that the shape we haven't seen before (like if the number of dimensions are different), this will help us generate faster cuda kernels. \r\n\r\nSo the TorchScript compilation time will be gone after compile and the first run :)\n<issue_comment>username_0: Thanks for the feedback!\r\n\r\nI have removed all in-place operations and added three kernel implementations:\r\n```python\r\n\r\nimport math\r\nimport torch\r\nimport time\r\nimport random\r\nimport sys\r\n\r\ndef kernel_native(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n    exp_avg.mul_(beta1).add_(1 - beta1, grad)\r\n    exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\r\n    denom = exp_avg_sq.sqrt().add_(eps)\r\n    param.addcdiv_(-step_size, exp_avg, denom)   \r\n\r\n@torch.jit.script\r\ndef kernel_jit_script1(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n    exp_avg *= beta1\r\n    exp_avg += (1 - beta1) + grad\r\n    exp_avg_sq *= beta2\r\n    exp_avg_sq += (1 - beta2)*(grad*grad)\r\n    param += -step_size * (exp_avg / exp_avg_sq.sqrt() + eps)\r\n\r\n@torch.jit.script\r\ndef kernel_jit_script2(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n    exp_avg[:] = exp_avg * beta1 + (1 - beta1) + grad\r\n    exp_avg_sq[:] = exp_avg_sq * beta2 + (1 - beta2)*(grad*grad)\r\n    param[:] = param -step_size * (exp_avg / exp_avg_sq.sqrt() + eps)\r\n\r\n@torch.jit.script\r\ndef kernel_jit_script3(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> Tuple[Tensor, Tensor, Tensor, Tensor]\r\n    exp_avg = exp_avg * beta1 + (1 - beta1) + grad\r\n    exp_avg_sq = exp_avg_sq * beta2 + (1 - beta2)*(grad*grad)\r\n    param = param -step_size * (exp_avg / exp_avg_sq.sqrt() + eps)\r\n    return (param, grad, exp_avg, exp_avg_sq)\r\n\r\n\r\nfrom numba import cuda\r\n@cuda.jit\r\ndef kernel_numba_cuda(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    i = cuda.grid(1)\r\n    if i > grad.size:\r\n        return\r\n\r\n    exp_avg[i] = exp_avg[i] * beta1 + (1 - beta1) + grad[i]\r\n    exp_avg_sq[i] = exp_avg_sq[i] * beta2 + (1 - beta2) *grad[i]*grad[i]\r\n    denom = math.sqrt(exp_avg_sq[i]) + eps\r\n    param[i] += -step_size * (exp_avg[i] / denom)\r\n\r\ndef kernel_numba(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n\r\n    import ctypes\r\n    import numpy as np\r\n    def get_devicendarray(t):\r\n        assert t.type() == 'torch.cuda.FloatTensor'\r\n        numpy_dtype = np.float32\r\n        itemsize = 4\r\n\r\n        ctx = cuda.cudadrv.driver.driver.get_context()\r\n        mp = cuda.cudadrv.driver.MemoryPointer(ctx, ctypes.c_ulong(t.data_ptr()), t.numel()*itemsize)\r\n        return cuda.cudadrv.devicearray.DeviceNDArray(t.numel(), itemsize, numpy_dtype(), gpu_data=mp, stream=torch.cuda.current_stream().cuda_stream)\r\n\r\n    numba_param = get_devicendarray(param)\r\n    numba_grad = get_devicendarray(grad)\r\n    numba_exp_avg = get_devicendarray(exp_avg)\r\n    numba_exp_avg_sq = get_devicendarray(exp_avg_sq)\r\n    threadsperblock = 32\r\n    blockspergrid = (param.numel() + (threadsperblock - 1)) // threadsperblock\r\n    kernel_numba_cuda[blockspergrid, threadsperblock](numba_param, numba_grad, numba_exp_avg, numba_exp_avg_sq, beta1, beta2, step_size, eps)\r\n\r\nkernels = [(\"native \", kernel_native), (\"numba  \", kernel_numba)]\r\nshape = (4194304*64,)\r\nparam, grad, exp_avg, exp_avg_sq = [torch.rand(shape).to(\"cuda\") for x in range(4)]\r\n[Truncated]\n        res = func(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps)\r\n        if res is not None:\r\n            param[:], _, exp_avg[:], exp_avg_sq[:] = res\r\n        torch.cuda.synchronize()\r\n        t2 = time.time()\r\n        print(\"%s: \" % name, t2-t1)\r\n```\r\nThe results are:\r\n```\r\nnative :  0.0323\r\nnumba  :  0.0147\r\nTScript1:  0.0369\r\nTScript2:  0.0352\r\nTScript3:  0.0280\r\n```\r\nTScript3 is now better than native but still far from numba. TScript3 is now generating a single CUDA kernel but the back-copying of the result now dominates the timing. If I remove the line `param[:], _, exp_avg[:], exp_avg_sq[:] = res` the performance is almost on pair with numba:\r\n```\r\nTScript3 (no back-copy):  0.01664\r\n```\r\nThe problem is I really do need to update the values :/\n<issue_comment>username_5: cc @mruberry"}
{"repo": "pytorch/pytorch", "issue_id": 475728990, "issue_number": 23655, "timestamp": "2019-08-02 00:01:35+00:00", "text": "@username_3 That made a big difference to my traced module too. Any other tricks like this? It slows down the non-JIT version, however.", "context": "<issue_start><issue_comment>Title: TorchScript Poor CUDA Performance \nusername_0: The performance of a TorchScript that handles the computation in the [Adam Optimizer](https://github.com/pytorch/pytorch/blob/master/torch/optim/adam.py) is worse than using the native PyTorch operations on CUDA. \r\n\r\n### Background\r\nI am trying to improve the GPU performance of the [Adam Optimizer](https://github.com/pytorch/pytorch/blob/master/torch/optim/adam.py) since it is the bottleneck in one of our workflows. \r\n\r\n### Results\r\nI have written a standalone script that compares PyTorch native tensor operations, TorchScript, and Numba (three runs each):\r\n```\r\nnative :  0.0506\r\nnative :  0.0322\r\nnative :  0.0322\r\nTScript:  0.1917\r\nTScript:  0.0442\r\nTScript:  0.0442\r\nnumba  :  0.1638\r\nnumba  :  0.0145\r\nnumba  :  0.0143\r\n```\r\nI suspect the problem is that TorchScript does not fuse all operations into a single CUDA. It generates two CUDA kernels (see the `PYTORCH_FUSION_DEBUG` output below)\r\n\r\n### Standalone Script\r\n```python\r\nimport math\r\nimport torch\r\nimport time\r\nimport random\r\nimport sys\r\n\r\ndef kernel_native(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n    exp_avg.mul_(beta1).add_(1 - beta1, grad)\r\n    exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\r\n    denom = exp_avg_sq.sqrt().add_(eps)\r\n    param.addcdiv_(-step_size, exp_avg, denom)   \r\n\r\n@torch.jit.script\r\ndef kernel_jit_script(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n    exp_avg.mul_(beta1)\r\n    exp_avg.add_((1 - beta1) + grad)\r\n    exp_avg_sq.mul_(beta2)\r\n    exp_avg_sq.add_((1 - beta2)*(grad*grad))\r\n    denom = exp_avg_sq.sqrt().add_(eps)\r\n    param += -step_size * (exp_avg / denom)\r\n\r\nfrom numba import cuda\r\n@cuda.jit\r\ndef kernel_numba_cuda(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    i = cuda.grid(1)\r\n    if i > grad.size:\r\n        return\r\n\r\n    exp_avg[i] = exp_avg[i] * beta1 + (1 - beta1) + grad[i]\r\n    exp_avg_sq[i] = exp_avg_sq[i] * beta2 + (1 - beta2) *grad[i]*grad[i]\r\n    denom = math.sqrt(exp_avg_sq[i]) + eps\r\n    param[i] += -step_size * (exp_avg[i] / denom)\r\n\r\ndef kernel_numba(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n\r\n    import ctypes\r\n    import numpy as np\r\n    def get_devicendarray(t):\r\n        assert t.type() == 'torch.cuda.FloatTensor'\r\n        numpy_dtype = np.float32\r\n        itemsize = 4\r\n\r\n        ctx = cuda.cudadrv.driver.driver.get_context()\r\n        mp = cuda.cudadrv.driver.MemoryPointer(ctx, ctypes.c_ulong(t.data_ptr()), t.numel()*itemsize)\r\n        return cuda.cudadrv.devicearray.DeviceNDArray(t.numel(), itemsize, numpy_dtype(), gpu_data=mp, stream=torch.cuda.current_stream().cuda_stream)\r\n\r\n    numba_param = get_devicendarray(param)\r\n    numba_grad = get_devicendarray(grad)\r\n    numba_exp_avg = get_devicendarray(exp_avg)\r\n    numba_exp_avg_sq = get_devicendarray(exp_avg_sq)\r\n    threadsperblock = 32\r\n    blockspergrid = (param.numel() + (threadsperblock - 1)) // threadsperblock\r\n    kernel_numba_cuda[blockspergrid, threadsperblock](numba_param, numba_grad, numba_exp_avg, numba_exp_avg_sq, beta1, beta2, step_size, eps)\r\n\r\nkernels = [(\"native \", kernel_native), (\"TScript\", kernel_jit_script), (\"numba  \", kernel_numba)]\r\n[Truncated]\n      size_t t1_dimIndex0 = t1_linearIndex ;\r\n      t1_offset += t1_dimIndex0 ;\r\n      IndexType t3_offset = 0;\r\n      IndexType t3_linearIndex = linearIndex;\r\n      \r\n      //printf(\"tensor t3 sizes[0] = %d, strides[0] = %d\\n\", t3.sizes[0],t3.strides[0]);\r\n      size_t t3_dimIndex0 = t3_linearIndex ;\r\n      t3_offset += t3_dimIndex0 ;\r\n      \r\n      // calculate the results\r\n      float n0 = __ldg(&t0.data[t0_offset]);\r\n      float n1 = __ldg(&t1.data[t1_offset]);\r\n      double n2 = s2;\r\n      float n3 = n0 / n1;\r\n      float n4 = n3 * ((float) n2);\r\n      t3.data[t3_offset] = n4;\r\n      \r\n    }\r\n}\r\n```\n<issue_comment>username_1: @username_0 can you try removing all the in-place operations in your scripted version?\r\nIIRC in-place ops block fusion.\r\n\r\nI was btw able to fuse most of sparse adam by removing those in-place ops, see https://github.com/pytorch/pytorch/pull/23522#issuecomment-516952462\n<issue_comment>username_2: cc: @username_3 and also relevant WIP PR: [JIT version of Adagrad](https://github.com/pytorch/pytorch/pull/23640)\n<issue_comment>username_3: Yeah the CUDA fusion does not support fusing in-place operations yet, Can you try the suggestion by @username_1 to see if you get a better perf?\n<issue_comment>username_3: @username_0 Also, it seems like you are only timing it in 3 iterations, and the perf numbers seems including the compilation time. The first time JIT/TorchScript sees an input it will specialize and do optimizations against that input, so when comparing performance please allow a compilation time and runtime optimization warmup for the input.\n<issue_comment>username_4: @username_3 That made a big difference to my traced module too. Any other tricks like this? It slows down the non-JIT version, however.\n<issue_comment>username_3: @username_4 That's pretty much all, let me summarize:\r\n\r\n1. JIT fusion does not support in-place operation fusion and reduction operation yet.\r\n2. we did the fusion code generation at the first time we see an input that the shape we haven't seen before (like if the number of dimensions are different), this will help us generate faster cuda kernels. \r\n\r\nSo the TorchScript compilation time will be gone after compile and the first run :)\n<issue_comment>username_0: Thanks for the feedback!\r\n\r\nI have removed all in-place operations and added three kernel implementations:\r\n```python\r\n\r\nimport math\r\nimport torch\r\nimport time\r\nimport random\r\nimport sys\r\n\r\ndef kernel_native(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n    exp_avg.mul_(beta1).add_(1 - beta1, grad)\r\n    exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\r\n    denom = exp_avg_sq.sqrt().add_(eps)\r\n    param.addcdiv_(-step_size, exp_avg, denom)   \r\n\r\n@torch.jit.script\r\ndef kernel_jit_script1(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n    exp_avg *= beta1\r\n    exp_avg += (1 - beta1) + grad\r\n    exp_avg_sq *= beta2\r\n    exp_avg_sq += (1 - beta2)*(grad*grad)\r\n    param += -step_size * (exp_avg / exp_avg_sq.sqrt() + eps)\r\n\r\n@torch.jit.script\r\ndef kernel_jit_script2(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n    exp_avg[:] = exp_avg * beta1 + (1 - beta1) + grad\r\n    exp_avg_sq[:] = exp_avg_sq * beta2 + (1 - beta2)*(grad*grad)\r\n    param[:] = param -step_size * (exp_avg / exp_avg_sq.sqrt() + eps)\r\n\r\n@torch.jit.script\r\ndef kernel_jit_script3(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> Tuple[Tensor, Tensor, Tensor, Tensor]\r\n    exp_avg = exp_avg * beta1 + (1 - beta1) + grad\r\n    exp_avg_sq = exp_avg_sq * beta2 + (1 - beta2)*(grad*grad)\r\n    param = param -step_size * (exp_avg / exp_avg_sq.sqrt() + eps)\r\n    return (param, grad, exp_avg, exp_avg_sq)\r\n\r\n\r\nfrom numba import cuda\r\n@cuda.jit\r\ndef kernel_numba_cuda(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    i = cuda.grid(1)\r\n    if i > grad.size:\r\n        return\r\n\r\n    exp_avg[i] = exp_avg[i] * beta1 + (1 - beta1) + grad[i]\r\n    exp_avg_sq[i] = exp_avg_sq[i] * beta2 + (1 - beta2) *grad[i]*grad[i]\r\n    denom = math.sqrt(exp_avg_sq[i]) + eps\r\n    param[i] += -step_size * (exp_avg[i] / denom)\r\n\r\ndef kernel_numba(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n\r\n    import ctypes\r\n    import numpy as np\r\n    def get_devicendarray(t):\r\n        assert t.type() == 'torch.cuda.FloatTensor'\r\n        numpy_dtype = np.float32\r\n        itemsize = 4\r\n\r\n        ctx = cuda.cudadrv.driver.driver.get_context()\r\n        mp = cuda.cudadrv.driver.MemoryPointer(ctx, ctypes.c_ulong(t.data_ptr()), t.numel()*itemsize)\r\n        return cuda.cudadrv.devicearray.DeviceNDArray(t.numel(), itemsize, numpy_dtype(), gpu_data=mp, stream=torch.cuda.current_stream().cuda_stream)\r\n\r\n    numba_param = get_devicendarray(param)\r\n    numba_grad = get_devicendarray(grad)\r\n    numba_exp_avg = get_devicendarray(exp_avg)\r\n    numba_exp_avg_sq = get_devicendarray(exp_avg_sq)\r\n    threadsperblock = 32\r\n    blockspergrid = (param.numel() + (threadsperblock - 1)) // threadsperblock\r\n    kernel_numba_cuda[blockspergrid, threadsperblock](numba_param, numba_grad, numba_exp_avg, numba_exp_avg_sq, beta1, beta2, step_size, eps)\r\n\r\nkernels = [(\"native \", kernel_native), (\"numba  \", kernel_numba)]\r\nshape = (4194304*64,)\r\nparam, grad, exp_avg, exp_avg_sq = [torch.rand(shape).to(\"cuda\") for x in range(4)]\r\n[Truncated]\n        res = func(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps)\r\n        if res is not None:\r\n            param[:], _, exp_avg[:], exp_avg_sq[:] = res\r\n        torch.cuda.synchronize()\r\n        t2 = time.time()\r\n        print(\"%s: \" % name, t2-t1)\r\n```\r\nThe results are:\r\n```\r\nnative :  0.0323\r\nnumba  :  0.0147\r\nTScript1:  0.0369\r\nTScript2:  0.0352\r\nTScript3:  0.0280\r\n```\r\nTScript3 is now better than native but still far from numba. TScript3 is now generating a single CUDA kernel but the back-copying of the result now dominates the timing. If I remove the line `param[:], _, exp_avg[:], exp_avg_sq[:] = res` the performance is almost on pair with numba:\r\n```\r\nTScript3 (no back-copy):  0.01664\r\n```\r\nThe problem is I really do need to update the values :/\n<issue_comment>username_5: cc @mruberry"}
{"repo": "pytorch/pytorch", "issue_id": 475728990, "issue_number": 23655, "timestamp": "2019-08-02 00:22:34+00:00", "text": "@username_4 That's pretty much all, let me summarize:\r\n\r\n1. JIT fusion does not support in-place operation fusion and reduction operation yet.\r\n2. we did the fusion code generation at the first time we see an input that the shape we haven't seen before (like if the number of dimensions are different), this will help us generate faster cuda kernels. \r\n\r\nSo the TorchScript compilation time will be gone after compile and the first run :)", "context": "<issue_start><issue_comment>Title: TorchScript Poor CUDA Performance \nusername_0: The performance of a TorchScript that handles the computation in the [Adam Optimizer](https://github.com/pytorch/pytorch/blob/master/torch/optim/adam.py) is worse than using the native PyTorch operations on CUDA. \r\n\r\n### Background\r\nI am trying to improve the GPU performance of the [Adam Optimizer](https://github.com/pytorch/pytorch/blob/master/torch/optim/adam.py) since it is the bottleneck in one of our workflows. \r\n\r\n### Results\r\nI have written a standalone script that compares PyTorch native tensor operations, TorchScript, and Numba (three runs each):\r\n```\r\nnative :  0.0506\r\nnative :  0.0322\r\nnative :  0.0322\r\nTScript:  0.1917\r\nTScript:  0.0442\r\nTScript:  0.0442\r\nnumba  :  0.1638\r\nnumba  :  0.0145\r\nnumba  :  0.0143\r\n```\r\nI suspect the problem is that TorchScript does not fuse all operations into a single CUDA. It generates two CUDA kernels (see the `PYTORCH_FUSION_DEBUG` output below)\r\n\r\n### Standalone Script\r\n```python\r\nimport math\r\nimport torch\r\nimport time\r\nimport random\r\nimport sys\r\n\r\ndef kernel_native(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n    exp_avg.mul_(beta1).add_(1 - beta1, grad)\r\n    exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\r\n    denom = exp_avg_sq.sqrt().add_(eps)\r\n    param.addcdiv_(-step_size, exp_avg, denom)   \r\n\r\n@torch.jit.script\r\ndef kernel_jit_script(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n    exp_avg.mul_(beta1)\r\n    exp_avg.add_((1 - beta1) + grad)\r\n    exp_avg_sq.mul_(beta2)\r\n    exp_avg_sq.add_((1 - beta2)*(grad*grad))\r\n    denom = exp_avg_sq.sqrt().add_(eps)\r\n    param += -step_size * (exp_avg / denom)\r\n\r\nfrom numba import cuda\r\n@cuda.jit\r\ndef kernel_numba_cuda(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    i = cuda.grid(1)\r\n    if i > grad.size:\r\n        return\r\n\r\n    exp_avg[i] = exp_avg[i] * beta1 + (1 - beta1) + grad[i]\r\n    exp_avg_sq[i] = exp_avg_sq[i] * beta2 + (1 - beta2) *grad[i]*grad[i]\r\n    denom = math.sqrt(exp_avg_sq[i]) + eps\r\n    param[i] += -step_size * (exp_avg[i] / denom)\r\n\r\ndef kernel_numba(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n\r\n    import ctypes\r\n    import numpy as np\r\n    def get_devicendarray(t):\r\n        assert t.type() == 'torch.cuda.FloatTensor'\r\n        numpy_dtype = np.float32\r\n        itemsize = 4\r\n\r\n        ctx = cuda.cudadrv.driver.driver.get_context()\r\n        mp = cuda.cudadrv.driver.MemoryPointer(ctx, ctypes.c_ulong(t.data_ptr()), t.numel()*itemsize)\r\n        return cuda.cudadrv.devicearray.DeviceNDArray(t.numel(), itemsize, numpy_dtype(), gpu_data=mp, stream=torch.cuda.current_stream().cuda_stream)\r\n\r\n    numba_param = get_devicendarray(param)\r\n    numba_grad = get_devicendarray(grad)\r\n    numba_exp_avg = get_devicendarray(exp_avg)\r\n    numba_exp_avg_sq = get_devicendarray(exp_avg_sq)\r\n    threadsperblock = 32\r\n    blockspergrid = (param.numel() + (threadsperblock - 1)) // threadsperblock\r\n    kernel_numba_cuda[blockspergrid, threadsperblock](numba_param, numba_grad, numba_exp_avg, numba_exp_avg_sq, beta1, beta2, step_size, eps)\r\n\r\nkernels = [(\"native \", kernel_native), (\"TScript\", kernel_jit_script), (\"numba  \", kernel_numba)]\r\n[Truncated]\n      size_t t1_dimIndex0 = t1_linearIndex ;\r\n      t1_offset += t1_dimIndex0 ;\r\n      IndexType t3_offset = 0;\r\n      IndexType t3_linearIndex = linearIndex;\r\n      \r\n      //printf(\"tensor t3 sizes[0] = %d, strides[0] = %d\\n\", t3.sizes[0],t3.strides[0]);\r\n      size_t t3_dimIndex0 = t3_linearIndex ;\r\n      t3_offset += t3_dimIndex0 ;\r\n      \r\n      // calculate the results\r\n      float n0 = __ldg(&t0.data[t0_offset]);\r\n      float n1 = __ldg(&t1.data[t1_offset]);\r\n      double n2 = s2;\r\n      float n3 = n0 / n1;\r\n      float n4 = n3 * ((float) n2);\r\n      t3.data[t3_offset] = n4;\r\n      \r\n    }\r\n}\r\n```\n<issue_comment>username_1: @username_0 can you try removing all the in-place operations in your scripted version?\r\nIIRC in-place ops block fusion.\r\n\r\nI was btw able to fuse most of sparse adam by removing those in-place ops, see https://github.com/pytorch/pytorch/pull/23522#issuecomment-516952462\n<issue_comment>username_2: cc: @username_3 and also relevant WIP PR: [JIT version of Adagrad](https://github.com/pytorch/pytorch/pull/23640)\n<issue_comment>username_3: Yeah the CUDA fusion does not support fusing in-place operations yet, Can you try the suggestion by @username_1 to see if you get a better perf?\n<issue_comment>username_3: @username_0 Also, it seems like you are only timing it in 3 iterations, and the perf numbers seems including the compilation time. The first time JIT/TorchScript sees an input it will specialize and do optimizations against that input, so when comparing performance please allow a compilation time and runtime optimization warmup for the input.\n<issue_comment>username_4: @username_3 That made a big difference to my traced module too. Any other tricks like this? It slows down the non-JIT version, however.\n<issue_comment>username_3: @username_4 That's pretty much all, let me summarize:\r\n\r\n1. JIT fusion does not support in-place operation fusion and reduction operation yet.\r\n2. we did the fusion code generation at the first time we see an input that the shape we haven't seen before (like if the number of dimensions are different), this will help us generate faster cuda kernels. \r\n\r\nSo the TorchScript compilation time will be gone after compile and the first run :)\n<issue_comment>username_0: Thanks for the feedback!\r\n\r\nI have removed all in-place operations and added three kernel implementations:\r\n```python\r\n\r\nimport math\r\nimport torch\r\nimport time\r\nimport random\r\nimport sys\r\n\r\ndef kernel_native(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n    exp_avg.mul_(beta1).add_(1 - beta1, grad)\r\n    exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\r\n    denom = exp_avg_sq.sqrt().add_(eps)\r\n    param.addcdiv_(-step_size, exp_avg, denom)   \r\n\r\n@torch.jit.script\r\ndef kernel_jit_script1(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n    exp_avg *= beta1\r\n    exp_avg += (1 - beta1) + grad\r\n    exp_avg_sq *= beta2\r\n    exp_avg_sq += (1 - beta2)*(grad*grad)\r\n    param += -step_size * (exp_avg / exp_avg_sq.sqrt() + eps)\r\n\r\n@torch.jit.script\r\ndef kernel_jit_script2(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n    exp_avg[:] = exp_avg * beta1 + (1 - beta1) + grad\r\n    exp_avg_sq[:] = exp_avg_sq * beta2 + (1 - beta2)*(grad*grad)\r\n    param[:] = param -step_size * (exp_avg / exp_avg_sq.sqrt() + eps)\r\n\r\n@torch.jit.script\r\ndef kernel_jit_script3(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> Tuple[Tensor, Tensor, Tensor, Tensor]\r\n    exp_avg = exp_avg * beta1 + (1 - beta1) + grad\r\n    exp_avg_sq = exp_avg_sq * beta2 + (1 - beta2)*(grad*grad)\r\n    param = param -step_size * (exp_avg / exp_avg_sq.sqrt() + eps)\r\n    return (param, grad, exp_avg, exp_avg_sq)\r\n\r\n\r\nfrom numba import cuda\r\n@cuda.jit\r\ndef kernel_numba_cuda(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    i = cuda.grid(1)\r\n    if i > grad.size:\r\n        return\r\n\r\n    exp_avg[i] = exp_avg[i] * beta1 + (1 - beta1) + grad[i]\r\n    exp_avg_sq[i] = exp_avg_sq[i] * beta2 + (1 - beta2) *grad[i]*grad[i]\r\n    denom = math.sqrt(exp_avg_sq[i]) + eps\r\n    param[i] += -step_size * (exp_avg[i] / denom)\r\n\r\ndef kernel_numba(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n\r\n    import ctypes\r\n    import numpy as np\r\n    def get_devicendarray(t):\r\n        assert t.type() == 'torch.cuda.FloatTensor'\r\n        numpy_dtype = np.float32\r\n        itemsize = 4\r\n\r\n        ctx = cuda.cudadrv.driver.driver.get_context()\r\n        mp = cuda.cudadrv.driver.MemoryPointer(ctx, ctypes.c_ulong(t.data_ptr()), t.numel()*itemsize)\r\n        return cuda.cudadrv.devicearray.DeviceNDArray(t.numel(), itemsize, numpy_dtype(), gpu_data=mp, stream=torch.cuda.current_stream().cuda_stream)\r\n\r\n    numba_param = get_devicendarray(param)\r\n    numba_grad = get_devicendarray(grad)\r\n    numba_exp_avg = get_devicendarray(exp_avg)\r\n    numba_exp_avg_sq = get_devicendarray(exp_avg_sq)\r\n    threadsperblock = 32\r\n    blockspergrid = (param.numel() + (threadsperblock - 1)) // threadsperblock\r\n    kernel_numba_cuda[blockspergrid, threadsperblock](numba_param, numba_grad, numba_exp_avg, numba_exp_avg_sq, beta1, beta2, step_size, eps)\r\n\r\nkernels = [(\"native \", kernel_native), (\"numba  \", kernel_numba)]\r\nshape = (4194304*64,)\r\nparam, grad, exp_avg, exp_avg_sq = [torch.rand(shape).to(\"cuda\") for x in range(4)]\r\n[Truncated]\n        res = func(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps)\r\n        if res is not None:\r\n            param[:], _, exp_avg[:], exp_avg_sq[:] = res\r\n        torch.cuda.synchronize()\r\n        t2 = time.time()\r\n        print(\"%s: \" % name, t2-t1)\r\n```\r\nThe results are:\r\n```\r\nnative :  0.0323\r\nnumba  :  0.0147\r\nTScript1:  0.0369\r\nTScript2:  0.0352\r\nTScript3:  0.0280\r\n```\r\nTScript3 is now better than native but still far from numba. TScript3 is now generating a single CUDA kernel but the back-copying of the result now dominates the timing. If I remove the line `param[:], _, exp_avg[:], exp_avg_sq[:] = res` the performance is almost on pair with numba:\r\n```\r\nTScript3 (no back-copy):  0.01664\r\n```\r\nThe problem is I really do need to update the values :/\n<issue_comment>username_5: cc @mruberry"}
{"repo": "pytorch/pytorch", "issue_id": 475728990, "issue_number": 23655, "timestamp": "2019-08-02 12:25:09+00:00", "text": "Thanks for the feedback!\r\n\r\nI have removed all in-place operations and added three kernel implementations:\r\n```python\r\n\r\nimport math\r\nimport torch\r\nimport time\r\nimport random\r\nimport sys\r\n\r\ndef kernel_native(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n    exp_avg.mul_(beta1).add_(1 - beta1, grad)\r\n    exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\r\n    denom = exp_avg_sq.sqrt().add_(eps)\r\n    param.addcdiv_(-step_size, exp_avg, denom)   \r\n\r\n@torch.jit.script\r\ndef kernel_jit_script1(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n    exp_avg *= beta1\r\n    exp_avg += (1 - beta1) + grad\r\n    exp_avg_sq *= beta2\r\n    exp_avg_sq += (1 - beta2)*(grad*grad)\r\n    param += -step_size * (exp_avg / exp_avg_sq.sqrt() + eps)\r\n\r\n@torch.jit.script\r\ndef kernel_jit_script2(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n    exp_avg[:] = exp_avg * beta1 + (1 - beta1) + grad\r\n    exp_avg_sq[:] = exp_avg_sq * beta2 + (1 - beta2)*(grad*grad)\r\n    param[:] = param -step_size * (exp_avg / exp_avg_sq.sqrt() + eps)\r\n\r\n@torch.jit.script\r\ndef kernel_jit_script3(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> Tuple[Tensor, Tensor, Tensor, Tensor]\r\n    exp_avg = exp_avg * beta1 + (1 - beta1) + grad\r\n    exp_avg_sq = exp_avg_sq * beta2 + (1 - beta2)*(grad*grad)\r\n    param = param -step_size * (exp_avg / exp_avg_sq.sqrt() + eps)\r\n    return (param, grad, exp_avg, exp_avg_sq)\r\n\r\n\r\nfrom numba import cuda\r\n@cuda.jit\r\ndef kernel_numba_cuda(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    i = cuda.grid(1)\r\n    if i > grad.size:\r\n        return\r\n\r\n    exp_avg[i] = exp_avg[i] * beta1 + (1 - beta1) + grad[i]\r\n    exp_avg_sq[i] = exp_avg_sq[i] * beta2 + (1 - beta2) *grad[i]*grad[i]\r\n    denom = math.sqrt(exp_avg_sq[i]) + eps\r\n    param[i] += -step_size * (exp_avg[i] / denom)\r\n\r\ndef kernel_numba(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n\r\n    import ctypes\r\n    import numpy as np\r\n    def get_devicendarray(t):\r\n        assert t.type() == 'torch.cuda.FloatTensor'\r\n        numpy_dtype = np.float32\r\n        itemsize = 4\r\n\r\n        ctx = cuda.cudadrv.driver.driver.get_context()\r\n        mp = cuda.cudadrv.driver.MemoryPointer(ctx, ctypes.c_ulong(t.data_ptr()), t.numel()*itemsize)\r\n        return cuda.cudadrv.devicearray.DeviceNDArray(t.numel(), itemsize, numpy_dtype(), gpu_data=mp, stream=torch.cuda.current_stream().cuda_stream)\r\n\r\n    numba_param = get_devicendarray(param)\r\n    numba_grad = get_devicendarray(grad)\r\n    numba_exp_avg = get_devicendarray(exp_avg)\r\n    numba_exp_avg_sq = get_devicendarray(exp_avg_sq)\r\n    threadsperblock = 32\r\n    blockspergrid = (param.numel() + (threadsperblock - 1)) // threadsperblock\r\n    kernel_numba_cuda[blockspergrid, threadsperblock](numba_param, numba_grad, numba_exp_avg, numba_exp_avg_sq, beta1, beta2, step_size, eps)\r\n\r\nkernels = [(\"native \", kernel_native), (\"numba  \", kernel_numba)]\r\nshape = (4194304*64,)\r\nparam, grad, exp_avg, exp_avg_sq = [torch.rand(shape).to(\"cuda\") for x in range(4)]\r\n[Truncated]\n        res = func(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps)\r\n        if res is not None:\r\n            param[:], _, exp_avg[:], exp_avg_sq[:] = res\r\n        torch.cuda.synchronize()\r\n        t2 = time.time()\r\n        print(\"%s: \" % name, t2-t1)\r\n```\r\nThe results are:\r\n```\r\nnative :  0.0323\r\nnumba  :  0.0147\r\nTScript1:  0.0369\r\nTScript2:  0.0352\r\nTScript3:  0.0280\r\n```\r\nTScript3 is now better than native but still far from numba. TScript3 is now generating a single CUDA kernel but the back-copying of the result now dominates the timing. If I remove the line `param[:], _, exp_avg[:], exp_avg_sq[:] = res` the performance is almost on pair with numba:\r\n```\r\nTScript3 (no back-copy):  0.01664\r\n```\r\nThe problem is I really do need to update the values :/", "context": "<issue_start><issue_comment>Title: TorchScript Poor CUDA Performance \nusername_0: The performance of a TorchScript that handles the computation in the [Adam Optimizer](https://github.com/pytorch/pytorch/blob/master/torch/optim/adam.py) is worse than using the native PyTorch operations on CUDA. \r\n\r\n### Background\r\nI am trying to improve the GPU performance of the [Adam Optimizer](https://github.com/pytorch/pytorch/blob/master/torch/optim/adam.py) since it is the bottleneck in one of our workflows. \r\n\r\n### Results\r\nI have written a standalone script that compares PyTorch native tensor operations, TorchScript, and Numba (three runs each):\r\n```\r\nnative :  0.0506\r\nnative :  0.0322\r\nnative :  0.0322\r\nTScript:  0.1917\r\nTScript:  0.0442\r\nTScript:  0.0442\r\nnumba  :  0.1638\r\nnumba  :  0.0145\r\nnumba  :  0.0143\r\n```\r\nI suspect the problem is that TorchScript does not fuse all operations into a single CUDA. It generates two CUDA kernels (see the `PYTORCH_FUSION_DEBUG` output below)\r\n\r\n### Standalone Script\r\n```python\r\nimport math\r\nimport torch\r\nimport time\r\nimport random\r\nimport sys\r\n\r\ndef kernel_native(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n    exp_avg.mul_(beta1).add_(1 - beta1, grad)\r\n    exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\r\n    denom = exp_avg_sq.sqrt().add_(eps)\r\n    param.addcdiv_(-step_size, exp_avg, denom)   \r\n\r\n@torch.jit.script\r\ndef kernel_jit_script(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n    exp_avg.mul_(beta1)\r\n    exp_avg.add_((1 - beta1) + grad)\r\n    exp_avg_sq.mul_(beta2)\r\n    exp_avg_sq.add_((1 - beta2)*(grad*grad))\r\n    denom = exp_avg_sq.sqrt().add_(eps)\r\n    param += -step_size * (exp_avg / denom)\r\n\r\nfrom numba import cuda\r\n@cuda.jit\r\ndef kernel_numba_cuda(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    i = cuda.grid(1)\r\n    if i > grad.size:\r\n        return\r\n\r\n    exp_avg[i] = exp_avg[i] * beta1 + (1 - beta1) + grad[i]\r\n    exp_avg_sq[i] = exp_avg_sq[i] * beta2 + (1 - beta2) *grad[i]*grad[i]\r\n    denom = math.sqrt(exp_avg_sq[i]) + eps\r\n    param[i] += -step_size * (exp_avg[i] / denom)\r\n\r\ndef kernel_numba(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n\r\n    import ctypes\r\n    import numpy as np\r\n    def get_devicendarray(t):\r\n        assert t.type() == 'torch.cuda.FloatTensor'\r\n        numpy_dtype = np.float32\r\n        itemsize = 4\r\n\r\n        ctx = cuda.cudadrv.driver.driver.get_context()\r\n        mp = cuda.cudadrv.driver.MemoryPointer(ctx, ctypes.c_ulong(t.data_ptr()), t.numel()*itemsize)\r\n        return cuda.cudadrv.devicearray.DeviceNDArray(t.numel(), itemsize, numpy_dtype(), gpu_data=mp, stream=torch.cuda.current_stream().cuda_stream)\r\n\r\n    numba_param = get_devicendarray(param)\r\n    numba_grad = get_devicendarray(grad)\r\n    numba_exp_avg = get_devicendarray(exp_avg)\r\n    numba_exp_avg_sq = get_devicendarray(exp_avg_sq)\r\n    threadsperblock = 32\r\n    blockspergrid = (param.numel() + (threadsperblock - 1)) // threadsperblock\r\n    kernel_numba_cuda[blockspergrid, threadsperblock](numba_param, numba_grad, numba_exp_avg, numba_exp_avg_sq, beta1, beta2, step_size, eps)\r\n\r\nkernels = [(\"native \", kernel_native), (\"TScript\", kernel_jit_script), (\"numba  \", kernel_numba)]\r\n[Truncated]\n      size_t t1_dimIndex0 = t1_linearIndex ;\r\n      t1_offset += t1_dimIndex0 ;\r\n      IndexType t3_offset = 0;\r\n      IndexType t3_linearIndex = linearIndex;\r\n      \r\n      //printf(\"tensor t3 sizes[0] = %d, strides[0] = %d\\n\", t3.sizes[0],t3.strides[0]);\r\n      size_t t3_dimIndex0 = t3_linearIndex ;\r\n      t3_offset += t3_dimIndex0 ;\r\n      \r\n      // calculate the results\r\n      float n0 = __ldg(&t0.data[t0_offset]);\r\n      float n1 = __ldg(&t1.data[t1_offset]);\r\n      double n2 = s2;\r\n      float n3 = n0 / n1;\r\n      float n4 = n3 * ((float) n2);\r\n      t3.data[t3_offset] = n4;\r\n      \r\n    }\r\n}\r\n```\n<issue_comment>username_1: @username_0 can you try removing all the in-place operations in your scripted version?\r\nIIRC in-place ops block fusion.\r\n\r\nI was btw able to fuse most of sparse adam by removing those in-place ops, see https://github.com/pytorch/pytorch/pull/23522#issuecomment-516952462\n<issue_comment>username_2: cc: @username_3 and also relevant WIP PR: [JIT version of Adagrad](https://github.com/pytorch/pytorch/pull/23640)\n<issue_comment>username_3: Yeah the CUDA fusion does not support fusing in-place operations yet, Can you try the suggestion by @username_1 to see if you get a better perf?\n<issue_comment>username_3: @username_0 Also, it seems like you are only timing it in 3 iterations, and the perf numbers seems including the compilation time. The first time JIT/TorchScript sees an input it will specialize and do optimizations against that input, so when comparing performance please allow a compilation time and runtime optimization warmup for the input.\n<issue_comment>username_4: @username_3 That made a big difference to my traced module too. Any other tricks like this? It slows down the non-JIT version, however.\n<issue_comment>username_3: @username_4 That's pretty much all, let me summarize:\r\n\r\n1. JIT fusion does not support in-place operation fusion and reduction operation yet.\r\n2. we did the fusion code generation at the first time we see an input that the shape we haven't seen before (like if the number of dimensions are different), this will help us generate faster cuda kernels. \r\n\r\nSo the TorchScript compilation time will be gone after compile and the first run :)\n<issue_comment>username_0: Thanks for the feedback!\r\n\r\nI have removed all in-place operations and added three kernel implementations:\r\n```python\r\n\r\nimport math\r\nimport torch\r\nimport time\r\nimport random\r\nimport sys\r\n\r\ndef kernel_native(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n    exp_avg.mul_(beta1).add_(1 - beta1, grad)\r\n    exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\r\n    denom = exp_avg_sq.sqrt().add_(eps)\r\n    param.addcdiv_(-step_size, exp_avg, denom)   \r\n\r\n@torch.jit.script\r\ndef kernel_jit_script1(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n    exp_avg *= beta1\r\n    exp_avg += (1 - beta1) + grad\r\n    exp_avg_sq *= beta2\r\n    exp_avg_sq += (1 - beta2)*(grad*grad)\r\n    param += -step_size * (exp_avg / exp_avg_sq.sqrt() + eps)\r\n\r\n@torch.jit.script\r\ndef kernel_jit_script2(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n    exp_avg[:] = exp_avg * beta1 + (1 - beta1) + grad\r\n    exp_avg_sq[:] = exp_avg_sq * beta2 + (1 - beta2)*(grad*grad)\r\n    param[:] = param -step_size * (exp_avg / exp_avg_sq.sqrt() + eps)\r\n\r\n@torch.jit.script\r\ndef kernel_jit_script3(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> Tuple[Tensor, Tensor, Tensor, Tensor]\r\n    exp_avg = exp_avg * beta1 + (1 - beta1) + grad\r\n    exp_avg_sq = exp_avg_sq * beta2 + (1 - beta2)*(grad*grad)\r\n    param = param -step_size * (exp_avg / exp_avg_sq.sqrt() + eps)\r\n    return (param, grad, exp_avg, exp_avg_sq)\r\n\r\n\r\nfrom numba import cuda\r\n@cuda.jit\r\ndef kernel_numba_cuda(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    i = cuda.grid(1)\r\n    if i > grad.size:\r\n        return\r\n\r\n    exp_avg[i] = exp_avg[i] * beta1 + (1 - beta1) + grad[i]\r\n    exp_avg_sq[i] = exp_avg_sq[i] * beta2 + (1 - beta2) *grad[i]*grad[i]\r\n    denom = math.sqrt(exp_avg_sq[i]) + eps\r\n    param[i] += -step_size * (exp_avg[i] / denom)\r\n\r\ndef kernel_numba(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n\r\n    import ctypes\r\n    import numpy as np\r\n    def get_devicendarray(t):\r\n        assert t.type() == 'torch.cuda.FloatTensor'\r\n        numpy_dtype = np.float32\r\n        itemsize = 4\r\n\r\n        ctx = cuda.cudadrv.driver.driver.get_context()\r\n        mp = cuda.cudadrv.driver.MemoryPointer(ctx, ctypes.c_ulong(t.data_ptr()), t.numel()*itemsize)\r\n        return cuda.cudadrv.devicearray.DeviceNDArray(t.numel(), itemsize, numpy_dtype(), gpu_data=mp, stream=torch.cuda.current_stream().cuda_stream)\r\n\r\n    numba_param = get_devicendarray(param)\r\n    numba_grad = get_devicendarray(grad)\r\n    numba_exp_avg = get_devicendarray(exp_avg)\r\n    numba_exp_avg_sq = get_devicendarray(exp_avg_sq)\r\n    threadsperblock = 32\r\n    blockspergrid = (param.numel() + (threadsperblock - 1)) // threadsperblock\r\n    kernel_numba_cuda[blockspergrid, threadsperblock](numba_param, numba_grad, numba_exp_avg, numba_exp_avg_sq, beta1, beta2, step_size, eps)\r\n\r\nkernels = [(\"native \", kernel_native), (\"numba  \", kernel_numba)]\r\nshape = (4194304*64,)\r\nparam, grad, exp_avg, exp_avg_sq = [torch.rand(shape).to(\"cuda\") for x in range(4)]\r\n[Truncated]\n        res = func(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps)\r\n        if res is not None:\r\n            param[:], _, exp_avg[:], exp_avg_sq[:] = res\r\n        torch.cuda.synchronize()\r\n        t2 = time.time()\r\n        print(\"%s: \" % name, t2-t1)\r\n```\r\nThe results are:\r\n```\r\nnative :  0.0323\r\nnumba  :  0.0147\r\nTScript1:  0.0369\r\nTScript2:  0.0352\r\nTScript3:  0.0280\r\n```\r\nTScript3 is now better than native but still far from numba. TScript3 is now generating a single CUDA kernel but the back-copying of the result now dominates the timing. If I remove the line `param[:], _, exp_avg[:], exp_avg_sq[:] = res` the performance is almost on pair with numba:\r\n```\r\nTScript3 (no back-copy):  0.01664\r\n```\r\nThe problem is I really do need to update the values :/\n<issue_comment>username_5: cc @mruberry"}
{"repo": "pytorch/pytorch", "issue_id": 475728990, "issue_number": 23655, "timestamp": "2019-08-12 16:48:20+00:00", "text": "cc @mruberry", "context": "<issue_start><issue_comment>Title: TorchScript Poor CUDA Performance \nusername_0: The performance of a TorchScript that handles the computation in the [Adam Optimizer](https://github.com/pytorch/pytorch/blob/master/torch/optim/adam.py) is worse than using the native PyTorch operations on CUDA. \r\n\r\n### Background\r\nI am trying to improve the GPU performance of the [Adam Optimizer](https://github.com/pytorch/pytorch/blob/master/torch/optim/adam.py) since it is the bottleneck in one of our workflows. \r\n\r\n### Results\r\nI have written a standalone script that compares PyTorch native tensor operations, TorchScript, and Numba (three runs each):\r\n```\r\nnative :  0.0506\r\nnative :  0.0322\r\nnative :  0.0322\r\nTScript:  0.1917\r\nTScript:  0.0442\r\nTScript:  0.0442\r\nnumba  :  0.1638\r\nnumba  :  0.0145\r\nnumba  :  0.0143\r\n```\r\nI suspect the problem is that TorchScript does not fuse all operations into a single CUDA. It generates two CUDA kernels (see the `PYTORCH_FUSION_DEBUG` output below)\r\n\r\n### Standalone Script\r\n```python\r\nimport math\r\nimport torch\r\nimport time\r\nimport random\r\nimport sys\r\n\r\ndef kernel_native(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n    exp_avg.mul_(beta1).add_(1 - beta1, grad)\r\n    exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\r\n    denom = exp_avg_sq.sqrt().add_(eps)\r\n    param.addcdiv_(-step_size, exp_avg, denom)   \r\n\r\n@torch.jit.script\r\ndef kernel_jit_script(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n    exp_avg.mul_(beta1)\r\n    exp_avg.add_((1 - beta1) + grad)\r\n    exp_avg_sq.mul_(beta2)\r\n    exp_avg_sq.add_((1 - beta2)*(grad*grad))\r\n    denom = exp_avg_sq.sqrt().add_(eps)\r\n    param += -step_size * (exp_avg / denom)\r\n\r\nfrom numba import cuda\r\n@cuda.jit\r\ndef kernel_numba_cuda(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    i = cuda.grid(1)\r\n    if i > grad.size:\r\n        return\r\n\r\n    exp_avg[i] = exp_avg[i] * beta1 + (1 - beta1) + grad[i]\r\n    exp_avg_sq[i] = exp_avg_sq[i] * beta2 + (1 - beta2) *grad[i]*grad[i]\r\n    denom = math.sqrt(exp_avg_sq[i]) + eps\r\n    param[i] += -step_size * (exp_avg[i] / denom)\r\n\r\ndef kernel_numba(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n\r\n    import ctypes\r\n    import numpy as np\r\n    def get_devicendarray(t):\r\n        assert t.type() == 'torch.cuda.FloatTensor'\r\n        numpy_dtype = np.float32\r\n        itemsize = 4\r\n\r\n        ctx = cuda.cudadrv.driver.driver.get_context()\r\n        mp = cuda.cudadrv.driver.MemoryPointer(ctx, ctypes.c_ulong(t.data_ptr()), t.numel()*itemsize)\r\n        return cuda.cudadrv.devicearray.DeviceNDArray(t.numel(), itemsize, numpy_dtype(), gpu_data=mp, stream=torch.cuda.current_stream().cuda_stream)\r\n\r\n    numba_param = get_devicendarray(param)\r\n    numba_grad = get_devicendarray(grad)\r\n    numba_exp_avg = get_devicendarray(exp_avg)\r\n    numba_exp_avg_sq = get_devicendarray(exp_avg_sq)\r\n    threadsperblock = 32\r\n    blockspergrid = (param.numel() + (threadsperblock - 1)) // threadsperblock\r\n    kernel_numba_cuda[blockspergrid, threadsperblock](numba_param, numba_grad, numba_exp_avg, numba_exp_avg_sq, beta1, beta2, step_size, eps)\r\n\r\nkernels = [(\"native \", kernel_native), (\"TScript\", kernel_jit_script), (\"numba  \", kernel_numba)]\r\n[Truncated]\n      size_t t1_dimIndex0 = t1_linearIndex ;\r\n      t1_offset += t1_dimIndex0 ;\r\n      IndexType t3_offset = 0;\r\n      IndexType t3_linearIndex = linearIndex;\r\n      \r\n      //printf(\"tensor t3 sizes[0] = %d, strides[0] = %d\\n\", t3.sizes[0],t3.strides[0]);\r\n      size_t t3_dimIndex0 = t3_linearIndex ;\r\n      t3_offset += t3_dimIndex0 ;\r\n      \r\n      // calculate the results\r\n      float n0 = __ldg(&t0.data[t0_offset]);\r\n      float n1 = __ldg(&t1.data[t1_offset]);\r\n      double n2 = s2;\r\n      float n3 = n0 / n1;\r\n      float n4 = n3 * ((float) n2);\r\n      t3.data[t3_offset] = n4;\r\n      \r\n    }\r\n}\r\n```\n<issue_comment>username_1: @username_0 can you try removing all the in-place operations in your scripted version?\r\nIIRC in-place ops block fusion.\r\n\r\nI was btw able to fuse most of sparse adam by removing those in-place ops, see https://github.com/pytorch/pytorch/pull/23522#issuecomment-516952462\n<issue_comment>username_2: cc: @username_3 and also relevant WIP PR: [JIT version of Adagrad](https://github.com/pytorch/pytorch/pull/23640)\n<issue_comment>username_3: Yeah the CUDA fusion does not support fusing in-place operations yet, Can you try the suggestion by @username_1 to see if you get a better perf?\n<issue_comment>username_3: @username_0 Also, it seems like you are only timing it in 3 iterations, and the perf numbers seems including the compilation time. The first time JIT/TorchScript sees an input it will specialize and do optimizations against that input, so when comparing performance please allow a compilation time and runtime optimization warmup for the input.\n<issue_comment>username_4: @username_3 That made a big difference to my traced module too. Any other tricks like this? It slows down the non-JIT version, however.\n<issue_comment>username_3: @username_4 That's pretty much all, let me summarize:\r\n\r\n1. JIT fusion does not support in-place operation fusion and reduction operation yet.\r\n2. we did the fusion code generation at the first time we see an input that the shape we haven't seen before (like if the number of dimensions are different), this will help us generate faster cuda kernels. \r\n\r\nSo the TorchScript compilation time will be gone after compile and the first run :)\n<issue_comment>username_0: Thanks for the feedback!\r\n\r\nI have removed all in-place operations and added three kernel implementations:\r\n```python\r\n\r\nimport math\r\nimport torch\r\nimport time\r\nimport random\r\nimport sys\r\n\r\ndef kernel_native(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n    exp_avg.mul_(beta1).add_(1 - beta1, grad)\r\n    exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\r\n    denom = exp_avg_sq.sqrt().add_(eps)\r\n    param.addcdiv_(-step_size, exp_avg, denom)   \r\n\r\n@torch.jit.script\r\ndef kernel_jit_script1(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n    exp_avg *= beta1\r\n    exp_avg += (1 - beta1) + grad\r\n    exp_avg_sq *= beta2\r\n    exp_avg_sq += (1 - beta2)*(grad*grad)\r\n    param += -step_size * (exp_avg / exp_avg_sq.sqrt() + eps)\r\n\r\n@torch.jit.script\r\ndef kernel_jit_script2(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n    exp_avg[:] = exp_avg * beta1 + (1 - beta1) + grad\r\n    exp_avg_sq[:] = exp_avg_sq * beta2 + (1 - beta2)*(grad*grad)\r\n    param[:] = param -step_size * (exp_avg / exp_avg_sq.sqrt() + eps)\r\n\r\n@torch.jit.script\r\ndef kernel_jit_script3(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> Tuple[Tensor, Tensor, Tensor, Tensor]\r\n    exp_avg = exp_avg * beta1 + (1 - beta1) + grad\r\n    exp_avg_sq = exp_avg_sq * beta2 + (1 - beta2)*(grad*grad)\r\n    param = param -step_size * (exp_avg / exp_avg_sq.sqrt() + eps)\r\n    return (param, grad, exp_avg, exp_avg_sq)\r\n\r\n\r\nfrom numba import cuda\r\n@cuda.jit\r\ndef kernel_numba_cuda(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    i = cuda.grid(1)\r\n    if i > grad.size:\r\n        return\r\n\r\n    exp_avg[i] = exp_avg[i] * beta1 + (1 - beta1) + grad[i]\r\n    exp_avg_sq[i] = exp_avg_sq[i] * beta2 + (1 - beta2) *grad[i]*grad[i]\r\n    denom = math.sqrt(exp_avg_sq[i]) + eps\r\n    param[i] += -step_size * (exp_avg[i] / denom)\r\n\r\ndef kernel_numba(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n\r\n    import ctypes\r\n    import numpy as np\r\n    def get_devicendarray(t):\r\n        assert t.type() == 'torch.cuda.FloatTensor'\r\n        numpy_dtype = np.float32\r\n        itemsize = 4\r\n\r\n        ctx = cuda.cudadrv.driver.driver.get_context()\r\n        mp = cuda.cudadrv.driver.MemoryPointer(ctx, ctypes.c_ulong(t.data_ptr()), t.numel()*itemsize)\r\n        return cuda.cudadrv.devicearray.DeviceNDArray(t.numel(), itemsize, numpy_dtype(), gpu_data=mp, stream=torch.cuda.current_stream().cuda_stream)\r\n\r\n    numba_param = get_devicendarray(param)\r\n    numba_grad = get_devicendarray(grad)\r\n    numba_exp_avg = get_devicendarray(exp_avg)\r\n    numba_exp_avg_sq = get_devicendarray(exp_avg_sq)\r\n    threadsperblock = 32\r\n    blockspergrid = (param.numel() + (threadsperblock - 1)) // threadsperblock\r\n    kernel_numba_cuda[blockspergrid, threadsperblock](numba_param, numba_grad, numba_exp_avg, numba_exp_avg_sq, beta1, beta2, step_size, eps)\r\n\r\nkernels = [(\"native \", kernel_native), (\"numba  \", kernel_numba)]\r\nshape = (4194304*64,)\r\nparam, grad, exp_avg, exp_avg_sq = [torch.rand(shape).to(\"cuda\") for x in range(4)]\r\n[Truncated]\n        res = func(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps)\r\n        if res is not None:\r\n            param[:], _, exp_avg[:], exp_avg_sq[:] = res\r\n        torch.cuda.synchronize()\r\n        t2 = time.time()\r\n        print(\"%s: \" % name, t2-t1)\r\n```\r\nThe results are:\r\n```\r\nnative :  0.0323\r\nnumba  :  0.0147\r\nTScript1:  0.0369\r\nTScript2:  0.0352\r\nTScript3:  0.0280\r\n```\r\nTScript3 is now better than native but still far from numba. TScript3 is now generating a single CUDA kernel but the back-copying of the result now dominates the timing. If I remove the line `param[:], _, exp_avg[:], exp_avg_sq[:] = res` the performance is almost on pair with numba:\r\n```\r\nTScript3 (no back-copy):  0.01664\r\n```\r\nThe problem is I really do need to update the values :/\n<issue_comment>username_5: cc @mruberry"}
{"repo": "pytorch/pytorch", "issue_id": 488354709, "issue_number": 25563, "timestamp": "2019-09-03T01:42:54Z", "text": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#25563 Fix binary op name inference to happen before shape checks**\n\nBefore, for binary ops, name inference occurred after shape checks. This\ndefeats the purposes for names because the names are supposed to tell\nthe user that i.e. their tensors are misaligned or that they are adding\nincompatible tensors.\n\nThis PR changes TensorIterator so that names are computed before shape checks and\npropagated after the binary ops are finished. In order to support this,\nthis PR makes the following changes:\n- adds a `names_` field to TensorIterator, similar to `shape_`. This is\nnecessary to hold the output names, that are computed in\n`compute_names`, until they are used in `propagate_names_to_outputs()`.\n\nDifferential Revision: [D17158869](https://our.internmc.facebook.com/intern/diff/D17158869)", "context": "<issue_start><issue_comment>Title: Fix binary op name inference to happen before shape checks\nusername_0: Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#25563 Fix binary op name inference to happen before shape checks**\n\nBefore, for binary ops, name inference occurred after shape checks. This\ndefeats the purposes for names because the names are supposed to tell\nthe user that i.e. their tensors are misaligned or that they are adding\nincompatible tensors.\n\nThis PR changes TensorIterator so that names are computed before shape checks and\npropagated after the binary ops are finished. In order to support this,\nthis PR makes the following changes:\n- adds a `names_` field to TensorIterator, similar to `shape_`. This is\nnecessary to hold the output names, that are computed in\n`compute_names`, until they are used in `propagate_names_to_outputs()`.\n\nDifferential Revision: [D17158869](https://our.internmc.facebook.com/intern/diff/D17158869)"}
{"repo": "pytorch/pytorch", "issue_id": 498366431, "issue_number": 26813, "timestamp": "2019-09-25T15:36:37Z", "text": "Stack from [ghstack](https://github.com/username_0/ghstack):\n* **#26813 Switch static dispatch to use extractLegacyTypeId.**\n\nAttempt to fix #26764", "context": "<issue_start><issue_comment>Title: Switch static dispatch to use extractLegacyTypeId.\nusername_0: Stack from [ghstack](https://github.com/username_0/ghstack):\n* **#26813 Switch static dispatch to use extractLegacyTypeId.**\n\nAttempt to fix #26764\n<issue_comment>username_1: I got a chance to quickly test this out with my changes that run a quantized model. Seems like the flow now hits assertions in the code where it expects to not get a variable.\r\n\r\nFirst hit - \r\n`abort_message: assertion \"terminating with uncaught exception of type std::runtime_error: !self.is_variable() INTERNAL ASSERT FAILED at../aten/src/ATen/quantized/Quantizer.cpp`\r\n\r\nSecond hit\r\n`abort_message: assertion \"terminating with uncaught exception of type std::runtime_error: !options.is_variable() INTERNAL ASSERT FAILED at../aten/src/ATen/native/TensorFactories.cpp`\r\n\r\nI was able to run the model after commenting out both those assert statements. I'm guessing there may be other parts of the code where similar change needs to be made.\n<issue_comment>username_0: OK. So basically, this patch is a bandaid: the real problem is there is Tensor-Variable confusion somewhere in the code here, and that is triggering the asserts. While I don't feel too bad about unblocking by removing the asserts on mobile only, if this is reproducible server side it indicates a very real problem. My guess would be in the new quantization code :)\r\n\r\nIs it at all possible to run these mobile models, but on server? That would help diagnose further,.\n<issue_comment>username_2: fyi, it is possible with the \"scripts/build_mobile.sh\" script I recently created, for exactly this purpose.\r\nBut for this specific problem it involves some other not-yet-landed changes from others so it's more complicated debugging process. I updated the issue with some new findings.\n<issue_comment>username_0: I think this is subsumed by #26908."}
{"repo": "pytorch/pytorch", "issue_id": 498366431, "issue_number": 26813, "timestamp": "2019-09-25 23:39:52+00:00", "text": "I got a chance to quickly test this out with my changes that run a quantized model. Seems like the flow now hits assertions in the code where it expects to not get a variable.\r\n\r\nFirst hit - \r\n`abort_message: assertion \"terminating with uncaught exception of type std::runtime_error: !self.is_variable() INTERNAL ASSERT FAILED at../aten/src/ATen/quantized/Quantizer.cpp`\r\n\r\nSecond hit\r\n`abort_message: assertion \"terminating with uncaught exception of type std::runtime_error: !options.is_variable() INTERNAL ASSERT FAILED at../aten/src/ATen/native/TensorFactories.cpp`\r\n\r\nI was able to run the model after commenting out both those assert statements. I'm guessing there may be other parts of the code where similar change needs to be made.", "context": "<issue_start><issue_comment>Title: Switch static dispatch to use extractLegacyTypeId.\nusername_0: Stack from [ghstack](https://github.com/username_0/ghstack):\n* **#26813 Switch static dispatch to use extractLegacyTypeId.**\n\nAttempt to fix #26764\n<issue_comment>username_1: I got a chance to quickly test this out with my changes that run a quantized model. Seems like the flow now hits assertions in the code where it expects to not get a variable.\r\n\r\nFirst hit - \r\n`abort_message: assertion \"terminating with uncaught exception of type std::runtime_error: !self.is_variable() INTERNAL ASSERT FAILED at../aten/src/ATen/quantized/Quantizer.cpp`\r\n\r\nSecond hit\r\n`abort_message: assertion \"terminating with uncaught exception of type std::runtime_error: !options.is_variable() INTERNAL ASSERT FAILED at../aten/src/ATen/native/TensorFactories.cpp`\r\n\r\nI was able to run the model after commenting out both those assert statements. I'm guessing there may be other parts of the code where similar change needs to be made.\n<issue_comment>username_0: OK. So basically, this patch is a bandaid: the real problem is there is Tensor-Variable confusion somewhere in the code here, and that is triggering the asserts. While I don't feel too bad about unblocking by removing the asserts on mobile only, if this is reproducible server side it indicates a very real problem. My guess would be in the new quantization code :)\r\n\r\nIs it at all possible to run these mobile models, but on server? That would help diagnose further,.\n<issue_comment>username_2: fyi, it is possible with the \"scripts/build_mobile.sh\" script I recently created, for exactly this purpose.\r\nBut for this specific problem it involves some other not-yet-landed changes from others so it's more complicated debugging process. I updated the issue with some new findings.\n<issue_comment>username_0: I think this is subsumed by #26908."}
{"repo": "pytorch/pytorch", "issue_id": 498366431, "issue_number": 26813, "timestamp": "2019-09-26 01:30:23+00:00", "text": "OK. So basically, this patch is a bandaid: the real problem is there is Tensor-Variable confusion somewhere in the code here, and that is triggering the asserts. While I don't feel too bad about unblocking by removing the asserts on mobile only, if this is reproducible server side it indicates a very real problem. My guess would be in the new quantization code :)\r\n\r\nIs it at all possible to run these mobile models, but on server? That would help diagnose further,.", "context": "<issue_start><issue_comment>Title: Switch static dispatch to use extractLegacyTypeId.\nusername_0: Stack from [ghstack](https://github.com/username_0/ghstack):\n* **#26813 Switch static dispatch to use extractLegacyTypeId.**\n\nAttempt to fix #26764\n<issue_comment>username_1: I got a chance to quickly test this out with my changes that run a quantized model. Seems like the flow now hits assertions in the code where it expects to not get a variable.\r\n\r\nFirst hit - \r\n`abort_message: assertion \"terminating with uncaught exception of type std::runtime_error: !self.is_variable() INTERNAL ASSERT FAILED at../aten/src/ATen/quantized/Quantizer.cpp`\r\n\r\nSecond hit\r\n`abort_message: assertion \"terminating with uncaught exception of type std::runtime_error: !options.is_variable() INTERNAL ASSERT FAILED at../aten/src/ATen/native/TensorFactories.cpp`\r\n\r\nI was able to run the model after commenting out both those assert statements. I'm guessing there may be other parts of the code where similar change needs to be made.\n<issue_comment>username_0: OK. So basically, this patch is a bandaid: the real problem is there is Tensor-Variable confusion somewhere in the code here, and that is triggering the asserts. While I don't feel too bad about unblocking by removing the asserts on mobile only, if this is reproducible server side it indicates a very real problem. My guess would be in the new quantization code :)\r\n\r\nIs it at all possible to run these mobile models, but on server? That would help diagnose further,.\n<issue_comment>username_2: fyi, it is possible with the \"scripts/build_mobile.sh\" script I recently created, for exactly this purpose.\r\nBut for this specific problem it involves some other not-yet-landed changes from others so it's more complicated debugging process. I updated the issue with some new findings.\n<issue_comment>username_0: I think this is subsumed by #26908."}
{"repo": "pytorch/pytorch", "issue_id": 498366431, "issue_number": 26813, "timestamp": "2019-09-26 05:15:34+00:00", "text": "fyi, it is possible with the \"scripts/build_mobile.sh\" script I recently created, for exactly this purpose.\r\nBut for this specific problem it involves some other not-yet-landed changes from others so it's more complicated debugging process. I updated the issue with some new findings.", "context": "<issue_start><issue_comment>Title: Switch static dispatch to use extractLegacyTypeId.\nusername_0: Stack from [ghstack](https://github.com/username_0/ghstack):\n* **#26813 Switch static dispatch to use extractLegacyTypeId.**\n\nAttempt to fix #26764\n<issue_comment>username_1: I got a chance to quickly test this out with my changes that run a quantized model. Seems like the flow now hits assertions in the code where it expects to not get a variable.\r\n\r\nFirst hit - \r\n`abort_message: assertion \"terminating with uncaught exception of type std::runtime_error: !self.is_variable() INTERNAL ASSERT FAILED at../aten/src/ATen/quantized/Quantizer.cpp`\r\n\r\nSecond hit\r\n`abort_message: assertion \"terminating with uncaught exception of type std::runtime_error: !options.is_variable() INTERNAL ASSERT FAILED at../aten/src/ATen/native/TensorFactories.cpp`\r\n\r\nI was able to run the model after commenting out both those assert statements. I'm guessing there may be other parts of the code where similar change needs to be made.\n<issue_comment>username_0: OK. So basically, this patch is a bandaid: the real problem is there is Tensor-Variable confusion somewhere in the code here, and that is triggering the asserts. While I don't feel too bad about unblocking by removing the asserts on mobile only, if this is reproducible server side it indicates a very real problem. My guess would be in the new quantization code :)\r\n\r\nIs it at all possible to run these mobile models, but on server? That would help diagnose further,.\n<issue_comment>username_2: fyi, it is possible with the \"scripts/build_mobile.sh\" script I recently created, for exactly this purpose.\r\nBut for this specific problem it involves some other not-yet-landed changes from others so it's more complicated debugging process. I updated the issue with some new findings.\n<issue_comment>username_0: I think this is subsumed by #26908."}
{"repo": "pytorch/pytorch", "issue_id": 498366431, "issue_number": 26813, "timestamp": "2019-09-26 18:44:26+00:00", "text": "I think this is subsumed by #26908.", "context": "<issue_start><issue_comment>Title: Switch static dispatch to use extractLegacyTypeId.\nusername_0: Stack from [ghstack](https://github.com/username_0/ghstack):\n* **#26813 Switch static dispatch to use extractLegacyTypeId.**\n\nAttempt to fix #26764\n<issue_comment>username_1: I got a chance to quickly test this out with my changes that run a quantized model. Seems like the flow now hits assertions in the code where it expects to not get a variable.\r\n\r\nFirst hit - \r\n`abort_message: assertion \"terminating with uncaught exception of type std::runtime_error: !self.is_variable() INTERNAL ASSERT FAILED at../aten/src/ATen/quantized/Quantizer.cpp`\r\n\r\nSecond hit\r\n`abort_message: assertion \"terminating with uncaught exception of type std::runtime_error: !options.is_variable() INTERNAL ASSERT FAILED at../aten/src/ATen/native/TensorFactories.cpp`\r\n\r\nI was able to run the model after commenting out both those assert statements. I'm guessing there may be other parts of the code where similar change needs to be made.\n<issue_comment>username_0: OK. So basically, this patch is a bandaid: the real problem is there is Tensor-Variable confusion somewhere in the code here, and that is triggering the asserts. While I don't feel too bad about unblocking by removing the asserts on mobile only, if this is reproducible server side it indicates a very real problem. My guess would be in the new quantization code :)\r\n\r\nIs it at all possible to run these mobile models, but on server? That would help diagnose further,.\n<issue_comment>username_2: fyi, it is possible with the \"scripts/build_mobile.sh\" script I recently created, for exactly this purpose.\r\nBut for this specific problem it involves some other not-yet-landed changes from others so it's more complicated debugging process. I updated the issue with some new findings.\n<issue_comment>username_0: I think this is subsumed by #26908."}
{"repo": "pytorch/pytorch", "issue_id": 550556777, "issue_number": 32266, "timestamp": "2020-01-16T03:47:40Z", "text": "", "context": "<issue_start><issue_comment>Title: [WIP] faster bailout tests\nusername_0: \n<issue_comment>username_1: ## :pill: CircleCI build failures summary and remediations\nAs of commit bf488282:\n\n\n* **1/1** failures introduced in this PR\n\n\n## Detailed failure analysis\n\nOne may explore the probable reasons each build failed interactively [on the Dr. CI website](https://dr.pytorch.org/commit-details.html?sha1=bf488282b9deb369e76de6cfffb0bdec2aed214f).\n\n### :detective: 1 new failure recognized by patterns\nThe following build failures do not appear to be due to upstream breakage:\n#### [![See CircleCI build](https://avatars0.githubusercontent.com/ml/7?s=12)](https://circleci.com/gh/pytorch/pytorch/4266171) pytorch_xla_linux_xenial_py3_6_clang7_build (1/1)\n**Step:** \"Build\" ([full log](https://dr.pytorch.org/api/view-log-full?build_id=86837350) | [pattern match details](https://dr.pytorch.org/build-details.html?build_id=86837350))\n<details>\n<summary>\n<code>Jan 16 05:27:23 torch_xla/csrc/aten_xla_type_default.cpp:9339:103: error: no member named 'DispatchKey' in namespace 'at'; did you mean 'Dispatcher'?</code>\n</summary>\n\n```\nJan 16 05:27:22 /opt/conda/lib/python3.6/site-packages/torch/include/ATen/core/dispatch/Dispatcher.h:35:18: note: 'Dispatcher' declared here \nJan 16 05:27:22 class CAFFE2_API Dispatcher final { \nJan 16 05:27:22                  ^ \nJan 16 05:27:23 torch_xla/csrc/aten_xla_type_default.cpp:9336:138: error: no member named 'DispatchKey' in namespace 'at'; did you mean 'Dispatcher'? \nJan 16 05:27:23       .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &, const at::Tensor &, bool), &AtenXlaTypeDefault::_cholesky_solve_helper>(at::DispatchKey::XLATensorId) \nJan 16 05:27:23                                                                                                                                      ~~~~^~~~~~~~~~~ \nJan 16 05:27:23                                                                                                                                          Dispatcher \nJan 16 05:27:23 /opt/conda/lib/python3.6/site-packages/torch/include/ATen/core/dispatch/Dispatcher.h:35:18: note: 'Dispatcher' declared here \nJan 16 05:27:23 class CAFFE2_API Dispatcher final { \nJan 16 05:27:23                  ^ \nJan 16 05:27:23 torch_xla/csrc/aten_xla_type_default.cpp:9339:103: error: no member named 'DispatchKey' in namespace 'at'; did you mean 'Dispatcher'? \nJan 16 05:27:23       .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &, bool), &AtenXlaTypeDefault::_coalesced_>(at::DispatchKey::XLATensorId) \nJan 16 05:27:23                                                                                                   ~~~~^~~~~~~~~~~ \nJan 16 05:27:23                                                                                                       Dispatcher \nJan 16 05:27:23 /opt/conda/lib/python3.6/site-packages/torch/include/ATen/core/dispatch/Dispatcher.h:35:18: note: 'Dispatcher' declared here \nJan 16 05:27:23 class CAFFE2_API Dispatcher final { \nJan 16 05:27:23                  ^ \nJan 16 05:27:23 fatal error: too many errors emitted, stopping now [-ferror-limit=] \nJan 16 05:27:23 20 errors generated. \npackages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.6/site-packages/torch/include/TH -I/opt/conda/lib/python3.6/site-packages/torch/include/THC -I/opt/conda/include/python3.6m -c torch_xla/csrc/layout_manager.cpp -o build/temp.linux-x86_64-3.6/torch_xla/csrc/layout_manager.o -std=c++14 -Wno-sign-compare -Wno-deprecated-declarations -Wno-return-type -Wno-macro-redefined -Wno-return-std-move -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=_XLAC -D_GLIBCXX_USE_CXX11_ABI=1 \n/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.6/site-packages/torch/include/TH -I/opt/conda/lib/python3.6/site-packages/torch/include/THC -I/opt/conda/include/python3.6m -c torch_xla/csrc/view.cpp -o build/temp.linux-x86_64-3.6/torch_xla/csrc/view.o -std=c++14 -Wno-sign-compare -Wno-deprecated-declarations -Wno-return-type -Wno-macro-redefined -Wno-return-std-move -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=_XLAC -D_GLIBCXX_USE_CXX11_ABI=1 \n```\n\n</details>\n\n\n\n---\n\n<details><summary>This comment was automatically generated by <a href=\"https://github.com/username_1/circleci-failure-tracker/tree/master/docs/from-pull-request-comment\">Dr. CI</a> (expand for details).</summary>Follow <a href=\"https://dr.pytorch.org/admin/comments-opt-out.html\">this link to opt-out</a> of these comments for your Pull Requests.\n\nPlease report bugs/suggestions on the <a href=\"https://github.com/username_1/circleci-failure-tracker/issues\">GitHub issue tracker</a>.\n</details>\n<issue_comment>username_0: I put together a small benchmark that runs 10,000 `prim::BailOut`s and no real ops. Even though, the fluctuations are huge on a devserver, `isCompatibleWith` is definitely much faster (4-8x).\r\n\r\n**baseline:**\r\n```\r\ntime = 0.224115446\r\ntime = 0.241094661\r\ntime = 0.206836688\r\ntime = 0.298701322\r\ntime = 0.236710434\r\ntime = 0.224483698\r\ntime = 0.224836235\r\ntime = 0.198836114\r\ntime = 0.187320452\r\ntime = 0.208056021\r\n```\r\n\r\n**isCompatibleWith:**\r\n\r\n```\r\ntime = 0.044660287\r\ntime = 0.045000739\r\ntime = 0.048079375\r\ntime = 0.085522459\r\ntime = 0.05170733\r\ntime = 0.047986804\r\ntime = 0.075046798\r\ntime = 0.075740901\r\ntime = 0.049676586\r\ntime = 0.046801684\r\n```"}
{"repo": "pytorch/pytorch", "issue_id": 550556777, "issue_number": 32266, "timestamp": "2020-01-16 05:31:36+00:00", "text": "## :pill: CircleCI build failures summary and remediations\nAs of commit bf488282:\n\n\n* **1/1** failures introduced in this PR\n\n\n## Detailed failure analysis\n\nOne may explore the probable reasons each build failed interactively [on the Dr. CI website](https://dr.pytorch.org/commit-details.html?sha1=bf488282b9deb369e76de6cfffb0bdec2aed214f).\n\n### :detective: 1 new failure recognized by patterns\nThe following build failures do not appear to be due to upstream breakage:\n#### [![See CircleCI build](https://avatars0.githubusercontent.com/ml/7?s=12)](https://circleci.com/gh/pytorch/pytorch/4266171) pytorch_xla_linux_xenial_py3_6_clang7_build (1/1)\n**Step:** \"Build\" ([full log](https://dr.pytorch.org/api/view-log-full?build_id=86837350) | [pattern match details](https://dr.pytorch.org/build-details.html?build_id=86837350))\n<details>\n<summary>\n<code>Jan 16 05:27:23 torch_xla/csrc/aten_xla_type_default.cpp:9339:103: error: no member named 'DispatchKey' in namespace 'at'; did you mean 'Dispatcher'?</code>\n</summary>\n\n```\nJan 16 05:27:22 /opt/conda/lib/python3.6/site-packages/torch/include/ATen/core/dispatch/Dispatcher.h:35:18: note: 'Dispatcher' declared here \nJan 16 05:27:22 class CAFFE2_API Dispatcher final { \nJan 16 05:27:22                  ^ \nJan 16 05:27:23 torch_xla/csrc/aten_xla_type_default.cpp:9336:138: error: no member named 'DispatchKey' in namespace 'at'; did you mean 'Dispatcher'? \nJan 16 05:27:23       .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &, const at::Tensor &, bool), &AtenXlaTypeDefault::_cholesky_solve_helper>(at::DispatchKey::XLATensorId) \nJan 16 05:27:23                                                                                                                                      ~~~~^~~~~~~~~~~ \nJan 16 05:27:23                                                                                                                                          Dispatcher \nJan 16 05:27:23 /opt/conda/lib/python3.6/site-packages/torch/include/ATen/core/dispatch/Dispatcher.h:35:18: note: 'Dispatcher' declared here \nJan 16 05:27:23 class CAFFE2_API Dispatcher final { \nJan 16 05:27:23                  ^ \nJan 16 05:27:23 torch_xla/csrc/aten_xla_type_default.cpp:9339:103: error: no member named 'DispatchKey' in namespace 'at'; did you mean 'Dispatcher'? \nJan 16 05:27:23       .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &, bool), &AtenXlaTypeDefault::_coalesced_>(at::DispatchKey::XLATensorId) \nJan 16 05:27:23                                                                                                   ~~~~^~~~~~~~~~~ \nJan 16 05:27:23                                                                                                       Dispatcher \nJan 16 05:27:23 /opt/conda/lib/python3.6/site-packages/torch/include/ATen/core/dispatch/Dispatcher.h:35:18: note: 'Dispatcher' declared here \nJan 16 05:27:23 class CAFFE2_API Dispatcher final { \nJan 16 05:27:23                  ^ \nJan 16 05:27:23 fatal error: too many errors emitted, stopping now [-ferror-limit=] \nJan 16 05:27:23 20 errors generated. \npackages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.6/site-packages/torch/include/TH -I/opt/conda/lib/python3.6/site-packages/torch/include/THC -I/opt/conda/include/python3.6m -c torch_xla/csrc/layout_manager.cpp -o build/temp.linux-x86_64-3.6/torch_xla/csrc/layout_manager.o -std=c++14 -Wno-sign-compare -Wno-deprecated-declarations -Wno-return-type -Wno-macro-redefined -Wno-return-std-move -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=_XLAC -D_GLIBCXX_USE_CXX11_ABI=1 \n/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.6/site-packages/torch/include/TH -I/opt/conda/lib/python3.6/site-packages/torch/include/THC -I/opt/conda/include/python3.6m -c torch_xla/csrc/view.cpp -o build/temp.linux-x86_64-3.6/torch_xla/csrc/view.o -std=c++14 -Wno-sign-compare -Wno-deprecated-declarations -Wno-return-type -Wno-macro-redefined -Wno-return-std-move -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=_XLAC -D_GLIBCXX_USE_CXX11_ABI=1 \n```\n\n</details>\n\n\n\n---\n\n<details><summary>This comment was automatically generated by <a href=\"https://github.com/username_1/circleci-failure-tracker/tree/master/docs/from-pull-request-comment\">Dr. CI</a> (expand for details).</summary>Follow <a href=\"https://dr.pytorch.org/admin/comments-opt-out.html\">this link to opt-out</a> of these comments for your Pull Requests.\n\nPlease report bugs/suggestions on the <a href=\"https://github.com/username_1/circleci-failure-tracker/issues\">GitHub issue tracker</a>.\n</details>", "context": "<issue_start><issue_comment>Title: [WIP] faster bailout tests\nusername_0: \n<issue_comment>username_1: ## :pill: CircleCI build failures summary and remediations\nAs of commit bf488282:\n\n\n* **1/1** failures introduced in this PR\n\n\n## Detailed failure analysis\n\nOne may explore the probable reasons each build failed interactively [on the Dr. CI website](https://dr.pytorch.org/commit-details.html?sha1=bf488282b9deb369e76de6cfffb0bdec2aed214f).\n\n### :detective: 1 new failure recognized by patterns\nThe following build failures do not appear to be due to upstream breakage:\n#### [![See CircleCI build](https://avatars0.githubusercontent.com/ml/7?s=12)](https://circleci.com/gh/pytorch/pytorch/4266171) pytorch_xla_linux_xenial_py3_6_clang7_build (1/1)\n**Step:** \"Build\" ([full log](https://dr.pytorch.org/api/view-log-full?build_id=86837350) | [pattern match details](https://dr.pytorch.org/build-details.html?build_id=86837350))\n<details>\n<summary>\n<code>Jan 16 05:27:23 torch_xla/csrc/aten_xla_type_default.cpp:9339:103: error: no member named 'DispatchKey' in namespace 'at'; did you mean 'Dispatcher'?</code>\n</summary>\n\n```\nJan 16 05:27:22 /opt/conda/lib/python3.6/site-packages/torch/include/ATen/core/dispatch/Dispatcher.h:35:18: note: 'Dispatcher' declared here \nJan 16 05:27:22 class CAFFE2_API Dispatcher final { \nJan 16 05:27:22                  ^ \nJan 16 05:27:23 torch_xla/csrc/aten_xla_type_default.cpp:9336:138: error: no member named 'DispatchKey' in namespace 'at'; did you mean 'Dispatcher'? \nJan 16 05:27:23       .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &, const at::Tensor &, bool), &AtenXlaTypeDefault::_cholesky_solve_helper>(at::DispatchKey::XLATensorId) \nJan 16 05:27:23                                                                                                                                      ~~~~^~~~~~~~~~~ \nJan 16 05:27:23                                                                                                                                          Dispatcher \nJan 16 05:27:23 /opt/conda/lib/python3.6/site-packages/torch/include/ATen/core/dispatch/Dispatcher.h:35:18: note: 'Dispatcher' declared here \nJan 16 05:27:23 class CAFFE2_API Dispatcher final { \nJan 16 05:27:23                  ^ \nJan 16 05:27:23 torch_xla/csrc/aten_xla_type_default.cpp:9339:103: error: no member named 'DispatchKey' in namespace 'at'; did you mean 'Dispatcher'? \nJan 16 05:27:23       .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &, bool), &AtenXlaTypeDefault::_coalesced_>(at::DispatchKey::XLATensorId) \nJan 16 05:27:23                                                                                                   ~~~~^~~~~~~~~~~ \nJan 16 05:27:23                                                                                                       Dispatcher \nJan 16 05:27:23 /opt/conda/lib/python3.6/site-packages/torch/include/ATen/core/dispatch/Dispatcher.h:35:18: note: 'Dispatcher' declared here \nJan 16 05:27:23 class CAFFE2_API Dispatcher final { \nJan 16 05:27:23                  ^ \nJan 16 05:27:23 fatal error: too many errors emitted, stopping now [-ferror-limit=] \nJan 16 05:27:23 20 errors generated. \npackages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.6/site-packages/torch/include/TH -I/opt/conda/lib/python3.6/site-packages/torch/include/THC -I/opt/conda/include/python3.6m -c torch_xla/csrc/layout_manager.cpp -o build/temp.linux-x86_64-3.6/torch_xla/csrc/layout_manager.o -std=c++14 -Wno-sign-compare -Wno-deprecated-declarations -Wno-return-type -Wno-macro-redefined -Wno-return-std-move -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=_XLAC -D_GLIBCXX_USE_CXX11_ABI=1 \n/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.6/site-packages/torch/include/TH -I/opt/conda/lib/python3.6/site-packages/torch/include/THC -I/opt/conda/include/python3.6m -c torch_xla/csrc/view.cpp -o build/temp.linux-x86_64-3.6/torch_xla/csrc/view.o -std=c++14 -Wno-sign-compare -Wno-deprecated-declarations -Wno-return-type -Wno-macro-redefined -Wno-return-std-move -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=_XLAC -D_GLIBCXX_USE_CXX11_ABI=1 \n```\n\n</details>\n\n\n\n---\n\n<details><summary>This comment was automatically generated by <a href=\"https://github.com/username_1/circleci-failure-tracker/tree/master/docs/from-pull-request-comment\">Dr. CI</a> (expand for details).</summary>Follow <a href=\"https://dr.pytorch.org/admin/comments-opt-out.html\">this link to opt-out</a> of these comments for your Pull Requests.\n\nPlease report bugs/suggestions on the <a href=\"https://github.com/username_1/circleci-failure-tracker/issues\">GitHub issue tracker</a>.\n</details>\n<issue_comment>username_0: I put together a small benchmark that runs 10,000 `prim::BailOut`s and no real ops. Even though, the fluctuations are huge on a devserver, `isCompatibleWith` is definitely much faster (4-8x).\r\n\r\n**baseline:**\r\n```\r\ntime = 0.224115446\r\ntime = 0.241094661\r\ntime = 0.206836688\r\ntime = 0.298701322\r\ntime = 0.236710434\r\ntime = 0.224483698\r\ntime = 0.224836235\r\ntime = 0.198836114\r\ntime = 0.187320452\r\ntime = 0.208056021\r\n```\r\n\r\n**isCompatibleWith:**\r\n\r\n```\r\ntime = 0.044660287\r\ntime = 0.045000739\r\ntime = 0.048079375\r\ntime = 0.085522459\r\ntime = 0.05170733\r\ntime = 0.047986804\r\ntime = 0.075046798\r\ntime = 0.075740901\r\ntime = 0.049676586\r\ntime = 0.046801684\r\n```"}
{"repo": "pytorch/pytorch", "issue_id": 550556777, "issue_number": 32266, "timestamp": "2020-01-21 20:05:52+00:00", "text": "I put together a small benchmark that runs 10,000 `prim::BailOut`s and no real ops. Even though, the fluctuations are huge on a devserver, `isCompatibleWith` is definitely much faster (4-8x).\r\n\r\n**baseline:**\r\n```\r\ntime = 0.224115446\r\ntime = 0.241094661\r\ntime = 0.206836688\r\ntime = 0.298701322\r\ntime = 0.236710434\r\ntime = 0.224483698\r\ntime = 0.224836235\r\ntime = 0.198836114\r\ntime = 0.187320452\r\ntime = 0.208056021\r\n```\r\n\r\n**isCompatibleWith:**\r\n\r\n```\r\ntime = 0.044660287\r\ntime = 0.045000739\r\ntime = 0.048079375\r\ntime = 0.085522459\r\ntime = 0.05170733\r\ntime = 0.047986804\r\ntime = 0.075046798\r\ntime = 0.075740901\r\ntime = 0.049676586\r\ntime = 0.046801684\r\n```", "context": "<issue_start><issue_comment>Title: [WIP] faster bailout tests\nusername_0: \n<issue_comment>username_1: ## :pill: CircleCI build failures summary and remediations\nAs of commit bf488282:\n\n\n* **1/1** failures introduced in this PR\n\n\n## Detailed failure analysis\n\nOne may explore the probable reasons each build failed interactively [on the Dr. CI website](https://dr.pytorch.org/commit-details.html?sha1=bf488282b9deb369e76de6cfffb0bdec2aed214f).\n\n### :detective: 1 new failure recognized by patterns\nThe following build failures do not appear to be due to upstream breakage:\n#### [![See CircleCI build](https://avatars0.githubusercontent.com/ml/7?s=12)](https://circleci.com/gh/pytorch/pytorch/4266171) pytorch_xla_linux_xenial_py3_6_clang7_build (1/1)\n**Step:** \"Build\" ([full log](https://dr.pytorch.org/api/view-log-full?build_id=86837350) | [pattern match details](https://dr.pytorch.org/build-details.html?build_id=86837350))\n<details>\n<summary>\n<code>Jan 16 05:27:23 torch_xla/csrc/aten_xla_type_default.cpp:9339:103: error: no member named 'DispatchKey' in namespace 'at'; did you mean 'Dispatcher'?</code>\n</summary>\n\n```\nJan 16 05:27:22 /opt/conda/lib/python3.6/site-packages/torch/include/ATen/core/dispatch/Dispatcher.h:35:18: note: 'Dispatcher' declared here \nJan 16 05:27:22 class CAFFE2_API Dispatcher final { \nJan 16 05:27:22                  ^ \nJan 16 05:27:23 torch_xla/csrc/aten_xla_type_default.cpp:9336:138: error: no member named 'DispatchKey' in namespace 'at'; did you mean 'Dispatcher'? \nJan 16 05:27:23       .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &, const at::Tensor &, bool), &AtenXlaTypeDefault::_cholesky_solve_helper>(at::DispatchKey::XLATensorId) \nJan 16 05:27:23                                                                                                                                      ~~~~^~~~~~~~~~~ \nJan 16 05:27:23                                                                                                                                          Dispatcher \nJan 16 05:27:23 /opt/conda/lib/python3.6/site-packages/torch/include/ATen/core/dispatch/Dispatcher.h:35:18: note: 'Dispatcher' declared here \nJan 16 05:27:23 class CAFFE2_API Dispatcher final { \nJan 16 05:27:23                  ^ \nJan 16 05:27:23 torch_xla/csrc/aten_xla_type_default.cpp:9339:103: error: no member named 'DispatchKey' in namespace 'at'; did you mean 'Dispatcher'? \nJan 16 05:27:23       .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &, bool), &AtenXlaTypeDefault::_coalesced_>(at::DispatchKey::XLATensorId) \nJan 16 05:27:23                                                                                                   ~~~~^~~~~~~~~~~ \nJan 16 05:27:23                                                                                                       Dispatcher \nJan 16 05:27:23 /opt/conda/lib/python3.6/site-packages/torch/include/ATen/core/dispatch/Dispatcher.h:35:18: note: 'Dispatcher' declared here \nJan 16 05:27:23 class CAFFE2_API Dispatcher final { \nJan 16 05:27:23                  ^ \nJan 16 05:27:23 fatal error: too many errors emitted, stopping now [-ferror-limit=] \nJan 16 05:27:23 20 errors generated. \npackages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.6/site-packages/torch/include/TH -I/opt/conda/lib/python3.6/site-packages/torch/include/THC -I/opt/conda/include/python3.6m -c torch_xla/csrc/layout_manager.cpp -o build/temp.linux-x86_64-3.6/torch_xla/csrc/layout_manager.o -std=c++14 -Wno-sign-compare -Wno-deprecated-declarations -Wno-return-type -Wno-macro-redefined -Wno-return-std-move -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=_XLAC -D_GLIBCXX_USE_CXX11_ABI=1 \n/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.6/site-packages/torch/include/TH -I/opt/conda/lib/python3.6/site-packages/torch/include/THC -I/opt/conda/include/python3.6m -c torch_xla/csrc/view.cpp -o build/temp.linux-x86_64-3.6/torch_xla/csrc/view.o -std=c++14 -Wno-sign-compare -Wno-deprecated-declarations -Wno-return-type -Wno-macro-redefined -Wno-return-std-move -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=_XLAC -D_GLIBCXX_USE_CXX11_ABI=1 \n```\n\n</details>\n\n\n\n---\n\n<details><summary>This comment was automatically generated by <a href=\"https://github.com/username_1/circleci-failure-tracker/tree/master/docs/from-pull-request-comment\">Dr. CI</a> (expand for details).</summary>Follow <a href=\"https://dr.pytorch.org/admin/comments-opt-out.html\">this link to opt-out</a> of these comments for your Pull Requests.\n\nPlease report bugs/suggestions on the <a href=\"https://github.com/username_1/circleci-failure-tracker/issues\">GitHub issue tracker</a>.\n</details>\n<issue_comment>username_0: I put together a small benchmark that runs 10,000 `prim::BailOut`s and no real ops. Even though, the fluctuations are huge on a devserver, `isCompatibleWith` is definitely much faster (4-8x).\r\n\r\n**baseline:**\r\n```\r\ntime = 0.224115446\r\ntime = 0.241094661\r\ntime = 0.206836688\r\ntime = 0.298701322\r\ntime = 0.236710434\r\ntime = 0.224483698\r\ntime = 0.224836235\r\ntime = 0.198836114\r\ntime = 0.187320452\r\ntime = 0.208056021\r\n```\r\n\r\n**isCompatibleWith:**\r\n\r\n```\r\ntime = 0.044660287\r\ntime = 0.045000739\r\ntime = 0.048079375\r\ntime = 0.085522459\r\ntime = 0.05170733\r\ntime = 0.047986804\r\ntime = 0.075046798\r\ntime = 0.075740901\r\ntime = 0.049676586\r\ntime = 0.046801684\r\n```"}
{"repo": "pytorch/pytorch", "issue_id": 557908112, "issue_number": 32856, "timestamp": "2020-01-31T04:38:14Z", "text": "", "context": "<issue_start><issue_comment>Title: D19359178\nusername_0: \n<issue_comment>username_1: ## :pill: CircleCI build failures summary and remediations\nAs of commit 227d7d51:\n\n\n* **1/1** failures introduced in this PR\n\n\n## Detailed failure analysis\n\nOne may explore the probable reasons each build failed interactively [on the Dr. CI website](https://dr.pytorch.org/commit-details.html?sha1=227d7d51342ff3de1964fefd3e8d4b282f8fdb8d).\n\n### :detective: 1 new failure recognized by patterns\nThe following build failures do not appear to be due to upstream breakage:\n#### [![See CircleCI build](https://avatars0.githubusercontent.com/ml/7?s=12)](https://circleci.com/gh/pytorch/pytorch/4388461) pytorch_linux_xenial_py3_6_gcc5_4_build (1/1)\n**Step:** \"Build\" ([full log](https://dr.pytorch.org/api/view-log-full?build_id=93622501) | [pattern match details](https://dr.pytorch.org/build-details.html?build_id=93622501))\n<details>\n<summary>\n<code>Automatic merge failed; fix conflicts and then commit the result.</code>\n</summary>\n\n```\n \n  git checkout -b <new-branch-name> \n \nHEAD is now at 227d7d5134 D19359178 \n+ git reset --hard 227d7d51342ff3de1964fefd3e8d4b282f8fdb8d \nHEAD is now at 227d7d5134 D19359178 \n+ git merge --allow-unrelated-histories --no-edit --no-ff ed10408cc64d1acf0e325a3dda01f7b911350052 \nFailed to merge submodule third_party/fbgemm (not checked out) \nAuto-merging third_party/fbgemm \nCONFLICT (submodule): Merge conflict in third_party/fbgemm \nAutomatic merge failed; fix conflicts and then commit the result. \n```\n\n</details>\n\n\n\n---\n\n<details><summary>This comment was automatically generated by <a href=\"https://github.com/username_1/circleci-failure-tracker/tree/master/docs/from-pull-request-comment\">Dr. CI</a> (expand for details).</summary>Follow <a href=\"https://dr.pytorch.org/admin/comments-opt-out.html\">this link to opt-out</a> of these comments for your Pull Requests.\n\nPlease report bugs/suggestions on the <a href=\"https://github.com/username_1/circleci-failure-tracker/issues\">GitHub issue tracker</a>.\n</details>"}
{"repo": "pytorch/pytorch", "issue_id": 557908112, "issue_number": 32856, "timestamp": "2020-01-31 06:45:52+00:00", "text": "## :pill: CircleCI build failures summary and remediations\nAs of commit 227d7d51:\n\n\n* **1/1** failures introduced in this PR\n\n\n## Detailed failure analysis\n\nOne may explore the probable reasons each build failed interactively [on the Dr. CI website](https://dr.pytorch.org/commit-details.html?sha1=227d7d51342ff3de1964fefd3e8d4b282f8fdb8d).\n\n### :detective: 1 new failure recognized by patterns\nThe following build failures do not appear to be due to upstream breakage:\n#### [![See CircleCI build](https://avatars0.githubusercontent.com/ml/7?s=12)](https://circleci.com/gh/pytorch/pytorch/4388461) pytorch_linux_xenial_py3_6_gcc5_4_build (1/1)\n**Step:** \"Build\" ([full log](https://dr.pytorch.org/api/view-log-full?build_id=93622501) | [pattern match details](https://dr.pytorch.org/build-details.html?build_id=93622501))\n<details>\n<summary>\n<code>Automatic merge failed; fix conflicts and then commit the result.</code>\n</summary>\n\n```\n \n  git checkout -b <new-branch-name> \n \nHEAD is now at 227d7d5134 D19359178 \n+ git reset --hard 227d7d51342ff3de1964fefd3e8d4b282f8fdb8d \nHEAD is now at 227d7d5134 D19359178 \n+ git merge --allow-unrelated-histories --no-edit --no-ff ed10408cc64d1acf0e325a3dda01f7b911350052 \nFailed to merge submodule third_party/fbgemm (not checked out) \nAuto-merging third_party/fbgemm \nCONFLICT (submodule): Merge conflict in third_party/fbgemm \nAutomatic merge failed; fix conflicts and then commit the result. \n```\n\n</details>\n\n\n\n---\n\n<details><summary>This comment was automatically generated by <a href=\"https://github.com/username_1/circleci-failure-tracker/tree/master/docs/from-pull-request-comment\">Dr. CI</a> (expand for details).</summary>Follow <a href=\"https://dr.pytorch.org/admin/comments-opt-out.html\">this link to opt-out</a> of these comments for your Pull Requests.\n\nPlease report bugs/suggestions on the <a href=\"https://github.com/username_1/circleci-failure-tracker/issues\">GitHub issue tracker</a>.\n</details>", "context": "<issue_start><issue_comment>Title: D19359178\nusername_0: \n<issue_comment>username_1: ## :pill: CircleCI build failures summary and remediations\nAs of commit 227d7d51:\n\n\n* **1/1** failures introduced in this PR\n\n\n## Detailed failure analysis\n\nOne may explore the probable reasons each build failed interactively [on the Dr. CI website](https://dr.pytorch.org/commit-details.html?sha1=227d7d51342ff3de1964fefd3e8d4b282f8fdb8d).\n\n### :detective: 1 new failure recognized by patterns\nThe following build failures do not appear to be due to upstream breakage:\n#### [![See CircleCI build](https://avatars0.githubusercontent.com/ml/7?s=12)](https://circleci.com/gh/pytorch/pytorch/4388461) pytorch_linux_xenial_py3_6_gcc5_4_build (1/1)\n**Step:** \"Build\" ([full log](https://dr.pytorch.org/api/view-log-full?build_id=93622501) | [pattern match details](https://dr.pytorch.org/build-details.html?build_id=93622501))\n<details>\n<summary>\n<code>Automatic merge failed; fix conflicts and then commit the result.</code>\n</summary>\n\n```\n \n  git checkout -b <new-branch-name> \n \nHEAD is now at 227d7d5134 D19359178 \n+ git reset --hard 227d7d51342ff3de1964fefd3e8d4b282f8fdb8d \nHEAD is now at 227d7d5134 D19359178 \n+ git merge --allow-unrelated-histories --no-edit --no-ff ed10408cc64d1acf0e325a3dda01f7b911350052 \nFailed to merge submodule third_party/fbgemm (not checked out) \nAuto-merging third_party/fbgemm \nCONFLICT (submodule): Merge conflict in third_party/fbgemm \nAutomatic merge failed; fix conflicts and then commit the result. \n```\n\n</details>\n\n\n\n---\n\n<details><summary>This comment was automatically generated by <a href=\"https://github.com/username_1/circleci-failure-tracker/tree/master/docs/from-pull-request-comment\">Dr. CI</a> (expand for details).</summary>Follow <a href=\"https://dr.pytorch.org/admin/comments-opt-out.html\">this link to opt-out</a> of these comments for your Pull Requests.\n\nPlease report bugs/suggestions on the <a href=\"https://github.com/username_1/circleci-failure-tracker/issues\">GitHub issue tracker</a>.\n</details>"}
{"repo": "pytorch/pytorch", "issue_id": 604295214, "issue_number": 37027, "timestamp": "2020-04-21T21:22:46Z", "text": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#37027 [WIP][rpc] Move _set_rpc_backand and RpcBackendOptions to use float instead of\ntimedelta**\n* #34650 per-RPC timeouts for rpc_sync and rpc_async\n\ntimedelta\n\nThe RPC timeout passed into rpc_sync and rpc_async after the below\nchange is now float, so we should make these APIs consistent.\n\nDifferential Revision: [D21125171](https://our.internmc.facebook.com/intern/diff/D21125171/)\n\n**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D21125171/)!", "context": "<issue_start><issue_comment>Title: [WIP][rpc] Move _set_rpc_backand and RpcBackendOptions to use float instead of\ntimedelta\nusername_0: Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#37027 [WIP][rpc] Move _set_rpc_backand and RpcBackendOptions to use float instead of\ntimedelta**\n* #34650 per-RPC timeouts for rpc_sync and rpc_async\n\ntimedelta\n\nThe RPC timeout passed into rpc_sync and rpc_async after the below\nchange is now float, so we should make these APIs consistent.\n\nDifferential Revision: [D21125171](https://our.internmc.facebook.com/intern/diff/D21125171/)\n\n**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D21125171/)!\n<issue_comment>username_1: OSS LGTM, but can you double-check the FB-only Thrift side? It looks like there's a build failure"}
{"repo": "pytorch/pytorch", "issue_id": 604295214, "issue_number": 37027, "timestamp": "2020-04-24 14:19:45+00:00", "text": "OSS LGTM, but can you double-check the FB-only Thrift side? It looks like there's a build failure", "context": "<issue_start><issue_comment>Title: [WIP][rpc] Move _set_rpc_backand and RpcBackendOptions to use float instead of\ntimedelta\nusername_0: Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#37027 [WIP][rpc] Move _set_rpc_backand and RpcBackendOptions to use float instead of\ntimedelta**\n* #34650 per-RPC timeouts for rpc_sync and rpc_async\n\ntimedelta\n\nThe RPC timeout passed into rpc_sync and rpc_async after the below\nchange is now float, so we should make these APIs consistent.\n\nDifferential Revision: [D21125171](https://our.internmc.facebook.com/intern/diff/D21125171/)\n\n**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D21125171/)!\n<issue_comment>username_1: OSS LGTM, but can you double-check the FB-only Thrift side? It looks like there's a build failure"}
{"repo": "pytorch/pytorch", "issue_id": 669008600, "issue_number": 42321, "timestamp": "2020-07-30T17:33:51Z", "text": "Stack from [ghstack](https://github.com/username_0/ghstack):\n* **#42321 Add missing space after -> for topk.values**\n\n\nDifferential Revision: [D22846520](https://our.internmc.facebook.com/intern/diff/D22846520)", "context": "<issue_start><issue_comment>Title: Add missing space after -> for topk.values\nusername_0: Stack from [ghstack](https://github.com/username_0/ghstack):\n* **#42321 Add missing space after -> for topk.values**\n\n\nDifferential Revision: [D22846520](https://our.internmc.facebook.com/intern/diff/D22846520)"}
{"repo": "pytorch/pytorch", "issue_id": 731323693, "issue_number": 46980, "timestamp": "2020-10-28 10:41:49+00:00", "text": "Steps to reproduce the behavior:\r\n\r\nCall torch.acos on a sparse tensor\r\nBelow is a code sample:\r\n\r\nimport torch\r\ni = torch.cuda.LongTensor([[0, 1, 1],\r\n                      [2, 0, 2],\r\n                      [2, 0, 2]])\r\n\r\nv  = torch.cuda.FloatTensor([3, -44, -5])\r\n\r\ns = torch.sparse_coo_tensor(i, v, torch.Size([2, 4, 4]), device=torch.device('cuda'))\r\nprint(s.acos())\r\n\r\nActual behaviour\r\nTraceback (most recent call last):\r\n  File \"sparse_acos.py\", line 9, in <module>\r\n    print(s.acos())\r\nRuntimeError: Could not run 'aten::acos.out' with arguments from the 'SparseCUDA' backend. 'aten::acos.out' is only available for these backends: [CPU, CUDA, BackendSelect, Named, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, Autocast, Batched, VmapMode].\r\n\r\nExpected behaviour\r\nacos of the sparse tensor should get calculated and output as a sparse tensor\r\n\r\nEnvironment\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.19.1\r\n[pip3] torch==1.8.0a0\r\n[pip3] torchtext==0.8.0a0+7e267d2\r\n[pip3] torchvision==0.8.0a0+be8192e\r\n[conda] blas 1.0 mkl\r\n[conda] cudatoolkit 10.2.89 hfd86e86_1\r\n[conda] magma-cuda102 2.5.2 1 pytorch\r\n[conda] mkl 2020.1 217\r\n[conda] mkl-include 2020.1 217\r\n[conda] mkl-service 2.3.0 py38he904b0f_0\r\n[conda] mkl_fft 1.1.0 py38h23d657b_0\r\n[conda] mkl_random 1.1.1 py38h0573a6f_0\r\n[conda] numpy 1.19.1 py38hbc911f0_0\r\n[conda] numpy-base 1.19.1 py38hfa32c7d_0\r\n[conda] torch 1.8.0a0 dev_0\r\n[conda] torchtext 0.8.0a0+7e267d2 pypi_0 pypi\r\n[conda] torchvision 0.8.0a0+be8192e pypi_0 pypi\r\n\r\nPyTorch Version: 1.8.0a0\r\nOS: Linux\r\nHow you installed PyTorch (conda, pip, source): source\r\nBuild command you used (if compiling from source): python setup.py develop\r\nPython version: 3.8\r\nCUDA/cuDNN version: 10.2", "context": "<issue_start><issue_comment>Title: torch.acos not supported for sparse layout\nusername_0: Steps to reproduce the behavior:\r\n\r\nCall torch.acos on a sparse tensor\r\nBelow is a code sample:\r\n\r\nimport torch\r\ni = torch.cuda.LongTensor([[0, 1, 1],\r\n                      [2, 0, 2],\r\n                      [2, 0, 2]])\r\n\r\nv  = torch.cuda.FloatTensor([3, -44, -5])\r\n\r\ns = torch.sparse_coo_tensor(i, v, torch.Size([2, 4, 4]), device=torch.device('cuda'))\r\nprint(s.acos())\r\n\r\nActual behaviour\r\nTraceback (most recent call last):\r\n  File \"sparse_acos.py\", line 9, in <module>\r\n    print(s.acos())\r\nRuntimeError: Could not run 'aten::acos.out' with arguments from the 'SparseCUDA' backend. 'aten::acos.out' is only available for these backends: [CPU, CUDA, BackendSelect, Named, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, Autocast, Batched, VmapMode].\r\n\r\nExpected behaviour\r\nacos of the sparse tensor should get calculated and output as a sparse tensor\r\n\r\nEnvironment\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.19.1\r\n[pip3] torch==1.8.0a0\r\n[pip3] torchtext==0.8.0a0+7e267d2\r\n[pip3] torchvision==0.8.0a0+be8192e\r\n[conda] blas 1.0 mkl\r\n[conda] cudatoolkit 10.2.89 hfd86e86_1\r\n[conda] magma-cuda102 2.5.2 1 pytorch\r\n[conda] mkl 2020.1 217\r\n[conda] mkl-include 2020.1 217\r\n[conda] mkl-service 2.3.0 py38he904b0f_0\r\n[conda] mkl_fft 1.1.0 py38h23d657b_0\r\n[conda] mkl_random 1.1.1 py38h0573a6f_0\r\n[conda] numpy 1.19.1 py38hbc911f0_0\r\n[conda] numpy-base 1.19.1 py38hfa32c7d_0\r\n[conda] torch 1.8.0a0 dev_0\r\n[conda] torchtext 0.8.0a0+7e267d2 pypi_0 pypi\r\n[conda] torchvision 0.8.0a0+be8192e pypi_0 pypi\r\n\r\nPyTorch Version: 1.8.0a0\r\nOS: Linux\r\nHow you installed PyTorch (conda, pip, source): source\r\nBuild command you used (if compiling from source): python setup.py develop\r\nPython version: 3.8\r\nCUDA/cuDNN version: 10.2\n<issue_comment>username_1: the result will be mostly dense so you probably should just `to_dense().acos()`\n<issue_comment>username_2: Hey @username_0, see questions on similar issues."}
{"repo": "pytorch/pytorch", "issue_id": 731323693, "issue_number": 46980, "timestamp": "2020-10-28 15:44:55+00:00", "text": "the result will be mostly dense so you probably should just `to_dense().acos()`", "context": "<issue_start><issue_comment>Title: torch.acos not supported for sparse layout\nusername_0: Steps to reproduce the behavior:\r\n\r\nCall torch.acos on a sparse tensor\r\nBelow is a code sample:\r\n\r\nimport torch\r\ni = torch.cuda.LongTensor([[0, 1, 1],\r\n                      [2, 0, 2],\r\n                      [2, 0, 2]])\r\n\r\nv  = torch.cuda.FloatTensor([3, -44, -5])\r\n\r\ns = torch.sparse_coo_tensor(i, v, torch.Size([2, 4, 4]), device=torch.device('cuda'))\r\nprint(s.acos())\r\n\r\nActual behaviour\r\nTraceback (most recent call last):\r\n  File \"sparse_acos.py\", line 9, in <module>\r\n    print(s.acos())\r\nRuntimeError: Could not run 'aten::acos.out' with arguments from the 'SparseCUDA' backend. 'aten::acos.out' is only available for these backends: [CPU, CUDA, BackendSelect, Named, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, Autocast, Batched, VmapMode].\r\n\r\nExpected behaviour\r\nacos of the sparse tensor should get calculated and output as a sparse tensor\r\n\r\nEnvironment\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.19.1\r\n[pip3] torch==1.8.0a0\r\n[pip3] torchtext==0.8.0a0+7e267d2\r\n[pip3] torchvision==0.8.0a0+be8192e\r\n[conda] blas 1.0 mkl\r\n[conda] cudatoolkit 10.2.89 hfd86e86_1\r\n[conda] magma-cuda102 2.5.2 1 pytorch\r\n[conda] mkl 2020.1 217\r\n[conda] mkl-include 2020.1 217\r\n[conda] mkl-service 2.3.0 py38he904b0f_0\r\n[conda] mkl_fft 1.1.0 py38h23d657b_0\r\n[conda] mkl_random 1.1.1 py38h0573a6f_0\r\n[conda] numpy 1.19.1 py38hbc911f0_0\r\n[conda] numpy-base 1.19.1 py38hfa32c7d_0\r\n[conda] torch 1.8.0a0 dev_0\r\n[conda] torchtext 0.8.0a0+7e267d2 pypi_0 pypi\r\n[conda] torchvision 0.8.0a0+be8192e pypi_0 pypi\r\n\r\nPyTorch Version: 1.8.0a0\r\nOS: Linux\r\nHow you installed PyTorch (conda, pip, source): source\r\nBuild command you used (if compiling from source): python setup.py develop\r\nPython version: 3.8\r\nCUDA/cuDNN version: 10.2\n<issue_comment>username_1: the result will be mostly dense so you probably should just `to_dense().acos()`\n<issue_comment>username_2: Hey @username_0, see questions on similar issues."}
{"repo": "pytorch/pytorch", "issue_id": 731323693, "issue_number": 46980, "timestamp": "2020-10-28 17:28:09+00:00", "text": "Hey @username_0, see questions on similar issues.", "context": "<issue_start><issue_comment>Title: torch.acos not supported for sparse layout\nusername_0: Steps to reproduce the behavior:\r\n\r\nCall torch.acos on a sparse tensor\r\nBelow is a code sample:\r\n\r\nimport torch\r\ni = torch.cuda.LongTensor([[0, 1, 1],\r\n                      [2, 0, 2],\r\n                      [2, 0, 2]])\r\n\r\nv  = torch.cuda.FloatTensor([3, -44, -5])\r\n\r\ns = torch.sparse_coo_tensor(i, v, torch.Size([2, 4, 4]), device=torch.device('cuda'))\r\nprint(s.acos())\r\n\r\nActual behaviour\r\nTraceback (most recent call last):\r\n  File \"sparse_acos.py\", line 9, in <module>\r\n    print(s.acos())\r\nRuntimeError: Could not run 'aten::acos.out' with arguments from the 'SparseCUDA' backend. 'aten::acos.out' is only available for these backends: [CPU, CUDA, BackendSelect, Named, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, Autocast, Batched, VmapMode].\r\n\r\nExpected behaviour\r\nacos of the sparse tensor should get calculated and output as a sparse tensor\r\n\r\nEnvironment\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.19.1\r\n[pip3] torch==1.8.0a0\r\n[pip3] torchtext==0.8.0a0+7e267d2\r\n[pip3] torchvision==0.8.0a0+be8192e\r\n[conda] blas 1.0 mkl\r\n[conda] cudatoolkit 10.2.89 hfd86e86_1\r\n[conda] magma-cuda102 2.5.2 1 pytorch\r\n[conda] mkl 2020.1 217\r\n[conda] mkl-include 2020.1 217\r\n[conda] mkl-service 2.3.0 py38he904b0f_0\r\n[conda] mkl_fft 1.1.0 py38h23d657b_0\r\n[conda] mkl_random 1.1.1 py38h0573a6f_0\r\n[conda] numpy 1.19.1 py38hbc911f0_0\r\n[conda] numpy-base 1.19.1 py38hfa32c7d_0\r\n[conda] torch 1.8.0a0 dev_0\r\n[conda] torchtext 0.8.0a0+7e267d2 pypi_0 pypi\r\n[conda] torchvision 0.8.0a0+be8192e pypi_0 pypi\r\n\r\nPyTorch Version: 1.8.0a0\r\nOS: Linux\r\nHow you installed PyTorch (conda, pip, source): source\r\nBuild command you used (if compiling from source): python setup.py develop\r\nPython version: 3.8\r\nCUDA/cuDNN version: 10.2\n<issue_comment>username_1: the result will be mostly dense so you probably should just `to_dense().acos()`\n<issue_comment>username_2: Hey @username_0, see questions on similar issues."}
{"repo": "facebook/react", "issue_id": 813286469, "issue_number": 20857, "timestamp": "2021-02-22 08:51:51+00:00", "text": "https://stackoverflow.com/questions/66306497/props-have-different-value-for-randomly-generated-ones-in-parent-react-js\r\n\r\nReact.StrictMode is changing the behaviour of logs in React App.\r\nReproducible here :  \r\nhttps://codesandbox.io/s/elegant-ellis-glhq7?file=/src/index.js (with StrictMode)\r\nhttps://stackblitz.com/edit/react-c2w4ay?file=src%2FApp.js (without StrictMode)\r\n\r\nIn both these cases the logs behaviour are different", "context": "<issue_start><issue_comment>Title: Bug: Issue with React.StrictMode\nusername_0: https://stackoverflow.com/questions/66306497/props-have-different-value-for-randomly-generated-ones-in-parent-react-js\r\n\r\nReact.StrictMode is changing the behaviour of logs in React App.\r\nReproducible here :  \r\nhttps://codesandbox.io/s/elegant-ellis-glhq7?file=/src/index.js (with StrictMode)\r\nhttps://stackblitz.com/edit/react-c2w4ay?file=src%2FApp.js (without StrictMode)\r\n\r\nIn both these cases the logs behaviour are different\n<issue_comment>username_1: This isn't a bug. This was the expected behavior as of PR #18547.\r\n\r\nIt's been discussed on a few other issues, e.g. #20090 and #17786.\r\n\r\nI'm going to close this issue since it doesn't add anything new. Feel free to leave a comment with this use case on one of those other issues though (or the PR).<issue_closed>\n<issue_comment>username_2: Error bug found\n<issue_comment>username_1: Please don't leave the same comment on multiple issues, @username_2.\n<issue_comment>username_2: Same as well\n<issue_comment>username_1: @username_2 This is the 5th meaningless comment you've left on an issue in the last 20 minutes. If it continues, I'll bloc you from leaving more.\n<issue_comment>username_2: Yes sure \ud83d\ude0a\n<issue_comment>username_2: What kind block, is it a new feature of React\n\nOn Mon, Feb 22, 2021, 19:45 Roy Thapa <anum.roy123@gmail.com> wrote:\n\n> Yes sure \ud83d\ude0a\n>\n>\n<issue_comment>username_3: To follow up on this, we've changed the logging behavior in 18 to be more intuitive.\r\nhttps://github.com/facebook/react/issues/21783#issuecomment-1083412766"}
{"repo": "facebook/react", "issue_id": 813286469, "issue_number": 20857, "timestamp": "2021-02-22 16:40:48+00:00", "text": "This isn't a bug. This was the expected behavior as of PR #18547.\r\n\r\nIt's been discussed on a few other issues, e.g. #20090 and #17786.\r\n\r\nI'm going to close this issue since it doesn't add anything new. Feel free to leave a comment with this use case on one of those other issues though (or the PR).", "context": "<issue_start><issue_comment>Title: Bug: Issue with React.StrictMode\nusername_0: https://stackoverflow.com/questions/66306497/props-have-different-value-for-randomly-generated-ones-in-parent-react-js\r\n\r\nReact.StrictMode is changing the behaviour of logs in React App.\r\nReproducible here :  \r\nhttps://codesandbox.io/s/elegant-ellis-glhq7?file=/src/index.js (with StrictMode)\r\nhttps://stackblitz.com/edit/react-c2w4ay?file=src%2FApp.js (without StrictMode)\r\n\r\nIn both these cases the logs behaviour are different\n<issue_comment>username_1: This isn't a bug. This was the expected behavior as of PR #18547.\r\n\r\nIt's been discussed on a few other issues, e.g. #20090 and #17786.\r\n\r\nI'm going to close this issue since it doesn't add anything new. Feel free to leave a comment with this use case on one of those other issues though (or the PR).<issue_closed>\n<issue_comment>username_2: Error bug found\n<issue_comment>username_1: Please don't leave the same comment on multiple issues, @username_2.\n<issue_comment>username_2: Same as well\n<issue_comment>username_1: @username_2 This is the 5th meaningless comment you've left on an issue in the last 20 minutes. If it continues, I'll bloc you from leaving more.\n<issue_comment>username_2: Yes sure \ud83d\ude0a\n<issue_comment>username_2: What kind block, is it a new feature of React\n\nOn Mon, Feb 22, 2021, 19:45 Roy Thapa <anum.roy123@gmail.com> wrote:\n\n> Yes sure \ud83d\ude0a\n>\n>\n<issue_comment>username_3: To follow up on this, we've changed the logging behavior in 18 to be more intuitive.\r\nhttps://github.com/facebook/react/issues/21783#issuecomment-1083412766"}
{"repo": "facebook/react", "issue_id": 813286469, "issue_number": 20857, "timestamp": "2021-02-22 16:40:48+00:00", "text": "", "context": "<issue_start><issue_comment>Title: Bug: Issue with React.StrictMode\nusername_0: https://stackoverflow.com/questions/66306497/props-have-different-value-for-randomly-generated-ones-in-parent-react-js\r\n\r\nReact.StrictMode is changing the behaviour of logs in React App.\r\nReproducible here :  \r\nhttps://codesandbox.io/s/elegant-ellis-glhq7?file=/src/index.js (with StrictMode)\r\nhttps://stackblitz.com/edit/react-c2w4ay?file=src%2FApp.js (without StrictMode)\r\n\r\nIn both these cases the logs behaviour are different\n<issue_comment>username_1: This isn't a bug. This was the expected behavior as of PR #18547.\r\n\r\nIt's been discussed on a few other issues, e.g. #20090 and #17786.\r\n\r\nI'm going to close this issue since it doesn't add anything new. Feel free to leave a comment with this use case on one of those other issues though (or the PR).<issue_closed>\n<issue_comment>username_2: Error bug found\n<issue_comment>username_1: Please don't leave the same comment on multiple issues, @username_2.\n<issue_comment>username_2: Same as well\n<issue_comment>username_1: @username_2 This is the 5th meaningless comment you've left on an issue in the last 20 minutes. If it continues, I'll bloc you from leaving more.\n<issue_comment>username_2: Yes sure \ud83d\ude0a\n<issue_comment>username_2: What kind block, is it a new feature of React\n\nOn Mon, Feb 22, 2021, 19:45 Roy Thapa <anum.roy123@gmail.com> wrote:\n\n> Yes sure \ud83d\ude0a\n>\n>\n<issue_comment>username_3: To follow up on this, we've changed the logging behavior in 18 to be more intuitive.\r\nhttps://github.com/facebook/react/issues/21783#issuecomment-1083412766"}
{"repo": "facebook/react", "issue_id": 813286469, "issue_number": 20857, "timestamp": "2021-02-22 17:14:17+00:00", "text": "Error bug found", "context": "<issue_start><issue_comment>Title: Bug: Issue with React.StrictMode\nusername_0: https://stackoverflow.com/questions/66306497/props-have-different-value-for-randomly-generated-ones-in-parent-react-js\r\n\r\nReact.StrictMode is changing the behaviour of logs in React App.\r\nReproducible here :  \r\nhttps://codesandbox.io/s/elegant-ellis-glhq7?file=/src/index.js (with StrictMode)\r\nhttps://stackblitz.com/edit/react-c2w4ay?file=src%2FApp.js (without StrictMode)\r\n\r\nIn both these cases the logs behaviour are different\n<issue_comment>username_1: This isn't a bug. This was the expected behavior as of PR #18547.\r\n\r\nIt's been discussed on a few other issues, e.g. #20090 and #17786.\r\n\r\nI'm going to close this issue since it doesn't add anything new. Feel free to leave a comment with this use case on one of those other issues though (or the PR).<issue_closed>\n<issue_comment>username_2: Error bug found\n<issue_comment>username_1: Please don't leave the same comment on multiple issues, @username_2.\n<issue_comment>username_2: Same as well\n<issue_comment>username_1: @username_2 This is the 5th meaningless comment you've left on an issue in the last 20 minutes. If it continues, I'll bloc you from leaving more.\n<issue_comment>username_2: Yes sure \ud83d\ude0a\n<issue_comment>username_2: What kind block, is it a new feature of React\n\nOn Mon, Feb 22, 2021, 19:45 Roy Thapa <anum.roy123@gmail.com> wrote:\n\n> Yes sure \ud83d\ude0a\n>\n>\n<issue_comment>username_3: To follow up on this, we've changed the logging behavior in 18 to be more intuitive.\r\nhttps://github.com/facebook/react/issues/21783#issuecomment-1083412766"}
{"repo": "facebook/react", "issue_id": 813286469, "issue_number": 20857, "timestamp": "2021-02-22 17:27:26+00:00", "text": "Please don't leave the same comment on multiple issues, @username_2.", "context": "<issue_start><issue_comment>Title: Bug: Issue with React.StrictMode\nusername_0: https://stackoverflow.com/questions/66306497/props-have-different-value-for-randomly-generated-ones-in-parent-react-js\r\n\r\nReact.StrictMode is changing the behaviour of logs in React App.\r\nReproducible here :  \r\nhttps://codesandbox.io/s/elegant-ellis-glhq7?file=/src/index.js (with StrictMode)\r\nhttps://stackblitz.com/edit/react-c2w4ay?file=src%2FApp.js (without StrictMode)\r\n\r\nIn both these cases the logs behaviour are different\n<issue_comment>username_1: This isn't a bug. This was the expected behavior as of PR #18547.\r\n\r\nIt's been discussed on a few other issues, e.g. #20090 and #17786.\r\n\r\nI'm going to close this issue since it doesn't add anything new. Feel free to leave a comment with this use case on one of those other issues though (or the PR).<issue_closed>\n<issue_comment>username_2: Error bug found\n<issue_comment>username_1: Please don't leave the same comment on multiple issues, @username_2.\n<issue_comment>username_2: Same as well\n<issue_comment>username_1: @username_2 This is the 5th meaningless comment you've left on an issue in the last 20 minutes. If it continues, I'll bloc you from leaving more.\n<issue_comment>username_2: Yes sure \ud83d\ude0a\n<issue_comment>username_2: What kind block, is it a new feature of React\n\nOn Mon, Feb 22, 2021, 19:45 Roy Thapa <anum.roy123@gmail.com> wrote:\n\n> Yes sure \ud83d\ude0a\n>\n>\n<issue_comment>username_3: To follow up on this, we've changed the logging behavior in 18 to be more intuitive.\r\nhttps://github.com/facebook/react/issues/21783#issuecomment-1083412766"}
{"repo": "facebook/react", "issue_id": 813286469, "issue_number": 20857, "timestamp": "2021-02-22 17:39:29+00:00", "text": "Same as well", "context": "<issue_start><issue_comment>Title: Bug: Issue with React.StrictMode\nusername_0: https://stackoverflow.com/questions/66306497/props-have-different-value-for-randomly-generated-ones-in-parent-react-js\r\n\r\nReact.StrictMode is changing the behaviour of logs in React App.\r\nReproducible here :  \r\nhttps://codesandbox.io/s/elegant-ellis-glhq7?file=/src/index.js (with StrictMode)\r\nhttps://stackblitz.com/edit/react-c2w4ay?file=src%2FApp.js (without StrictMode)\r\n\r\nIn both these cases the logs behaviour are different\n<issue_comment>username_1: This isn't a bug. This was the expected behavior as of PR #18547.\r\n\r\nIt's been discussed on a few other issues, e.g. #20090 and #17786.\r\n\r\nI'm going to close this issue since it doesn't add anything new. Feel free to leave a comment with this use case on one of those other issues though (or the PR).<issue_closed>\n<issue_comment>username_2: Error bug found\n<issue_comment>username_1: Please don't leave the same comment on multiple issues, @username_2.\n<issue_comment>username_2: Same as well\n<issue_comment>username_1: @username_2 This is the 5th meaningless comment you've left on an issue in the last 20 minutes. If it continues, I'll bloc you from leaving more.\n<issue_comment>username_2: Yes sure \ud83d\ude0a\n<issue_comment>username_2: What kind block, is it a new feature of React\n\nOn Mon, Feb 22, 2021, 19:45 Roy Thapa <anum.roy123@gmail.com> wrote:\n\n> Yes sure \ud83d\ude0a\n>\n>\n<issue_comment>username_3: To follow up on this, we've changed the logging behavior in 18 to be more intuitive.\r\nhttps://github.com/facebook/react/issues/21783#issuecomment-1083412766"}
{"repo": "facebook/react", "issue_id": 813286469, "issue_number": 20857, "timestamp": "2021-02-22 17:42:16+00:00", "text": "@username_2 This is the 5th meaningless comment you've left on an issue in the last 20 minutes. If it continues, I'll bloc you from leaving more.", "context": "<issue_start><issue_comment>Title: Bug: Issue with React.StrictMode\nusername_0: https://stackoverflow.com/questions/66306497/props-have-different-value-for-randomly-generated-ones-in-parent-react-js\r\n\r\nReact.StrictMode is changing the behaviour of logs in React App.\r\nReproducible here :  \r\nhttps://codesandbox.io/s/elegant-ellis-glhq7?file=/src/index.js (with StrictMode)\r\nhttps://stackblitz.com/edit/react-c2w4ay?file=src%2FApp.js (without StrictMode)\r\n\r\nIn both these cases the logs behaviour are different\n<issue_comment>username_1: This isn't a bug. This was the expected behavior as of PR #18547.\r\n\r\nIt's been discussed on a few other issues, e.g. #20090 and #17786.\r\n\r\nI'm going to close this issue since it doesn't add anything new. Feel free to leave a comment with this use case on one of those other issues though (or the PR).<issue_closed>\n<issue_comment>username_2: Error bug found\n<issue_comment>username_1: Please don't leave the same comment on multiple issues, @username_2.\n<issue_comment>username_2: Same as well\n<issue_comment>username_1: @username_2 This is the 5th meaningless comment you've left on an issue in the last 20 minutes. If it continues, I'll bloc you from leaving more.\n<issue_comment>username_2: Yes sure \ud83d\ude0a\n<issue_comment>username_2: What kind block, is it a new feature of React\n\nOn Mon, Feb 22, 2021, 19:45 Roy Thapa <anum.roy123@gmail.com> wrote:\n\n> Yes sure \ud83d\ude0a\n>\n>\n<issue_comment>username_3: To follow up on this, we've changed the logging behavior in 18 to be more intuitive.\r\nhttps://github.com/facebook/react/issues/21783#issuecomment-1083412766"}
{"repo": "facebook/react", "issue_id": 813286469, "issue_number": 20857, "timestamp": "2021-02-22 17:45:53+00:00", "text": "Yes sure \ud83d\ude0a", "context": "<issue_start><issue_comment>Title: Bug: Issue with React.StrictMode\nusername_0: https://stackoverflow.com/questions/66306497/props-have-different-value-for-randomly-generated-ones-in-parent-react-js\r\n\r\nReact.StrictMode is changing the behaviour of logs in React App.\r\nReproducible here :  \r\nhttps://codesandbox.io/s/elegant-ellis-glhq7?file=/src/index.js (with StrictMode)\r\nhttps://stackblitz.com/edit/react-c2w4ay?file=src%2FApp.js (without StrictMode)\r\n\r\nIn both these cases the logs behaviour are different\n<issue_comment>username_1: This isn't a bug. This was the expected behavior as of PR #18547.\r\n\r\nIt's been discussed on a few other issues, e.g. #20090 and #17786.\r\n\r\nI'm going to close this issue since it doesn't add anything new. Feel free to leave a comment with this use case on one of those other issues though (or the PR).<issue_closed>\n<issue_comment>username_2: Error bug found\n<issue_comment>username_1: Please don't leave the same comment on multiple issues, @username_2.\n<issue_comment>username_2: Same as well\n<issue_comment>username_1: @username_2 This is the 5th meaningless comment you've left on an issue in the last 20 minutes. If it continues, I'll bloc you from leaving more.\n<issue_comment>username_2: Yes sure \ud83d\ude0a\n<issue_comment>username_2: What kind block, is it a new feature of React\n\nOn Mon, Feb 22, 2021, 19:45 Roy Thapa <anum.roy123@gmail.com> wrote:\n\n> Yes sure \ud83d\ude0a\n>\n>\n<issue_comment>username_3: To follow up on this, we've changed the logging behavior in 18 to be more intuitive.\r\nhttps://github.com/facebook/react/issues/21783#issuecomment-1083412766"}
{"repo": "facebook/react", "issue_id": 813286469, "issue_number": 20857, "timestamp": "2021-02-22 17:49:03+00:00", "text": "What kind block, is it a new feature of React\n\nOn Mon, Feb 22, 2021, 19:45 Roy Thapa <lyhxr@example.com> wrote:\n\n> Yes sure \ud83d\ude0a\n>\n>", "context": "<issue_start><issue_comment>Title: Bug: Issue with React.StrictMode\nusername_0: https://stackoverflow.com/questions/66306497/props-have-different-value-for-randomly-generated-ones-in-parent-react-js\r\n\r\nReact.StrictMode is changing the behaviour of logs in React App.\r\nReproducible here :  \r\nhttps://codesandbox.io/s/elegant-ellis-glhq7?file=/src/index.js (with StrictMode)\r\nhttps://stackblitz.com/edit/react-c2w4ay?file=src%2FApp.js (without StrictMode)\r\n\r\nIn both these cases the logs behaviour are different\n<issue_comment>username_1: This isn't a bug. This was the expected behavior as of PR #18547.\r\n\r\nIt's been discussed on a few other issues, e.g. #20090 and #17786.\r\n\r\nI'm going to close this issue since it doesn't add anything new. Feel free to leave a comment with this use case on one of those other issues though (or the PR).<issue_closed>\n<issue_comment>username_2: Error bug found\n<issue_comment>username_1: Please don't leave the same comment on multiple issues, @username_2.\n<issue_comment>username_2: Same as well\n<issue_comment>username_1: @username_2 This is the 5th meaningless comment you've left on an issue in the last 20 minutes. If it continues, I'll bloc you from leaving more.\n<issue_comment>username_2: Yes sure \ud83d\ude0a\n<issue_comment>username_2: What kind block, is it a new feature of React\n\nOn Mon, Feb 22, 2021, 19:45 Roy Thapa <anum.roy123@gmail.com> wrote:\n\n> Yes sure \ud83d\ude0a\n>\n>\n<issue_comment>username_3: To follow up on this, we've changed the logging behavior in 18 to be more intuitive.\r\nhttps://github.com/facebook/react/issues/21783#issuecomment-1083412766"}
{"repo": "facebook/react", "issue_id": 813286469, "issue_number": 20857, "timestamp": "2022-03-30 17:41:20+00:00", "text": "To follow up on this, we've changed the logging behavior in 18 to be more intuitive.\r\nhttps://github.com/facebook/react/issues/21783#issuecomment-1083412766", "context": "<issue_start><issue_comment>Title: Bug: Issue with React.StrictMode\nusername_0: https://stackoverflow.com/questions/66306497/props-have-different-value-for-randomly-generated-ones-in-parent-react-js\r\n\r\nReact.StrictMode is changing the behaviour of logs in React App.\r\nReproducible here :  \r\nhttps://codesandbox.io/s/elegant-ellis-glhq7?file=/src/index.js (with StrictMode)\r\nhttps://stackblitz.com/edit/react-c2w4ay?file=src%2FApp.js (without StrictMode)\r\n\r\nIn both these cases the logs behaviour are different\n<issue_comment>username_1: This isn't a bug. This was the expected behavior as of PR #18547.\r\n\r\nIt's been discussed on a few other issues, e.g. #20090 and #17786.\r\n\r\nI'm going to close this issue since it doesn't add anything new. Feel free to leave a comment with this use case on one of those other issues though (or the PR).<issue_closed>\n<issue_comment>username_2: Error bug found\n<issue_comment>username_1: Please don't leave the same comment on multiple issues, @username_2.\n<issue_comment>username_2: Same as well\n<issue_comment>username_1: @username_2 This is the 5th meaningless comment you've left on an issue in the last 20 minutes. If it continues, I'll bloc you from leaving more.\n<issue_comment>username_2: Yes sure \ud83d\ude0a\n<issue_comment>username_2: What kind block, is it a new feature of React\n\nOn Mon, Feb 22, 2021, 19:45 Roy Thapa <anum.roy123@gmail.com> wrote:\n\n> Yes sure \ud83d\ude0a\n>\n>\n<issue_comment>username_3: To follow up on this, we've changed the logging behavior in 18 to be more intuitive.\r\nhttps://github.com/facebook/react/issues/21783#issuecomment-1083412766"}
{"repo": "pytorch/pytorch", "issue_id": 817748716, "issue_number": 52953, "timestamp": "2021-02-26T23:31:44Z", "text": "Fixes #5255\r\n\r\nThe current PR is a WIP. I tried to implement the `cumulative_trapezoid`.\r\n\r\nI saw that combining both the numpy implementation of trapz https://github.com/numpy/numpy/blob/master/numpy/lib/function_base.py#L4082 and the scipy implementation of the cumulative trapezoid would yield that to implement the cumulative trapezoid, we will just have to add the cumulative sum step at the end.\r\n\r\nI submitted this PR to get feedback about my approach. After that the next steps for this PR will be:\r\n1. Implement the other version of the same function when `dx` is constant.\r\n2. Implement another version of the function that take the `intial` parameter, similar to the scipy function.\r\n3. Add tests.", "context": "<issue_start><issue_comment>Title: [WIP] Implement `cumulative_trapezoid` operator\nusername_0: Fixes #5255\r\n\r\nThe current PR is a WIP. I tried to implement the `cumulative_trapezoid`.\r\n\r\nI saw that combining both the numpy implementation of trapz https://github.com/numpy/numpy/blob/master/numpy/lib/function_base.py#L4082 and the scipy implementation of the cumulative trapezoid would yield that to implement the cumulative trapezoid, we will just have to add the cumulative sum step at the end.\r\n\r\nI submitted this PR to get feedback about my approach. After that the next steps for this PR will be:\r\n1. Implement the other version of the same function when `dx` is constant.\r\n2. Implement another version of the function that take the `intial` parameter, similar to the scipy function.\r\n3. Add tests.\n<issue_comment>username_1: This is a good start @username_0. The most important thing to add next is probably a test that compares with the SciPy function.\n<issue_comment>username_0: Thanks a lot for reviewing this. Definitely that needs a lot of documentation and testing + adding `initial` argument to support the scipy behavior. That will be my next step\n<issue_comment>username_2: This operator was implemented in #61615."}
{"repo": "pytorch/pytorch", "issue_id": 817748716, "issue_number": 52953, "timestamp": "2021-03-01 15:04:48+00:00", "text": "This is a good start @username_0. The most important thing to add next is probably a test that compares with the SciPy function.", "context": "<issue_start><issue_comment>Title: [WIP] Implement `cumulative_trapezoid` operator\nusername_0: Fixes #5255\r\n\r\nThe current PR is a WIP. I tried to implement the `cumulative_trapezoid`.\r\n\r\nI saw that combining both the numpy implementation of trapz https://github.com/numpy/numpy/blob/master/numpy/lib/function_base.py#L4082 and the scipy implementation of the cumulative trapezoid would yield that to implement the cumulative trapezoid, we will just have to add the cumulative sum step at the end.\r\n\r\nI submitted this PR to get feedback about my approach. After that the next steps for this PR will be:\r\n1. Implement the other version of the same function when `dx` is constant.\r\n2. Implement another version of the function that take the `intial` parameter, similar to the scipy function.\r\n3. Add tests.\n<issue_comment>username_1: This is a good start @username_0. The most important thing to add next is probably a test that compares with the SciPy function.\n<issue_comment>username_0: Thanks a lot for reviewing this. Definitely that needs a lot of documentation and testing + adding `initial` argument to support the scipy behavior. That will be my next step\n<issue_comment>username_2: This operator was implemented in #61615."}
{"repo": "pytorch/pytorch", "issue_id": 817748716, "issue_number": 52953, "timestamp": "2021-03-01 23:15:51+00:00", "text": "Thanks a lot for reviewing this. Definitely that needs a lot of documentation and testing + adding `initial` argument to support the scipy behavior. That will be my next step", "context": "<issue_start><issue_comment>Title: [WIP] Implement `cumulative_trapezoid` operator\nusername_0: Fixes #5255\r\n\r\nThe current PR is a WIP. I tried to implement the `cumulative_trapezoid`.\r\n\r\nI saw that combining both the numpy implementation of trapz https://github.com/numpy/numpy/blob/master/numpy/lib/function_base.py#L4082 and the scipy implementation of the cumulative trapezoid would yield that to implement the cumulative trapezoid, we will just have to add the cumulative sum step at the end.\r\n\r\nI submitted this PR to get feedback about my approach. After that the next steps for this PR will be:\r\n1. Implement the other version of the same function when `dx` is constant.\r\n2. Implement another version of the function that take the `intial` parameter, similar to the scipy function.\r\n3. Add tests.\n<issue_comment>username_1: This is a good start @username_0. The most important thing to add next is probably a test that compares with the SciPy function.\n<issue_comment>username_0: Thanks a lot for reviewing this. Definitely that needs a lot of documentation and testing + adding `initial` argument to support the scipy behavior. That will be my next step\n<issue_comment>username_2: This operator was implemented in #61615."}
{"repo": "pytorch/pytorch", "issue_id": 817748716, "issue_number": 52953, "timestamp": "2021-08-03 15:06:31+00:00", "text": "This operator was implemented in #61615.", "context": "<issue_start><issue_comment>Title: [WIP] Implement `cumulative_trapezoid` operator\nusername_0: Fixes #5255\r\n\r\nThe current PR is a WIP. I tried to implement the `cumulative_trapezoid`.\r\n\r\nI saw that combining both the numpy implementation of trapz https://github.com/numpy/numpy/blob/master/numpy/lib/function_base.py#L4082 and the scipy implementation of the cumulative trapezoid would yield that to implement the cumulative trapezoid, we will just have to add the cumulative sum step at the end.\r\n\r\nI submitted this PR to get feedback about my approach. After that the next steps for this PR will be:\r\n1. Implement the other version of the same function when `dx` is constant.\r\n2. Implement another version of the function that take the `intial` parameter, similar to the scipy function.\r\n3. Add tests.\n<issue_comment>username_1: This is a good start @username_0. The most important thing to add next is probably a test that compares with the SciPy function.\n<issue_comment>username_0: Thanks a lot for reviewing this. Definitely that needs a lot of documentation and testing + adding `initial` argument to support the scipy behavior. That will be my next step\n<issue_comment>username_2: This operator was implemented in #61615."}
{"repo": "pytorch/pytorch", "issue_id": 863241380, "issue_number": 56532, "timestamp": "2021-04-20T21:05:09Z", "text": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* (to be filled)\n\nThis PR fixes a subtle issue with the finalizer implementation of `_PeriodicTimer`.\n\nWe avoid using a regular finalizer (a.k.a. `__del__`) for stopping the timer as joining a daemon thread during the interpreter shutdown can cause deadlocks. The `weakref.finalize` is a superior alternative that provides a consistent behavior regardless of the GC implementation.\n\nDifferential Revision: [D27889289](https://our.internmc.facebook.com/intern/diff/D27889289/)", "context": "<issue_start><issue_comment>Title: [4/n] [torch/elastic] Fix the finalizer of PeriodicTimer\nusername_0: Stack from [ghstack](https://github.com/ezyang/ghstack):\n* (to be filled)\n\nThis PR fixes a subtle issue with the finalizer implementation of `_PeriodicTimer`.\n\nWe avoid using a regular finalizer (a.k.a. `__del__`) for stopping the timer as joining a daemon thread during the interpreter shutdown can cause deadlocks. The `weakref.finalize` is a superior alternative that provides a consistent behavior regardless of the GC implementation.\n\nDifferential Revision: [D27889289](https://our.internmc.facebook.com/intern/diff/D27889289/)"}
{"repo": "pytorch/pytorch", "issue_id": 933116811, "issue_number": 60991, "timestamp": "2021-06-29 21:25:58+00:00", "text": "Here are two APIs that are needed to bridge the scheduling gap in reductions.\r\n  1. API to shuffle statements in a block\r\n  2. API to compress all the buffers in the kernel.\r\n\r\nThanks to @bertmaher for pointing these out.", "context": "<issue_start><issue_comment>Title: [NNC] APIs needed to bridge scheduling gap in reductions\nusername_0: Here are two APIs that are needed to bridge the scheduling gap in reductions.\r\n  1. API to shuffle statements in a block\r\n  2. API to compress all the buffers in the kernel.\r\n\r\nThanks to @bertmaher for pointing these out."}
{"repo": "pytorch/pytorch", "issue_id": 941068492, "issue_number": 61480, "timestamp": "2021-07-09T21:13:03Z", "text": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* (to be filled)\n\nAppend mobile_info.json and producer_info.json into extra_files and parse the jsons from \u201cmodel_info.json\u201d in onExitLoadModel.\n\nDifferential Revision: [D29608014](https://our.internmc.facebook.com/intern/diff/D29608014/)\n\n**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D29608014/)!", "context": "<issue_start><issue_comment>Title: Apply for MOBILE_MODULE_LOAD_STATS Logging\nusername_0: Stack from [ghstack](https://github.com/ezyang/ghstack):\n* (to be filled)\n\nAppend mobile_info.json and producer_info.json into extra_files and parse the jsons from \u201cmodel_info.json\u201d in onExitLoadModel.\n\nDifferential Revision: [D29608014](https://our.internmc.facebook.com/intern/diff/D29608014/)\n\n**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D29608014/)!"}
{"repo": "pytorch/pytorch", "issue_id": 988233250, "issue_number": 64502, "timestamp": "2021-09-04 07:25:55+00:00", "text": "```python\r\nimport torch\r\nimport numpy as np\r\n\r\nt = torch.tensor([1, 0, 1, 0])\r\n# t = torch.tensor([[1, 0, 1, 0], [0, 1, 0, 0]])\r\n# t = torch.ones(3, 3, 1)\r\n\r\ntorch.testing.assert_allclose(torch.nonzero(t), np.argwhere(t.numpy()), rtol=0, atol=0)\r\ntorch.testing.assert_allclose(torch.nonzero(t), np.nonzero(t.numpy()), rtol=0, atol=0) # error\r\n```", "context": "<issue_start><issue_comment>Title: [numpy] torch.nonzero is similar np.argwhere not np.nonzero\nusername_0: ```python\r\nimport torch\r\nimport numpy as np\r\n\r\nt = torch.tensor([1, 0, 1, 0])\r\n# t = torch.tensor([[1, 0, 1, 0], [0, 1, 0, 0]])\r\n# t = torch.ones(3, 3, 1)\r\n\r\ntorch.testing.assert_allclose(torch.nonzero(t), np.argwhere(t.numpy()), rtol=0, atol=0)\r\ntorch.testing.assert_allclose(torch.nonzero(t), np.nonzero(t.numpy()), rtol=0, atol=0) # error\r\n```\n<issue_comment>username_1: cc @saketh-are who's been looking at implementing argwhere\n<issue_comment>username_0: I have a [draft](https://github.com/pytorch/pytorch/pull/64257) for the same\n<issue_comment>username_2: It doesn't match the numpy default because of the default value of the `as_tuple` keyword. That is discussed in detail in gh-45499.\n<issue_comment>username_2: For the decision of making a backwards-incompatible change to `nonzero` or not, see gh-68174.\n<issue_comment>username_1: Note we now have `torch.argwhere`: https://pytorch.org/docs/master/generated/torch.argwhere.html?highlight=argwhere#torch.argwhere."}
{"repo": "pytorch/pytorch", "issue_id": 988233250, "issue_number": 64502, "timestamp": "2021-09-05 02:45:31+00:00", "text": "cc @saketh-are who's been looking at implementing argwhere", "context": "<issue_start><issue_comment>Title: [numpy] torch.nonzero is similar np.argwhere not np.nonzero\nusername_0: ```python\r\nimport torch\r\nimport numpy as np\r\n\r\nt = torch.tensor([1, 0, 1, 0])\r\n# t = torch.tensor([[1, 0, 1, 0], [0, 1, 0, 0]])\r\n# t = torch.ones(3, 3, 1)\r\n\r\ntorch.testing.assert_allclose(torch.nonzero(t), np.argwhere(t.numpy()), rtol=0, atol=0)\r\ntorch.testing.assert_allclose(torch.nonzero(t), np.nonzero(t.numpy()), rtol=0, atol=0) # error\r\n```\n<issue_comment>username_1: cc @saketh-are who's been looking at implementing argwhere\n<issue_comment>username_0: I have a [draft](https://github.com/pytorch/pytorch/pull/64257) for the same\n<issue_comment>username_2: It doesn't match the numpy default because of the default value of the `as_tuple` keyword. That is discussed in detail in gh-45499.\n<issue_comment>username_2: For the decision of making a backwards-incompatible change to `nonzero` or not, see gh-68174.\n<issue_comment>username_1: Note we now have `torch.argwhere`: https://pytorch.org/docs/master/generated/torch.argwhere.html?highlight=argwhere#torch.argwhere."}
{"repo": "pytorch/pytorch", "issue_id": 988233250, "issue_number": 64502, "timestamp": "2021-09-05 05:23:49+00:00", "text": "I have a [draft](https://github.com/pytorch/pytorch/pull/64257) for the same", "context": "<issue_start><issue_comment>Title: [numpy] torch.nonzero is similar np.argwhere not np.nonzero\nusername_0: ```python\r\nimport torch\r\nimport numpy as np\r\n\r\nt = torch.tensor([1, 0, 1, 0])\r\n# t = torch.tensor([[1, 0, 1, 0], [0, 1, 0, 0]])\r\n# t = torch.ones(3, 3, 1)\r\n\r\ntorch.testing.assert_allclose(torch.nonzero(t), np.argwhere(t.numpy()), rtol=0, atol=0)\r\ntorch.testing.assert_allclose(torch.nonzero(t), np.nonzero(t.numpy()), rtol=0, atol=0) # error\r\n```\n<issue_comment>username_1: cc @saketh-are who's been looking at implementing argwhere\n<issue_comment>username_0: I have a [draft](https://github.com/pytorch/pytorch/pull/64257) for the same\n<issue_comment>username_2: It doesn't match the numpy default because of the default value of the `as_tuple` keyword. That is discussed in detail in gh-45499.\n<issue_comment>username_2: For the decision of making a backwards-incompatible change to `nonzero` or not, see gh-68174.\n<issue_comment>username_1: Note we now have `torch.argwhere`: https://pytorch.org/docs/master/generated/torch.argwhere.html?highlight=argwhere#torch.argwhere."}
{"repo": "pytorch/pytorch", "issue_id": 988233250, "issue_number": 64502, "timestamp": "2021-09-05 19:03:45+00:00", "text": "It doesn't match the numpy default because of the default value of the `as_tuple` keyword. That is discussed in detail in gh-45499.", "context": "<issue_start><issue_comment>Title: [numpy] torch.nonzero is similar np.argwhere not np.nonzero\nusername_0: ```python\r\nimport torch\r\nimport numpy as np\r\n\r\nt = torch.tensor([1, 0, 1, 0])\r\n# t = torch.tensor([[1, 0, 1, 0], [0, 1, 0, 0]])\r\n# t = torch.ones(3, 3, 1)\r\n\r\ntorch.testing.assert_allclose(torch.nonzero(t), np.argwhere(t.numpy()), rtol=0, atol=0)\r\ntorch.testing.assert_allclose(torch.nonzero(t), np.nonzero(t.numpy()), rtol=0, atol=0) # error\r\n```\n<issue_comment>username_1: cc @saketh-are who's been looking at implementing argwhere\n<issue_comment>username_0: I have a [draft](https://github.com/pytorch/pytorch/pull/64257) for the same\n<issue_comment>username_2: It doesn't match the numpy default because of the default value of the `as_tuple` keyword. That is discussed in detail in gh-45499.\n<issue_comment>username_2: For the decision of making a backwards-incompatible change to `nonzero` or not, see gh-68174.\n<issue_comment>username_1: Note we now have `torch.argwhere`: https://pytorch.org/docs/master/generated/torch.argwhere.html?highlight=argwhere#torch.argwhere."}
{"repo": "pytorch/pytorch", "issue_id": 988233250, "issue_number": 64502, "timestamp": "2022-01-03 17:05:06+00:00", "text": "For the decision of making a backwards-incompatible change to `nonzero` or not, see gh-68174.", "context": "<issue_start><issue_comment>Title: [numpy] torch.nonzero is similar np.argwhere not np.nonzero\nusername_0: ```python\r\nimport torch\r\nimport numpy as np\r\n\r\nt = torch.tensor([1, 0, 1, 0])\r\n# t = torch.tensor([[1, 0, 1, 0], [0, 1, 0, 0]])\r\n# t = torch.ones(3, 3, 1)\r\n\r\ntorch.testing.assert_allclose(torch.nonzero(t), np.argwhere(t.numpy()), rtol=0, atol=0)\r\ntorch.testing.assert_allclose(torch.nonzero(t), np.nonzero(t.numpy()), rtol=0, atol=0) # error\r\n```\n<issue_comment>username_1: cc @saketh-are who's been looking at implementing argwhere\n<issue_comment>username_0: I have a [draft](https://github.com/pytorch/pytorch/pull/64257) for the same\n<issue_comment>username_2: It doesn't match the numpy default because of the default value of the `as_tuple` keyword. That is discussed in detail in gh-45499.\n<issue_comment>username_2: For the decision of making a backwards-incompatible change to `nonzero` or not, see gh-68174.\n<issue_comment>username_1: Note we now have `torch.argwhere`: https://pytorch.org/docs/master/generated/torch.argwhere.html?highlight=argwhere#torch.argwhere."}
{"repo": "pytorch/pytorch", "issue_id": 988233250, "issue_number": 64502, "timestamp": "2022-01-26 06:23:10+00:00", "text": "Note we now have `torch.argwhere`: https://pytorch.org/docs/master/generated/torch.argwhere.html?highlight=argwhere#torch.argwhere.", "context": "<issue_start><issue_comment>Title: [numpy] torch.nonzero is similar np.argwhere not np.nonzero\nusername_0: ```python\r\nimport torch\r\nimport numpy as np\r\n\r\nt = torch.tensor([1, 0, 1, 0])\r\n# t = torch.tensor([[1, 0, 1, 0], [0, 1, 0, 0]])\r\n# t = torch.ones(3, 3, 1)\r\n\r\ntorch.testing.assert_allclose(torch.nonzero(t), np.argwhere(t.numpy()), rtol=0, atol=0)\r\ntorch.testing.assert_allclose(torch.nonzero(t), np.nonzero(t.numpy()), rtol=0, atol=0) # error\r\n```\n<issue_comment>username_1: cc @saketh-are who's been looking at implementing argwhere\n<issue_comment>username_0: I have a [draft](https://github.com/pytorch/pytorch/pull/64257) for the same\n<issue_comment>username_2: It doesn't match the numpy default because of the default value of the `as_tuple` keyword. That is discussed in detail in gh-45499.\n<issue_comment>username_2: For the decision of making a backwards-incompatible change to `nonzero` or not, see gh-68174.\n<issue_comment>username_1: Note we now have `torch.argwhere`: https://pytorch.org/docs/master/generated/torch.argwhere.html?highlight=argwhere#torch.argwhere."}
{"repo": "pytorch/pytorch", "issue_id": 1067664370, "issue_number": 69122, "timestamp": "2021-11-30T20:51:54Z", "text": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* (to be filled)\n\nSummary:\n\nForking this for now so that we can make changes as we need, the changes can be merged back to torch.fx\nlater\n\nTest Plan:\n```\npython test/test_quantization.py TestQuantizeFx\npython test/test_quantization.py TestQuantizeFxOps\n```\n\nImported from OSS\n\nReviewed By: vkuzo\n\nDifferential Revision: D32537713\n\nfbshipit-source-id: 326598d13645fcc28ef2c66baaac6a077b80fd0c", "context": "<issue_start><issue_comment>Title: [quant][graphmode][fx] Fork subgraph_rewriter from torch.fx to quantization (#68228)\nusername_0: Stack from [ghstack](https://github.com/ezyang/ghstack):\n* (to be filled)\n\nSummary:\n\nForking this for now so that we can make changes as we need, the changes can be merged back to torch.fx\nlater\n\nTest Plan:\n```\npython test/test_quantization.py TestQuantizeFx\npython test/test_quantization.py TestQuantizeFxOps\n```\n\nImported from OSS\n\nReviewed By: vkuzo\n\nDifferential Revision: D32537713\n\nfbshipit-source-id: 326598d13645fcc28ef2c66baaac6a077b80fd0c"}
{"repo": "facebook/react", "issue_id": 110798082, "issue_number": 5121, "timestamp": "2015-10-10 14:40:56+00:00", "text": "Did not know where to post this best.\r\n\r\nSince some weeks, the layout looks broken on https://facebook.github.io/react/html-jsx.html\r\n\r\nGoogle Chrome 43, Linux\r\n\r\nScreenshot:\r\n![selection_003](https://cloud.githubusercontent.com/assets/417542/10411670/7a7529c8-6f6d-11e5-9c97-ef8c2d1b3755.png)", "context": "<issue_start><issue_comment>Title: Page messed up: facebook.github.io/react/html-jsx.html\nusername_0: Did not know where to post this best.\r\n\r\nSince some weeks, the layout looks broken on https://facebook.github.io/react/html-jsx.html\r\n\r\nGoogle Chrome 43, Linux\r\n\r\nScreenshot:\r\n![selection_003](https://cloud.githubusercontent.com/assets/417542/10411670/7a7529c8-6f6d-11e5-9c97-ef8c2d1b3755.png)\n<issue_comment>username_1: Not Linux specific; broken in all browsers / operating systems.  This is probably a good first bug if someone wants to submit a pull request.\n<issue_comment>username_2: Closed in #5123.<issue_closed>"}
{"repo": "facebook/react", "issue_id": 110798082, "issue_number": 5121, "timestamp": "2015-10-10 17:16:35+00:00", "text": "Not Linux specific; broken in all browsers / operating systems.  This is probably a good first bug if someone wants to submit a pull request.", "context": "<issue_start><issue_comment>Title: Page messed up: facebook.github.io/react/html-jsx.html\nusername_0: Did not know where to post this best.\r\n\r\nSince some weeks, the layout looks broken on https://facebook.github.io/react/html-jsx.html\r\n\r\nGoogle Chrome 43, Linux\r\n\r\nScreenshot:\r\n![selection_003](https://cloud.githubusercontent.com/assets/417542/10411670/7a7529c8-6f6d-11e5-9c97-ef8c2d1b3755.png)\n<issue_comment>username_1: Not Linux specific; broken in all browsers / operating systems.  This is probably a good first bug if someone wants to submit a pull request.\n<issue_comment>username_2: Closed in #5123.<issue_closed>"}
{"repo": "facebook/react", "issue_id": 110798082, "issue_number": 5121, "timestamp": "2015-10-11 02:16:39+00:00", "text": "Closed in #5123.", "context": "<issue_start><issue_comment>Title: Page messed up: facebook.github.io/react/html-jsx.html\nusername_0: Did not know where to post this best.\r\n\r\nSince some weeks, the layout looks broken on https://facebook.github.io/react/html-jsx.html\r\n\r\nGoogle Chrome 43, Linux\r\n\r\nScreenshot:\r\n![selection_003](https://cloud.githubusercontent.com/assets/417542/10411670/7a7529c8-6f6d-11e5-9c97-ef8c2d1b3755.png)\n<issue_comment>username_1: Not Linux specific; broken in all browsers / operating systems.  This is probably a good first bug if someone wants to submit a pull request.\n<issue_comment>username_2: Closed in #5123.<issue_closed>"}
{"repo": "facebook/react", "issue_id": 110798082, "issue_number": 5121, "timestamp": "2015-10-11 22:01:22+00:00", "text": "", "context": "<issue_start><issue_comment>Title: Page messed up: facebook.github.io/react/html-jsx.html\nusername_0: Did not know where to post this best.\r\n\r\nSince some weeks, the layout looks broken on https://facebook.github.io/react/html-jsx.html\r\n\r\nGoogle Chrome 43, Linux\r\n\r\nScreenshot:\r\n![selection_003](https://cloud.githubusercontent.com/assets/417542/10411670/7a7529c8-6f6d-11e5-9c97-ef8c2d1b3755.png)\n<issue_comment>username_1: Not Linux specific; broken in all browsers / operating systems.  This is probably a good first bug if someone wants to submit a pull request.\n<issue_comment>username_2: Closed in #5123.<issue_closed>"}
{"repo": "facebook/react", "issue_id": 260420589, "issue_number": 10821, "timestamp": "2017-09-25T21:27:03Z", "text": "This PR updates the [Thinking In React example](https://facebook.github.io/react/docs/thinking-in-react.html) by moving the code that filters the products from the `ProductTable` component to the `FilterableProductTable` component. See issue #10808 for more details.", "context": "<issue_start><issue_comment>Title: Update thinking-in-react.md to filter products in FilterableProductTable\nusername_0: This PR updates the [Thinking In React example](https://facebook.github.io/react/docs/thinking-in-react.html) by moving the code that filters the products from the `ProductTable` component to the `FilterableProductTable` component. See issue #10808 for more details.\n<issue_comment>username_0: Looks like #10846 created some conflicts with this PR - specifically with the embedded CodePen examples. I will update this PR.\n<issue_comment>username_1: Thank you for submitting this PR! \ud83d\ude04 \r\n\r\nUnfortunately the documentation and source code for reactjs.org has been moved to a different repository: https://github.com/reactjs/reactjs.org\r\n\r\nIf you are willing, please open a new PR there. Sorry for the inconvenience!\r\n\r\n(For more backstory on why we moved to a new repo, see issue #11075)"}
{"repo": "facebook/react", "issue_id": 260420589, "issue_number": 10821, "timestamp": "2017-09-27 16:45:38+00:00", "text": "Looks like #10846 created some conflicts with this PR - specifically with the embedded CodePen examples. I will update this PR.", "context": "<issue_start><issue_comment>Title: Update thinking-in-react.md to filter products in FilterableProductTable\nusername_0: This PR updates the [Thinking In React example](https://facebook.github.io/react/docs/thinking-in-react.html) by moving the code that filters the products from the `ProductTable` component to the `FilterableProductTable` component. See issue #10808 for more details.\n<issue_comment>username_0: Looks like #10846 created some conflicts with this PR - specifically with the embedded CodePen examples. I will update this PR.\n<issue_comment>username_1: Thank you for submitting this PR! \ud83d\ude04 \r\n\r\nUnfortunately the documentation and source code for reactjs.org has been moved to a different repository: https://github.com/reactjs/reactjs.org\r\n\r\nIf you are willing, please open a new PR there. Sorry for the inconvenience!\r\n\r\n(For more backstory on why we moved to a new repo, see issue #11075)"}
{"repo": "facebook/react", "issue_id": 260420589, "issue_number": 10821, "timestamp": "2017-10-06 17:19:57+00:00", "text": "Thank you for submitting this PR! \ud83d\ude04 \r\n\r\nUnfortunately the documentation and source code for reactjs.org has been moved to a different repository: https://github.com/reactjs/reactjs.org\r\n\r\nIf you are willing, please open a new PR there. Sorry for the inconvenience!\r\n\r\n(For more backstory on why we moved to a new repo, see issue #11075)", "context": "<issue_start><issue_comment>Title: Update thinking-in-react.md to filter products in FilterableProductTable\nusername_0: This PR updates the [Thinking In React example](https://facebook.github.io/react/docs/thinking-in-react.html) by moving the code that filters the products from the `ProductTable` component to the `FilterableProductTable` component. See issue #10808 for more details.\n<issue_comment>username_0: Looks like #10846 created some conflicts with this PR - specifically with the embedded CodePen examples. I will update this PR.\n<issue_comment>username_1: Thank you for submitting this PR! \ud83d\ude04 \r\n\r\nUnfortunately the documentation and source code for reactjs.org has been moved to a different repository: https://github.com/reactjs/reactjs.org\r\n\r\nIf you are willing, please open a new PR there. Sorry for the inconvenience!\r\n\r\n(For more backstory on why we moved to a new repo, see issue #11075)"}
{"repo": "facebook/react", "issue_id": 272851505, "issue_number": 11510, "timestamp": "2017-11-10 08:08:26+00:00", "text": "See https://github.com/reactjs/reactjs.org/issues/268#issuecomment-343402728.\r\n\r\nFeels like an easy mistake to make, and easy for us to detect.", "context": "<issue_start><issue_comment>Title: Warn when component has PropTypes rather than propTypes\nusername_0: See https://github.com/reactjs/reactjs.org/issues/268#issuecomment-343402728.\r\n\r\nFeels like an easy mistake to make, and easy for us to detect.\n<issue_comment>username_1: hey can I take this ?\n<issue_comment>username_2: @username_1  Just do it.\n<issue_comment>username_1: could you please guide me to where to start with this issue? thank you\n<issue_comment>username_0: Search the code for where we read `.propTypes` and add the check there.\n<issue_comment>username_1: got it.\n<issue_comment>username_3: is this done?\n<issue_comment>username_0: It's taken by @username_1.\n<issue_comment>username_4: @username_0 Should this be closed now?<issue_closed>"}
{"repo": "facebook/react", "issue_id": 272851505, "issue_number": 11510, "timestamp": "2017-11-10 08:13:11+00:00", "text": "hey can I take this ?", "context": "<issue_start><issue_comment>Title: Warn when component has PropTypes rather than propTypes\nusername_0: See https://github.com/reactjs/reactjs.org/issues/268#issuecomment-343402728.\r\n\r\nFeels like an easy mistake to make, and easy for us to detect.\n<issue_comment>username_1: hey can I take this ?\n<issue_comment>username_2: @username_1  Just do it.\n<issue_comment>username_1: could you please guide me to where to start with this issue? thank you\n<issue_comment>username_0: Search the code for where we read `.propTypes` and add the check there.\n<issue_comment>username_1: got it.\n<issue_comment>username_3: is this done?\n<issue_comment>username_0: It's taken by @username_1.\n<issue_comment>username_4: @username_0 Should this be closed now?<issue_closed>"}
{"repo": "facebook/react", "issue_id": 272851505, "issue_number": 11510, "timestamp": "2017-11-10 09:00:26+00:00", "text": "@username_1  Just do it.", "context": "<issue_start><issue_comment>Title: Warn when component has PropTypes rather than propTypes\nusername_0: See https://github.com/reactjs/reactjs.org/issues/268#issuecomment-343402728.\r\n\r\nFeels like an easy mistake to make, and easy for us to detect.\n<issue_comment>username_1: hey can I take this ?\n<issue_comment>username_2: @username_1  Just do it.\n<issue_comment>username_1: could you please guide me to where to start with this issue? thank you\n<issue_comment>username_0: Search the code for where we read `.propTypes` and add the check there.\n<issue_comment>username_1: got it.\n<issue_comment>username_3: is this done?\n<issue_comment>username_0: It's taken by @username_1.\n<issue_comment>username_4: @username_0 Should this be closed now?<issue_closed>"}
{"repo": "facebook/react", "issue_id": 272851505, "issue_number": 11510, "timestamp": "2017-11-10 09:04:02+00:00", "text": "could you please guide me to where to start with this issue? thank you", "context": "<issue_start><issue_comment>Title: Warn when component has PropTypes rather than propTypes\nusername_0: See https://github.com/reactjs/reactjs.org/issues/268#issuecomment-343402728.\r\n\r\nFeels like an easy mistake to make, and easy for us to detect.\n<issue_comment>username_1: hey can I take this ?\n<issue_comment>username_2: @username_1  Just do it.\n<issue_comment>username_1: could you please guide me to where to start with this issue? thank you\n<issue_comment>username_0: Search the code for where we read `.propTypes` and add the check there.\n<issue_comment>username_1: got it.\n<issue_comment>username_3: is this done?\n<issue_comment>username_0: It's taken by @username_1.\n<issue_comment>username_4: @username_0 Should this be closed now?<issue_closed>"}
{"repo": "facebook/react", "issue_id": 272851505, "issue_number": 11510, "timestamp": "2017-11-10 09:10:46+00:00", "text": "Search the code for where we read `.propTypes` and add the check there.", "context": "<issue_start><issue_comment>Title: Warn when component has PropTypes rather than propTypes\nusername_0: See https://github.com/reactjs/reactjs.org/issues/268#issuecomment-343402728.\r\n\r\nFeels like an easy mistake to make, and easy for us to detect.\n<issue_comment>username_1: hey can I take this ?\n<issue_comment>username_2: @username_1  Just do it.\n<issue_comment>username_1: could you please guide me to where to start with this issue? thank you\n<issue_comment>username_0: Search the code for where we read `.propTypes` and add the check there.\n<issue_comment>username_1: got it.\n<issue_comment>username_3: is this done?\n<issue_comment>username_0: It's taken by @username_1.\n<issue_comment>username_4: @username_0 Should this be closed now?<issue_closed>"}
{"repo": "facebook/react", "issue_id": 272851505, "issue_number": 11510, "timestamp": "2017-11-10 09:24:50+00:00", "text": "got it.", "context": "<issue_start><issue_comment>Title: Warn when component has PropTypes rather than propTypes\nusername_0: See https://github.com/reactjs/reactjs.org/issues/268#issuecomment-343402728.\r\n\r\nFeels like an easy mistake to make, and easy for us to detect.\n<issue_comment>username_1: hey can I take this ?\n<issue_comment>username_2: @username_1  Just do it.\n<issue_comment>username_1: could you please guide me to where to start with this issue? thank you\n<issue_comment>username_0: Search the code for where we read `.propTypes` and add the check there.\n<issue_comment>username_1: got it.\n<issue_comment>username_3: is this done?\n<issue_comment>username_0: It's taken by @username_1.\n<issue_comment>username_4: @username_0 Should this be closed now?<issue_closed>"}
{"repo": "facebook/react", "issue_id": 272851505, "issue_number": 11510, "timestamp": "2017-11-10 19:33:59+00:00", "text": "is this done?", "context": "<issue_start><issue_comment>Title: Warn when component has PropTypes rather than propTypes\nusername_0: See https://github.com/reactjs/reactjs.org/issues/268#issuecomment-343402728.\r\n\r\nFeels like an easy mistake to make, and easy for us to detect.\n<issue_comment>username_1: hey can I take this ?\n<issue_comment>username_2: @username_1  Just do it.\n<issue_comment>username_1: could you please guide me to where to start with this issue? thank you\n<issue_comment>username_0: Search the code for where we read `.propTypes` and add the check there.\n<issue_comment>username_1: got it.\n<issue_comment>username_3: is this done?\n<issue_comment>username_0: It's taken by @username_1.\n<issue_comment>username_4: @username_0 Should this be closed now?<issue_closed>"}
{"repo": "facebook/react", "issue_id": 272851505, "issue_number": 11510, "timestamp": "2017-11-10 19:38:23+00:00", "text": "It's taken by @username_1.", "context": "<issue_start><issue_comment>Title: Warn when component has PropTypes rather than propTypes\nusername_0: See https://github.com/reactjs/reactjs.org/issues/268#issuecomment-343402728.\r\n\r\nFeels like an easy mistake to make, and easy for us to detect.\n<issue_comment>username_1: hey can I take this ?\n<issue_comment>username_2: @username_1  Just do it.\n<issue_comment>username_1: could you please guide me to where to start with this issue? thank you\n<issue_comment>username_0: Search the code for where we read `.propTypes` and add the check there.\n<issue_comment>username_1: got it.\n<issue_comment>username_3: is this done?\n<issue_comment>username_0: It's taken by @username_1.\n<issue_comment>username_4: @username_0 Should this be closed now?<issue_closed>"}
{"repo": "facebook/react", "issue_id": 272851505, "issue_number": 11510, "timestamp": "2017-12-05 06:25:05+00:00", "text": "@username_0 Should this be closed now?", "context": "<issue_start><issue_comment>Title: Warn when component has PropTypes rather than propTypes\nusername_0: See https://github.com/reactjs/reactjs.org/issues/268#issuecomment-343402728.\r\n\r\nFeels like an easy mistake to make, and easy for us to detect.\n<issue_comment>username_1: hey can I take this ?\n<issue_comment>username_2: @username_1  Just do it.\n<issue_comment>username_1: could you please guide me to where to start with this issue? thank you\n<issue_comment>username_0: Search the code for where we read `.propTypes` and add the check there.\n<issue_comment>username_1: got it.\n<issue_comment>username_3: is this done?\n<issue_comment>username_0: It's taken by @username_1.\n<issue_comment>username_4: @username_0 Should this be closed now?<issue_closed>"}
{"repo": "facebook/react", "issue_id": 272851505, "issue_number": 11510, "timestamp": "2017-12-05 11:25:10+00:00", "text": "", "context": "<issue_start><issue_comment>Title: Warn when component has PropTypes rather than propTypes\nusername_0: See https://github.com/reactjs/reactjs.org/issues/268#issuecomment-343402728.\r\n\r\nFeels like an easy mistake to make, and easy for us to detect.\n<issue_comment>username_1: hey can I take this ?\n<issue_comment>username_2: @username_1  Just do it.\n<issue_comment>username_1: could you please guide me to where to start with this issue? thank you\n<issue_comment>username_0: Search the code for where we read `.propTypes` and add the check there.\n<issue_comment>username_1: got it.\n<issue_comment>username_3: is this done?\n<issue_comment>username_0: It's taken by @username_1.\n<issue_comment>username_4: @username_0 Should this be closed now?<issue_closed>"}
{"repo": "facebook/react", "issue_id": 290555006, "issue_number": 12072, "timestamp": "2018-01-22T17:45:57Z", "text": "**Do you want to request a *feature* or report a *bug*?**\r\nReporting a bug.\r\n\r\n**What is the current behavior?**\r\nBasically I have many controlled `<textarea/>` fields in the app that I am currently developing and I normally only want them to re-render when their values have changed through their onChange event, but they re-render every time setState() gets called anywhere within the component or any parent component.\r\n\r\nI even tried creating a custom component which only contains a `<textarea/>` and setting shouldComponentUpdate() to return false but it seems to ignore that command completely. It's weird because in this case the console.log()s in my child component's render() function don't get executed but the field gets re-rendered nonetheless.\r\n\r\nThis issue is causing some major performance problems in my app. I hope someone addresses this issue as fast as possible so I can continue working on my react project. Maybe someone can provide a temporary workaround? Please let me know if there's something I'm missing or if I'm using this field incorrectly.\r\n\r\n**If the current behavior is a bug, please provide the steps to reproduce and if possible a minimal demo of the problem. Your bug will get fixed much faster if we can run your code and it doesn't have dependencies other than React. Paste the link to your JSFiddle (https://jsfiddle.net/Luktwrdm/) or CodeSandbox (https://codesandbox.io/s/new) example below:**\r\nI have isolated the problem in a new empty app and you can see the behavior in the following gif:\r\nhttps://gfycat.com/DentalExcitableIndri\r\n\r\nThe code is very simple. You can recreate this problem by creating 2 controlled fields, one `<input/>` and another `<textarea/>`. Then go to your browser, inspect the `<textarea/>` element and input something in the basic input field. The `<textarea/>` will re-render on every new typed letter in the `<input/>` field but not the other way around.\r\n```javascript\r\nimport React, { Component } from 'react';\r\nimport logo from './logo.svg';\r\nimport './App.css';\r\n\r\nclass App extends Component {\r\n\tconstructor(props){\r\n\t\tsuper(props);\r\n\t\tthis.state = {\r\n\t\t\tta1 : \"ta1\",\r\n\t\t\tta2 : \"ta2\",\r\n\t\t\tta3 : \"ta3\",\r\n\t\t\ti1 : \"i1\",\r\n\t\t\ti2 : \"i2\",\r\n\t\t\ti3 : \"i3\"\r\n\t\t};\r\n\t\tthis.handleInputChange = this.handleInputChange.bind(this);\r\n\t}\r\n\r\n\thandleInputChange(event){\r\n\t\tthis.setState({\r\n\t\t\t[event.target.name] : event.target.value\r\n\t\t});\r\n\t}\r\n\r\n\trender() {\r\n\t\treturn (\r\n\t\t\t<div className=\"App\">\r\n\t\t\t\t<header className=\"App-header\">\r\n\t\t\t\t\t<img src={logo} className=\"App-logo\" alt=\"logo\" />\r\n\t\t\t\t\t<h1 className=\"App-title\">Welcome to React</h1>\r\n\t\t\t\t</header>\r\n\t\t\t\t<p className=\"App-intro\">\r\n\t\t\t\t\ttest\r\n\t\t\t\t</p>\r\n\t\t\t\t<div className=\"row\">\r\n\t\t\t\t\t<div className=\"column\">\r\n\t\t\t\t\t\ttextareas\r\n\t\t\t\t\t\t<textarea value={this.state.ta1} name=\"ta1\" onChange={this.handleInputChange}/>\r\n\t\t\t\t\t\t<textarea value={this.state.ta2} name=\"ta2\" onChange={this.handleInputChange}/>\r\n\t\t\t\t\t\t<textarea value={this.state.ta3} name=\"ta3\" onChange={this.handleInputChange}/>\r\n\t\t\t\t\t</div>\r\n\t\t\t\t\t<div className=\"column\">\r\n\t\t\t\t\t\tinput fields\r\n\t\t\t\t\t\t<input value={this.state.i1} name=\"i1\" onChange={this.handleInputChange}/>\r\n\t\t\t\t\t\t<input value={this.state.i2} name=\"i2\" onChange={this.handleInputChange}/>\r\n\t\t\t\t\t\t<input value={this.state.i3} name=\"i3\" onChange={this.handleInputChange}/>\r\n\t\t\t\t\t</div>\r\n\t\t\t\t</div>\r\n\t\t\t</div>\r\n\t\t);\r\n\t}\r\n}\r\n\r\nexport default App;\r\n```\r\n\r\n**What is the expected behavior?**\r\nThe `<textarea/>` field should only re-render when its data is changed.\r\n\r\n**Which versions of React, and which browser / OS are affected by this issue? Did this work in previous versions of React?**\r\nI was able to recreate this problem on React versions 16.2.0 (latest) and 15.6.1. I did not test on other versions so I am not sure if this worked correctly before. I'm using Chrome 62.0.3202.94 64-bit (latest) and Firefox 57.0.4 64-bit (latest) on Ubuntu 14.04.", "context": "<issue_start><issue_comment>Title: All controlled <textarea/> fields re-render on any setState() call even though their data has not changed.\nusername_0: **Do you want to request a *feature* or report a *bug*?**\r\nReporting a bug.\r\n\r\n**What is the current behavior?**\r\nBasically I have many controlled `<textarea/>` fields in the app that I am currently developing and I normally only want them to re-render when their values have changed through their onChange event, but they re-render every time setState() gets called anywhere within the component or any parent component.\r\n\r\nI even tried creating a custom component which only contains a `<textarea/>` and setting shouldComponentUpdate() to return false but it seems to ignore that command completely. It's weird because in this case the console.log()s in my child component's render() function don't get executed but the field gets re-rendered nonetheless.\r\n\r\nThis issue is causing some major performance problems in my app. I hope someone addresses this issue as fast as possible so I can continue working on my react project. Maybe someone can provide a temporary workaround? Please let me know if there's something I'm missing or if I'm using this field incorrectly.\r\n\r\n**If the current behavior is a bug, please provide the steps to reproduce and if possible a minimal demo of the problem. Your bug will get fixed much faster if we can run your code and it doesn't have dependencies other than React. Paste the link to your JSFiddle (https://jsfiddle.net/Luktwrdm/) or CodeSandbox (https://codesandbox.io/s/new) example below:**\r\nI have isolated the problem in a new empty app and you can see the behavior in the following gif:\r\nhttps://gfycat.com/DentalExcitableIndri\r\n\r\nThe code is very simple. You can recreate this problem by creating 2 controlled fields, one `<input/>` and another `<textarea/>`. Then go to your browser, inspect the `<textarea/>` element and input something in the basic input field. The `<textarea/>` will re-render on every new typed letter in the `<input/>` field but not the other way around.\r\n```javascript\r\nimport React, { Component } from 'react';\r\nimport logo from './logo.svg';\r\nimport './App.css';\r\n\r\nclass App extends Component {\r\n\tconstructor(props){\r\n\t\tsuper(props);\r\n\t\tthis.state = {\r\n\t\t\tta1 : \"ta1\",\r\n\t\t\tta2 : \"ta2\",\r\n\t\t\tta3 : \"ta3\",\r\n\t\t\ti1 : \"i1\",\r\n\t\t\ti2 : \"i2\",\r\n\t\t\ti3 : \"i3\"\r\n\t\t};\r\n\t\tthis.handleInputChange = this.handleInputChange.bind(this);\r\n\t}\r\n\r\n\thandleInputChange(event){\r\n\t\tthis.setState({\r\n\t\t\t[event.target.name] : event.target.value\r\n\t\t});\r\n\t}\r\n\r\n\trender() {\r\n\t\treturn (\r\n\t\t\t<div className=\"App\">\r\n\t\t\t\t<header className=\"App-header\">\r\n\t\t\t\t\t<img src={logo} className=\"App-logo\" alt=\"logo\" />\r\n\t\t\t\t\t<h1 className=\"App-title\">Welcome to React</h1>\r\n\t\t\t\t</header>\r\n\t\t\t\t<p className=\"App-intro\">\r\n\t\t\t\t\ttest\r\n\t\t\t\t</p>\r\n\t\t\t\t<div className=\"row\">\r\n\t\t\t\t\t<div className=\"column\">\r\n\t\t\t\t\t\ttextareas\r\n\t\t\t\t\t\t<textarea value={this.state.ta1} name=\"ta1\" onChange={this.handleInputChange}/>\r\n\t\t\t\t\t\t<textarea value={this.state.ta2} name=\"ta2\" onChange={this.handleInputChange}/>\r\n\t\t\t\t\t\t<textarea value={this.state.ta3} name=\"ta3\" onChange={this.handleInputChange}/>\r\n\t\t\t\t\t</div>\r\n\t\t\t\t\t<div className=\"column\">\r\n\t\t\t\t\t\tinput fields\r\n\t\t\t\t\t\t<input value={this.state.i1} name=\"i1\" onChange={this.handleInputChange}/>\r\n\t\t\t\t\t\t<input value={this.state.i2} name=\"i2\" onChange={this.handleInputChange}/>\r\n\t\t\t\t\t\t<input value={this.state.i3} name=\"i3\" onChange={this.handleInputChange}/>\r\n\t\t\t\t\t</div>\r\n\t\t\t\t</div>\r\n\t\t\t</div>\r\n\t\t);\r\n\t}\r\n}\r\n\r\nexport default App;\r\n```\r\n\r\n**What is the expected behavior?**\r\nThe `<textarea/>` field should only re-render when its data is changed.\r\n\r\n**Which versions of React, and which browser / OS are affected by this issue? Did this work in previous versions of React?**\r\nI was able to recreate this problem on React versions 16.2.0 (latest) and 15.6.1. I did not test on other versions so I am not sure if this worked correctly before. I'm using Chrome 62.0.3202.94 64-bit (latest) and Firefox 57.0.4 64-bit (latest) on Ubuntu 14.04.\n<issue_comment>username_1: Do you want to look into why this happens?\n<issue_comment>username_0: I'd love to help, but I'm not sure if I would be able to locate the bug in the source code myself. I'm guessing it has something to do with React's diffing algorithm when updating a component, although I could be wrong.\n<issue_comment>username_2: try to set key for each input and textarea\n<issue_comment>username_3: It looks like this is due to [`updateWrapper` in `ReactDOMTextArea`](https://github.com/facebook/react/blame/master/packages/react-dom/src/client/ReactDOMFiberTextarea.js#L142-L144). It's setting `node.defaultValue` because `props.defaultValue` does not exist, and that's triggering the update. By the same token, it will also [set `node.defaultValue` if `props.defaultValue` is defined](https://github.com/facebook/react/blame/master/packages/react-dom/src/client/ReactDOMFiberTextarea.js#L146-L148), so no matter what it's updating `node.defaultValue` and causing the element to re-attach it's text node child.\n<issue_comment>username_4: @username_0  I think the PR #12080  will resolve the problem.\r\nThanks, @username_3  for the tip +1:\n<issue_comment>username_0: Any ETA on when this change will be pushed to master and released?<issue_closed>"}
{"repo": "facebook/react", "issue_id": 290555006, "issue_number": 12072, "timestamp": "2018-01-22 18:08:28+00:00", "text": "Do you want to look into why this happens?", "context": "<issue_start><issue_comment>Title: All controlled <textarea/> fields re-render on any setState() call even though their data has not changed.\nusername_0: **Do you want to request a *feature* or report a *bug*?**\r\nReporting a bug.\r\n\r\n**What is the current behavior?**\r\nBasically I have many controlled `<textarea/>` fields in the app that I am currently developing and I normally only want them to re-render when their values have changed through their onChange event, but they re-render every time setState() gets called anywhere within the component or any parent component.\r\n\r\nI even tried creating a custom component which only contains a `<textarea/>` and setting shouldComponentUpdate() to return false but it seems to ignore that command completely. It's weird because in this case the console.log()s in my child component's render() function don't get executed but the field gets re-rendered nonetheless.\r\n\r\nThis issue is causing some major performance problems in my app. I hope someone addresses this issue as fast as possible so I can continue working on my react project. Maybe someone can provide a temporary workaround? Please let me know if there's something I'm missing or if I'm using this field incorrectly.\r\n\r\n**If the current behavior is a bug, please provide the steps to reproduce and if possible a minimal demo of the problem. Your bug will get fixed much faster if we can run your code and it doesn't have dependencies other than React. Paste the link to your JSFiddle (https://jsfiddle.net/Luktwrdm/) or CodeSandbox (https://codesandbox.io/s/new) example below:**\r\nI have isolated the problem in a new empty app and you can see the behavior in the following gif:\r\nhttps://gfycat.com/DentalExcitableIndri\r\n\r\nThe code is very simple. You can recreate this problem by creating 2 controlled fields, one `<input/>` and another `<textarea/>`. Then go to your browser, inspect the `<textarea/>` element and input something in the basic input field. The `<textarea/>` will re-render on every new typed letter in the `<input/>` field but not the other way around.\r\n```javascript\r\nimport React, { Component } from 'react';\r\nimport logo from './logo.svg';\r\nimport './App.css';\r\n\r\nclass App extends Component {\r\n\tconstructor(props){\r\n\t\tsuper(props);\r\n\t\tthis.state = {\r\n\t\t\tta1 : \"ta1\",\r\n\t\t\tta2 : \"ta2\",\r\n\t\t\tta3 : \"ta3\",\r\n\t\t\ti1 : \"i1\",\r\n\t\t\ti2 : \"i2\",\r\n\t\t\ti3 : \"i3\"\r\n\t\t};\r\n\t\tthis.handleInputChange = this.handleInputChange.bind(this);\r\n\t}\r\n\r\n\thandleInputChange(event){\r\n\t\tthis.setState({\r\n\t\t\t[event.target.name] : event.target.value\r\n\t\t});\r\n\t}\r\n\r\n\trender() {\r\n\t\treturn (\r\n\t\t\t<div className=\"App\">\r\n\t\t\t\t<header className=\"App-header\">\r\n\t\t\t\t\t<img src={logo} className=\"App-logo\" alt=\"logo\" />\r\n\t\t\t\t\t<h1 className=\"App-title\">Welcome to React</h1>\r\n\t\t\t\t</header>\r\n\t\t\t\t<p className=\"App-intro\">\r\n\t\t\t\t\ttest\r\n\t\t\t\t</p>\r\n\t\t\t\t<div className=\"row\">\r\n\t\t\t\t\t<div className=\"column\">\r\n\t\t\t\t\t\ttextareas\r\n\t\t\t\t\t\t<textarea value={this.state.ta1} name=\"ta1\" onChange={this.handleInputChange}/>\r\n\t\t\t\t\t\t<textarea value={this.state.ta2} name=\"ta2\" onChange={this.handleInputChange}/>\r\n\t\t\t\t\t\t<textarea value={this.state.ta3} name=\"ta3\" onChange={this.handleInputChange}/>\r\n\t\t\t\t\t</div>\r\n\t\t\t\t\t<div className=\"column\">\r\n\t\t\t\t\t\tinput fields\r\n\t\t\t\t\t\t<input value={this.state.i1} name=\"i1\" onChange={this.handleInputChange}/>\r\n\t\t\t\t\t\t<input value={this.state.i2} name=\"i2\" onChange={this.handleInputChange}/>\r\n\t\t\t\t\t\t<input value={this.state.i3} name=\"i3\" onChange={this.handleInputChange}/>\r\n\t\t\t\t\t</div>\r\n\t\t\t\t</div>\r\n\t\t\t</div>\r\n\t\t);\r\n\t}\r\n}\r\n\r\nexport default App;\r\n```\r\n\r\n**What is the expected behavior?**\r\nThe `<textarea/>` field should only re-render when its data is changed.\r\n\r\n**Which versions of React, and which browser / OS are affected by this issue? Did this work in previous versions of React?**\r\nI was able to recreate this problem on React versions 16.2.0 (latest) and 15.6.1. I did not test on other versions so I am not sure if this worked correctly before. I'm using Chrome 62.0.3202.94 64-bit (latest) and Firefox 57.0.4 64-bit (latest) on Ubuntu 14.04.\n<issue_comment>username_1: Do you want to look into why this happens?\n<issue_comment>username_0: I'd love to help, but I'm not sure if I would be able to locate the bug in the source code myself. I'm guessing it has something to do with React's diffing algorithm when updating a component, although I could be wrong.\n<issue_comment>username_2: try to set key for each input and textarea\n<issue_comment>username_3: It looks like this is due to [`updateWrapper` in `ReactDOMTextArea`](https://github.com/facebook/react/blame/master/packages/react-dom/src/client/ReactDOMFiberTextarea.js#L142-L144). It's setting `node.defaultValue` because `props.defaultValue` does not exist, and that's triggering the update. By the same token, it will also [set `node.defaultValue` if `props.defaultValue` is defined](https://github.com/facebook/react/blame/master/packages/react-dom/src/client/ReactDOMFiberTextarea.js#L146-L148), so no matter what it's updating `node.defaultValue` and causing the element to re-attach it's text node child.\n<issue_comment>username_4: @username_0  I think the PR #12080  will resolve the problem.\r\nThanks, @username_3  for the tip +1:\n<issue_comment>username_0: Any ETA on when this change will be pushed to master and released?<issue_closed>"}
{"repo": "facebook/react", "issue_id": 290555006, "issue_number": 12072, "timestamp": "2018-01-22 18:16:29+00:00", "text": "I'd love to help, but I'm not sure if I would be able to locate the bug in the source code myself. I'm guessing it has something to do with React's diffing algorithm when updating a component, although I could be wrong.", "context": "<issue_start><issue_comment>Title: All controlled <textarea/> fields re-render on any setState() call even though their data has not changed.\nusername_0: **Do you want to request a *feature* or report a *bug*?**\r\nReporting a bug.\r\n\r\n**What is the current behavior?**\r\nBasically I have many controlled `<textarea/>` fields in the app that I am currently developing and I normally only want them to re-render when their values have changed through their onChange event, but they re-render every time setState() gets called anywhere within the component or any parent component.\r\n\r\nI even tried creating a custom component which only contains a `<textarea/>` and setting shouldComponentUpdate() to return false but it seems to ignore that command completely. It's weird because in this case the console.log()s in my child component's render() function don't get executed but the field gets re-rendered nonetheless.\r\n\r\nThis issue is causing some major performance problems in my app. I hope someone addresses this issue as fast as possible so I can continue working on my react project. Maybe someone can provide a temporary workaround? Please let me know if there's something I'm missing or if I'm using this field incorrectly.\r\n\r\n**If the current behavior is a bug, please provide the steps to reproduce and if possible a minimal demo of the problem. Your bug will get fixed much faster if we can run your code and it doesn't have dependencies other than React. Paste the link to your JSFiddle (https://jsfiddle.net/Luktwrdm/) or CodeSandbox (https://codesandbox.io/s/new) example below:**\r\nI have isolated the problem in a new empty app and you can see the behavior in the following gif:\r\nhttps://gfycat.com/DentalExcitableIndri\r\n\r\nThe code is very simple. You can recreate this problem by creating 2 controlled fields, one `<input/>` and another `<textarea/>`. Then go to your browser, inspect the `<textarea/>` element and input something in the basic input field. The `<textarea/>` will re-render on every new typed letter in the `<input/>` field but not the other way around.\r\n```javascript\r\nimport React, { Component } from 'react';\r\nimport logo from './logo.svg';\r\nimport './App.css';\r\n\r\nclass App extends Component {\r\n\tconstructor(props){\r\n\t\tsuper(props);\r\n\t\tthis.state = {\r\n\t\t\tta1 : \"ta1\",\r\n\t\t\tta2 : \"ta2\",\r\n\t\t\tta3 : \"ta3\",\r\n\t\t\ti1 : \"i1\",\r\n\t\t\ti2 : \"i2\",\r\n\t\t\ti3 : \"i3\"\r\n\t\t};\r\n\t\tthis.handleInputChange = this.handleInputChange.bind(this);\r\n\t}\r\n\r\n\thandleInputChange(event){\r\n\t\tthis.setState({\r\n\t\t\t[event.target.name] : event.target.value\r\n\t\t});\r\n\t}\r\n\r\n\trender() {\r\n\t\treturn (\r\n\t\t\t<div className=\"App\">\r\n\t\t\t\t<header className=\"App-header\">\r\n\t\t\t\t\t<img src={logo} className=\"App-logo\" alt=\"logo\" />\r\n\t\t\t\t\t<h1 className=\"App-title\">Welcome to React</h1>\r\n\t\t\t\t</header>\r\n\t\t\t\t<p className=\"App-intro\">\r\n\t\t\t\t\ttest\r\n\t\t\t\t</p>\r\n\t\t\t\t<div className=\"row\">\r\n\t\t\t\t\t<div className=\"column\">\r\n\t\t\t\t\t\ttextareas\r\n\t\t\t\t\t\t<textarea value={this.state.ta1} name=\"ta1\" onChange={this.handleInputChange}/>\r\n\t\t\t\t\t\t<textarea value={this.state.ta2} name=\"ta2\" onChange={this.handleInputChange}/>\r\n\t\t\t\t\t\t<textarea value={this.state.ta3} name=\"ta3\" onChange={this.handleInputChange}/>\r\n\t\t\t\t\t</div>\r\n\t\t\t\t\t<div className=\"column\">\r\n\t\t\t\t\t\tinput fields\r\n\t\t\t\t\t\t<input value={this.state.i1} name=\"i1\" onChange={this.handleInputChange}/>\r\n\t\t\t\t\t\t<input value={this.state.i2} name=\"i2\" onChange={this.handleInputChange}/>\r\n\t\t\t\t\t\t<input value={this.state.i3} name=\"i3\" onChange={this.handleInputChange}/>\r\n\t\t\t\t\t</div>\r\n\t\t\t\t</div>\r\n\t\t\t</div>\r\n\t\t);\r\n\t}\r\n}\r\n\r\nexport default App;\r\n```\r\n\r\n**What is the expected behavior?**\r\nThe `<textarea/>` field should only re-render when its data is changed.\r\n\r\n**Which versions of React, and which browser / OS are affected by this issue? Did this work in previous versions of React?**\r\nI was able to recreate this problem on React versions 16.2.0 (latest) and 15.6.1. I did not test on other versions so I am not sure if this worked correctly before. I'm using Chrome 62.0.3202.94 64-bit (latest) and Firefox 57.0.4 64-bit (latest) on Ubuntu 14.04.\n<issue_comment>username_1: Do you want to look into why this happens?\n<issue_comment>username_0: I'd love to help, but I'm not sure if I would be able to locate the bug in the source code myself. I'm guessing it has something to do with React's diffing algorithm when updating a component, although I could be wrong.\n<issue_comment>username_2: try to set key for each input and textarea\n<issue_comment>username_3: It looks like this is due to [`updateWrapper` in `ReactDOMTextArea`](https://github.com/facebook/react/blame/master/packages/react-dom/src/client/ReactDOMFiberTextarea.js#L142-L144). It's setting `node.defaultValue` because `props.defaultValue` does not exist, and that's triggering the update. By the same token, it will also [set `node.defaultValue` if `props.defaultValue` is defined](https://github.com/facebook/react/blame/master/packages/react-dom/src/client/ReactDOMFiberTextarea.js#L146-L148), so no matter what it's updating `node.defaultValue` and causing the element to re-attach it's text node child.\n<issue_comment>username_4: @username_0  I think the PR #12080  will resolve the problem.\r\nThanks, @username_3  for the tip +1:\n<issue_comment>username_0: Any ETA on when this change will be pushed to master and released?<issue_closed>"}
{"repo": "facebook/react", "issue_id": 290555006, "issue_number": 12072, "timestamp": "2018-01-22 19:15:13+00:00", "text": "try to set key for each input and textarea", "context": "<issue_start><issue_comment>Title: All controlled <textarea/> fields re-render on any setState() call even though their data has not changed.\nusername_0: **Do you want to request a *feature* or report a *bug*?**\r\nReporting a bug.\r\n\r\n**What is the current behavior?**\r\nBasically I have many controlled `<textarea/>` fields in the app that I am currently developing and I normally only want them to re-render when their values have changed through their onChange event, but they re-render every time setState() gets called anywhere within the component or any parent component.\r\n\r\nI even tried creating a custom component which only contains a `<textarea/>` and setting shouldComponentUpdate() to return false but it seems to ignore that command completely. It's weird because in this case the console.log()s in my child component's render() function don't get executed but the field gets re-rendered nonetheless.\r\n\r\nThis issue is causing some major performance problems in my app. I hope someone addresses this issue as fast as possible so I can continue working on my react project. Maybe someone can provide a temporary workaround? Please let me know if there's something I'm missing or if I'm using this field incorrectly.\r\n\r\n**If the current behavior is a bug, please provide the steps to reproduce and if possible a minimal demo of the problem. Your bug will get fixed much faster if we can run your code and it doesn't have dependencies other than React. Paste the link to your JSFiddle (https://jsfiddle.net/Luktwrdm/) or CodeSandbox (https://codesandbox.io/s/new) example below:**\r\nI have isolated the problem in a new empty app and you can see the behavior in the following gif:\r\nhttps://gfycat.com/DentalExcitableIndri\r\n\r\nThe code is very simple. You can recreate this problem by creating 2 controlled fields, one `<input/>` and another `<textarea/>`. Then go to your browser, inspect the `<textarea/>` element and input something in the basic input field. The `<textarea/>` will re-render on every new typed letter in the `<input/>` field but not the other way around.\r\n```javascript\r\nimport React, { Component } from 'react';\r\nimport logo from './logo.svg';\r\nimport './App.css';\r\n\r\nclass App extends Component {\r\n\tconstructor(props){\r\n\t\tsuper(props);\r\n\t\tthis.state = {\r\n\t\t\tta1 : \"ta1\",\r\n\t\t\tta2 : \"ta2\",\r\n\t\t\tta3 : \"ta3\",\r\n\t\t\ti1 : \"i1\",\r\n\t\t\ti2 : \"i2\",\r\n\t\t\ti3 : \"i3\"\r\n\t\t};\r\n\t\tthis.handleInputChange = this.handleInputChange.bind(this);\r\n\t}\r\n\r\n\thandleInputChange(event){\r\n\t\tthis.setState({\r\n\t\t\t[event.target.name] : event.target.value\r\n\t\t});\r\n\t}\r\n\r\n\trender() {\r\n\t\treturn (\r\n\t\t\t<div className=\"App\">\r\n\t\t\t\t<header className=\"App-header\">\r\n\t\t\t\t\t<img src={logo} className=\"App-logo\" alt=\"logo\" />\r\n\t\t\t\t\t<h1 className=\"App-title\">Welcome to React</h1>\r\n\t\t\t\t</header>\r\n\t\t\t\t<p className=\"App-intro\">\r\n\t\t\t\t\ttest\r\n\t\t\t\t</p>\r\n\t\t\t\t<div className=\"row\">\r\n\t\t\t\t\t<div className=\"column\">\r\n\t\t\t\t\t\ttextareas\r\n\t\t\t\t\t\t<textarea value={this.state.ta1} name=\"ta1\" onChange={this.handleInputChange}/>\r\n\t\t\t\t\t\t<textarea value={this.state.ta2} name=\"ta2\" onChange={this.handleInputChange}/>\r\n\t\t\t\t\t\t<textarea value={this.state.ta3} name=\"ta3\" onChange={this.handleInputChange}/>\r\n\t\t\t\t\t</div>\r\n\t\t\t\t\t<div className=\"column\">\r\n\t\t\t\t\t\tinput fields\r\n\t\t\t\t\t\t<input value={this.state.i1} name=\"i1\" onChange={this.handleInputChange}/>\r\n\t\t\t\t\t\t<input value={this.state.i2} name=\"i2\" onChange={this.handleInputChange}/>\r\n\t\t\t\t\t\t<input value={this.state.i3} name=\"i3\" onChange={this.handleInputChange}/>\r\n\t\t\t\t\t</div>\r\n\t\t\t\t</div>\r\n\t\t\t</div>\r\n\t\t);\r\n\t}\r\n}\r\n\r\nexport default App;\r\n```\r\n\r\n**What is the expected behavior?**\r\nThe `<textarea/>` field should only re-render when its data is changed.\r\n\r\n**Which versions of React, and which browser / OS are affected by this issue? Did this work in previous versions of React?**\r\nI was able to recreate this problem on React versions 16.2.0 (latest) and 15.6.1. I did not test on other versions so I am not sure if this worked correctly before. I'm using Chrome 62.0.3202.94 64-bit (latest) and Firefox 57.0.4 64-bit (latest) on Ubuntu 14.04.\n<issue_comment>username_1: Do you want to look into why this happens?\n<issue_comment>username_0: I'd love to help, but I'm not sure if I would be able to locate the bug in the source code myself. I'm guessing it has something to do with React's diffing algorithm when updating a component, although I could be wrong.\n<issue_comment>username_2: try to set key for each input and textarea\n<issue_comment>username_3: It looks like this is due to [`updateWrapper` in `ReactDOMTextArea`](https://github.com/facebook/react/blame/master/packages/react-dom/src/client/ReactDOMFiberTextarea.js#L142-L144). It's setting `node.defaultValue` because `props.defaultValue` does not exist, and that's triggering the update. By the same token, it will also [set `node.defaultValue` if `props.defaultValue` is defined](https://github.com/facebook/react/blame/master/packages/react-dom/src/client/ReactDOMFiberTextarea.js#L146-L148), so no matter what it's updating `node.defaultValue` and causing the element to re-attach it's text node child.\n<issue_comment>username_4: @username_0  I think the PR #12080  will resolve the problem.\r\nThanks, @username_3  for the tip +1:\n<issue_comment>username_0: Any ETA on when this change will be pushed to master and released?<issue_closed>"}
{"repo": "facebook/react", "issue_id": 290555006, "issue_number": 12072, "timestamp": "2018-01-22 21:54:13+00:00", "text": "It looks like this is due to [`updateWrapper` in `ReactDOMTextArea`](https://github.com/facebook/react/blame/master/packages/react-dom/src/client/ReactDOMFiberTextarea.js#L142-L144). It's setting `node.defaultValue` because `props.defaultValue` does not exist, and that's triggering the update. By the same token, it will also [set `node.defaultValue` if `props.defaultValue` is defined](https://github.com/facebook/react/blame/master/packages/react-dom/src/client/ReactDOMFiberTextarea.js#L146-L148), so no matter what it's updating `node.defaultValue` and causing the element to re-attach it's text node child.", "context": "<issue_start><issue_comment>Title: All controlled <textarea/> fields re-render on any setState() call even though their data has not changed.\nusername_0: **Do you want to request a *feature* or report a *bug*?**\r\nReporting a bug.\r\n\r\n**What is the current behavior?**\r\nBasically I have many controlled `<textarea/>` fields in the app that I am currently developing and I normally only want them to re-render when their values have changed through their onChange event, but they re-render every time setState() gets called anywhere within the component or any parent component.\r\n\r\nI even tried creating a custom component which only contains a `<textarea/>` and setting shouldComponentUpdate() to return false but it seems to ignore that command completely. It's weird because in this case the console.log()s in my child component's render() function don't get executed but the field gets re-rendered nonetheless.\r\n\r\nThis issue is causing some major performance problems in my app. I hope someone addresses this issue as fast as possible so I can continue working on my react project. Maybe someone can provide a temporary workaround? Please let me know if there's something I'm missing or if I'm using this field incorrectly.\r\n\r\n**If the current behavior is a bug, please provide the steps to reproduce and if possible a minimal demo of the problem. Your bug will get fixed much faster if we can run your code and it doesn't have dependencies other than React. Paste the link to your JSFiddle (https://jsfiddle.net/Luktwrdm/) or CodeSandbox (https://codesandbox.io/s/new) example below:**\r\nI have isolated the problem in a new empty app and you can see the behavior in the following gif:\r\nhttps://gfycat.com/DentalExcitableIndri\r\n\r\nThe code is very simple. You can recreate this problem by creating 2 controlled fields, one `<input/>` and another `<textarea/>`. Then go to your browser, inspect the `<textarea/>` element and input something in the basic input field. The `<textarea/>` will re-render on every new typed letter in the `<input/>` field but not the other way around.\r\n```javascript\r\nimport React, { Component } from 'react';\r\nimport logo from './logo.svg';\r\nimport './App.css';\r\n\r\nclass App extends Component {\r\n\tconstructor(props){\r\n\t\tsuper(props);\r\n\t\tthis.state = {\r\n\t\t\tta1 : \"ta1\",\r\n\t\t\tta2 : \"ta2\",\r\n\t\t\tta3 : \"ta3\",\r\n\t\t\ti1 : \"i1\",\r\n\t\t\ti2 : \"i2\",\r\n\t\t\ti3 : \"i3\"\r\n\t\t};\r\n\t\tthis.handleInputChange = this.handleInputChange.bind(this);\r\n\t}\r\n\r\n\thandleInputChange(event){\r\n\t\tthis.setState({\r\n\t\t\t[event.target.name] : event.target.value\r\n\t\t});\r\n\t}\r\n\r\n\trender() {\r\n\t\treturn (\r\n\t\t\t<div className=\"App\">\r\n\t\t\t\t<header className=\"App-header\">\r\n\t\t\t\t\t<img src={logo} className=\"App-logo\" alt=\"logo\" />\r\n\t\t\t\t\t<h1 className=\"App-title\">Welcome to React</h1>\r\n\t\t\t\t</header>\r\n\t\t\t\t<p className=\"App-intro\">\r\n\t\t\t\t\ttest\r\n\t\t\t\t</p>\r\n\t\t\t\t<div className=\"row\">\r\n\t\t\t\t\t<div className=\"column\">\r\n\t\t\t\t\t\ttextareas\r\n\t\t\t\t\t\t<textarea value={this.state.ta1} name=\"ta1\" onChange={this.handleInputChange}/>\r\n\t\t\t\t\t\t<textarea value={this.state.ta2} name=\"ta2\" onChange={this.handleInputChange}/>\r\n\t\t\t\t\t\t<textarea value={this.state.ta3} name=\"ta3\" onChange={this.handleInputChange}/>\r\n\t\t\t\t\t</div>\r\n\t\t\t\t\t<div className=\"column\">\r\n\t\t\t\t\t\tinput fields\r\n\t\t\t\t\t\t<input value={this.state.i1} name=\"i1\" onChange={this.handleInputChange}/>\r\n\t\t\t\t\t\t<input value={this.state.i2} name=\"i2\" onChange={this.handleInputChange}/>\r\n\t\t\t\t\t\t<input value={this.state.i3} name=\"i3\" onChange={this.handleInputChange}/>\r\n\t\t\t\t\t</div>\r\n\t\t\t\t</div>\r\n\t\t\t</div>\r\n\t\t);\r\n\t}\r\n}\r\n\r\nexport default App;\r\n```\r\n\r\n**What is the expected behavior?**\r\nThe `<textarea/>` field should only re-render when its data is changed.\r\n\r\n**Which versions of React, and which browser / OS are affected by this issue? Did this work in previous versions of React?**\r\nI was able to recreate this problem on React versions 16.2.0 (latest) and 15.6.1. I did not test on other versions so I am not sure if this worked correctly before. I'm using Chrome 62.0.3202.94 64-bit (latest) and Firefox 57.0.4 64-bit (latest) on Ubuntu 14.04.\n<issue_comment>username_1: Do you want to look into why this happens?\n<issue_comment>username_0: I'd love to help, but I'm not sure if I would be able to locate the bug in the source code myself. I'm guessing it has something to do with React's diffing algorithm when updating a component, although I could be wrong.\n<issue_comment>username_2: try to set key for each input and textarea\n<issue_comment>username_3: It looks like this is due to [`updateWrapper` in `ReactDOMTextArea`](https://github.com/facebook/react/blame/master/packages/react-dom/src/client/ReactDOMFiberTextarea.js#L142-L144). It's setting `node.defaultValue` because `props.defaultValue` does not exist, and that's triggering the update. By the same token, it will also [set `node.defaultValue` if `props.defaultValue` is defined](https://github.com/facebook/react/blame/master/packages/react-dom/src/client/ReactDOMFiberTextarea.js#L146-L148), so no matter what it's updating `node.defaultValue` and causing the element to re-attach it's text node child.\n<issue_comment>username_4: @username_0  I think the PR #12080  will resolve the problem.\r\nThanks, @username_3  for the tip +1:\n<issue_comment>username_0: Any ETA on when this change will be pushed to master and released?<issue_closed>"}
{"repo": "facebook/react", "issue_id": 290555006, "issue_number": 12072, "timestamp": "2018-01-23 18:55:34+00:00", "text": "@username_0  I think the PR #12080  will resolve the problem.\r\nThanks, @username_3  for the tip +1:", "context": "<issue_start><issue_comment>Title: All controlled <textarea/> fields re-render on any setState() call even though their data has not changed.\nusername_0: **Do you want to request a *feature* or report a *bug*?**\r\nReporting a bug.\r\n\r\n**What is the current behavior?**\r\nBasically I have many controlled `<textarea/>` fields in the app that I am currently developing and I normally only want them to re-render when their values have changed through their onChange event, but they re-render every time setState() gets called anywhere within the component or any parent component.\r\n\r\nI even tried creating a custom component which only contains a `<textarea/>` and setting shouldComponentUpdate() to return false but it seems to ignore that command completely. It's weird because in this case the console.log()s in my child component's render() function don't get executed but the field gets re-rendered nonetheless.\r\n\r\nThis issue is causing some major performance problems in my app. I hope someone addresses this issue as fast as possible so I can continue working on my react project. Maybe someone can provide a temporary workaround? Please let me know if there's something I'm missing or if I'm using this field incorrectly.\r\n\r\n**If the current behavior is a bug, please provide the steps to reproduce and if possible a minimal demo of the problem. Your bug will get fixed much faster if we can run your code and it doesn't have dependencies other than React. Paste the link to your JSFiddle (https://jsfiddle.net/Luktwrdm/) or CodeSandbox (https://codesandbox.io/s/new) example below:**\r\nI have isolated the problem in a new empty app and you can see the behavior in the following gif:\r\nhttps://gfycat.com/DentalExcitableIndri\r\n\r\nThe code is very simple. You can recreate this problem by creating 2 controlled fields, one `<input/>` and another `<textarea/>`. Then go to your browser, inspect the `<textarea/>` element and input something in the basic input field. The `<textarea/>` will re-render on every new typed letter in the `<input/>` field but not the other way around.\r\n```javascript\r\nimport React, { Component } from 'react';\r\nimport logo from './logo.svg';\r\nimport './App.css';\r\n\r\nclass App extends Component {\r\n\tconstructor(props){\r\n\t\tsuper(props);\r\n\t\tthis.state = {\r\n\t\t\tta1 : \"ta1\",\r\n\t\t\tta2 : \"ta2\",\r\n\t\t\tta3 : \"ta3\",\r\n\t\t\ti1 : \"i1\",\r\n\t\t\ti2 : \"i2\",\r\n\t\t\ti3 : \"i3\"\r\n\t\t};\r\n\t\tthis.handleInputChange = this.handleInputChange.bind(this);\r\n\t}\r\n\r\n\thandleInputChange(event){\r\n\t\tthis.setState({\r\n\t\t\t[event.target.name] : event.target.value\r\n\t\t});\r\n\t}\r\n\r\n\trender() {\r\n\t\treturn (\r\n\t\t\t<div className=\"App\">\r\n\t\t\t\t<header className=\"App-header\">\r\n\t\t\t\t\t<img src={logo} className=\"App-logo\" alt=\"logo\" />\r\n\t\t\t\t\t<h1 className=\"App-title\">Welcome to React</h1>\r\n\t\t\t\t</header>\r\n\t\t\t\t<p className=\"App-intro\">\r\n\t\t\t\t\ttest\r\n\t\t\t\t</p>\r\n\t\t\t\t<div className=\"row\">\r\n\t\t\t\t\t<div className=\"column\">\r\n\t\t\t\t\t\ttextareas\r\n\t\t\t\t\t\t<textarea value={this.state.ta1} name=\"ta1\" onChange={this.handleInputChange}/>\r\n\t\t\t\t\t\t<textarea value={this.state.ta2} name=\"ta2\" onChange={this.handleInputChange}/>\r\n\t\t\t\t\t\t<textarea value={this.state.ta3} name=\"ta3\" onChange={this.handleInputChange}/>\r\n\t\t\t\t\t</div>\r\n\t\t\t\t\t<div className=\"column\">\r\n\t\t\t\t\t\tinput fields\r\n\t\t\t\t\t\t<input value={this.state.i1} name=\"i1\" onChange={this.handleInputChange}/>\r\n\t\t\t\t\t\t<input value={this.state.i2} name=\"i2\" onChange={this.handleInputChange}/>\r\n\t\t\t\t\t\t<input value={this.state.i3} name=\"i3\" onChange={this.handleInputChange}/>\r\n\t\t\t\t\t</div>\r\n\t\t\t\t</div>\r\n\t\t\t</div>\r\n\t\t);\r\n\t}\r\n}\r\n\r\nexport default App;\r\n```\r\n\r\n**What is the expected behavior?**\r\nThe `<textarea/>` field should only re-render when its data is changed.\r\n\r\n**Which versions of React, and which browser / OS are affected by this issue? Did this work in previous versions of React?**\r\nI was able to recreate this problem on React versions 16.2.0 (latest) and 15.6.1. I did not test on other versions so I am not sure if this worked correctly before. I'm using Chrome 62.0.3202.94 64-bit (latest) and Firefox 57.0.4 64-bit (latest) on Ubuntu 14.04.\n<issue_comment>username_1: Do you want to look into why this happens?\n<issue_comment>username_0: I'd love to help, but I'm not sure if I would be able to locate the bug in the source code myself. I'm guessing it has something to do with React's diffing algorithm when updating a component, although I could be wrong.\n<issue_comment>username_2: try to set key for each input and textarea\n<issue_comment>username_3: It looks like this is due to [`updateWrapper` in `ReactDOMTextArea`](https://github.com/facebook/react/blame/master/packages/react-dom/src/client/ReactDOMFiberTextarea.js#L142-L144). It's setting `node.defaultValue` because `props.defaultValue` does not exist, and that's triggering the update. By the same token, it will also [set `node.defaultValue` if `props.defaultValue` is defined](https://github.com/facebook/react/blame/master/packages/react-dom/src/client/ReactDOMFiberTextarea.js#L146-L148), so no matter what it's updating `node.defaultValue` and causing the element to re-attach it's text node child.\n<issue_comment>username_4: @username_0  I think the PR #12080  will resolve the problem.\r\nThanks, @username_3  for the tip +1:\n<issue_comment>username_0: Any ETA on when this change will be pushed to master and released?<issue_closed>"}
{"repo": "facebook/react", "issue_id": 290555006, "issue_number": 12072, "timestamp": "2018-01-24 16:11:12+00:00", "text": "Any ETA on when this change will be pushed to master and released?", "context": "<issue_start><issue_comment>Title: All controlled <textarea/> fields re-render on any setState() call even though their data has not changed.\nusername_0: **Do you want to request a *feature* or report a *bug*?**\r\nReporting a bug.\r\n\r\n**What is the current behavior?**\r\nBasically I have many controlled `<textarea/>` fields in the app that I am currently developing and I normally only want them to re-render when their values have changed through their onChange event, but they re-render every time setState() gets called anywhere within the component or any parent component.\r\n\r\nI even tried creating a custom component which only contains a `<textarea/>` and setting shouldComponentUpdate() to return false but it seems to ignore that command completely. It's weird because in this case the console.log()s in my child component's render() function don't get executed but the field gets re-rendered nonetheless.\r\n\r\nThis issue is causing some major performance problems in my app. I hope someone addresses this issue as fast as possible so I can continue working on my react project. Maybe someone can provide a temporary workaround? Please let me know if there's something I'm missing or if I'm using this field incorrectly.\r\n\r\n**If the current behavior is a bug, please provide the steps to reproduce and if possible a minimal demo of the problem. Your bug will get fixed much faster if we can run your code and it doesn't have dependencies other than React. Paste the link to your JSFiddle (https://jsfiddle.net/Luktwrdm/) or CodeSandbox (https://codesandbox.io/s/new) example below:**\r\nI have isolated the problem in a new empty app and you can see the behavior in the following gif:\r\nhttps://gfycat.com/DentalExcitableIndri\r\n\r\nThe code is very simple. You can recreate this problem by creating 2 controlled fields, one `<input/>` and another `<textarea/>`. Then go to your browser, inspect the `<textarea/>` element and input something in the basic input field. The `<textarea/>` will re-render on every new typed letter in the `<input/>` field but not the other way around.\r\n```javascript\r\nimport React, { Component } from 'react';\r\nimport logo from './logo.svg';\r\nimport './App.css';\r\n\r\nclass App extends Component {\r\n\tconstructor(props){\r\n\t\tsuper(props);\r\n\t\tthis.state = {\r\n\t\t\tta1 : \"ta1\",\r\n\t\t\tta2 : \"ta2\",\r\n\t\t\tta3 : \"ta3\",\r\n\t\t\ti1 : \"i1\",\r\n\t\t\ti2 : \"i2\",\r\n\t\t\ti3 : \"i3\"\r\n\t\t};\r\n\t\tthis.handleInputChange = this.handleInputChange.bind(this);\r\n\t}\r\n\r\n\thandleInputChange(event){\r\n\t\tthis.setState({\r\n\t\t\t[event.target.name] : event.target.value\r\n\t\t});\r\n\t}\r\n\r\n\trender() {\r\n\t\treturn (\r\n\t\t\t<div className=\"App\">\r\n\t\t\t\t<header className=\"App-header\">\r\n\t\t\t\t\t<img src={logo} className=\"App-logo\" alt=\"logo\" />\r\n\t\t\t\t\t<h1 className=\"App-title\">Welcome to React</h1>\r\n\t\t\t\t</header>\r\n\t\t\t\t<p className=\"App-intro\">\r\n\t\t\t\t\ttest\r\n\t\t\t\t</p>\r\n\t\t\t\t<div className=\"row\">\r\n\t\t\t\t\t<div className=\"column\">\r\n\t\t\t\t\t\ttextareas\r\n\t\t\t\t\t\t<textarea value={this.state.ta1} name=\"ta1\" onChange={this.handleInputChange}/>\r\n\t\t\t\t\t\t<textarea value={this.state.ta2} name=\"ta2\" onChange={this.handleInputChange}/>\r\n\t\t\t\t\t\t<textarea value={this.state.ta3} name=\"ta3\" onChange={this.handleInputChange}/>\r\n\t\t\t\t\t</div>\r\n\t\t\t\t\t<div className=\"column\">\r\n\t\t\t\t\t\tinput fields\r\n\t\t\t\t\t\t<input value={this.state.i1} name=\"i1\" onChange={this.handleInputChange}/>\r\n\t\t\t\t\t\t<input value={this.state.i2} name=\"i2\" onChange={this.handleInputChange}/>\r\n\t\t\t\t\t\t<input value={this.state.i3} name=\"i3\" onChange={this.handleInputChange}/>\r\n\t\t\t\t\t</div>\r\n\t\t\t\t</div>\r\n\t\t\t</div>\r\n\t\t);\r\n\t}\r\n}\r\n\r\nexport default App;\r\n```\r\n\r\n**What is the expected behavior?**\r\nThe `<textarea/>` field should only re-render when its data is changed.\r\n\r\n**Which versions of React, and which browser / OS are affected by this issue? Did this work in previous versions of React?**\r\nI was able to recreate this problem on React versions 16.2.0 (latest) and 15.6.1. I did not test on other versions so I am not sure if this worked correctly before. I'm using Chrome 62.0.3202.94 64-bit (latest) and Firefox 57.0.4 64-bit (latest) on Ubuntu 14.04.\n<issue_comment>username_1: Do you want to look into why this happens?\n<issue_comment>username_0: I'd love to help, but I'm not sure if I would be able to locate the bug in the source code myself. I'm guessing it has something to do with React's diffing algorithm when updating a component, although I could be wrong.\n<issue_comment>username_2: try to set key for each input and textarea\n<issue_comment>username_3: It looks like this is due to [`updateWrapper` in `ReactDOMTextArea`](https://github.com/facebook/react/blame/master/packages/react-dom/src/client/ReactDOMFiberTextarea.js#L142-L144). It's setting `node.defaultValue` because `props.defaultValue` does not exist, and that's triggering the update. By the same token, it will also [set `node.defaultValue` if `props.defaultValue` is defined](https://github.com/facebook/react/blame/master/packages/react-dom/src/client/ReactDOMFiberTextarea.js#L146-L148), so no matter what it's updating `node.defaultValue` and causing the element to re-attach it's text node child.\n<issue_comment>username_4: @username_0  I think the PR #12080  will resolve the problem.\r\nThanks, @username_3  for the tip +1:\n<issue_comment>username_0: Any ETA on when this change will be pushed to master and released?<issue_closed>"}
{"repo": "facebook/react", "issue_id": 290555006, "issue_number": 12072, "timestamp": "2018-09-14 23:09:08+00:00", "text": "", "context": "<issue_start><issue_comment>Title: All controlled <textarea/> fields re-render on any setState() call even though their data has not changed.\nusername_0: **Do you want to request a *feature* or report a *bug*?**\r\nReporting a bug.\r\n\r\n**What is the current behavior?**\r\nBasically I have many controlled `<textarea/>` fields in the app that I am currently developing and I normally only want them to re-render when their values have changed through their onChange event, but they re-render every time setState() gets called anywhere within the component or any parent component.\r\n\r\nI even tried creating a custom component which only contains a `<textarea/>` and setting shouldComponentUpdate() to return false but it seems to ignore that command completely. It's weird because in this case the console.log()s in my child component's render() function don't get executed but the field gets re-rendered nonetheless.\r\n\r\nThis issue is causing some major performance problems in my app. I hope someone addresses this issue as fast as possible so I can continue working on my react project. Maybe someone can provide a temporary workaround? Please let me know if there's something I'm missing or if I'm using this field incorrectly.\r\n\r\n**If the current behavior is a bug, please provide the steps to reproduce and if possible a minimal demo of the problem. Your bug will get fixed much faster if we can run your code and it doesn't have dependencies other than React. Paste the link to your JSFiddle (https://jsfiddle.net/Luktwrdm/) or CodeSandbox (https://codesandbox.io/s/new) example below:**\r\nI have isolated the problem in a new empty app and you can see the behavior in the following gif:\r\nhttps://gfycat.com/DentalExcitableIndri\r\n\r\nThe code is very simple. You can recreate this problem by creating 2 controlled fields, one `<input/>` and another `<textarea/>`. Then go to your browser, inspect the `<textarea/>` element and input something in the basic input field. The `<textarea/>` will re-render on every new typed letter in the `<input/>` field but not the other way around.\r\n```javascript\r\nimport React, { Component } from 'react';\r\nimport logo from './logo.svg';\r\nimport './App.css';\r\n\r\nclass App extends Component {\r\n\tconstructor(props){\r\n\t\tsuper(props);\r\n\t\tthis.state = {\r\n\t\t\tta1 : \"ta1\",\r\n\t\t\tta2 : \"ta2\",\r\n\t\t\tta3 : \"ta3\",\r\n\t\t\ti1 : \"i1\",\r\n\t\t\ti2 : \"i2\",\r\n\t\t\ti3 : \"i3\"\r\n\t\t};\r\n\t\tthis.handleInputChange = this.handleInputChange.bind(this);\r\n\t}\r\n\r\n\thandleInputChange(event){\r\n\t\tthis.setState({\r\n\t\t\t[event.target.name] : event.target.value\r\n\t\t});\r\n\t}\r\n\r\n\trender() {\r\n\t\treturn (\r\n\t\t\t<div className=\"App\">\r\n\t\t\t\t<header className=\"App-header\">\r\n\t\t\t\t\t<img src={logo} className=\"App-logo\" alt=\"logo\" />\r\n\t\t\t\t\t<h1 className=\"App-title\">Welcome to React</h1>\r\n\t\t\t\t</header>\r\n\t\t\t\t<p className=\"App-intro\">\r\n\t\t\t\t\ttest\r\n\t\t\t\t</p>\r\n\t\t\t\t<div className=\"row\">\r\n\t\t\t\t\t<div className=\"column\">\r\n\t\t\t\t\t\ttextareas\r\n\t\t\t\t\t\t<textarea value={this.state.ta1} name=\"ta1\" onChange={this.handleInputChange}/>\r\n\t\t\t\t\t\t<textarea value={this.state.ta2} name=\"ta2\" onChange={this.handleInputChange}/>\r\n\t\t\t\t\t\t<textarea value={this.state.ta3} name=\"ta3\" onChange={this.handleInputChange}/>\r\n\t\t\t\t\t</div>\r\n\t\t\t\t\t<div className=\"column\">\r\n\t\t\t\t\t\tinput fields\r\n\t\t\t\t\t\t<input value={this.state.i1} name=\"i1\" onChange={this.handleInputChange}/>\r\n\t\t\t\t\t\t<input value={this.state.i2} name=\"i2\" onChange={this.handleInputChange}/>\r\n\t\t\t\t\t\t<input value={this.state.i3} name=\"i3\" onChange={this.handleInputChange}/>\r\n\t\t\t\t\t</div>\r\n\t\t\t\t</div>\r\n\t\t\t</div>\r\n\t\t);\r\n\t}\r\n}\r\n\r\nexport default App;\r\n```\r\n\r\n**What is the expected behavior?**\r\nThe `<textarea/>` field should only re-render when its data is changed.\r\n\r\n**Which versions of React, and which browser / OS are affected by this issue? Did this work in previous versions of React?**\r\nI was able to recreate this problem on React versions 16.2.0 (latest) and 15.6.1. I did not test on other versions so I am not sure if this worked correctly before. I'm using Chrome 62.0.3202.94 64-bit (latest) and Firefox 57.0.4 64-bit (latest) on Ubuntu 14.04.\n<issue_comment>username_1: Do you want to look into why this happens?\n<issue_comment>username_0: I'd love to help, but I'm not sure if I would be able to locate the bug in the source code myself. I'm guessing it has something to do with React's diffing algorithm when updating a component, although I could be wrong.\n<issue_comment>username_2: try to set key for each input and textarea\n<issue_comment>username_3: It looks like this is due to [`updateWrapper` in `ReactDOMTextArea`](https://github.com/facebook/react/blame/master/packages/react-dom/src/client/ReactDOMFiberTextarea.js#L142-L144). It's setting `node.defaultValue` because `props.defaultValue` does not exist, and that's triggering the update. By the same token, it will also [set `node.defaultValue` if `props.defaultValue` is defined](https://github.com/facebook/react/blame/master/packages/react-dom/src/client/ReactDOMFiberTextarea.js#L146-L148), so no matter what it's updating `node.defaultValue` and causing the element to re-attach it's text node child.\n<issue_comment>username_4: @username_0  I think the PR #12080  will resolve the problem.\r\nThanks, @username_3  for the tip +1:\n<issue_comment>username_0: Any ETA on when this change will be pushed to master and released?<issue_closed>"}
{"repo": "pytorch/pytorch", "issue_id": 342899294, "issue_number": 9607, "timestamp": "2018-07-19T21:40:56Z", "text": "fixes #9589 #9507 #9502 #9390", "context": "<issue_start><issue_comment>Title: docs fixes\nusername_0: fixes #9589 #9507 #9502 #9390\n<issue_comment>username_1: Do you want to consider documenting `scatter_add_` (https://github.com/pytorch/pytorch/issues/4176) as a part of this PR?\n<issue_comment>username_0: @username_1 Added! Let me know if the doc I wrote can be improved.\n<issue_comment>username_1: Whoops, sorry about the delay.\n<issue_comment>username_0: @username_1 no worries... the fb bot was too fast."}
{"repo": "pytorch/pytorch", "issue_id": 342899294, "issue_number": 9607, "timestamp": "2018-07-20 14:42:45+00:00", "text": "Do you want to consider documenting `scatter_add_` (https://github.com/pytorch/pytorch/issues/4176) as a part of this PR?", "context": "<issue_start><issue_comment>Title: docs fixes\nusername_0: fixes #9589 #9507 #9502 #9390\n<issue_comment>username_1: Do you want to consider documenting `scatter_add_` (https://github.com/pytorch/pytorch/issues/4176) as a part of this PR?\n<issue_comment>username_0: @username_1 Added! Let me know if the doc I wrote can be improved.\n<issue_comment>username_1: Whoops, sorry about the delay.\n<issue_comment>username_0: @username_1 no worries... the fb bot was too fast."}
{"repo": "pytorch/pytorch", "issue_id": 342899294, "issue_number": 9607, "timestamp": "2018-07-20 14:51:54+00:00", "text": "@username_1 Added! Let me know if the doc I wrote can be improved.", "context": "<issue_start><issue_comment>Title: docs fixes\nusername_0: fixes #9589 #9507 #9502 #9390\n<issue_comment>username_1: Do you want to consider documenting `scatter_add_` (https://github.com/pytorch/pytorch/issues/4176) as a part of this PR?\n<issue_comment>username_0: @username_1 Added! Let me know if the doc I wrote can be improved.\n<issue_comment>username_1: Whoops, sorry about the delay.\n<issue_comment>username_0: @username_1 no worries... the fb bot was too fast."}
{"repo": "pytorch/pytorch", "issue_id": 342899294, "issue_number": 9607, "timestamp": "2018-07-20 14:58:06+00:00", "text": "Whoops, sorry about the delay.", "context": "<issue_start><issue_comment>Title: docs fixes\nusername_0: fixes #9589 #9507 #9502 #9390\n<issue_comment>username_1: Do you want to consider documenting `scatter_add_` (https://github.com/pytorch/pytorch/issues/4176) as a part of this PR?\n<issue_comment>username_0: @username_1 Added! Let me know if the doc I wrote can be improved.\n<issue_comment>username_1: Whoops, sorry about the delay.\n<issue_comment>username_0: @username_1 no worries... the fb bot was too fast."}
{"repo": "pytorch/pytorch", "issue_id": 342899294, "issue_number": 9607, "timestamp": "2018-07-20 14:58:47+00:00", "text": "@username_1 no worries... the fb bot was too fast.", "context": "<issue_start><issue_comment>Title: docs fixes\nusername_0: fixes #9589 #9507 #9502 #9390\n<issue_comment>username_1: Do you want to consider documenting `scatter_add_` (https://github.com/pytorch/pytorch/issues/4176) as a part of this PR?\n<issue_comment>username_0: @username_1 Added! Let me know if the doc I wrote can be improved.\n<issue_comment>username_1: Whoops, sorry about the delay.\n<issue_comment>username_0: @username_1 no worries... the fb bot was too fast."}
{"repo": "pytorch/pytorch", "issue_id": 438432359, "issue_number": 19921, "timestamp": "2019-04-29T17:48:54Z", "text": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* #19885 Cleanup includes in c10/core/CPUAllocator.cpp.\n* #19924 Cleanup includes in torch/csrc/*\n* #19923 Cleanup includes in torch/csrc/autograd/*\n* #19922 Cleanup includes in torch/csrc/jit/*\n* **#19921 Cleanup includes in torch/csrc/jit/script/***\n\nDifferential Revision: [D15125017](https://our.internmc.facebook.com/intern/diff/D15125017)", "context": "<issue_start><issue_comment>Title: Cleanup includes in torch/csrc/jit/script/*\nusername_0: Stack from [ghstack](https://github.com/ezyang/ghstack):\n* #19885 Cleanup includes in c10/core/CPUAllocator.cpp.\n* #19924 Cleanup includes in torch/csrc/*\n* #19923 Cleanup includes in torch/csrc/autograd/*\n* #19922 Cleanup includes in torch/csrc/jit/*\n* **#19921 Cleanup includes in torch/csrc/jit/script/***\n\nDifferential Revision: [D15125017](https://our.internmc.facebook.com/intern/diff/D15125017)"}
{"repo": "pytorch/pytorch", "issue_id": 452726576, "issue_number": 21433, "timestamp": "2019-06-05T21:30:53Z", "text": "This moves all the actual functionality of `WeakScriptModuleProxy` into\r\na method that creates a `ScriptModule` directly and copies into it.\r\n\r\nThe existing weak script stuff now just makes `ScriptModule`s, so `WeakScriptModuleProxy` isn't necessary anymore\r\n\r\nDifferential Revision: [D15681650](https://our.internmc.facebook.com/intern/diff/15681650/)", "context": "<issue_start><issue_comment>Title: [jit] Delete WeakScriptModuleProxy\nusername_0: This moves all the actual functionality of `WeakScriptModuleProxy` into\r\na method that creates a `ScriptModule` directly and copies into it.\r\n\r\nThe existing weak script stuff now just makes `ScriptModule`s, so `WeakScriptModuleProxy` isn't necessary anymore\r\n\r\nDifferential Revision: [D15681650](https://our.internmc.facebook.com/intern/diff/15681650/)\n<issue_comment>username_1: Looks like cuda tests are failing (memory leak?)"}
{"repo": "pytorch/pytorch", "issue_id": 452726576, "issue_number": 21433, "timestamp": "2019-06-18 21:48:56+00:00", "text": "Looks like cuda tests are failing (memory leak?)", "context": "<issue_start><issue_comment>Title: [jit] Delete WeakScriptModuleProxy\nusername_0: This moves all the actual functionality of `WeakScriptModuleProxy` into\r\na method that creates a `ScriptModule` directly and copies into it.\r\n\r\nThe existing weak script stuff now just makes `ScriptModule`s, so `WeakScriptModuleProxy` isn't necessary anymore\r\n\r\nDifferential Revision: [D15681650](https://our.internmc.facebook.com/intern/diff/15681650/)\n<issue_comment>username_1: Looks like cuda tests are failing (memory leak?)"}
{"repo": "pytorch/pytorch", "issue_id": 499094993, "issue_number": 26918, "timestamp": "2019-09-26T20:17:30Z", "text": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#26918 [quantization] Make quantized max_pool2d error message more specific and less silly**\n* #26916 [quantization] Support ceil_mode in quantized maxpool\n\nDifferential Revision: [D17609624](https://our.internmc.facebook.com/intern/diff/D17609624)", "context": "<issue_start><issue_comment>Title: [quantization] Make quantized max_pool2d error message more specific and less silly\nusername_0: Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#26918 [quantization] Make quantized max_pool2d error message more specific and less silly**\n* #26916 [quantization] Support ceil_mode in quantized maxpool\n\nDifferential Revision: [D17609624](https://our.internmc.facebook.com/intern/diff/D17609624)"}
{"repo": "facebook/react", "issue_id": 519924494, "issue_number": 17314, "timestamp": "2019-11-08 10:07:07+00:00", "text": "<!--\r\n  Note: if the issue is about documentation or the website, please file it at:\r\n  https://github.com/reactjs/reactjs.org/issues/new\r\n-->\r\n\r\n**Do you want to request a *feature* or report a *bug*?**\r\n**Bug** - I think? \r\n\r\n**What is the current behavior?**\r\n\r\nIn Concurrent Mode, it appears that if a render is interrupted, if a component is using `useSubscription` the interrupted update is lost, which leads to \"tearing\" \r\n\r\nThe following codesandbox uses `useSubscription` with a RxJS `BehaviorSubject`, mimicking the example from here: https://www.npmjs.com/package/use-subscription#subscribing-to-observables \r\n\r\nIn the sandbox, clicking on the \"Increment Remote Count\" button triggers the RxJs `BehaviorSubject` to increment. This is done outside of the React event handler (ie: via `window.addEventLIstener` and so the updates are not batched together. The update to render the numbers is artificially slowed down. \r\n\r\nIf you click the \"Increment Remote Count\" button multiple times, the update works as expected. \r\n\r\nIf you interrupt the update, via clicking the \"increment local count\" \r\n\r\n\r\nSo the Steps to reproduce look like: \r\n1. Click the \"Increment Remote Count\" button once\r\n2. Before the update is committed to the DOM, click the \"Increment Local Count\" update. \r\n3. The first update is \"lost\" ie; the output looks like: \r\n\r\n![image](https://user-images.githubusercontent.com/787007/68468141-27255f80-0263-11ea-9afb-07964e21d9d9.png)\r\n\r\n\r\n**If the current behavior is a bug, please provide the steps to reproduce and if possible a minimal demo of the problem. Your bug will get fixed much faster if we can run your code and it doesn't have dependencies other than React. Paste the link to your JSFiddle (https://jsfiddle.net/Luktwrdm/) or CodeSandbox (https://codesandbox.io/s/new) example below:**\r\n\r\nhttps://jwenc.csb.app/ \\ https://codesandbox.io/s/usesubscriptionconcurrentlosingupdates-jwenc \r\n\r\n**What is the expected behavior?**\r\n\r\nI'd expect there to be a commit as the above screenshot, but I'd then expect there to be a follow-up commit that restores the consistency. In other words, I'd expect in the above picture for everything to be `1` \r\n\r\n**Which versions of React, and which browser / OS are affected by this issue? Did this work in previous versions of React?**\r\n\r\n`0.0.0-experimental-f6b8d31a7`\r\n\r\n\r\nI'd be willing to try to take a stab at writing a React test for this, if needed?", "context": "<issue_start><issue_comment>Title: Concurrent Mode and UseSubscription with RxJS \"lose\" updates \nusername_0: <!--\r\n  Note: if the issue is about documentation or the website, please file it at:\r\n  https://github.com/reactjs/reactjs.org/issues/new\r\n-->\r\n\r\n**Do you want to request a *feature* or report a *bug*?**\r\n**Bug** - I think? \r\n\r\n**What is the current behavior?**\r\n\r\nIn Concurrent Mode, it appears that if a render is interrupted, if a component is using `useSubscription` the interrupted update is lost, which leads to \"tearing\" \r\n\r\nThe following codesandbox uses `useSubscription` with a RxJS `BehaviorSubject`, mimicking the example from here: https://www.npmjs.com/package/use-subscription#subscribing-to-observables \r\n\r\nIn the sandbox, clicking on the \"Increment Remote Count\" button triggers the RxJs `BehaviorSubject` to increment. This is done outside of the React event handler (ie: via `window.addEventLIstener` and so the updates are not batched together. The update to render the numbers is artificially slowed down. \r\n\r\nIf you click the \"Increment Remote Count\" button multiple times, the update works as expected. \r\n\r\nIf you interrupt the update, via clicking the \"increment local count\" \r\n\r\n\r\nSo the Steps to reproduce look like: \r\n1. Click the \"Increment Remote Count\" button once\r\n2. Before the update is committed to the DOM, click the \"Increment Local Count\" update. \r\n3. The first update is \"lost\" ie; the output looks like: \r\n\r\n![image](https://user-images.githubusercontent.com/787007/68468141-27255f80-0263-11ea-9afb-07964e21d9d9.png)\r\n\r\n\r\n**If the current behavior is a bug, please provide the steps to reproduce and if possible a minimal demo of the problem. Your bug will get fixed much faster if we can run your code and it doesn't have dependencies other than React. Paste the link to your JSFiddle (https://jsfiddle.net/Luktwrdm/) or CodeSandbox (https://codesandbox.io/s/new) example below:**\r\n\r\nhttps://jwenc.csb.app/ \\ https://codesandbox.io/s/usesubscriptionconcurrentlosingupdates-jwenc \r\n\r\n**What is the expected behavior?**\r\n\r\nI'd expect there to be a commit as the above screenshot, but I'd then expect there to be a follow-up commit that restores the consistency. In other words, I'd expect in the above picture for everything to be `1` \r\n\r\n**Which versions of React, and which browser / OS are affected by this issue? Did this work in previous versions of React?**\r\n\r\n`0.0.0-experimental-f6b8d31a7`\r\n\r\n\r\nI'd be willing to try to take a stab at writing a React test for this, if needed?\n<issue_comment>username_1: https://github.com/facebook/react/issues/16396#issuecomment-547193356\r\nHere's codesandbox I created.\n<issue_comment>username_0: Started looking into why this happens - https://github.com/facebook/react/pull/17336\r\n\r\nSeems like an issue somewhere in `updateSimpleMemoComponent`\n<issue_comment>username_1: Maybe related: https://github.com/facebook/react/issues/17318#issuecomment-552167774\n<issue_comment>username_2: yes i already signaled here : https://github.com/facebook/react/issues/17318#issuecomment-552160358\n<issue_comment>username_3: Reported similar issue some time ago: https://github.com/facebook/react/issues/17028<issue_closed>\n<issue_comment>username_4: Thanks for the bug report!\r\n\r\nI believe this was fixed by https://github.com/facebook/react/pull/18091\r\n\r\nYou can confirm using `react@0.0.0-experimental-ea6ed3dbb` and `react-dom@react@0.0.0-experimental-ea6ed3dbb`.\r\n\r\nI'm going to close this issue, but if the bug persists please comment and we'll reopen.\n<issue_comment>username_0: Rebased https://github.com/facebook/react/pull/17336/files and the test was green - thanks!\n<issue_comment>username_4: @username_0 Thank you for that PR! I feel a bit guilty that it got buried and I didn't notice it. But I really appreciate that you took the time to write a unit test \ud83d\ude2eWe'll try to do a better job keeping track of our PR queue!\n<issue_comment>username_0: No worries, was a nice learning experience"}
{"repo": "facebook/react", "issue_id": 519924494, "issue_number": 17314, "timestamp": "2019-11-08 11:21:44+00:00", "text": "https://github.com/facebook/react/issues/16396#issuecomment-547193356\r\nHere's codesandbox I created.", "context": "<issue_start><issue_comment>Title: Concurrent Mode and UseSubscription with RxJS \"lose\" updates \nusername_0: <!--\r\n  Note: if the issue is about documentation or the website, please file it at:\r\n  https://github.com/reactjs/reactjs.org/issues/new\r\n-->\r\n\r\n**Do you want to request a *feature* or report a *bug*?**\r\n**Bug** - I think? \r\n\r\n**What is the current behavior?**\r\n\r\nIn Concurrent Mode, it appears that if a render is interrupted, if a component is using `useSubscription` the interrupted update is lost, which leads to \"tearing\" \r\n\r\nThe following codesandbox uses `useSubscription` with a RxJS `BehaviorSubject`, mimicking the example from here: https://www.npmjs.com/package/use-subscription#subscribing-to-observables \r\n\r\nIn the sandbox, clicking on the \"Increment Remote Count\" button triggers the RxJs `BehaviorSubject` to increment. This is done outside of the React event handler (ie: via `window.addEventLIstener` and so the updates are not batched together. The update to render the numbers is artificially slowed down. \r\n\r\nIf you click the \"Increment Remote Count\" button multiple times, the update works as expected. \r\n\r\nIf you interrupt the update, via clicking the \"increment local count\" \r\n\r\n\r\nSo the Steps to reproduce look like: \r\n1. Click the \"Increment Remote Count\" button once\r\n2. Before the update is committed to the DOM, click the \"Increment Local Count\" update. \r\n3. The first update is \"lost\" ie; the output looks like: \r\n\r\n![image](https://user-images.githubusercontent.com/787007/68468141-27255f80-0263-11ea-9afb-07964e21d9d9.png)\r\n\r\n\r\n**If the current behavior is a bug, please provide the steps to reproduce and if possible a minimal demo of the problem. Your bug will get fixed much faster if we can run your code and it doesn't have dependencies other than React. Paste the link to your JSFiddle (https://jsfiddle.net/Luktwrdm/) or CodeSandbox (https://codesandbox.io/s/new) example below:**\r\n\r\nhttps://jwenc.csb.app/ \\ https://codesandbox.io/s/usesubscriptionconcurrentlosingupdates-jwenc \r\n\r\n**What is the expected behavior?**\r\n\r\nI'd expect there to be a commit as the above screenshot, but I'd then expect there to be a follow-up commit that restores the consistency. In other words, I'd expect in the above picture for everything to be `1` \r\n\r\n**Which versions of React, and which browser / OS are affected by this issue? Did this work in previous versions of React?**\r\n\r\n`0.0.0-experimental-f6b8d31a7`\r\n\r\n\r\nI'd be willing to try to take a stab at writing a React test for this, if needed?\n<issue_comment>username_1: https://github.com/facebook/react/issues/16396#issuecomment-547193356\r\nHere's codesandbox I created.\n<issue_comment>username_0: Started looking into why this happens - https://github.com/facebook/react/pull/17336\r\n\r\nSeems like an issue somewhere in `updateSimpleMemoComponent`\n<issue_comment>username_1: Maybe related: https://github.com/facebook/react/issues/17318#issuecomment-552167774\n<issue_comment>username_2: yes i already signaled here : https://github.com/facebook/react/issues/17318#issuecomment-552160358\n<issue_comment>username_3: Reported similar issue some time ago: https://github.com/facebook/react/issues/17028<issue_closed>\n<issue_comment>username_4: Thanks for the bug report!\r\n\r\nI believe this was fixed by https://github.com/facebook/react/pull/18091\r\n\r\nYou can confirm using `react@0.0.0-experimental-ea6ed3dbb` and `react-dom@react@0.0.0-experimental-ea6ed3dbb`.\r\n\r\nI'm going to close this issue, but if the bug persists please comment and we'll reopen.\n<issue_comment>username_0: Rebased https://github.com/facebook/react/pull/17336/files and the test was green - thanks!\n<issue_comment>username_4: @username_0 Thank you for that PR! I feel a bit guilty that it got buried and I didn't notice it. But I really appreciate that you took the time to write a unit test \ud83d\ude2eWe'll try to do a better job keeping track of our PR queue!\n<issue_comment>username_0: No worries, was a nice learning experience"}
{"repo": "facebook/react", "issue_id": 519924494, "issue_number": 17314, "timestamp": "2019-11-14 21:43:59+00:00", "text": "Started looking into why this happens - https://github.com/facebook/react/pull/17336\r\n\r\nSeems like an issue somewhere in `updateSimpleMemoComponent`", "context": "<issue_start><issue_comment>Title: Concurrent Mode and UseSubscription with RxJS \"lose\" updates \nusername_0: <!--\r\n  Note: if the issue is about documentation or the website, please file it at:\r\n  https://github.com/reactjs/reactjs.org/issues/new\r\n-->\r\n\r\n**Do you want to request a *feature* or report a *bug*?**\r\n**Bug** - I think? \r\n\r\n**What is the current behavior?**\r\n\r\nIn Concurrent Mode, it appears that if a render is interrupted, if a component is using `useSubscription` the interrupted update is lost, which leads to \"tearing\" \r\n\r\nThe following codesandbox uses `useSubscription` with a RxJS `BehaviorSubject`, mimicking the example from here: https://www.npmjs.com/package/use-subscription#subscribing-to-observables \r\n\r\nIn the sandbox, clicking on the \"Increment Remote Count\" button triggers the RxJs `BehaviorSubject` to increment. This is done outside of the React event handler (ie: via `window.addEventLIstener` and so the updates are not batched together. The update to render the numbers is artificially slowed down. \r\n\r\nIf you click the \"Increment Remote Count\" button multiple times, the update works as expected. \r\n\r\nIf you interrupt the update, via clicking the \"increment local count\" \r\n\r\n\r\nSo the Steps to reproduce look like: \r\n1. Click the \"Increment Remote Count\" button once\r\n2. Before the update is committed to the DOM, click the \"Increment Local Count\" update. \r\n3. The first update is \"lost\" ie; the output looks like: \r\n\r\n![image](https://user-images.githubusercontent.com/787007/68468141-27255f80-0263-11ea-9afb-07964e21d9d9.png)\r\n\r\n\r\n**If the current behavior is a bug, please provide the steps to reproduce and if possible a minimal demo of the problem. Your bug will get fixed much faster if we can run your code and it doesn't have dependencies other than React. Paste the link to your JSFiddle (https://jsfiddle.net/Luktwrdm/) or CodeSandbox (https://codesandbox.io/s/new) example below:**\r\n\r\nhttps://jwenc.csb.app/ \\ https://codesandbox.io/s/usesubscriptionconcurrentlosingupdates-jwenc \r\n\r\n**What is the expected behavior?**\r\n\r\nI'd expect there to be a commit as the above screenshot, but I'd then expect there to be a follow-up commit that restores the consistency. In other words, I'd expect in the above picture for everything to be `1` \r\n\r\n**Which versions of React, and which browser / OS are affected by this issue? Did this work in previous versions of React?**\r\n\r\n`0.0.0-experimental-f6b8d31a7`\r\n\r\n\r\nI'd be willing to try to take a stab at writing a React test for this, if needed?\n<issue_comment>username_1: https://github.com/facebook/react/issues/16396#issuecomment-547193356\r\nHere's codesandbox I created.\n<issue_comment>username_0: Started looking into why this happens - https://github.com/facebook/react/pull/17336\r\n\r\nSeems like an issue somewhere in `updateSimpleMemoComponent`\n<issue_comment>username_1: Maybe related: https://github.com/facebook/react/issues/17318#issuecomment-552167774\n<issue_comment>username_2: yes i already signaled here : https://github.com/facebook/react/issues/17318#issuecomment-552160358\n<issue_comment>username_3: Reported similar issue some time ago: https://github.com/facebook/react/issues/17028<issue_closed>\n<issue_comment>username_4: Thanks for the bug report!\r\n\r\nI believe this was fixed by https://github.com/facebook/react/pull/18091\r\n\r\nYou can confirm using `react@0.0.0-experimental-ea6ed3dbb` and `react-dom@react@0.0.0-experimental-ea6ed3dbb`.\r\n\r\nI'm going to close this issue, but if the bug persists please comment and we'll reopen.\n<issue_comment>username_0: Rebased https://github.com/facebook/react/pull/17336/files and the test was green - thanks!\n<issue_comment>username_4: @username_0 Thank you for that PR! I feel a bit guilty that it got buried and I didn't notice it. But I really appreciate that you took the time to write a unit test \ud83d\ude2eWe'll try to do a better job keeping track of our PR queue!\n<issue_comment>username_0: No worries, was a nice learning experience"}
{"repo": "facebook/react", "issue_id": 519924494, "issue_number": 17314, "timestamp": "2019-11-14 23:16:03+00:00", "text": "Maybe related: https://github.com/facebook/react/issues/17318#issuecomment-552167774", "context": "<issue_start><issue_comment>Title: Concurrent Mode and UseSubscription with RxJS \"lose\" updates \nusername_0: <!--\r\n  Note: if the issue is about documentation or the website, please file it at:\r\n  https://github.com/reactjs/reactjs.org/issues/new\r\n-->\r\n\r\n**Do you want to request a *feature* or report a *bug*?**\r\n**Bug** - I think? \r\n\r\n**What is the current behavior?**\r\n\r\nIn Concurrent Mode, it appears that if a render is interrupted, if a component is using `useSubscription` the interrupted update is lost, which leads to \"tearing\" \r\n\r\nThe following codesandbox uses `useSubscription` with a RxJS `BehaviorSubject`, mimicking the example from here: https://www.npmjs.com/package/use-subscription#subscribing-to-observables \r\n\r\nIn the sandbox, clicking on the \"Increment Remote Count\" button triggers the RxJs `BehaviorSubject` to increment. This is done outside of the React event handler (ie: via `window.addEventLIstener` and so the updates are not batched together. The update to render the numbers is artificially slowed down. \r\n\r\nIf you click the \"Increment Remote Count\" button multiple times, the update works as expected. \r\n\r\nIf you interrupt the update, via clicking the \"increment local count\" \r\n\r\n\r\nSo the Steps to reproduce look like: \r\n1. Click the \"Increment Remote Count\" button once\r\n2. Before the update is committed to the DOM, click the \"Increment Local Count\" update. \r\n3. The first update is \"lost\" ie; the output looks like: \r\n\r\n![image](https://user-images.githubusercontent.com/787007/68468141-27255f80-0263-11ea-9afb-07964e21d9d9.png)\r\n\r\n\r\n**If the current behavior is a bug, please provide the steps to reproduce and if possible a minimal demo of the problem. Your bug will get fixed much faster if we can run your code and it doesn't have dependencies other than React. Paste the link to your JSFiddle (https://jsfiddle.net/Luktwrdm/) or CodeSandbox (https://codesandbox.io/s/new) example below:**\r\n\r\nhttps://jwenc.csb.app/ \\ https://codesandbox.io/s/usesubscriptionconcurrentlosingupdates-jwenc \r\n\r\n**What is the expected behavior?**\r\n\r\nI'd expect there to be a commit as the above screenshot, but I'd then expect there to be a follow-up commit that restores the consistency. In other words, I'd expect in the above picture for everything to be `1` \r\n\r\n**Which versions of React, and which browser / OS are affected by this issue? Did this work in previous versions of React?**\r\n\r\n`0.0.0-experimental-f6b8d31a7`\r\n\r\n\r\nI'd be willing to try to take a stab at writing a React test for this, if needed?\n<issue_comment>username_1: https://github.com/facebook/react/issues/16396#issuecomment-547193356\r\nHere's codesandbox I created.\n<issue_comment>username_0: Started looking into why this happens - https://github.com/facebook/react/pull/17336\r\n\r\nSeems like an issue somewhere in `updateSimpleMemoComponent`\n<issue_comment>username_1: Maybe related: https://github.com/facebook/react/issues/17318#issuecomment-552167774\n<issue_comment>username_2: yes i already signaled here : https://github.com/facebook/react/issues/17318#issuecomment-552160358\n<issue_comment>username_3: Reported similar issue some time ago: https://github.com/facebook/react/issues/17028<issue_closed>\n<issue_comment>username_4: Thanks for the bug report!\r\n\r\nI believe this was fixed by https://github.com/facebook/react/pull/18091\r\n\r\nYou can confirm using `react@0.0.0-experimental-ea6ed3dbb` and `react-dom@react@0.0.0-experimental-ea6ed3dbb`.\r\n\r\nI'm going to close this issue, but if the bug persists please comment and we'll reopen.\n<issue_comment>username_0: Rebased https://github.com/facebook/react/pull/17336/files and the test was green - thanks!\n<issue_comment>username_4: @username_0 Thank you for that PR! I feel a bit guilty that it got buried and I didn't notice it. But I really appreciate that you took the time to write a unit test \ud83d\ude2eWe'll try to do a better job keeping track of our PR queue!\n<issue_comment>username_0: No worries, was a nice learning experience"}
{"repo": "facebook/react", "issue_id": 519924494, "issue_number": 17314, "timestamp": "2019-11-17 21:46:40+00:00", "text": "yes i already signaled here : https://github.com/facebook/react/issues/17318#issuecomment-552160358", "context": "<issue_start><issue_comment>Title: Concurrent Mode and UseSubscription with RxJS \"lose\" updates \nusername_0: <!--\r\n  Note: if the issue is about documentation or the website, please file it at:\r\n  https://github.com/reactjs/reactjs.org/issues/new\r\n-->\r\n\r\n**Do you want to request a *feature* or report a *bug*?**\r\n**Bug** - I think? \r\n\r\n**What is the current behavior?**\r\n\r\nIn Concurrent Mode, it appears that if a render is interrupted, if a component is using `useSubscription` the interrupted update is lost, which leads to \"tearing\" \r\n\r\nThe following codesandbox uses `useSubscription` with a RxJS `BehaviorSubject`, mimicking the example from here: https://www.npmjs.com/package/use-subscription#subscribing-to-observables \r\n\r\nIn the sandbox, clicking on the \"Increment Remote Count\" button triggers the RxJs `BehaviorSubject` to increment. This is done outside of the React event handler (ie: via `window.addEventLIstener` and so the updates are not batched together. The update to render the numbers is artificially slowed down. \r\n\r\nIf you click the \"Increment Remote Count\" button multiple times, the update works as expected. \r\n\r\nIf you interrupt the update, via clicking the \"increment local count\" \r\n\r\n\r\nSo the Steps to reproduce look like: \r\n1. Click the \"Increment Remote Count\" button once\r\n2. Before the update is committed to the DOM, click the \"Increment Local Count\" update. \r\n3. The first update is \"lost\" ie; the output looks like: \r\n\r\n![image](https://user-images.githubusercontent.com/787007/68468141-27255f80-0263-11ea-9afb-07964e21d9d9.png)\r\n\r\n\r\n**If the current behavior is a bug, please provide the steps to reproduce and if possible a minimal demo of the problem. Your bug will get fixed much faster if we can run your code and it doesn't have dependencies other than React. Paste the link to your JSFiddle (https://jsfiddle.net/Luktwrdm/) or CodeSandbox (https://codesandbox.io/s/new) example below:**\r\n\r\nhttps://jwenc.csb.app/ \\ https://codesandbox.io/s/usesubscriptionconcurrentlosingupdates-jwenc \r\n\r\n**What is the expected behavior?**\r\n\r\nI'd expect there to be a commit as the above screenshot, but I'd then expect there to be a follow-up commit that restores the consistency. In other words, I'd expect in the above picture for everything to be `1` \r\n\r\n**Which versions of React, and which browser / OS are affected by this issue? Did this work in previous versions of React?**\r\n\r\n`0.0.0-experimental-f6b8d31a7`\r\n\r\n\r\nI'd be willing to try to take a stab at writing a React test for this, if needed?\n<issue_comment>username_1: https://github.com/facebook/react/issues/16396#issuecomment-547193356\r\nHere's codesandbox I created.\n<issue_comment>username_0: Started looking into why this happens - https://github.com/facebook/react/pull/17336\r\n\r\nSeems like an issue somewhere in `updateSimpleMemoComponent`\n<issue_comment>username_1: Maybe related: https://github.com/facebook/react/issues/17318#issuecomment-552167774\n<issue_comment>username_2: yes i already signaled here : https://github.com/facebook/react/issues/17318#issuecomment-552160358\n<issue_comment>username_3: Reported similar issue some time ago: https://github.com/facebook/react/issues/17028<issue_closed>\n<issue_comment>username_4: Thanks for the bug report!\r\n\r\nI believe this was fixed by https://github.com/facebook/react/pull/18091\r\n\r\nYou can confirm using `react@0.0.0-experimental-ea6ed3dbb` and `react-dom@react@0.0.0-experimental-ea6ed3dbb`.\r\n\r\nI'm going to close this issue, but if the bug persists please comment and we'll reopen.\n<issue_comment>username_0: Rebased https://github.com/facebook/react/pull/17336/files and the test was green - thanks!\n<issue_comment>username_4: @username_0 Thank you for that PR! I feel a bit guilty that it got buried and I didn't notice it. But I really appreciate that you took the time to write a unit test \ud83d\ude2eWe'll try to do a better job keeping track of our PR queue!\n<issue_comment>username_0: No worries, was a nice learning experience"}
{"repo": "facebook/react", "issue_id": 519924494, "issue_number": 17314, "timestamp": "2019-11-23 06:57:16+00:00", "text": "Reported similar issue some time ago: https://github.com/facebook/react/issues/17028", "context": "<issue_start><issue_comment>Title: Concurrent Mode and UseSubscription with RxJS \"lose\" updates \nusername_0: <!--\r\n  Note: if the issue is about documentation or the website, please file it at:\r\n  https://github.com/reactjs/reactjs.org/issues/new\r\n-->\r\n\r\n**Do you want to request a *feature* or report a *bug*?**\r\n**Bug** - I think? \r\n\r\n**What is the current behavior?**\r\n\r\nIn Concurrent Mode, it appears that if a render is interrupted, if a component is using `useSubscription` the interrupted update is lost, which leads to \"tearing\" \r\n\r\nThe following codesandbox uses `useSubscription` with a RxJS `BehaviorSubject`, mimicking the example from here: https://www.npmjs.com/package/use-subscription#subscribing-to-observables \r\n\r\nIn the sandbox, clicking on the \"Increment Remote Count\" button triggers the RxJs `BehaviorSubject` to increment. This is done outside of the React event handler (ie: via `window.addEventLIstener` and so the updates are not batched together. The update to render the numbers is artificially slowed down. \r\n\r\nIf you click the \"Increment Remote Count\" button multiple times, the update works as expected. \r\n\r\nIf you interrupt the update, via clicking the \"increment local count\" \r\n\r\n\r\nSo the Steps to reproduce look like: \r\n1. Click the \"Increment Remote Count\" button once\r\n2. Before the update is committed to the DOM, click the \"Increment Local Count\" update. \r\n3. The first update is \"lost\" ie; the output looks like: \r\n\r\n![image](https://user-images.githubusercontent.com/787007/68468141-27255f80-0263-11ea-9afb-07964e21d9d9.png)\r\n\r\n\r\n**If the current behavior is a bug, please provide the steps to reproduce and if possible a minimal demo of the problem. Your bug will get fixed much faster if we can run your code and it doesn't have dependencies other than React. Paste the link to your JSFiddle (https://jsfiddle.net/Luktwrdm/) or CodeSandbox (https://codesandbox.io/s/new) example below:**\r\n\r\nhttps://jwenc.csb.app/ \\ https://codesandbox.io/s/usesubscriptionconcurrentlosingupdates-jwenc \r\n\r\n**What is the expected behavior?**\r\n\r\nI'd expect there to be a commit as the above screenshot, but I'd then expect there to be a follow-up commit that restores the consistency. In other words, I'd expect in the above picture for everything to be `1` \r\n\r\n**Which versions of React, and which browser / OS are affected by this issue? Did this work in previous versions of React?**\r\n\r\n`0.0.0-experimental-f6b8d31a7`\r\n\r\n\r\nI'd be willing to try to take a stab at writing a React test for this, if needed?\n<issue_comment>username_1: https://github.com/facebook/react/issues/16396#issuecomment-547193356\r\nHere's codesandbox I created.\n<issue_comment>username_0: Started looking into why this happens - https://github.com/facebook/react/pull/17336\r\n\r\nSeems like an issue somewhere in `updateSimpleMemoComponent`\n<issue_comment>username_1: Maybe related: https://github.com/facebook/react/issues/17318#issuecomment-552167774\n<issue_comment>username_2: yes i already signaled here : https://github.com/facebook/react/issues/17318#issuecomment-552160358\n<issue_comment>username_3: Reported similar issue some time ago: https://github.com/facebook/react/issues/17028<issue_closed>\n<issue_comment>username_4: Thanks for the bug report!\r\n\r\nI believe this was fixed by https://github.com/facebook/react/pull/18091\r\n\r\nYou can confirm using `react@0.0.0-experimental-ea6ed3dbb` and `react-dom@react@0.0.0-experimental-ea6ed3dbb`.\r\n\r\nI'm going to close this issue, but if the bug persists please comment and we'll reopen.\n<issue_comment>username_0: Rebased https://github.com/facebook/react/pull/17336/files and the test was green - thanks!\n<issue_comment>username_4: @username_0 Thank you for that PR! I feel a bit guilty that it got buried and I didn't notice it. But I really appreciate that you took the time to write a unit test \ud83d\ude2eWe'll try to do a better job keeping track of our PR queue!\n<issue_comment>username_0: No worries, was a nice learning experience"}
{"repo": "facebook/react", "issue_id": 519924494, "issue_number": 17314, "timestamp": "2020-02-21 00:56:05+00:00", "text": "", "context": "<issue_start><issue_comment>Title: Concurrent Mode and UseSubscription with RxJS \"lose\" updates \nusername_0: <!--\r\n  Note: if the issue is about documentation or the website, please file it at:\r\n  https://github.com/reactjs/reactjs.org/issues/new\r\n-->\r\n\r\n**Do you want to request a *feature* or report a *bug*?**\r\n**Bug** - I think? \r\n\r\n**What is the current behavior?**\r\n\r\nIn Concurrent Mode, it appears that if a render is interrupted, if a component is using `useSubscription` the interrupted update is lost, which leads to \"tearing\" \r\n\r\nThe following codesandbox uses `useSubscription` with a RxJS `BehaviorSubject`, mimicking the example from here: https://www.npmjs.com/package/use-subscription#subscribing-to-observables \r\n\r\nIn the sandbox, clicking on the \"Increment Remote Count\" button triggers the RxJs `BehaviorSubject` to increment. This is done outside of the React event handler (ie: via `window.addEventLIstener` and so the updates are not batched together. The update to render the numbers is artificially slowed down. \r\n\r\nIf you click the \"Increment Remote Count\" button multiple times, the update works as expected. \r\n\r\nIf you interrupt the update, via clicking the \"increment local count\" \r\n\r\n\r\nSo the Steps to reproduce look like: \r\n1. Click the \"Increment Remote Count\" button once\r\n2. Before the update is committed to the DOM, click the \"Increment Local Count\" update. \r\n3. The first update is \"lost\" ie; the output looks like: \r\n\r\n![image](https://user-images.githubusercontent.com/787007/68468141-27255f80-0263-11ea-9afb-07964e21d9d9.png)\r\n\r\n\r\n**If the current behavior is a bug, please provide the steps to reproduce and if possible a minimal demo of the problem. Your bug will get fixed much faster if we can run your code and it doesn't have dependencies other than React. Paste the link to your JSFiddle (https://jsfiddle.net/Luktwrdm/) or CodeSandbox (https://codesandbox.io/s/new) example below:**\r\n\r\nhttps://jwenc.csb.app/ \\ https://codesandbox.io/s/usesubscriptionconcurrentlosingupdates-jwenc \r\n\r\n**What is the expected behavior?**\r\n\r\nI'd expect there to be a commit as the above screenshot, but I'd then expect there to be a follow-up commit that restores the consistency. In other words, I'd expect in the above picture for everything to be `1` \r\n\r\n**Which versions of React, and which browser / OS are affected by this issue? Did this work in previous versions of React?**\r\n\r\n`0.0.0-experimental-f6b8d31a7`\r\n\r\n\r\nI'd be willing to try to take a stab at writing a React test for this, if needed?\n<issue_comment>username_1: https://github.com/facebook/react/issues/16396#issuecomment-547193356\r\nHere's codesandbox I created.\n<issue_comment>username_0: Started looking into why this happens - https://github.com/facebook/react/pull/17336\r\n\r\nSeems like an issue somewhere in `updateSimpleMemoComponent`\n<issue_comment>username_1: Maybe related: https://github.com/facebook/react/issues/17318#issuecomment-552167774\n<issue_comment>username_2: yes i already signaled here : https://github.com/facebook/react/issues/17318#issuecomment-552160358\n<issue_comment>username_3: Reported similar issue some time ago: https://github.com/facebook/react/issues/17028<issue_closed>\n<issue_comment>username_4: Thanks for the bug report!\r\n\r\nI believe this was fixed by https://github.com/facebook/react/pull/18091\r\n\r\nYou can confirm using `react@0.0.0-experimental-ea6ed3dbb` and `react-dom@react@0.0.0-experimental-ea6ed3dbb`.\r\n\r\nI'm going to close this issue, but if the bug persists please comment and we'll reopen.\n<issue_comment>username_0: Rebased https://github.com/facebook/react/pull/17336/files and the test was green - thanks!\n<issue_comment>username_4: @username_0 Thank you for that PR! I feel a bit guilty that it got buried and I didn't notice it. But I really appreciate that you took the time to write a unit test \ud83d\ude2eWe'll try to do a better job keeping track of our PR queue!\n<issue_comment>username_0: No worries, was a nice learning experience"}
{"repo": "facebook/react", "issue_id": 519924494, "issue_number": 17314, "timestamp": "2020-02-21 00:56:05+00:00", "text": "Thanks for the bug report!\r\n\r\nI believe this was fixed by https://github.com/facebook/react/pull/18091\r\n\r\nYou can confirm using `react@0.0.0-experimental-ea6ed3dbb` and `react-dom@react@0.0.0-experimental-ea6ed3dbb`.\r\n\r\nI'm going to close this issue, but if the bug persists please comment and we'll reopen.", "context": "<issue_start><issue_comment>Title: Concurrent Mode and UseSubscription with RxJS \"lose\" updates \nusername_0: <!--\r\n  Note: if the issue is about documentation or the website, please file it at:\r\n  https://github.com/reactjs/reactjs.org/issues/new\r\n-->\r\n\r\n**Do you want to request a *feature* or report a *bug*?**\r\n**Bug** - I think? \r\n\r\n**What is the current behavior?**\r\n\r\nIn Concurrent Mode, it appears that if a render is interrupted, if a component is using `useSubscription` the interrupted update is lost, which leads to \"tearing\" \r\n\r\nThe following codesandbox uses `useSubscription` with a RxJS `BehaviorSubject`, mimicking the example from here: https://www.npmjs.com/package/use-subscription#subscribing-to-observables \r\n\r\nIn the sandbox, clicking on the \"Increment Remote Count\" button triggers the RxJs `BehaviorSubject` to increment. This is done outside of the React event handler (ie: via `window.addEventLIstener` and so the updates are not batched together. The update to render the numbers is artificially slowed down. \r\n\r\nIf you click the \"Increment Remote Count\" button multiple times, the update works as expected. \r\n\r\nIf you interrupt the update, via clicking the \"increment local count\" \r\n\r\n\r\nSo the Steps to reproduce look like: \r\n1. Click the \"Increment Remote Count\" button once\r\n2. Before the update is committed to the DOM, click the \"Increment Local Count\" update. \r\n3. The first update is \"lost\" ie; the output looks like: \r\n\r\n![image](https://user-images.githubusercontent.com/787007/68468141-27255f80-0263-11ea-9afb-07964e21d9d9.png)\r\n\r\n\r\n**If the current behavior is a bug, please provide the steps to reproduce and if possible a minimal demo of the problem. Your bug will get fixed much faster if we can run your code and it doesn't have dependencies other than React. Paste the link to your JSFiddle (https://jsfiddle.net/Luktwrdm/) or CodeSandbox (https://codesandbox.io/s/new) example below:**\r\n\r\nhttps://jwenc.csb.app/ \\ https://codesandbox.io/s/usesubscriptionconcurrentlosingupdates-jwenc \r\n\r\n**What is the expected behavior?**\r\n\r\nI'd expect there to be a commit as the above screenshot, but I'd then expect there to be a follow-up commit that restores the consistency. In other words, I'd expect in the above picture for everything to be `1` \r\n\r\n**Which versions of React, and which browser / OS are affected by this issue? Did this work in previous versions of React?**\r\n\r\n`0.0.0-experimental-f6b8d31a7`\r\n\r\n\r\nI'd be willing to try to take a stab at writing a React test for this, if needed?\n<issue_comment>username_1: https://github.com/facebook/react/issues/16396#issuecomment-547193356\r\nHere's codesandbox I created.\n<issue_comment>username_0: Started looking into why this happens - https://github.com/facebook/react/pull/17336\r\n\r\nSeems like an issue somewhere in `updateSimpleMemoComponent`\n<issue_comment>username_1: Maybe related: https://github.com/facebook/react/issues/17318#issuecomment-552167774\n<issue_comment>username_2: yes i already signaled here : https://github.com/facebook/react/issues/17318#issuecomment-552160358\n<issue_comment>username_3: Reported similar issue some time ago: https://github.com/facebook/react/issues/17028<issue_closed>\n<issue_comment>username_4: Thanks for the bug report!\r\n\r\nI believe this was fixed by https://github.com/facebook/react/pull/18091\r\n\r\nYou can confirm using `react@0.0.0-experimental-ea6ed3dbb` and `react-dom@react@0.0.0-experimental-ea6ed3dbb`.\r\n\r\nI'm going to close this issue, but if the bug persists please comment and we'll reopen.\n<issue_comment>username_0: Rebased https://github.com/facebook/react/pull/17336/files and the test was green - thanks!\n<issue_comment>username_4: @username_0 Thank you for that PR! I feel a bit guilty that it got buried and I didn't notice it. But I really appreciate that you took the time to write a unit test \ud83d\ude2eWe'll try to do a better job keeping track of our PR queue!\n<issue_comment>username_0: No worries, was a nice learning experience"}
{"repo": "facebook/react", "issue_id": 519924494, "issue_number": 17314, "timestamp": "2020-02-21 01:09:10+00:00", "text": "Rebased https://github.com/facebook/react/pull/17336/files and the test was green - thanks!", "context": "<issue_start><issue_comment>Title: Concurrent Mode and UseSubscription with RxJS \"lose\" updates \nusername_0: <!--\r\n  Note: if the issue is about documentation or the website, please file it at:\r\n  https://github.com/reactjs/reactjs.org/issues/new\r\n-->\r\n\r\n**Do you want to request a *feature* or report a *bug*?**\r\n**Bug** - I think? \r\n\r\n**What is the current behavior?**\r\n\r\nIn Concurrent Mode, it appears that if a render is interrupted, if a component is using `useSubscription` the interrupted update is lost, which leads to \"tearing\" \r\n\r\nThe following codesandbox uses `useSubscription` with a RxJS `BehaviorSubject`, mimicking the example from here: https://www.npmjs.com/package/use-subscription#subscribing-to-observables \r\n\r\nIn the sandbox, clicking on the \"Increment Remote Count\" button triggers the RxJs `BehaviorSubject` to increment. This is done outside of the React event handler (ie: via `window.addEventLIstener` and so the updates are not batched together. The update to render the numbers is artificially slowed down. \r\n\r\nIf you click the \"Increment Remote Count\" button multiple times, the update works as expected. \r\n\r\nIf you interrupt the update, via clicking the \"increment local count\" \r\n\r\n\r\nSo the Steps to reproduce look like: \r\n1. Click the \"Increment Remote Count\" button once\r\n2. Before the update is committed to the DOM, click the \"Increment Local Count\" update. \r\n3. The first update is \"lost\" ie; the output looks like: \r\n\r\n![image](https://user-images.githubusercontent.com/787007/68468141-27255f80-0263-11ea-9afb-07964e21d9d9.png)\r\n\r\n\r\n**If the current behavior is a bug, please provide the steps to reproduce and if possible a minimal demo of the problem. Your bug will get fixed much faster if we can run your code and it doesn't have dependencies other than React. Paste the link to your JSFiddle (https://jsfiddle.net/Luktwrdm/) or CodeSandbox (https://codesandbox.io/s/new) example below:**\r\n\r\nhttps://jwenc.csb.app/ \\ https://codesandbox.io/s/usesubscriptionconcurrentlosingupdates-jwenc \r\n\r\n**What is the expected behavior?**\r\n\r\nI'd expect there to be a commit as the above screenshot, but I'd then expect there to be a follow-up commit that restores the consistency. In other words, I'd expect in the above picture for everything to be `1` \r\n\r\n**Which versions of React, and which browser / OS are affected by this issue? Did this work in previous versions of React?**\r\n\r\n`0.0.0-experimental-f6b8d31a7`\r\n\r\n\r\nI'd be willing to try to take a stab at writing a React test for this, if needed?\n<issue_comment>username_1: https://github.com/facebook/react/issues/16396#issuecomment-547193356\r\nHere's codesandbox I created.\n<issue_comment>username_0: Started looking into why this happens - https://github.com/facebook/react/pull/17336\r\n\r\nSeems like an issue somewhere in `updateSimpleMemoComponent`\n<issue_comment>username_1: Maybe related: https://github.com/facebook/react/issues/17318#issuecomment-552167774\n<issue_comment>username_2: yes i already signaled here : https://github.com/facebook/react/issues/17318#issuecomment-552160358\n<issue_comment>username_3: Reported similar issue some time ago: https://github.com/facebook/react/issues/17028<issue_closed>\n<issue_comment>username_4: Thanks for the bug report!\r\n\r\nI believe this was fixed by https://github.com/facebook/react/pull/18091\r\n\r\nYou can confirm using `react@0.0.0-experimental-ea6ed3dbb` and `react-dom@react@0.0.0-experimental-ea6ed3dbb`.\r\n\r\nI'm going to close this issue, but if the bug persists please comment and we'll reopen.\n<issue_comment>username_0: Rebased https://github.com/facebook/react/pull/17336/files and the test was green - thanks!\n<issue_comment>username_4: @username_0 Thank you for that PR! I feel a bit guilty that it got buried and I didn't notice it. But I really appreciate that you took the time to write a unit test \ud83d\ude2eWe'll try to do a better job keeping track of our PR queue!\n<issue_comment>username_0: No worries, was a nice learning experience"}
{"repo": "facebook/react", "issue_id": 519924494, "issue_number": 17314, "timestamp": "2020-02-21 01:26:19+00:00", "text": "@username_0 Thank you for that PR! I feel a bit guilty that it got buried and I didn't notice it. But I really appreciate that you took the time to write a unit test \ud83d\ude2eWe'll try to do a better job keeping track of our PR queue!", "context": "<issue_start><issue_comment>Title: Concurrent Mode and UseSubscription with RxJS \"lose\" updates \nusername_0: <!--\r\n  Note: if the issue is about documentation or the website, please file it at:\r\n  https://github.com/reactjs/reactjs.org/issues/new\r\n-->\r\n\r\n**Do you want to request a *feature* or report a *bug*?**\r\n**Bug** - I think? \r\n\r\n**What is the current behavior?**\r\n\r\nIn Concurrent Mode, it appears that if a render is interrupted, if a component is using `useSubscription` the interrupted update is lost, which leads to \"tearing\" \r\n\r\nThe following codesandbox uses `useSubscription` with a RxJS `BehaviorSubject`, mimicking the example from here: https://www.npmjs.com/package/use-subscription#subscribing-to-observables \r\n\r\nIn the sandbox, clicking on the \"Increment Remote Count\" button triggers the RxJs `BehaviorSubject` to increment. This is done outside of the React event handler (ie: via `window.addEventLIstener` and so the updates are not batched together. The update to render the numbers is artificially slowed down. \r\n\r\nIf you click the \"Increment Remote Count\" button multiple times, the update works as expected. \r\n\r\nIf you interrupt the update, via clicking the \"increment local count\" \r\n\r\n\r\nSo the Steps to reproduce look like: \r\n1. Click the \"Increment Remote Count\" button once\r\n2. Before the update is committed to the DOM, click the \"Increment Local Count\" update. \r\n3. The first update is \"lost\" ie; the output looks like: \r\n\r\n![image](https://user-images.githubusercontent.com/787007/68468141-27255f80-0263-11ea-9afb-07964e21d9d9.png)\r\n\r\n\r\n**If the current behavior is a bug, please provide the steps to reproduce and if possible a minimal demo of the problem. Your bug will get fixed much faster if we can run your code and it doesn't have dependencies other than React. Paste the link to your JSFiddle (https://jsfiddle.net/Luktwrdm/) or CodeSandbox (https://codesandbox.io/s/new) example below:**\r\n\r\nhttps://jwenc.csb.app/ \\ https://codesandbox.io/s/usesubscriptionconcurrentlosingupdates-jwenc \r\n\r\n**What is the expected behavior?**\r\n\r\nI'd expect there to be a commit as the above screenshot, but I'd then expect there to be a follow-up commit that restores the consistency. In other words, I'd expect in the above picture for everything to be `1` \r\n\r\n**Which versions of React, and which browser / OS are affected by this issue? Did this work in previous versions of React?**\r\n\r\n`0.0.0-experimental-f6b8d31a7`\r\n\r\n\r\nI'd be willing to try to take a stab at writing a React test for this, if needed?\n<issue_comment>username_1: https://github.com/facebook/react/issues/16396#issuecomment-547193356\r\nHere's codesandbox I created.\n<issue_comment>username_0: Started looking into why this happens - https://github.com/facebook/react/pull/17336\r\n\r\nSeems like an issue somewhere in `updateSimpleMemoComponent`\n<issue_comment>username_1: Maybe related: https://github.com/facebook/react/issues/17318#issuecomment-552167774\n<issue_comment>username_2: yes i already signaled here : https://github.com/facebook/react/issues/17318#issuecomment-552160358\n<issue_comment>username_3: Reported similar issue some time ago: https://github.com/facebook/react/issues/17028<issue_closed>\n<issue_comment>username_4: Thanks for the bug report!\r\n\r\nI believe this was fixed by https://github.com/facebook/react/pull/18091\r\n\r\nYou can confirm using `react@0.0.0-experimental-ea6ed3dbb` and `react-dom@react@0.0.0-experimental-ea6ed3dbb`.\r\n\r\nI'm going to close this issue, but if the bug persists please comment and we'll reopen.\n<issue_comment>username_0: Rebased https://github.com/facebook/react/pull/17336/files and the test was green - thanks!\n<issue_comment>username_4: @username_0 Thank you for that PR! I feel a bit guilty that it got buried and I didn't notice it. But I really appreciate that you took the time to write a unit test \ud83d\ude2eWe'll try to do a better job keeping track of our PR queue!\n<issue_comment>username_0: No worries, was a nice learning experience"}
{"repo": "facebook/react", "issue_id": 519924494, "issue_number": 17314, "timestamp": "2020-02-21 01:48:00+00:00", "text": "No worries, was a nice learning experience", "context": "<issue_start><issue_comment>Title: Concurrent Mode and UseSubscription with RxJS \"lose\" updates \nusername_0: <!--\r\n  Note: if the issue is about documentation or the website, please file it at:\r\n  https://github.com/reactjs/reactjs.org/issues/new\r\n-->\r\n\r\n**Do you want to request a *feature* or report a *bug*?**\r\n**Bug** - I think? \r\n\r\n**What is the current behavior?**\r\n\r\nIn Concurrent Mode, it appears that if a render is interrupted, if a component is using `useSubscription` the interrupted update is lost, which leads to \"tearing\" \r\n\r\nThe following codesandbox uses `useSubscription` with a RxJS `BehaviorSubject`, mimicking the example from here: https://www.npmjs.com/package/use-subscription#subscribing-to-observables \r\n\r\nIn the sandbox, clicking on the \"Increment Remote Count\" button triggers the RxJs `BehaviorSubject` to increment. This is done outside of the React event handler (ie: via `window.addEventLIstener` and so the updates are not batched together. The update to render the numbers is artificially slowed down. \r\n\r\nIf you click the \"Increment Remote Count\" button multiple times, the update works as expected. \r\n\r\nIf you interrupt the update, via clicking the \"increment local count\" \r\n\r\n\r\nSo the Steps to reproduce look like: \r\n1. Click the \"Increment Remote Count\" button once\r\n2. Before the update is committed to the DOM, click the \"Increment Local Count\" update. \r\n3. The first update is \"lost\" ie; the output looks like: \r\n\r\n![image](https://user-images.githubusercontent.com/787007/68468141-27255f80-0263-11ea-9afb-07964e21d9d9.png)\r\n\r\n\r\n**If the current behavior is a bug, please provide the steps to reproduce and if possible a minimal demo of the problem. Your bug will get fixed much faster if we can run your code and it doesn't have dependencies other than React. Paste the link to your JSFiddle (https://jsfiddle.net/Luktwrdm/) or CodeSandbox (https://codesandbox.io/s/new) example below:**\r\n\r\nhttps://jwenc.csb.app/ \\ https://codesandbox.io/s/usesubscriptionconcurrentlosingupdates-jwenc \r\n\r\n**What is the expected behavior?**\r\n\r\nI'd expect there to be a commit as the above screenshot, but I'd then expect there to be a follow-up commit that restores the consistency. In other words, I'd expect in the above picture for everything to be `1` \r\n\r\n**Which versions of React, and which browser / OS are affected by this issue? Did this work in previous versions of React?**\r\n\r\n`0.0.0-experimental-f6b8d31a7`\r\n\r\n\r\nI'd be willing to try to take a stab at writing a React test for this, if needed?\n<issue_comment>username_1: https://github.com/facebook/react/issues/16396#issuecomment-547193356\r\nHere's codesandbox I created.\n<issue_comment>username_0: Started looking into why this happens - https://github.com/facebook/react/pull/17336\r\n\r\nSeems like an issue somewhere in `updateSimpleMemoComponent`\n<issue_comment>username_1: Maybe related: https://github.com/facebook/react/issues/17318#issuecomment-552167774\n<issue_comment>username_2: yes i already signaled here : https://github.com/facebook/react/issues/17318#issuecomment-552160358\n<issue_comment>username_3: Reported similar issue some time ago: https://github.com/facebook/react/issues/17028<issue_closed>\n<issue_comment>username_4: Thanks for the bug report!\r\n\r\nI believe this was fixed by https://github.com/facebook/react/pull/18091\r\n\r\nYou can confirm using `react@0.0.0-experimental-ea6ed3dbb` and `react-dom@react@0.0.0-experimental-ea6ed3dbb`.\r\n\r\nI'm going to close this issue, but if the bug persists please comment and we'll reopen.\n<issue_comment>username_0: Rebased https://github.com/facebook/react/pull/17336/files and the test was green - thanks!\n<issue_comment>username_4: @username_0 Thank you for that PR! I feel a bit guilty that it got buried and I didn't notice it. But I really appreciate that you took the time to write a unit test \ud83d\ude2eWe'll try to do a better job keeping track of our PR queue!\n<issue_comment>username_0: No worries, was a nice learning experience"}
{"repo": "facebook/react", "issue_id": 566892272, "issue_number": 18065, "timestamp": "2020-02-18 13:27:21+00:00", "text": "The useEffect() hook produces false positive warnings concerning \"React Hook useEffect has missing dependencies\" in rare cases.\r\n\r\nReact version: 16.12.0\r\n\r\n## Steps To Reproduce\r\n\r\nThere are situations in which \"React Hook useEffect has missing dependencies\" is not correct concerning the problem and implementation logic at hand. Example:\r\n\r\n```ts\r\nconst [current, send] = useMachine(machine);\r\nconst currentMatchesSignedIn = current.matches('signedIn');\r\n\r\nuseEffect(() => {\r\n    if (currentMatchesSignedIn) {\r\n        send('UPDATE_ENTRIES');\r\n    }\r\n  }, [currentMatchesSignedIn]);\r\n```\r\n\r\n`current` represents the current state, `send` is a function that the effect should not depend upon. It would be wrong if the effect would depend upon a change of it.\r\n\r\nI had other circumstance in which the warning was plainly wrong, that didn't involve Xstate. It would be nice if there was a way to disable this warning for this rare cases.", "context": "<issue_start><issue_comment>Title: False positives with warning \"React Hook useEffect has missing dependencies\"\nusername_0: The useEffect() hook produces false positive warnings concerning \"React Hook useEffect has missing dependencies\" in rare cases.\r\n\r\nReact version: 16.12.0\r\n\r\n## Steps To Reproduce\r\n\r\nThere are situations in which \"React Hook useEffect has missing dependencies\" is not correct concerning the problem and implementation logic at hand. Example:\r\n\r\n```ts\r\nconst [current, send] = useMachine(machine);\r\nconst currentMatchesSignedIn = current.matches('signedIn');\r\n\r\nuseEffect(() => {\r\n    if (currentMatchesSignedIn) {\r\n        send('UPDATE_ENTRIES');\r\n    }\r\n  }, [currentMatchesSignedIn]);\r\n```\r\n\r\n`current` represents the current state, `send` is a function that the effect should not depend upon. It would be wrong if the effect would depend upon a change of it.\r\n\r\nI had other circumstance in which the warning was plainly wrong, that didn't involve Xstate. It would be nice if there was a way to disable this warning for this rare cases.\n<issue_comment>username_1: Isn't that what `eslint-disable-line` / `eslint-disable-next-line` do? (To be clear, I am not advising you use them in this case.)<issue_closed>\n<issue_comment>username_2: As @username_1 says, the warning is correct. If `send` is truly stable, include it in deps.\n<issue_comment>username_0: Thanks, understood. I'll invest the time to do the necessary changes."}
{"repo": "facebook/react", "issue_id": 566892272, "issue_number": 18065, "timestamp": "2020-02-24 02:58:43+00:00", "text": "Isn't that what `eslint-disable-line` / `eslint-disable-next-line` do? (To be clear, I am not advising you use them in this case.)", "context": "<issue_start><issue_comment>Title: False positives with warning \"React Hook useEffect has missing dependencies\"\nusername_0: The useEffect() hook produces false positive warnings concerning \"React Hook useEffect has missing dependencies\" in rare cases.\r\n\r\nReact version: 16.12.0\r\n\r\n## Steps To Reproduce\r\n\r\nThere are situations in which \"React Hook useEffect has missing dependencies\" is not correct concerning the problem and implementation logic at hand. Example:\r\n\r\n```ts\r\nconst [current, send] = useMachine(machine);\r\nconst currentMatchesSignedIn = current.matches('signedIn');\r\n\r\nuseEffect(() => {\r\n    if (currentMatchesSignedIn) {\r\n        send('UPDATE_ENTRIES');\r\n    }\r\n  }, [currentMatchesSignedIn]);\r\n```\r\n\r\n`current` represents the current state, `send` is a function that the effect should not depend upon. It would be wrong if the effect would depend upon a change of it.\r\n\r\nI had other circumstance in which the warning was plainly wrong, that didn't involve Xstate. It would be nice if there was a way to disable this warning for this rare cases.\n<issue_comment>username_1: Isn't that what `eslint-disable-line` / `eslint-disable-next-line` do? (To be clear, I am not advising you use them in this case.)<issue_closed>\n<issue_comment>username_2: As @username_1 says, the warning is correct. If `send` is truly stable, include it in deps.\n<issue_comment>username_0: Thanks, understood. I'll invest the time to do the necessary changes."}
{"repo": "facebook/react", "issue_id": 566892272, "issue_number": 18065, "timestamp": "2020-02-24 15:27:05+00:00", "text": "", "context": "<issue_start><issue_comment>Title: False positives with warning \"React Hook useEffect has missing dependencies\"\nusername_0: The useEffect() hook produces false positive warnings concerning \"React Hook useEffect has missing dependencies\" in rare cases.\r\n\r\nReact version: 16.12.0\r\n\r\n## Steps To Reproduce\r\n\r\nThere are situations in which \"React Hook useEffect has missing dependencies\" is not correct concerning the problem and implementation logic at hand. Example:\r\n\r\n```ts\r\nconst [current, send] = useMachine(machine);\r\nconst currentMatchesSignedIn = current.matches('signedIn');\r\n\r\nuseEffect(() => {\r\n    if (currentMatchesSignedIn) {\r\n        send('UPDATE_ENTRIES');\r\n    }\r\n  }, [currentMatchesSignedIn]);\r\n```\r\n\r\n`current` represents the current state, `send` is a function that the effect should not depend upon. It would be wrong if the effect would depend upon a change of it.\r\n\r\nI had other circumstance in which the warning was plainly wrong, that didn't involve Xstate. It would be nice if there was a way to disable this warning for this rare cases.\n<issue_comment>username_1: Isn't that what `eslint-disable-line` / `eslint-disable-next-line` do? (To be clear, I am not advising you use them in this case.)<issue_closed>\n<issue_comment>username_2: As @username_1 says, the warning is correct. If `send` is truly stable, include it in deps.\n<issue_comment>username_0: Thanks, understood. I'll invest the time to do the necessary changes."}
{"repo": "facebook/react", "issue_id": 566892272, "issue_number": 18065, "timestamp": "2020-02-24 15:27:29+00:00", "text": "As @username_1 says, the warning is correct. If `send` is truly stable, include it in deps.", "context": "<issue_start><issue_comment>Title: False positives with warning \"React Hook useEffect has missing dependencies\"\nusername_0: The useEffect() hook produces false positive warnings concerning \"React Hook useEffect has missing dependencies\" in rare cases.\r\n\r\nReact version: 16.12.0\r\n\r\n## Steps To Reproduce\r\n\r\nThere are situations in which \"React Hook useEffect has missing dependencies\" is not correct concerning the problem and implementation logic at hand. Example:\r\n\r\n```ts\r\nconst [current, send] = useMachine(machine);\r\nconst currentMatchesSignedIn = current.matches('signedIn');\r\n\r\nuseEffect(() => {\r\n    if (currentMatchesSignedIn) {\r\n        send('UPDATE_ENTRIES');\r\n    }\r\n  }, [currentMatchesSignedIn]);\r\n```\r\n\r\n`current` represents the current state, `send` is a function that the effect should not depend upon. It would be wrong if the effect would depend upon a change of it.\r\n\r\nI had other circumstance in which the warning was plainly wrong, that didn't involve Xstate. It would be nice if there was a way to disable this warning for this rare cases.\n<issue_comment>username_1: Isn't that what `eslint-disable-line` / `eslint-disable-next-line` do? (To be clear, I am not advising you use them in this case.)<issue_closed>\n<issue_comment>username_2: As @username_1 says, the warning is correct. If `send` is truly stable, include it in deps.\n<issue_comment>username_0: Thanks, understood. I'll invest the time to do the necessary changes."}
{"repo": "facebook/react", "issue_id": 566892272, "issue_number": 18065, "timestamp": "2020-02-26 10:45:24+00:00", "text": "Thanks, understood. I'll invest the time to do the necessary changes.", "context": "<issue_start><issue_comment>Title: False positives with warning \"React Hook useEffect has missing dependencies\"\nusername_0: The useEffect() hook produces false positive warnings concerning \"React Hook useEffect has missing dependencies\" in rare cases.\r\n\r\nReact version: 16.12.0\r\n\r\n## Steps To Reproduce\r\n\r\nThere are situations in which \"React Hook useEffect has missing dependencies\" is not correct concerning the problem and implementation logic at hand. Example:\r\n\r\n```ts\r\nconst [current, send] = useMachine(machine);\r\nconst currentMatchesSignedIn = current.matches('signedIn');\r\n\r\nuseEffect(() => {\r\n    if (currentMatchesSignedIn) {\r\n        send('UPDATE_ENTRIES');\r\n    }\r\n  }, [currentMatchesSignedIn]);\r\n```\r\n\r\n`current` represents the current state, `send` is a function that the effect should not depend upon. It would be wrong if the effect would depend upon a change of it.\r\n\r\nI had other circumstance in which the warning was plainly wrong, that didn't involve Xstate. It would be nice if there was a way to disable this warning for this rare cases.\n<issue_comment>username_1: Isn't that what `eslint-disable-line` / `eslint-disable-next-line` do? (To be clear, I am not advising you use them in this case.)<issue_closed>\n<issue_comment>username_2: As @username_1 says, the warning is correct. If `send` is truly stable, include it in deps.\n<issue_comment>username_0: Thanks, understood. I'll invest the time to do the necessary changes."}
{"repo": "pytorch/pytorch", "issue_id": 617506552, "issue_number": 38396, "timestamp": "2020-05-13 14:40:32+00:00", "text": "https://ci.pytorch.org/jenkins/job/pytorch-builds/job/py3.6-clang7-rocmdeb-ubuntu16.04-test1/31353/console\r\n\r\n```\r\n09:24:32 ======================================================================\r\n09:24:32 FAIL: test_activations_bfloat16_cuda (__main__.TestNNDeviceTypeCUDA)\r\n09:24:32 ----------------------------------------------------------------------\r\n09:24:32 Traceback (most recent call last):\r\n09:24:32   File \"/var/lib/jenkins/.local/lib/python3.6/site-packages/torch/testing/_internal/common_utils.py\", line 753, in wrapper\r\n09:24:32     method(*args, **kwargs)\r\n09:24:32   File \"/var/lib/jenkins/.local/lib/python3.6/site-packages/torch/testing/_internal/common_device_type.py\", line 210, in instantiated_test\r\n09:24:32     return test(self, device_arg)\r\n09:24:32   File \"/var/lib/jenkins/.local/lib/python3.6/site-packages/torch/testing/_internal/common_device_type.py\", line 465, in only_fn\r\n09:24:32     return fn(slf, device, *args, **kwargs)\r\n09:24:32   File \"/var/lib/jenkins/.local/lib/python3.6/site-packages/torch/testing/_internal/common_device_type.py\", line 404, in dep_fn\r\n09:24:32     return fn(slf, device, *args, **kwargs)\r\n09:24:32   File \"test_nn.py\", line 11063, in test_activations_bfloat16\r\n09:24:32     self._test_bfloat16_ops(torch.nn.ReLU(), device, inp_dims=(5), prec=1e-2)\r\n09:24:32   File \"test_nn.py\", line 11057, in _test_bfloat16_ops\r\n09:24:32     self.assertEqual(out1, out2, atol=prec)\r\n09:24:32   File \"/var/lib/jenkins/.local/lib/python3.6/site-packages/torch/testing/_internal/common_utils.py\", line 989, in assertEqual\r\n09:24:32     assertTensorsEqual(x, y)\r\n09:24:32   File \"/var/lib/jenkins/.local/lib/python3.6/site-packages/torch/testing/_internal/common_utils.py\", line 924, in assertTensorsEqual\r\n09:24:32     self.assertEqual(a.dtype, b.dtype)\r\n09:24:32   File \"/var/lib/jenkins/.local/lib/python3.6/site-packages/torch/testing/_internal/common_utils.py\", line 1022, in assertEqual\r\n09:24:32     super().assertEqual(x, y, message)\r\n09:24:32 AssertionError: torch.float32 != torch.bfloat16 : \r\n09:24:32 \r\n09:24:32 ======================================================================\r\n09:24:32 FAIL: test_pooling_bfloat16_cuda (__main__.TestNNDeviceTypeCUDA)\r\n09:24:32 ----------------------------------------------------------------------\r\n09:24:32 Traceback (most recent call last):\r\n09:24:32   File \"/var/lib/jenkins/.local/lib/python3.6/site-packages/torch/testing/_internal/common_utils.py\", line 753, in wrapper\r\n09:24:32     method(*args, **kwargs)\r\n09:24:32   File \"/var/lib/jenkins/.local/lib/python3.6/site-packages/torch/testing/_internal/common_device_type.py\", line 210, in instantiated_test\r\n09:24:32     return test(self, device_arg)\r\n09:24:32   File \"/var/lib/jenkins/.local/lib/python3.6/site-packages/torch/testing/_internal/common_device_type.py\", line 465, in only_fn\r\n09:24:32     return fn(slf, device, *args, **kwargs)\r\n09:24:32   File \"/var/lib/jenkins/.local/lib/python3.6/site-packages/torch/testing/_internal/common_device_type.py\", line 404, in dep_fn\r\n09:24:32     return fn(slf, device, *args, **kwargs)\r\n09:24:32   File \"test_nn.py\", line 11074, in test_pooling_bfloat16\r\n09:24:32     self._test_bfloat16_ops(torch.nn.AvgPool1d(3, stride=2), device, inp_dims=(8, 4, 16), prec=0.05)\r\n09:24:32   File \"test_nn.py\", line 11057, in _test_bfloat16_ops\r\n09:24:32     self.assertEqual(out1, out2, atol=prec)\r\n09:24:32   File \"/var/lib/jenkins/.local/lib/python3.6/site-packages/torch/testing/_internal/common_utils.py\", line 989, in assertEqual\r\n09:24:32     assertTensorsEqual(x, y)\r\n09:24:32   File \"/var/lib/jenkins/.local/lib/python3.6/site-packages/torch/testing/_internal/common_utils.py\", line 924, in assertTensorsEqual\r\n09:24:32     self.assertEqual(a.dtype, b.dtype)\r\n09:24:32   File \"/var/lib/jenkins/.local/lib/python3.6/site-packages/torch/testing/_internal/common_utils.py\", line 1022, in assertEqual\r\n09:24:32     super().assertEqual(x, y, message)\r\n09:24:32 AssertionError: torch.float32 != torch.bfloat16 : \r\n09:24:32 \r\n09:24:32 ======================================================================\r\n09:24:32 FAIL: test_softmax_bfloat16_cuda (__main__.TestNNDeviceTypeCUDA)\r\n09:24:32 ----------------------------------------------------------------------\r\n09:24:32 Traceback (most recent call last):\r\n09:24:32   File \"/var/lib/jenkins/.local/lib/python3.6/site-packages/torch/testing/_internal/common_utils.py\", line 753, in wrapper\r\n09:24:32     method(*args, **kwargs)\r\n09:24:32   File \"/var/lib/jenkins/.local/lib/python3.6/site-packages/torch/testing/_internal/common_device_type.py\", line 210, in instantiated_test\r\n09:24:32     return test(self, device_arg)\r\n09:24:32   File \"/var/lib/jenkins/.local/lib/python3.6/site-packages/torch/testing/_internal/common_device_type.py\", line 465, in only_fn\r\n09:24:32     return fn(slf, device, *args, **kwargs)\r\n09:24:32   File \"/var/lib/jenkins/.local/lib/python3.6/site-packages/torch/testing/_internal/common_device_type.py\", line 404, in dep_fn\r\n09:24:32     return fn(slf, device, *args, **kwargs)\r\n09:24:32   File \"test_nn.py\", line 11084, in test_softmax_bfloat16\r\n09:24:32     self._test_bfloat16_ops(torch.nn.Softmax(dim=1), device, inp_dims=(16, 32), prec=1e-2)\r\n09:24:32   File \"test_nn.py\", line 11057, in _test_bfloat16_ops\r\n09:24:32     self.assertEqual(out1, out2, atol=prec)\r\n09:24:32   File \"/var/lib/jenkins/.local/lib/python3.6/site-packages/torch/testing/_internal/common_utils.py\", line 989, in assertEqual\r\n09:24:32     assertTensorsEqual(x, y)\r\n09:24:32   File \"/var/lib/jenkins/.local/lib/python3.6/site-packages/torch/testing/_internal/common_utils.py\", line 924, in assertTensorsEqual\r\n09:24:32     self.assertEqual(a.dtype, b.dtype)\r\n09:24:32   File \"/var/lib/jenkins/.local/lib/python3.6/site-packages/torch/testing/_internal/common_utils.py\", line 1022, in assertEqual\r\n09:24:32     super().assertEqual(x, y, message)\r\n09:24:32 AssertionError: torch.float32 != torch.bfloat16 :\r\n```", "context": "<issue_start><issue_comment>Title: [CI] rocmdeb-ubuntu : test_*_bfloat16_cuda (__main__.TestNNDeviceTypeCUDA)\nusername_0: https://ci.pytorch.org/jenkins/job/pytorch-builds/job/py3.6-clang7-rocmdeb-ubuntu16.04-test1/31353/console\r\n\r\n```\r\n09:24:32 ======================================================================\r\n09:24:32 FAIL: test_activations_bfloat16_cuda (__main__.TestNNDeviceTypeCUDA)\r\n09:24:32 ----------------------------------------------------------------------\r\n09:24:32 Traceback (most recent call last):\r\n09:24:32   File \"/var/lib/jenkins/.local/lib/python3.6/site-packages/torch/testing/_internal/common_utils.py\", line 753, in wrapper\r\n09:24:32     method(*args, **kwargs)\r\n09:24:32   File \"/var/lib/jenkins/.local/lib/python3.6/site-packages/torch/testing/_internal/common_device_type.py\", line 210, in instantiated_test\r\n09:24:32     return test(self, device_arg)\r\n09:24:32   File \"/var/lib/jenkins/.local/lib/python3.6/site-packages/torch/testing/_internal/common_device_type.py\", line 465, in only_fn\r\n09:24:32     return fn(slf, device, *args, **kwargs)\r\n09:24:32   File \"/var/lib/jenkins/.local/lib/python3.6/site-packages/torch/testing/_internal/common_device_type.py\", line 404, in dep_fn\r\n09:24:32     return fn(slf, device, *args, **kwargs)\r\n09:24:32   File \"test_nn.py\", line 11063, in test_activations_bfloat16\r\n09:24:32     self._test_bfloat16_ops(torch.nn.ReLU(), device, inp_dims=(5), prec=1e-2)\r\n09:24:32   File \"test_nn.py\", line 11057, in _test_bfloat16_ops\r\n09:24:32     self.assertEqual(out1, out2, atol=prec)\r\n09:24:32   File \"/var/lib/jenkins/.local/lib/python3.6/site-packages/torch/testing/_internal/common_utils.py\", line 989, in assertEqual\r\n09:24:32     assertTensorsEqual(x, y)\r\n09:24:32   File \"/var/lib/jenkins/.local/lib/python3.6/site-packages/torch/testing/_internal/common_utils.py\", line 924, in assertTensorsEqual\r\n09:24:32     self.assertEqual(a.dtype, b.dtype)\r\n09:24:32   File \"/var/lib/jenkins/.local/lib/python3.6/site-packages/torch/testing/_internal/common_utils.py\", line 1022, in assertEqual\r\n09:24:32     super().assertEqual(x, y, message)\r\n09:24:32 AssertionError: torch.float32 != torch.bfloat16 : \r\n09:24:32 \r\n09:24:32 ======================================================================\r\n09:24:32 FAIL: test_pooling_bfloat16_cuda (__main__.TestNNDeviceTypeCUDA)\r\n09:24:32 ----------------------------------------------------------------------\r\n09:24:32 Traceback (most recent call last):\r\n09:24:32   File \"/var/lib/jenkins/.local/lib/python3.6/site-packages/torch/testing/_internal/common_utils.py\", line 753, in wrapper\r\n09:24:32     method(*args, **kwargs)\r\n09:24:32   File \"/var/lib/jenkins/.local/lib/python3.6/site-packages/torch/testing/_internal/common_device_type.py\", line 210, in instantiated_test\r\n09:24:32     return test(self, device_arg)\r\n09:24:32   File \"/var/lib/jenkins/.local/lib/python3.6/site-packages/torch/testing/_internal/common_device_type.py\", line 465, in only_fn\r\n09:24:32     return fn(slf, device, *args, **kwargs)\r\n09:24:32   File \"/var/lib/jenkins/.local/lib/python3.6/site-packages/torch/testing/_internal/common_device_type.py\", line 404, in dep_fn\r\n09:24:32     return fn(slf, device, *args, **kwargs)\r\n09:24:32   File \"test_nn.py\", line 11074, in test_pooling_bfloat16\r\n09:24:32     self._test_bfloat16_ops(torch.nn.AvgPool1d(3, stride=2), device, inp_dims=(8, 4, 16), prec=0.05)\r\n09:24:32   File \"test_nn.py\", line 11057, in _test_bfloat16_ops\r\n09:24:32     self.assertEqual(out1, out2, atol=prec)\r\n09:24:32   File \"/var/lib/jenkins/.local/lib/python3.6/site-packages/torch/testing/_internal/common_utils.py\", line 989, in assertEqual\r\n09:24:32     assertTensorsEqual(x, y)\r\n09:24:32   File \"/var/lib/jenkins/.local/lib/python3.6/site-packages/torch/testing/_internal/common_utils.py\", line 924, in assertTensorsEqual\r\n09:24:32     self.assertEqual(a.dtype, b.dtype)\r\n09:24:32   File \"/var/lib/jenkins/.local/lib/python3.6/site-packages/torch/testing/_internal/common_utils.py\", line 1022, in assertEqual\r\n09:24:32     super().assertEqual(x, y, message)\r\n09:24:32 AssertionError: torch.float32 != torch.bfloat16 : \r\n09:24:32 \r\n09:24:32 ======================================================================\r\n09:24:32 FAIL: test_softmax_bfloat16_cuda (__main__.TestNNDeviceTypeCUDA)\r\n09:24:32 ----------------------------------------------------------------------\r\n09:24:32 Traceback (most recent call last):\r\n09:24:32   File \"/var/lib/jenkins/.local/lib/python3.6/site-packages/torch/testing/_internal/common_utils.py\", line 753, in wrapper\r\n09:24:32     method(*args, **kwargs)\r\n09:24:32   File \"/var/lib/jenkins/.local/lib/python3.6/site-packages/torch/testing/_internal/common_device_type.py\", line 210, in instantiated_test\r\n09:24:32     return test(self, device_arg)\r\n09:24:32   File \"/var/lib/jenkins/.local/lib/python3.6/site-packages/torch/testing/_internal/common_device_type.py\", line 465, in only_fn\r\n09:24:32     return fn(slf, device, *args, **kwargs)\r\n09:24:32   File \"/var/lib/jenkins/.local/lib/python3.6/site-packages/torch/testing/_internal/common_device_type.py\", line 404, in dep_fn\r\n09:24:32     return fn(slf, device, *args, **kwargs)\r\n09:24:32   File \"test_nn.py\", line 11084, in test_softmax_bfloat16\r\n09:24:32     self._test_bfloat16_ops(torch.nn.Softmax(dim=1), device, inp_dims=(16, 32), prec=1e-2)\r\n09:24:32   File \"test_nn.py\", line 11057, in _test_bfloat16_ops\r\n09:24:32     self.assertEqual(out1, out2, atol=prec)\r\n09:24:32   File \"/var/lib/jenkins/.local/lib/python3.6/site-packages/torch/testing/_internal/common_utils.py\", line 989, in assertEqual\r\n09:24:32     assertTensorsEqual(x, y)\r\n09:24:32   File \"/var/lib/jenkins/.local/lib/python3.6/site-packages/torch/testing/_internal/common_utils.py\", line 924, in assertTensorsEqual\r\n09:24:32     self.assertEqual(a.dtype, b.dtype)\r\n09:24:32   File \"/var/lib/jenkins/.local/lib/python3.6/site-packages/torch/testing/_internal/common_utils.py\", line 1022, in assertEqual\r\n09:24:32     super().assertEqual(x, y, message)\r\n09:24:32 AssertionError: torch.float32 != torch.bfloat16 :\r\n```\n<issue_comment>username_1: These tests are enabled and passing in latest ROCm CI.<issue_closed>"}
{"repo": "pytorch/pytorch", "issue_id": 617506552, "issue_number": 38396, "timestamp": "2020-11-24 23:47:29+00:00", "text": "These tests are enabled and passing in latest ROCm CI.", "context": "<issue_start><issue_comment>Title: [CI] rocmdeb-ubuntu : test_*_bfloat16_cuda (__main__.TestNNDeviceTypeCUDA)\nusername_0: https://ci.pytorch.org/jenkins/job/pytorch-builds/job/py3.6-clang7-rocmdeb-ubuntu16.04-test1/31353/console\r\n\r\n```\r\n09:24:32 ======================================================================\r\n09:24:32 FAIL: test_activations_bfloat16_cuda (__main__.TestNNDeviceTypeCUDA)\r\n09:24:32 ----------------------------------------------------------------------\r\n09:24:32 Traceback (most recent call last):\r\n09:24:32   File \"/var/lib/jenkins/.local/lib/python3.6/site-packages/torch/testing/_internal/common_utils.py\", line 753, in wrapper\r\n09:24:32     method(*args, **kwargs)\r\n09:24:32   File \"/var/lib/jenkins/.local/lib/python3.6/site-packages/torch/testing/_internal/common_device_type.py\", line 210, in instantiated_test\r\n09:24:32     return test(self, device_arg)\r\n09:24:32   File \"/var/lib/jenkins/.local/lib/python3.6/site-packages/torch/testing/_internal/common_device_type.py\", line 465, in only_fn\r\n09:24:32     return fn(slf, device, *args, **kwargs)\r\n09:24:32   File \"/var/lib/jenkins/.local/lib/python3.6/site-packages/torch/testing/_internal/common_device_type.py\", line 404, in dep_fn\r\n09:24:32     return fn(slf, device, *args, **kwargs)\r\n09:24:32   File \"test_nn.py\", line 11063, in test_activations_bfloat16\r\n09:24:32     self._test_bfloat16_ops(torch.nn.ReLU(), device, inp_dims=(5), prec=1e-2)\r\n09:24:32   File \"test_nn.py\", line 11057, in _test_bfloat16_ops\r\n09:24:32     self.assertEqual(out1, out2, atol=prec)\r\n09:24:32   File \"/var/lib/jenkins/.local/lib/python3.6/site-packages/torch/testing/_internal/common_utils.py\", line 989, in assertEqual\r\n09:24:32     assertTensorsEqual(x, y)\r\n09:24:32   File \"/var/lib/jenkins/.local/lib/python3.6/site-packages/torch/testing/_internal/common_utils.py\", line 924, in assertTensorsEqual\r\n09:24:32     self.assertEqual(a.dtype, b.dtype)\r\n09:24:32   File \"/var/lib/jenkins/.local/lib/python3.6/site-packages/torch/testing/_internal/common_utils.py\", line 1022, in assertEqual\r\n09:24:32     super().assertEqual(x, y, message)\r\n09:24:32 AssertionError: torch.float32 != torch.bfloat16 : \r\n09:24:32 \r\n09:24:32 ======================================================================\r\n09:24:32 FAIL: test_pooling_bfloat16_cuda (__main__.TestNNDeviceTypeCUDA)\r\n09:24:32 ----------------------------------------------------------------------\r\n09:24:32 Traceback (most recent call last):\r\n09:24:32   File \"/var/lib/jenkins/.local/lib/python3.6/site-packages/torch/testing/_internal/common_utils.py\", line 753, in wrapper\r\n09:24:32     method(*args, **kwargs)\r\n09:24:32   File \"/var/lib/jenkins/.local/lib/python3.6/site-packages/torch/testing/_internal/common_device_type.py\", line 210, in instantiated_test\r\n09:24:32     return test(self, device_arg)\r\n09:24:32   File \"/var/lib/jenkins/.local/lib/python3.6/site-packages/torch/testing/_internal/common_device_type.py\", line 465, in only_fn\r\n09:24:32     return fn(slf, device, *args, **kwargs)\r\n09:24:32   File \"/var/lib/jenkins/.local/lib/python3.6/site-packages/torch/testing/_internal/common_device_type.py\", line 404, in dep_fn\r\n09:24:32     return fn(slf, device, *args, **kwargs)\r\n09:24:32   File \"test_nn.py\", line 11074, in test_pooling_bfloat16\r\n09:24:32     self._test_bfloat16_ops(torch.nn.AvgPool1d(3, stride=2), device, inp_dims=(8, 4, 16), prec=0.05)\r\n09:24:32   File \"test_nn.py\", line 11057, in _test_bfloat16_ops\r\n09:24:32     self.assertEqual(out1, out2, atol=prec)\r\n09:24:32   File \"/var/lib/jenkins/.local/lib/python3.6/site-packages/torch/testing/_internal/common_utils.py\", line 989, in assertEqual\r\n09:24:32     assertTensorsEqual(x, y)\r\n09:24:32   File \"/var/lib/jenkins/.local/lib/python3.6/site-packages/torch/testing/_internal/common_utils.py\", line 924, in assertTensorsEqual\r\n09:24:32     self.assertEqual(a.dtype, b.dtype)\r\n09:24:32   File \"/var/lib/jenkins/.local/lib/python3.6/site-packages/torch/testing/_internal/common_utils.py\", line 1022, in assertEqual\r\n09:24:32     super().assertEqual(x, y, message)\r\n09:24:32 AssertionError: torch.float32 != torch.bfloat16 : \r\n09:24:32 \r\n09:24:32 ======================================================================\r\n09:24:32 FAIL: test_softmax_bfloat16_cuda (__main__.TestNNDeviceTypeCUDA)\r\n09:24:32 ----------------------------------------------------------------------\r\n09:24:32 Traceback (most recent call last):\r\n09:24:32   File \"/var/lib/jenkins/.local/lib/python3.6/site-packages/torch/testing/_internal/common_utils.py\", line 753, in wrapper\r\n09:24:32     method(*args, **kwargs)\r\n09:24:32   File \"/var/lib/jenkins/.local/lib/python3.6/site-packages/torch/testing/_internal/common_device_type.py\", line 210, in instantiated_test\r\n09:24:32     return test(self, device_arg)\r\n09:24:32   File \"/var/lib/jenkins/.local/lib/python3.6/site-packages/torch/testing/_internal/common_device_type.py\", line 465, in only_fn\r\n09:24:32     return fn(slf, device, *args, **kwargs)\r\n09:24:32   File \"/var/lib/jenkins/.local/lib/python3.6/site-packages/torch/testing/_internal/common_device_type.py\", line 404, in dep_fn\r\n09:24:32     return fn(slf, device, *args, **kwargs)\r\n09:24:32   File \"test_nn.py\", line 11084, in test_softmax_bfloat16\r\n09:24:32     self._test_bfloat16_ops(torch.nn.Softmax(dim=1), device, inp_dims=(16, 32), prec=1e-2)\r\n09:24:32   File \"test_nn.py\", line 11057, in _test_bfloat16_ops\r\n09:24:32     self.assertEqual(out1, out2, atol=prec)\r\n09:24:32   File \"/var/lib/jenkins/.local/lib/python3.6/site-packages/torch/testing/_internal/common_utils.py\", line 989, in assertEqual\r\n09:24:32     assertTensorsEqual(x, y)\r\n09:24:32   File \"/var/lib/jenkins/.local/lib/python3.6/site-packages/torch/testing/_internal/common_utils.py\", line 924, in assertTensorsEqual\r\n09:24:32     self.assertEqual(a.dtype, b.dtype)\r\n09:24:32   File \"/var/lib/jenkins/.local/lib/python3.6/site-packages/torch/testing/_internal/common_utils.py\", line 1022, in assertEqual\r\n09:24:32     super().assertEqual(x, y, message)\r\n09:24:32 AssertionError: torch.float32 != torch.bfloat16 :\r\n```\n<issue_comment>username_1: These tests are enabled and passing in latest ROCm CI.<issue_closed>"}
{"repo": "pytorch/pytorch", "issue_id": 617506552, "issue_number": 38396, "timestamp": "2020-11-24 23:47:30+00:00", "text": "", "context": "<issue_start><issue_comment>Title: [CI] rocmdeb-ubuntu : test_*_bfloat16_cuda (__main__.TestNNDeviceTypeCUDA)\nusername_0: https://ci.pytorch.org/jenkins/job/pytorch-builds/job/py3.6-clang7-rocmdeb-ubuntu16.04-test1/31353/console\r\n\r\n```\r\n09:24:32 ======================================================================\r\n09:24:32 FAIL: test_activations_bfloat16_cuda (__main__.TestNNDeviceTypeCUDA)\r\n09:24:32 ----------------------------------------------------------------------\r\n09:24:32 Traceback (most recent call last):\r\n09:24:32   File \"/var/lib/jenkins/.local/lib/python3.6/site-packages/torch/testing/_internal/common_utils.py\", line 753, in wrapper\r\n09:24:32     method(*args, **kwargs)\r\n09:24:32   File \"/var/lib/jenkins/.local/lib/python3.6/site-packages/torch/testing/_internal/common_device_type.py\", line 210, in instantiated_test\r\n09:24:32     return test(self, device_arg)\r\n09:24:32   File \"/var/lib/jenkins/.local/lib/python3.6/site-packages/torch/testing/_internal/common_device_type.py\", line 465, in only_fn\r\n09:24:32     return fn(slf, device, *args, **kwargs)\r\n09:24:32   File \"/var/lib/jenkins/.local/lib/python3.6/site-packages/torch/testing/_internal/common_device_type.py\", line 404, in dep_fn\r\n09:24:32     return fn(slf, device, *args, **kwargs)\r\n09:24:32   File \"test_nn.py\", line 11063, in test_activations_bfloat16\r\n09:24:32     self._test_bfloat16_ops(torch.nn.ReLU(), device, inp_dims=(5), prec=1e-2)\r\n09:24:32   File \"test_nn.py\", line 11057, in _test_bfloat16_ops\r\n09:24:32     self.assertEqual(out1, out2, atol=prec)\r\n09:24:32   File \"/var/lib/jenkins/.local/lib/python3.6/site-packages/torch/testing/_internal/common_utils.py\", line 989, in assertEqual\r\n09:24:32     assertTensorsEqual(x, y)\r\n09:24:32   File \"/var/lib/jenkins/.local/lib/python3.6/site-packages/torch/testing/_internal/common_utils.py\", line 924, in assertTensorsEqual\r\n09:24:32     self.assertEqual(a.dtype, b.dtype)\r\n09:24:32   File \"/var/lib/jenkins/.local/lib/python3.6/site-packages/torch/testing/_internal/common_utils.py\", line 1022, in assertEqual\r\n09:24:32     super().assertEqual(x, y, message)\r\n09:24:32 AssertionError: torch.float32 != torch.bfloat16 : \r\n09:24:32 \r\n09:24:32 ======================================================================\r\n09:24:32 FAIL: test_pooling_bfloat16_cuda (__main__.TestNNDeviceTypeCUDA)\r\n09:24:32 ----------------------------------------------------------------------\r\n09:24:32 Traceback (most recent call last):\r\n09:24:32   File \"/var/lib/jenkins/.local/lib/python3.6/site-packages/torch/testing/_internal/common_utils.py\", line 753, in wrapper\r\n09:24:32     method(*args, **kwargs)\r\n09:24:32   File \"/var/lib/jenkins/.local/lib/python3.6/site-packages/torch/testing/_internal/common_device_type.py\", line 210, in instantiated_test\r\n09:24:32     return test(self, device_arg)\r\n09:24:32   File \"/var/lib/jenkins/.local/lib/python3.6/site-packages/torch/testing/_internal/common_device_type.py\", line 465, in only_fn\r\n09:24:32     return fn(slf, device, *args, **kwargs)\r\n09:24:32   File \"/var/lib/jenkins/.local/lib/python3.6/site-packages/torch/testing/_internal/common_device_type.py\", line 404, in dep_fn\r\n09:24:32     return fn(slf, device, *args, **kwargs)\r\n09:24:32   File \"test_nn.py\", line 11074, in test_pooling_bfloat16\r\n09:24:32     self._test_bfloat16_ops(torch.nn.AvgPool1d(3, stride=2), device, inp_dims=(8, 4, 16), prec=0.05)\r\n09:24:32   File \"test_nn.py\", line 11057, in _test_bfloat16_ops\r\n09:24:32     self.assertEqual(out1, out2, atol=prec)\r\n09:24:32   File \"/var/lib/jenkins/.local/lib/python3.6/site-packages/torch/testing/_internal/common_utils.py\", line 989, in assertEqual\r\n09:24:32     assertTensorsEqual(x, y)\r\n09:24:32   File \"/var/lib/jenkins/.local/lib/python3.6/site-packages/torch/testing/_internal/common_utils.py\", line 924, in assertTensorsEqual\r\n09:24:32     self.assertEqual(a.dtype, b.dtype)\r\n09:24:32   File \"/var/lib/jenkins/.local/lib/python3.6/site-packages/torch/testing/_internal/common_utils.py\", line 1022, in assertEqual\r\n09:24:32     super().assertEqual(x, y, message)\r\n09:24:32 AssertionError: torch.float32 != torch.bfloat16 : \r\n09:24:32 \r\n09:24:32 ======================================================================\r\n09:24:32 FAIL: test_softmax_bfloat16_cuda (__main__.TestNNDeviceTypeCUDA)\r\n09:24:32 ----------------------------------------------------------------------\r\n09:24:32 Traceback (most recent call last):\r\n09:24:32   File \"/var/lib/jenkins/.local/lib/python3.6/site-packages/torch/testing/_internal/common_utils.py\", line 753, in wrapper\r\n09:24:32     method(*args, **kwargs)\r\n09:24:32   File \"/var/lib/jenkins/.local/lib/python3.6/site-packages/torch/testing/_internal/common_device_type.py\", line 210, in instantiated_test\r\n09:24:32     return test(self, device_arg)\r\n09:24:32   File \"/var/lib/jenkins/.local/lib/python3.6/site-packages/torch/testing/_internal/common_device_type.py\", line 465, in only_fn\r\n09:24:32     return fn(slf, device, *args, **kwargs)\r\n09:24:32   File \"/var/lib/jenkins/.local/lib/python3.6/site-packages/torch/testing/_internal/common_device_type.py\", line 404, in dep_fn\r\n09:24:32     return fn(slf, device, *args, **kwargs)\r\n09:24:32   File \"test_nn.py\", line 11084, in test_softmax_bfloat16\r\n09:24:32     self._test_bfloat16_ops(torch.nn.Softmax(dim=1), device, inp_dims=(16, 32), prec=1e-2)\r\n09:24:32   File \"test_nn.py\", line 11057, in _test_bfloat16_ops\r\n09:24:32     self.assertEqual(out1, out2, atol=prec)\r\n09:24:32   File \"/var/lib/jenkins/.local/lib/python3.6/site-packages/torch/testing/_internal/common_utils.py\", line 989, in assertEqual\r\n09:24:32     assertTensorsEqual(x, y)\r\n09:24:32   File \"/var/lib/jenkins/.local/lib/python3.6/site-packages/torch/testing/_internal/common_utils.py\", line 924, in assertTensorsEqual\r\n09:24:32     self.assertEqual(a.dtype, b.dtype)\r\n09:24:32   File \"/var/lib/jenkins/.local/lib/python3.6/site-packages/torch/testing/_internal/common_utils.py\", line 1022, in assertEqual\r\n09:24:32     super().assertEqual(x, y, message)\r\n09:24:32 AssertionError: torch.float32 != torch.bfloat16 :\r\n```\n<issue_comment>username_1: These tests are enabled and passing in latest ROCm CI.<issue_closed>"}
{"repo": "pytorch/pytorch", "issue_id": 631127261, "issue_number": 39535, "timestamp": "2020-06-04T20:34:16Z", "text": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#39535 Fix possible deadlock in _wait_all_workers**\n\nThis is my understanding of what could happen: on workerN (N != 0), `_wait_all_workers_sequence_id_to_states`, which is a `defaultdict`, is accessed twice: once in the body of `_wait_all_workers` (by the \"main thread\" of workerN) and once in `_set_proceed_shutdown_signal`, called by worker0 through a RPC call. I think the two could race and access the `_wait_all_workers_sequence_id_to_states` at the same time, and thus create two separate copies of `WaitAllWorkersStates`. One of those threads would wait  on the event of one copy, but the other thread would set the event of the other copy. This lead to a deadlock, as the main thread would end up waiting forever.\n\nDifferential Revision: [D21889752](https://our.internmc.facebook.com/intern/diff/D21889752/)", "context": "<issue_start><issue_comment>Title: Fix possible deadlock in _wait_all_workers\nusername_0: Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#39535 Fix possible deadlock in _wait_all_workers**\n\nThis is my understanding of what could happen: on workerN (N != 0), `_wait_all_workers_sequence_id_to_states`, which is a `defaultdict`, is accessed twice: once in the body of `_wait_all_workers` (by the \"main thread\" of workerN) and once in `_set_proceed_shutdown_signal`, called by worker0 through a RPC call. I think the two could race and access the `_wait_all_workers_sequence_id_to_states` at the same time, and thus create two separate copies of `WaitAllWorkersStates`. One of those threads would wait  on the event of one copy, but the other thread would set the event of the other copy. This lead to a deadlock, as the main thread would end up waiting forever.\n\nDifferential Revision: [D21889752](https://our.internmc.facebook.com/intern/diff/D21889752/)"}
{"repo": "pytorch/pytorch", "issue_id": 683350371, "issue_number": 43398, "timestamp": "2020-08-21T07:05:54Z", "text": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#43398 [quant][WIP] Fusion + tests + whole bunch of other things...**\n* #43393 [quant] BatchNorm1d\n* #43392 [quant] Factored out common code in batchnorm.py\n* #43391 [quant] Fixing activation_post_process coming from a wrong fused batchnorm2d location\n\nThis PR will be split into several, and is currently WIP.", "context": "<issue_start><issue_comment>Title: [quant][WIP] Fusion + tests + whole bunch of other things...\nusername_0: Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#43398 [quant][WIP] Fusion + tests + whole bunch of other things...**\n* #43393 [quant] BatchNorm1d\n* #43392 [quant] Factored out common code in batchnorm.py\n* #43391 [quant] Fixing activation_post_process coming from a wrong fused batchnorm2d location\n\nThis PR will be split into several, and is currently WIP."}
{"repo": "pytorch/pytorch", "issue_id": 766991822, "issue_number": 49367, "timestamp": "2020-12-14T22:43:52Z", "text": "Stack from [ghstack](https://github.com/username_2/ghstack):\n* **#49367 [PyTorch Mobile] Skip generating code for BoxedKernelWrapper when building or mobile**\n\nWhen building for mobile, we don't need BoxedKernelWrapper since it's used to call boxed implementations of functions with from an unboxed call context (i.e. the wrapper converts an unboxed call into a call to a boxed operator implementation, and then unboxes the return value to pass on to the original caller). This is done using template expansion since the types are all known at build time.\n\nThis results in generated code that is practically unused.\n\nDifferential Revision: [D25521462](https://our.internmc.facebook.com/intern/diff/D25521462/)", "context": "<issue_start><issue_comment>Title: [PyTorch Mobile] Skip generating code for BoxedKernelWrapper when building or mobile\nusername_0: Stack from [ghstack](https://github.com/username_2/ghstack):\n* **#49367 [PyTorch Mobile] Skip generating code for BoxedKernelWrapper when building or mobile**\n\nWhen building for mobile, we don't need BoxedKernelWrapper since it's used to call boxed implementations of functions with from an unboxed call context (i.e. the wrapper converts an unboxed call into a call to a boxed operator implementation, and then unboxes the return value to pass on to the original caller). This is done using template expansion since the types are all known at build time.\n\nThis results in generated code that is practically unused.\n\nDifferential Revision: [D25521462](https://our.internmc.facebook.com/intern/diff/D25521462/)\n<issue_comment>username_1: This might work right now but will break when we start using backend fallbacks for things. Not sure if we should land this. We're considering backend fallbacks for some things that are relevant for mobile too, say Variable kernels.\n<issue_comment>username_0: @username_1 Thanks for the additional context! The savings are in the range of 37KiB-59KiB depending which app you look at, so not life changing, but we are entering the stage of long tail of small size improvements (@username_2 's note about efficiency resonates with size work too). That said, if this will cause some problems for future work, there's no reason to land it. Will revisit when the backend fallback work is under way/landed.\n<issue_comment>username_1: This would be relatively easy to revert once we start using backend fallbacks. I'm not strongly opposed to landing it, I'll leave it up to you. We just should be aware that this is a temporary win and we will likely have to revert it with the backend fallback work.\n<issue_comment>username_2: This is fine, but I'd like a Note saying why the ifdef is there and under what circumstances it might be removed."}
{"repo": "pytorch/pytorch", "issue_id": 766991822, "issue_number": 49367, "timestamp": "2021-01-08 18:16:11+00:00", "text": "This might work right now but will break when we start using backend fallbacks for things. Not sure if we should land this. We're considering backend fallbacks for some things that are relevant for mobile too, say Variable kernels.", "context": "<issue_start><issue_comment>Title: [PyTorch Mobile] Skip generating code for BoxedKernelWrapper when building or mobile\nusername_0: Stack from [ghstack](https://github.com/username_2/ghstack):\n* **#49367 [PyTorch Mobile] Skip generating code for BoxedKernelWrapper when building or mobile**\n\nWhen building for mobile, we don't need BoxedKernelWrapper since it's used to call boxed implementations of functions with from an unboxed call context (i.e. the wrapper converts an unboxed call into a call to a boxed operator implementation, and then unboxes the return value to pass on to the original caller). This is done using template expansion since the types are all known at build time.\n\nThis results in generated code that is practically unused.\n\nDifferential Revision: [D25521462](https://our.internmc.facebook.com/intern/diff/D25521462/)\n<issue_comment>username_1: This might work right now but will break when we start using backend fallbacks for things. Not sure if we should land this. We're considering backend fallbacks for some things that are relevant for mobile too, say Variable kernels.\n<issue_comment>username_0: @username_1 Thanks for the additional context! The savings are in the range of 37KiB-59KiB depending which app you look at, so not life changing, but we are entering the stage of long tail of small size improvements (@username_2 's note about efficiency resonates with size work too). That said, if this will cause some problems for future work, there's no reason to land it. Will revisit when the backend fallback work is under way/landed.\n<issue_comment>username_1: This would be relatively easy to revert once we start using backend fallbacks. I'm not strongly opposed to landing it, I'll leave it up to you. We just should be aware that this is a temporary win and we will likely have to revert it with the backend fallback work.\n<issue_comment>username_2: This is fine, but I'd like a Note saying why the ifdef is there and under what circumstances it might be removed."}
{"repo": "pytorch/pytorch", "issue_id": 766991822, "issue_number": 49367, "timestamp": "2021-01-08 19:36:43+00:00", "text": "@username_1 Thanks for the additional context! The savings are in the range of 37KiB-59KiB depending which app you look at, so not life changing, but we are entering the stage of long tail of small size improvements (@username_2 's note about efficiency resonates with size work too). That said, if this will cause some problems for future work, there's no reason to land it. Will revisit when the backend fallback work is under way/landed.", "context": "<issue_start><issue_comment>Title: [PyTorch Mobile] Skip generating code for BoxedKernelWrapper when building or mobile\nusername_0: Stack from [ghstack](https://github.com/username_2/ghstack):\n* **#49367 [PyTorch Mobile] Skip generating code for BoxedKernelWrapper when building or mobile**\n\nWhen building for mobile, we don't need BoxedKernelWrapper since it's used to call boxed implementations of functions with from an unboxed call context (i.e. the wrapper converts an unboxed call into a call to a boxed operator implementation, and then unboxes the return value to pass on to the original caller). This is done using template expansion since the types are all known at build time.\n\nThis results in generated code that is practically unused.\n\nDifferential Revision: [D25521462](https://our.internmc.facebook.com/intern/diff/D25521462/)\n<issue_comment>username_1: This might work right now but will break when we start using backend fallbacks for things. Not sure if we should land this. We're considering backend fallbacks for some things that are relevant for mobile too, say Variable kernels.\n<issue_comment>username_0: @username_1 Thanks for the additional context! The savings are in the range of 37KiB-59KiB depending which app you look at, so not life changing, but we are entering the stage of long tail of small size improvements (@username_2 's note about efficiency resonates with size work too). That said, if this will cause some problems for future work, there's no reason to land it. Will revisit when the backend fallback work is under way/landed.\n<issue_comment>username_1: This would be relatively easy to revert once we start using backend fallbacks. I'm not strongly opposed to landing it, I'll leave it up to you. We just should be aware that this is a temporary win and we will likely have to revert it with the backend fallback work.\n<issue_comment>username_2: This is fine, but I'd like a Note saying why the ifdef is there and under what circumstances it might be removed."}
{"repo": "pytorch/pytorch", "issue_id": 766991822, "issue_number": 49367, "timestamp": "2021-01-08 20:08:47+00:00", "text": "This would be relatively easy to revert once we start using backend fallbacks. I'm not strongly opposed to landing it, I'll leave it up to you. We just should be aware that this is a temporary win and we will likely have to revert it with the backend fallback work.", "context": "<issue_start><issue_comment>Title: [PyTorch Mobile] Skip generating code for BoxedKernelWrapper when building or mobile\nusername_0: Stack from [ghstack](https://github.com/username_2/ghstack):\n* **#49367 [PyTorch Mobile] Skip generating code for BoxedKernelWrapper when building or mobile**\n\nWhen building for mobile, we don't need BoxedKernelWrapper since it's used to call boxed implementations of functions with from an unboxed call context (i.e. the wrapper converts an unboxed call into a call to a boxed operator implementation, and then unboxes the return value to pass on to the original caller). This is done using template expansion since the types are all known at build time.\n\nThis results in generated code that is practically unused.\n\nDifferential Revision: [D25521462](https://our.internmc.facebook.com/intern/diff/D25521462/)\n<issue_comment>username_1: This might work right now but will break when we start using backend fallbacks for things. Not sure if we should land this. We're considering backend fallbacks for some things that are relevant for mobile too, say Variable kernels.\n<issue_comment>username_0: @username_1 Thanks for the additional context! The savings are in the range of 37KiB-59KiB depending which app you look at, so not life changing, but we are entering the stage of long tail of small size improvements (@username_2 's note about efficiency resonates with size work too). That said, if this will cause some problems for future work, there's no reason to land it. Will revisit when the backend fallback work is under way/landed.\n<issue_comment>username_1: This would be relatively easy to revert once we start using backend fallbacks. I'm not strongly opposed to landing it, I'll leave it up to you. We just should be aware that this is a temporary win and we will likely have to revert it with the backend fallback work.\n<issue_comment>username_2: This is fine, but I'd like a Note saying why the ifdef is there and under what circumstances it might be removed."}
{"repo": "pytorch/pytorch", "issue_id": 766991822, "issue_number": 49367, "timestamp": "2021-01-11 15:27:32+00:00", "text": "This is fine, but I'd like a Note saying why the ifdef is there and under what circumstances it might be removed.", "context": "<issue_start><issue_comment>Title: [PyTorch Mobile] Skip generating code for BoxedKernelWrapper when building or mobile\nusername_0: Stack from [ghstack](https://github.com/username_2/ghstack):\n* **#49367 [PyTorch Mobile] Skip generating code for BoxedKernelWrapper when building or mobile**\n\nWhen building for mobile, we don't need BoxedKernelWrapper since it's used to call boxed implementations of functions with from an unboxed call context (i.e. the wrapper converts an unboxed call into a call to a boxed operator implementation, and then unboxes the return value to pass on to the original caller). This is done using template expansion since the types are all known at build time.\n\nThis results in generated code that is practically unused.\n\nDifferential Revision: [D25521462](https://our.internmc.facebook.com/intern/diff/D25521462/)\n<issue_comment>username_1: This might work right now but will break when we start using backend fallbacks for things. Not sure if we should land this. We're considering backend fallbacks for some things that are relevant for mobile too, say Variable kernels.\n<issue_comment>username_0: @username_1 Thanks for the additional context! The savings are in the range of 37KiB-59KiB depending which app you look at, so not life changing, but we are entering the stage of long tail of small size improvements (@username_2 's note about efficiency resonates with size work too). That said, if this will cause some problems for future work, there's no reason to land it. Will revisit when the backend fallback work is under way/landed.\n<issue_comment>username_1: This would be relatively easy to revert once we start using backend fallbacks. I'm not strongly opposed to landing it, I'll leave it up to you. We just should be aware that this is a temporary win and we will likely have to revert it with the backend fallback work.\n<issue_comment>username_2: This is fine, but I'd like a Note saying why the ifdef is there and under what circumstances it might be removed."}
{"repo": "pytorch/pytorch", "issue_id": 781484897, "issue_number": 50213, "timestamp": "2021-01-07 17:32:33+00:00", "text": "array([[-1.1256918e-01,  5.3110137e+00, -3.6077526e+01],\r\n       [-1.2552988e-02, -3.9551861e-04,  2.7541189e+01]], dtype=float32)\r\n```\r\n\r\nthis violates https://github.com/pytorch/pytorch/wiki/Developer-FAQ#how-does-out-work-in-pytorch\r\n\r\n```\r\nCollecting environment information...\r\nPyTorch version: 1.8.0.dev20201222\r\nIs debug build: False\r\nCUDA used to build PyTorch: 9.2\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: CentOS Linux 7 (Core) (x86_64)\r\nGCC version: (GCC) 4.8.5 20150623 (Red Hat 4.8.5-44)\r\nClang version: Could not collect\r\nCMake version: version 3.14.0\r\n\r\nPython version: 3.7 (64-bit runtime)\r\nIs CUDA available: True\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration:\r\nGPU 0: Tesla M40\r\nGPU 1: Tesla M40\r\n\r\nNvidia driver version: 418.126.02\r\ncuDNN version: /usr/local/cuda-9.2/targets/x86_64-linux/lib/libcudnn.so.7.4.2\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.18.1\r\n[pip] pytorch-lightning==0.9.0\r\n[pip] torch==1.8.0.dev20201222\r\n[pip] torchaudio==0.8.0a0+1398187\r\n[pip] torchvision==0.9.0.dev20201222\r\n[conda] blas                      1.0                         mkl\r\n[conda] cudatoolkit               9.2                           0\r\n[conda] mkl                       2020.1                      217\r\n[conda] mkl-include               2020.1                      217\r\n[conda] mkl-service               2.3.0            py37he904b0f_0\r\n[conda] mkl_fft                   1.0.14           py37ha843d7b_0\r\n[conda] mkl_random                1.1.0            py37hd6b4f25_0\r\n[conda] numpy                     1.18.1           py37h4f9e942_0\r\n[conda] numpy-base                1.18.1           py37hde5b4d6_1\r\n[conda] pytorch                   1.8.0.dev20201222 py3.7_cuda9.2.148_cudnn7.6.3_0    pytorch-nightly\r\n[conda] pytorch-lightning         0.9.0                     <pip>\r\n[conda] torch                     1.8.0.dev20201130+cpu           <pip>\r\n[conda] torchaudio                0.8.0.dev20201130           <pip>\r\n[conda] torchaudio                0.8.0.dev20201222            py37    pytorch-nightly\r\n[conda] torchvision               0.9.0.dev20201222       py37_cu92    pytorch-nightly\r\n```", "context": "<issue_start><issue_comment>Title: torch.float_power out= and inplace variant errors on non-matching output dtype instead of casting\nusername_0: array([[-1.1256918e-01,  5.3110137e+00, -3.6077526e+01],\r\n       [-1.2552988e-02, -3.9551861e-04,  2.7541189e+01]], dtype=float32)\r\n```\r\n\r\nthis violates https://github.com/pytorch/pytorch/wiki/Developer-FAQ#how-does-out-work-in-pytorch\r\n\r\n```\r\nCollecting environment information...\r\nPyTorch version: 1.8.0.dev20201222\r\nIs debug build: False\r\nCUDA used to build PyTorch: 9.2\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: CentOS Linux 7 (Core) (x86_64)\r\nGCC version: (GCC) 4.8.5 20150623 (Red Hat 4.8.5-44)\r\nClang version: Could not collect\r\nCMake version: version 3.14.0\r\n\r\nPython version: 3.7 (64-bit runtime)\r\nIs CUDA available: True\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration:\r\nGPU 0: Tesla M40\r\nGPU 1: Tesla M40\r\n\r\nNvidia driver version: 418.126.02\r\ncuDNN version: /usr/local/cuda-9.2/targets/x86_64-linux/lib/libcudnn.so.7.4.2\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.18.1\r\n[pip] pytorch-lightning==0.9.0\r\n[pip] torch==1.8.0.dev20201222\r\n[pip] torchaudio==0.8.0a0+1398187\r\n[pip] torchvision==0.9.0.dev20201222\r\n[conda] blas                      1.0                         mkl\r\n[conda] cudatoolkit               9.2                           0\r\n[conda] mkl                       2020.1                      217\r\n[conda] mkl-include               2020.1                      217\r\n[conda] mkl-service               2.3.0            py37he904b0f_0\r\n[conda] mkl_fft                   1.0.14           py37ha843d7b_0\r\n[conda] mkl_random                1.1.0            py37hd6b4f25_0\r\n[conda] numpy                     1.18.1           py37h4f9e942_0\r\n[conda] numpy-base                1.18.1           py37hde5b4d6_1\r\n[conda] pytorch                   1.8.0.dev20201222 py3.7_cuda9.2.148_cudnn7.6.3_0    pytorch-nightly\r\n[conda] pytorch-lightning         0.9.0                     <pip>\r\n[conda] torch                     1.8.0.dev20201130+cpu           <pip>\r\n[conda] torchaudio                0.8.0.dev20201130           <pip>\r\n[conda] torchaudio                0.8.0.dev20201222            py37    pytorch-nightly\r\n[conda] torchvision               0.9.0.dev20201222       py37_cu92    pytorch-nightly\r\n```\n<issue_comment>username_1: When addressing https://github.com/pytorch/pytorch/issues/49468 we should be sure it catches issues like this."}
{"repo": "pytorch/pytorch", "issue_id": 781484897, "issue_number": 50213, "timestamp": "2021-01-08 00:28:35+00:00", "text": "When addressing https://github.com/pytorch/pytorch/issues/49468 we should be sure it catches issues like this.", "context": "<issue_start><issue_comment>Title: torch.float_power out= and inplace variant errors on non-matching output dtype instead of casting\nusername_0: array([[-1.1256918e-01,  5.3110137e+00, -3.6077526e+01],\r\n       [-1.2552988e-02, -3.9551861e-04,  2.7541189e+01]], dtype=float32)\r\n```\r\n\r\nthis violates https://github.com/pytorch/pytorch/wiki/Developer-FAQ#how-does-out-work-in-pytorch\r\n\r\n```\r\nCollecting environment information...\r\nPyTorch version: 1.8.0.dev20201222\r\nIs debug build: False\r\nCUDA used to build PyTorch: 9.2\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: CentOS Linux 7 (Core) (x86_64)\r\nGCC version: (GCC) 4.8.5 20150623 (Red Hat 4.8.5-44)\r\nClang version: Could not collect\r\nCMake version: version 3.14.0\r\n\r\nPython version: 3.7 (64-bit runtime)\r\nIs CUDA available: True\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration:\r\nGPU 0: Tesla M40\r\nGPU 1: Tesla M40\r\n\r\nNvidia driver version: 418.126.02\r\ncuDNN version: /usr/local/cuda-9.2/targets/x86_64-linux/lib/libcudnn.so.7.4.2\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.18.1\r\n[pip] pytorch-lightning==0.9.0\r\n[pip] torch==1.8.0.dev20201222\r\n[pip] torchaudio==0.8.0a0+1398187\r\n[pip] torchvision==0.9.0.dev20201222\r\n[conda] blas                      1.0                         mkl\r\n[conda] cudatoolkit               9.2                           0\r\n[conda] mkl                       2020.1                      217\r\n[conda] mkl-include               2020.1                      217\r\n[conda] mkl-service               2.3.0            py37he904b0f_0\r\n[conda] mkl_fft                   1.0.14           py37ha843d7b_0\r\n[conda] mkl_random                1.1.0            py37hd6b4f25_0\r\n[conda] numpy                     1.18.1           py37h4f9e942_0\r\n[conda] numpy-base                1.18.1           py37hde5b4d6_1\r\n[conda] pytorch                   1.8.0.dev20201222 py3.7_cuda9.2.148_cudnn7.6.3_0    pytorch-nightly\r\n[conda] pytorch-lightning         0.9.0                     <pip>\r\n[conda] torch                     1.8.0.dev20201130+cpu           <pip>\r\n[conda] torchaudio                0.8.0.dev20201130           <pip>\r\n[conda] torchaudio                0.8.0.dev20201222            py37    pytorch-nightly\r\n[conda] torchvision               0.9.0.dev20201222       py37_cu92    pytorch-nightly\r\n```\n<issue_comment>username_1: When addressing https://github.com/pytorch/pytorch/issues/49468 we should be sure it catches issues like this."}
{"repo": "pytorch/pytorch", "issue_id": 868007964, "issue_number": 56939, "timestamp": "2021-04-26T18:39:38Z", "text": "Stack from [ghstack](https://github.com/username_0/ghstack):\n* **#56939 Delete grandfathered Caffe2 dispatch keys.**\n\nThese never have kernels registered to them and are effectively useless.\nWhat I am not so sure if we allocate tensors to them or not; if we do\nI cannot use asserts and I need to ensure we just return undefined\nor something equivalent.", "context": "<issue_start><issue_comment>Title: Delete grandfathered Caffe2 dispatch keys.\nusername_0: Stack from [ghstack](https://github.com/username_0/ghstack):\n* **#56939 Delete grandfathered Caffe2 dispatch keys.**\n\nThese never have kernels registered to them and are effectively useless.\nWhat I am not so sure if we allocate tensors to them or not; if we do\nI cannot use asserts and I need to ensure we just return undefined\nor something equivalent.\n<issue_comment>username_0: @bhosmer yeah yeah, let me do that"}
{"repo": "pytorch/pytorch", "issue_id": 868007964, "issue_number": 56939, "timestamp": "2021-04-27 02:04:12+00:00", "text": "@bhosmer yeah yeah, let me do that", "context": "<issue_start><issue_comment>Title: Delete grandfathered Caffe2 dispatch keys.\nusername_0: Stack from [ghstack](https://github.com/username_0/ghstack):\n* **#56939 Delete grandfathered Caffe2 dispatch keys.**\n\nThese never have kernels registered to them and are effectively useless.\nWhat I am not so sure if we allocate tensors to them or not; if we do\nI cannot use asserts and I need to ensure we just return undefined\nor something equivalent.\n<issue_comment>username_0: @bhosmer yeah yeah, let me do that"}
{"repo": "pytorch/pytorch", "issue_id": 973134051, "issue_number": 63441, "timestamp": "2021-08-17 23:19:50+00:00", "text": "## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\nI am trying to run the mnist script (https://github.com/pytorch/examples/blob/master/mnist/main.py) on a cpu.\r\nImporting OpenCV in the script causes the script to get stuck indefinitely on loss.backward()\r\nThis happens even if opencv is not used anywhere in the script, importing it is causing this problem.\r\n\r\nOn stepping through the code using gdb, the stack trace is not visible. Instead of the current executing functions for each thread, gdb shows question marks.\r\n\r\n## Environment\r\n\r\nPyTorch version: 1.9.0\r\nIs debug build: False\r\nCUDA used to build PyTorch: None\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 20.04.2 LTS (x86_64)\r\nGCC version: (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0\r\nClang version: Could not collect\r\nCMake version: version 3.16.3\r\nLibc version: glibc-2.31\r\n\r\nPython version: 3.8.10 | packaged by conda-forge | (default, May 11 2021, 07:01:05)  [GCC 9.3.0] (64-bit runtime)\r\nPython platform: Linux-5.4.0-1054-aws-x86_64-with-glibc2.10\r\nIs CUDA available: False\r\nCUDA runtime version: No CUDA\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.19.1\r\n[pip3] sagemaker-pytorch-training==2.4.0\r\n[pip3] torch==1.9.0\r\n[pip3] torchvision==0.10.0\r\n[conda] mkl                       2020.2                      256    anaconda\r\n[conda] mkl-include               2020.2                      256    anaconda\r\n[conda] numpy                     1.19.1           py38h30dfecb_0    anaconda\r\n[conda] numpy-base                1.19.1           py38h75fe3a5_0    anaconda\r\n[conda] sagemaker-pytorch-training 2.4.0                    pypi_0    pypi\r\n[conda] torch                     1.9.0                    pypi_0    pypi\r\n[conda] torchvision               0.10.0                   pypi_0    pypi", "context": "<issue_start><issue_comment>Title: OpenCV causes backpropagation to get stuck\nusername_0: ## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\nI am trying to run the mnist script (https://github.com/pytorch/examples/blob/master/mnist/main.py) on a cpu.\r\nImporting OpenCV in the script causes the script to get stuck indefinitely on loss.backward()\r\nThis happens even if opencv is not used anywhere in the script, importing it is causing this problem.\r\n\r\nOn stepping through the code using gdb, the stack trace is not visible. Instead of the current executing functions for each thread, gdb shows question marks.\r\n\r\n## Environment\r\n\r\nPyTorch version: 1.9.0\r\nIs debug build: False\r\nCUDA used to build PyTorch: None\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 20.04.2 LTS (x86_64)\r\nGCC version: (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0\r\nClang version: Could not collect\r\nCMake version: version 3.16.3\r\nLibc version: glibc-2.31\r\n\r\nPython version: 3.8.10 | packaged by conda-forge | (default, May 11 2021, 07:01:05)  [GCC 9.3.0] (64-bit runtime)\r\nPython platform: Linux-5.4.0-1054-aws-x86_64-with-glibc2.10\r\nIs CUDA available: False\r\nCUDA runtime version: No CUDA\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.19.1\r\n[pip3] sagemaker-pytorch-training==2.4.0\r\n[pip3] torch==1.9.0\r\n[pip3] torchvision==0.10.0\r\n[conda] mkl                       2020.2                      256    anaconda\r\n[conda] mkl-include               2020.2                      256    anaconda\r\n[conda] numpy                     1.19.1           py38h30dfecb_0    anaconda\r\n[conda] numpy-base                1.19.1           py38h75fe3a5_0    anaconda\r\n[conda] sagemaker-pytorch-training 2.4.0                    pypi_0    pypi\r\n[conda] torch                     1.9.0                    pypi_0    pypi\r\n[conda] torchvision               0.10.0                   pypi_0    pypi"}
{"repo": "pytorch/pytorch", "issue_id": 1066417257, "issue_number": 69017, "timestamp": "2021-11-29 19:31:31+00:00", "text": "## \ud83d\udc1b Bug\r\n\r\n\"distributed/algorithms/ddp_comm_hooks/test_ddp_hooks\", \"distributed/algorithms/quantization/test_quantization\" tests do not run in CI, i.e. they are not contained in `DISTRIBUTED_TESTS` list here: https://github.com/pytorch/pytorch/blob/master/test/run_test.py#L302", "context": "<issue_start><issue_comment>Title: distributed/algorithms/ddp_comm_hooks/test_ddp_hooks and distributed/algorithms/quantization/test_quantization don't run in OSS CI\nusername_0: ## \ud83d\udc1b Bug\r\n\r\n\"distributed/algorithms/ddp_comm_hooks/test_ddp_hooks\", \"distributed/algorithms/quantization/test_quantization\" tests do not run in CI, i.e. they are not contained in `DISTRIBUTED_TESTS` list here: https://github.com/pytorch/pytorch/blob/master/test/run_test.py#L302"}
{"repo": "pytorch/pytorch", "issue_id": 1091443235, "issue_number": 70538, "timestamp": "2021-12-31T06:42:51Z", "text": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* (to be filled)\n\nDifferential Revision: [D33375195](https://our.internmc.facebook.com/intern/diff/D33375195/)", "context": "<issue_start><issue_comment>Title: Enable upgraders in TS server\nusername_0: Stack from [ghstack](https://github.com/ezyang/ghstack):\n* (to be filled)\n\nDifferential Revision: [D33375195](https://our.internmc.facebook.com/intern/diff/D33375195/)\n<issue_comment>username_0: @username_0 has imported this pull request.  If you are a Facebook employee, you can view this diff [on Phabricator](https://www.internalfb.com/diff/D33375195).\n<issue_comment>username_0: @username_0 has imported this pull request.  If you are a Facebook employee, you can view this diff [on Phabricator](https://www.internalfb.com/diff/D33375195).\n<issue_comment>username_0: @username_0 has imported this pull request.  If you are a Facebook employee, you can view this diff [on Phabricator](https://www.internalfb.com/diff/D33375195).\n<issue_comment>username_0: @username_0 has imported this pull request.  If you are a Facebook employee, you can view this diff [on Phabricator](https://www.internalfb.com/diff/D33375195)."}
{"repo": "pytorch/pytorch", "issue_id": 1091443235, "issue_number": 70538, "timestamp": "2021-12-31 07:42:16+00:00", "text": "@username_0 has imported this pull request.  If you are a Facebook employee, you can view this diff [on Phabricator](https://www.internalfb.com/diff/D33375195).", "context": "<issue_start><issue_comment>Title: Enable upgraders in TS server\nusername_0: Stack from [ghstack](https://github.com/ezyang/ghstack):\n* (to be filled)\n\nDifferential Revision: [D33375195](https://our.internmc.facebook.com/intern/diff/D33375195/)\n<issue_comment>username_0: @username_0 has imported this pull request.  If you are a Facebook employee, you can view this diff [on Phabricator](https://www.internalfb.com/diff/D33375195).\n<issue_comment>username_0: @username_0 has imported this pull request.  If you are a Facebook employee, you can view this diff [on Phabricator](https://www.internalfb.com/diff/D33375195).\n<issue_comment>username_0: @username_0 has imported this pull request.  If you are a Facebook employee, you can view this diff [on Phabricator](https://www.internalfb.com/diff/D33375195).\n<issue_comment>username_0: @username_0 has imported this pull request.  If you are a Facebook employee, you can view this diff [on Phabricator](https://www.internalfb.com/diff/D33375195)."}
{"repo": "pytorch/pytorch", "issue_id": 1091443235, "issue_number": 70538, "timestamp": "2021-12-31 08:11:29+00:00", "text": "@username_0 has imported this pull request.  If you are a Facebook employee, you can view this diff [on Phabricator](https://www.internalfb.com/diff/D33375195).", "context": "<issue_start><issue_comment>Title: Enable upgraders in TS server\nusername_0: Stack from [ghstack](https://github.com/ezyang/ghstack):\n* (to be filled)\n\nDifferential Revision: [D33375195](https://our.internmc.facebook.com/intern/diff/D33375195/)\n<issue_comment>username_0: @username_0 has imported this pull request.  If you are a Facebook employee, you can view this diff [on Phabricator](https://www.internalfb.com/diff/D33375195).\n<issue_comment>username_0: @username_0 has imported this pull request.  If you are a Facebook employee, you can view this diff [on Phabricator](https://www.internalfb.com/diff/D33375195).\n<issue_comment>username_0: @username_0 has imported this pull request.  If you are a Facebook employee, you can view this diff [on Phabricator](https://www.internalfb.com/diff/D33375195).\n<issue_comment>username_0: @username_0 has imported this pull request.  If you are a Facebook employee, you can view this diff [on Phabricator](https://www.internalfb.com/diff/D33375195)."}
{"repo": "pytorch/pytorch", "issue_id": 1091443235, "issue_number": 70538, "timestamp": "2022-01-03 21:31:45+00:00", "text": "@username_0 has imported this pull request.  If you are a Facebook employee, you can view this diff [on Phabricator](https://www.internalfb.com/diff/D33375195).", "context": "<issue_start><issue_comment>Title: Enable upgraders in TS server\nusername_0: Stack from [ghstack](https://github.com/ezyang/ghstack):\n* (to be filled)\n\nDifferential Revision: [D33375195](https://our.internmc.facebook.com/intern/diff/D33375195/)\n<issue_comment>username_0: @username_0 has imported this pull request.  If you are a Facebook employee, you can view this diff [on Phabricator](https://www.internalfb.com/diff/D33375195).\n<issue_comment>username_0: @username_0 has imported this pull request.  If you are a Facebook employee, you can view this diff [on Phabricator](https://www.internalfb.com/diff/D33375195).\n<issue_comment>username_0: @username_0 has imported this pull request.  If you are a Facebook employee, you can view this diff [on Phabricator](https://www.internalfb.com/diff/D33375195).\n<issue_comment>username_0: @username_0 has imported this pull request.  If you are a Facebook employee, you can view this diff [on Phabricator](https://www.internalfb.com/diff/D33375195)."}
{"repo": "pytorch/pytorch", "issue_id": 1091443235, "issue_number": 70538, "timestamp": "2022-01-06 17:46:03+00:00", "text": "@username_0 has imported this pull request.  If you are a Facebook employee, you can view this diff [on Phabricator](https://www.internalfb.com/diff/D33375195).", "context": "<issue_start><issue_comment>Title: Enable upgraders in TS server\nusername_0: Stack from [ghstack](https://github.com/ezyang/ghstack):\n* (to be filled)\n\nDifferential Revision: [D33375195](https://our.internmc.facebook.com/intern/diff/D33375195/)\n<issue_comment>username_0: @username_0 has imported this pull request.  If you are a Facebook employee, you can view this diff [on Phabricator](https://www.internalfb.com/diff/D33375195).\n<issue_comment>username_0: @username_0 has imported this pull request.  If you are a Facebook employee, you can view this diff [on Phabricator](https://www.internalfb.com/diff/D33375195).\n<issue_comment>username_0: @username_0 has imported this pull request.  If you are a Facebook employee, you can view this diff [on Phabricator](https://www.internalfb.com/diff/D33375195).\n<issue_comment>username_0: @username_0 has imported this pull request.  If you are a Facebook employee, you can view this diff [on Phabricator](https://www.internalfb.com/diff/D33375195)."}
{"repo": "pytorch/pytorch", "issue_id": 1114601827, "issue_number": 71809, "timestamp": "2022-01-26 03:59:06+00:00", "text": "### \ud83d\udc1b Describe the bug\n\nPlatforms: linux\r\n\r\nCulprit seems to be #71791, tagging @anjali411 \r\n\r\nExample failure (now they show up as unexpected successes):\r\nhttps://github.com/pytorch/pytorch/runs/4945515438?check_suite_focus=true\r\n\r\nMore failures: https://hud2.pytorch.org/minihud?name_filter=linux-xenial-cuda11.3-py3.7-gcc7%20/%20test%20(default,%201,%202,%20linux.4xlarge.nvidia.gpu)\n\n### Versions\n\nIn CI", "context": "<issue_start><issue_comment>Title: DISABLED test_fn_fwgrad_bwgrad_gradient (__main__.TestGradients)\nusername_0: ### \ud83d\udc1b Describe the bug\n\nPlatforms: linux\r\n\r\nCulprit seems to be #71791, tagging @anjali411 \r\n\r\nExample failure (now they show up as unexpected successes):\r\nhttps://github.com/pytorch/pytorch/runs/4945515438?check_suite_focus=true\r\n\r\nMore failures: https://hud2.pytorch.org/minihud?name_filter=linux-xenial-cuda11.3-py3.7-gcc7%20/%20test%20(default,%201,%202,%20linux.4xlarge.nvidia.gpu)\n\n### Versions\n\nIn CI\n<issue_comment>username_0: Also testing the code I recently commited in https://github.com/pytorch/pytorch/pull/71499<issue_closed>\n<issue_comment>username_0: Closing as revert should fix https://github.com/pytorch/pytorch/pull/71611#issuecomment-1022399970"}
{"repo": "pytorch/pytorch", "issue_id": 1114601827, "issue_number": 71809, "timestamp": "2022-01-26 04:00:00+00:00", "text": "Also testing the code I recently commited in https://github.com/pytorch/pytorch/pull/71499", "context": "<issue_start><issue_comment>Title: DISABLED test_fn_fwgrad_bwgrad_gradient (__main__.TestGradients)\nusername_0: ### \ud83d\udc1b Describe the bug\n\nPlatforms: linux\r\n\r\nCulprit seems to be #71791, tagging @anjali411 \r\n\r\nExample failure (now they show up as unexpected successes):\r\nhttps://github.com/pytorch/pytorch/runs/4945515438?check_suite_focus=true\r\n\r\nMore failures: https://hud2.pytorch.org/minihud?name_filter=linux-xenial-cuda11.3-py3.7-gcc7%20/%20test%20(default,%201,%202,%20linux.4xlarge.nvidia.gpu)\n\n### Versions\n\nIn CI\n<issue_comment>username_0: Also testing the code I recently commited in https://github.com/pytorch/pytorch/pull/71499<issue_closed>\n<issue_comment>username_0: Closing as revert should fix https://github.com/pytorch/pytorch/pull/71611#issuecomment-1022399970"}
{"repo": "pytorch/pytorch", "issue_id": 1114601827, "issue_number": 71809, "timestamp": "2022-01-26 17:14:02+00:00", "text": "", "context": "<issue_start><issue_comment>Title: DISABLED test_fn_fwgrad_bwgrad_gradient (__main__.TestGradients)\nusername_0: ### \ud83d\udc1b Describe the bug\n\nPlatforms: linux\r\n\r\nCulprit seems to be #71791, tagging @anjali411 \r\n\r\nExample failure (now they show up as unexpected successes):\r\nhttps://github.com/pytorch/pytorch/runs/4945515438?check_suite_focus=true\r\n\r\nMore failures: https://hud2.pytorch.org/minihud?name_filter=linux-xenial-cuda11.3-py3.7-gcc7%20/%20test%20(default,%201,%202,%20linux.4xlarge.nvidia.gpu)\n\n### Versions\n\nIn CI\n<issue_comment>username_0: Also testing the code I recently commited in https://github.com/pytorch/pytorch/pull/71499<issue_closed>\n<issue_comment>username_0: Closing as revert should fix https://github.com/pytorch/pytorch/pull/71611#issuecomment-1022399970"}
{"repo": "pytorch/pytorch", "issue_id": 1114601827, "issue_number": 71809, "timestamp": "2022-01-26 17:14:02+00:00", "text": "Closing as revert should fix https://github.com/pytorch/pytorch/pull/71611#issuecomment-1022399970", "context": "<issue_start><issue_comment>Title: DISABLED test_fn_fwgrad_bwgrad_gradient (__main__.TestGradients)\nusername_0: ### \ud83d\udc1b Describe the bug\n\nPlatforms: linux\r\n\r\nCulprit seems to be #71791, tagging @anjali411 \r\n\r\nExample failure (now they show up as unexpected successes):\r\nhttps://github.com/pytorch/pytorch/runs/4945515438?check_suite_focus=true\r\n\r\nMore failures: https://hud2.pytorch.org/minihud?name_filter=linux-xenial-cuda11.3-py3.7-gcc7%20/%20test%20(default,%201,%202,%20linux.4xlarge.nvidia.gpu)\n\n### Versions\n\nIn CI\n<issue_comment>username_0: Also testing the code I recently commited in https://github.com/pytorch/pytorch/pull/71499<issue_closed>\n<issue_comment>username_0: Closing as revert should fix https://github.com/pytorch/pytorch/pull/71611#issuecomment-1022399970"}
{"repo": "facebook/react", "issue_id": 107302404, "issue_number": 4914, "timestamp": "2015-09-19T01:28:02Z", "text": "There are a couple changes here. The most important are that this brings us up to compatibility with 0.14 (mostly just using `ReactDOM` where appropriate). The other thing is that we stop trying to use JSXTransformer, which we aren't even building so these are all broken right now. And lastly, I took this opportunity to stop bundling 3rd party code (es5-shim, sham, console, jquery, bootstrap) and just point at CDNs. This does mean that the examples will require an internet connection to actually run but I think that's ok - I don't care about the airplane case as much anymore.\r\n\r\nI left the [server-rendering example](https://github.com/facebook/react/tree/master/examples/server-rendering) alone because I think I want to delete it. It's confusing to follow and run, and I think there are enough resources around on server rendering.", "context": "<issue_start><issue_comment>Title: Update examples for Babel, 0.14; remove 3rd party code\nusername_0: There are a couple changes here. The most important are that this brings us up to compatibility with 0.14 (mostly just using `ReactDOM` where appropriate). The other thing is that we stop trying to use JSXTransformer, which we aren't even building so these are all broken right now. And lastly, I took this opportunity to stop bundling 3rd party code (es5-shim, sham, console, jquery, bootstrap) and just point at CDNs. This does mean that the examples will require an internet connection to actually run but I think that's ok - I don't care about the airplane case as much anymore.\r\n\r\nI left the [server-rendering example](https://github.com/facebook/react/tree/master/examples/server-rendering) alone because I think I want to delete it. It's confusing to follow and run, and I think there are enough resources around on server rendering.\n<issue_comment>username_1: :+1:"}
{"repo": "facebook/react", "issue_id": 107302404, "issue_number": 4914, "timestamp": "2015-09-24 21:55:45+00:00", "text": ":+1:", "context": "<issue_start><issue_comment>Title: Update examples for Babel, 0.14; remove 3rd party code\nusername_0: There are a couple changes here. The most important are that this brings us up to compatibility with 0.14 (mostly just using `ReactDOM` where appropriate). The other thing is that we stop trying to use JSXTransformer, which we aren't even building so these are all broken right now. And lastly, I took this opportunity to stop bundling 3rd party code (es5-shim, sham, console, jquery, bootstrap) and just point at CDNs. This does mean that the examples will require an internet connection to actually run but I think that's ok - I don't care about the airplane case as much anymore.\r\n\r\nI left the [server-rendering example](https://github.com/facebook/react/tree/master/examples/server-rendering) alone because I think I want to delete it. It's confusing to follow and run, and I think there are enough resources around on server rendering.\n<issue_comment>username_1: :+1:"}
{"repo": "facebook/react", "issue_id": 362375314, "issue_number": 13703, "timestamp": "2018-09-20 21:30:20+00:00", "text": "The display names of components that are based on React.forwardRef are currently completely ignored (=> name is always \"ForwardRef\" ).\r\nThe display name is neither used in prop type validation error messages nor in \"React Developer Tools\".\r\nI think this is a bug.\r\n\r\nSnippet:\r\n```javascript\r\nconst MyComponent = React.forwardRef((props, ref) => {\r\n  return '[MyComponent]'\r\n})\r\n\r\nMyComponent.displayName = 'MyComponent' // <- not working\r\n\r\nMyComponent.propTypes = {\r\n  someProp: () => new Error('Simulated prop type error')\r\n}\r\n```\r\n\r\nHere's the full example:\r\n\r\nEdit:\r\nhttps://jsbin.com/mujarizozu/edit?html,js,output\r\n\r\nPreview:\r\nhttps://output.jsbin.com/mujarizozu", "context": "<issue_start><issue_comment>Title: displayName not supported in combination with forwardRef\nusername_0: The display names of components that are based on React.forwardRef are currently completely ignored (=> name is always \"ForwardRef\" ).\r\nThe display name is neither used in prop type validation error messages nor in \"React Developer Tools\".\r\nI think this is a bug.\r\n\r\nSnippet:\r\n```javascript\r\nconst MyComponent = React.forwardRef((props, ref) => {\r\n  return '[MyComponent]'\r\n})\r\n\r\nMyComponent.displayName = 'MyComponent' // <- not working\r\n\r\nMyComponent.propTypes = {\r\n  someProp: () => new Error('Simulated prop type error')\r\n}\r\n```\r\n\r\nHere's the full example:\r\n\r\nEdit:\r\nhttps://jsbin.com/mujarizozu/edit?html,js,output\r\n\r\nPreview:\r\nhttps://output.jsbin.com/mujarizozu\n<issue_comment>username_1: Check out the docs:\r\nhttps://reactjs.org/docs/forwarding-refs.html#displaying-a-custom-name-in-devtools\r\n\r\nThe function you pass to `React.forwardRef` needs to be named, rather than the object returned by `React.forwardRef`\r\n\r\nUpdated code: https://codesandbox.io/s/oqy4q045qy\r\nPreview: https://oqy4q045qy.codesandbox.io/<issue_closed>\n<issue_comment>username_0: Oooops, like always: It helps if you can read ;-)\r\nThanks a lot @username_1 - I've overlooked that part in the documentation :-("}
{"repo": "facebook/react", "issue_id": 362375314, "issue_number": 13703, "timestamp": "2018-09-20 21:53:54+00:00", "text": "Check out the docs:\r\nhttps://reactjs.org/docs/forwarding-refs.html#displaying-a-custom-name-in-devtools\r\n\r\nThe function you pass to `React.forwardRef` needs to be named, rather than the object returned by `React.forwardRef`\r\n\r\nUpdated code: https://codesandbox.io/s/oqy4q045qy\r\nPreview: https://oqy4q045qy.codesandbox.io/", "context": "<issue_start><issue_comment>Title: displayName not supported in combination with forwardRef\nusername_0: The display names of components that are based on React.forwardRef are currently completely ignored (=> name is always \"ForwardRef\" ).\r\nThe display name is neither used in prop type validation error messages nor in \"React Developer Tools\".\r\nI think this is a bug.\r\n\r\nSnippet:\r\n```javascript\r\nconst MyComponent = React.forwardRef((props, ref) => {\r\n  return '[MyComponent]'\r\n})\r\n\r\nMyComponent.displayName = 'MyComponent' // <- not working\r\n\r\nMyComponent.propTypes = {\r\n  someProp: () => new Error('Simulated prop type error')\r\n}\r\n```\r\n\r\nHere's the full example:\r\n\r\nEdit:\r\nhttps://jsbin.com/mujarizozu/edit?html,js,output\r\n\r\nPreview:\r\nhttps://output.jsbin.com/mujarizozu\n<issue_comment>username_1: Check out the docs:\r\nhttps://reactjs.org/docs/forwarding-refs.html#displaying-a-custom-name-in-devtools\r\n\r\nThe function you pass to `React.forwardRef` needs to be named, rather than the object returned by `React.forwardRef`\r\n\r\nUpdated code: https://codesandbox.io/s/oqy4q045qy\r\nPreview: https://oqy4q045qy.codesandbox.io/<issue_closed>\n<issue_comment>username_0: Oooops, like always: It helps if you can read ;-)\r\nThanks a lot @username_1 - I've overlooked that part in the documentation :-("}
{"repo": "facebook/react", "issue_id": 362375314, "issue_number": 13703, "timestamp": "2018-09-20 21:53:55+00:00", "text": "", "context": "<issue_start><issue_comment>Title: displayName not supported in combination with forwardRef\nusername_0: The display names of components that are based on React.forwardRef are currently completely ignored (=> name is always \"ForwardRef\" ).\r\nThe display name is neither used in prop type validation error messages nor in \"React Developer Tools\".\r\nI think this is a bug.\r\n\r\nSnippet:\r\n```javascript\r\nconst MyComponent = React.forwardRef((props, ref) => {\r\n  return '[MyComponent]'\r\n})\r\n\r\nMyComponent.displayName = 'MyComponent' // <- not working\r\n\r\nMyComponent.propTypes = {\r\n  someProp: () => new Error('Simulated prop type error')\r\n}\r\n```\r\n\r\nHere's the full example:\r\n\r\nEdit:\r\nhttps://jsbin.com/mujarizozu/edit?html,js,output\r\n\r\nPreview:\r\nhttps://output.jsbin.com/mujarizozu\n<issue_comment>username_1: Check out the docs:\r\nhttps://reactjs.org/docs/forwarding-refs.html#displaying-a-custom-name-in-devtools\r\n\r\nThe function you pass to `React.forwardRef` needs to be named, rather than the object returned by `React.forwardRef`\r\n\r\nUpdated code: https://codesandbox.io/s/oqy4q045qy\r\nPreview: https://oqy4q045qy.codesandbox.io/<issue_closed>\n<issue_comment>username_0: Oooops, like always: It helps if you can read ;-)\r\nThanks a lot @username_1 - I've overlooked that part in the documentation :-("}
{"repo": "facebook/react", "issue_id": 362375314, "issue_number": 13703, "timestamp": "2018-09-20 22:01:20+00:00", "text": "Oooops, like always: It helps if you can read ;-)\r\nThanks a lot @username_1 - I've overlooked that part in the documentation :-(", "context": "<issue_start><issue_comment>Title: displayName not supported in combination with forwardRef\nusername_0: The display names of components that are based on React.forwardRef are currently completely ignored (=> name is always \"ForwardRef\" ).\r\nThe display name is neither used in prop type validation error messages nor in \"React Developer Tools\".\r\nI think this is a bug.\r\n\r\nSnippet:\r\n```javascript\r\nconst MyComponent = React.forwardRef((props, ref) => {\r\n  return '[MyComponent]'\r\n})\r\n\r\nMyComponent.displayName = 'MyComponent' // <- not working\r\n\r\nMyComponent.propTypes = {\r\n  someProp: () => new Error('Simulated prop type error')\r\n}\r\n```\r\n\r\nHere's the full example:\r\n\r\nEdit:\r\nhttps://jsbin.com/mujarizozu/edit?html,js,output\r\n\r\nPreview:\r\nhttps://output.jsbin.com/mujarizozu\n<issue_comment>username_1: Check out the docs:\r\nhttps://reactjs.org/docs/forwarding-refs.html#displaying-a-custom-name-in-devtools\r\n\r\nThe function you pass to `React.forwardRef` needs to be named, rather than the object returned by `React.forwardRef`\r\n\r\nUpdated code: https://codesandbox.io/s/oqy4q045qy\r\nPreview: https://oqy4q045qy.codesandbox.io/<issue_closed>\n<issue_comment>username_0: Oooops, like always: It helps if you can read ;-)\r\nThanks a lot @username_1 - I've overlooked that part in the documentation :-("}
{"repo": "pytorch/pytorch", "issue_id": 459442784, "issue_number": 22102, "timestamp": "2019-06-22T06:22:52Z", "text": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* #22103 Swap detection order in randperm_out_cuda to avoid unnecessary conversion from float when the input is small.\n* **#22102 Support Half type in randperm.**", "context": "<issue_start><issue_comment>Title: Support Half type in randperm.\nusername_0: Stack from [ghstack](https://github.com/ezyang/ghstack):\n* #22103 Swap detection order in randperm_out_cuda to avoid unnecessary conversion from float when the input is small.\n* **#22102 Support Half type in randperm.**\n<issue_comment>username_0: @pytorchbot retest this please\n<issue_comment>username_0: @pytorchbot retest this please\n<issue_comment>username_1: CI failures look unrelated."}
{"repo": "pytorch/pytorch", "issue_id": 459442784, "issue_number": 22102, "timestamp": "2019-06-22 16:35:28+00:00", "text": "@pytorchbot retest this please", "context": "<issue_start><issue_comment>Title: Support Half type in randperm.\nusername_0: Stack from [ghstack](https://github.com/ezyang/ghstack):\n* #22103 Swap detection order in randperm_out_cuda to avoid unnecessary conversion from float when the input is small.\n* **#22102 Support Half type in randperm.**\n<issue_comment>username_0: @pytorchbot retest this please\n<issue_comment>username_0: @pytorchbot retest this please\n<issue_comment>username_1: CI failures look unrelated."}
{"repo": "pytorch/pytorch", "issue_id": 459442784, "issue_number": 22102, "timestamp": "2019-06-22 22:04:44+00:00", "text": "@pytorchbot retest this please", "context": "<issue_start><issue_comment>Title: Support Half type in randperm.\nusername_0: Stack from [ghstack](https://github.com/ezyang/ghstack):\n* #22103 Swap detection order in randperm_out_cuda to avoid unnecessary conversion from float when the input is small.\n* **#22102 Support Half type in randperm.**\n<issue_comment>username_0: @pytorchbot retest this please\n<issue_comment>username_0: @pytorchbot retest this please\n<issue_comment>username_1: CI failures look unrelated."}
{"repo": "pytorch/pytorch", "issue_id": 459442784, "issue_number": 22102, "timestamp": "2019-07-02 17:10:08+00:00", "text": "CI failures look unrelated.", "context": "<issue_start><issue_comment>Title: Support Half type in randperm.\nusername_0: Stack from [ghstack](https://github.com/ezyang/ghstack):\n* #22103 Swap detection order in randperm_out_cuda to avoid unnecessary conversion from float when the input is small.\n* **#22102 Support Half type in randperm.**\n<issue_comment>username_0: @pytorchbot retest this please\n<issue_comment>username_0: @pytorchbot retest this please\n<issue_comment>username_1: CI failures look unrelated."}
{"repo": "pytorch/pytorch", "issue_id": 478262268, "issue_number": 24001, "timestamp": "2019-08-08 05:43:19+00:00", "text": "```pytb\r\n======================================================================\r\nFAIL: test_matmul_small_brute_force_2d_Nd (__main__.TestTorch)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"test_torch.py\", line 8960, in test_matmul_small_brute_force_2d_Nd\r\n    self.check_single_matmul(x, y, (o, n, p))\r\n  File \"test_torch.py\", line 8895, in check_single_matmul\r\n    self.assertTrue(expected.flags['C_CONTIGUOUS'])\r\nAssertionError: False is not true\r\n```", "context": "<issue_start><issue_comment>Title: [v1.2.0] test_matmul_small_brute_force_2d_Nd failed in Windows builds\nusername_0: ```pytb\r\n======================================================================\r\nFAIL: test_matmul_small_brute_force_2d_Nd (__main__.TestTorch)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"test_torch.py\", line 8960, in test_matmul_small_brute_force_2d_Nd\r\n    self.check_single_matmul(x, y, (o, n, p))\r\n  File \"test_torch.py\", line 8895, in check_single_matmul\r\n    self.assertTrue(expected.flags['C_CONTIGUOUS'])\r\nAssertionError: False is not true\r\n```\n<issue_comment>username_0: @username_2 thought it maybe just because the numpy version is old.\n<issue_comment>username_1: I wrote that test, actually the line is only there to check that numpy and pytorch do the same thing (and the test has always passed so far).\r\n\r\nBut if some numpy versions (or MKL?) don't return a c-contiguous result there's no reason to keep this check.\n<issue_comment>username_2: yea it looks like on numpy v1.1 on Windows (probably a particular install) is failing that assert. we can remove it\n<issue_comment>username_0: @username_1 Would you please delete them in both master and v1.2.0?\n<issue_comment>username_2: i've just patched v1.2.0\n<issue_comment>username_1: @username_0 Yes, for master there's #24012 now."}
{"repo": "pytorch/pytorch", "issue_id": 478262268, "issue_number": 24001, "timestamp": "2019-08-08 05:52:23+00:00", "text": "@username_2 thought it maybe just because the numpy version is old.", "context": "<issue_start><issue_comment>Title: [v1.2.0] test_matmul_small_brute_force_2d_Nd failed in Windows builds\nusername_0: ```pytb\r\n======================================================================\r\nFAIL: test_matmul_small_brute_force_2d_Nd (__main__.TestTorch)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"test_torch.py\", line 8960, in test_matmul_small_brute_force_2d_Nd\r\n    self.check_single_matmul(x, y, (o, n, p))\r\n  File \"test_torch.py\", line 8895, in check_single_matmul\r\n    self.assertTrue(expected.flags['C_CONTIGUOUS'])\r\nAssertionError: False is not true\r\n```\n<issue_comment>username_0: @username_2 thought it maybe just because the numpy version is old.\n<issue_comment>username_1: I wrote that test, actually the line is only there to check that numpy and pytorch do the same thing (and the test has always passed so far).\r\n\r\nBut if some numpy versions (or MKL?) don't return a c-contiguous result there's no reason to keep this check.\n<issue_comment>username_2: yea it looks like on numpy v1.1 on Windows (probably a particular install) is failing that assert. we can remove it\n<issue_comment>username_0: @username_1 Would you please delete them in both master and v1.2.0?\n<issue_comment>username_2: i've just patched v1.2.0\n<issue_comment>username_1: @username_0 Yes, for master there's #24012 now."}
{"repo": "pytorch/pytorch", "issue_id": 478262268, "issue_number": 24001, "timestamp": "2019-08-08 12:31:10+00:00", "text": "I wrote that test, actually the line is only there to check that numpy and pytorch do the same thing (and the test has always passed so far).\r\n\r\nBut if some numpy versions (or MKL?) don't return a c-contiguous result there's no reason to keep this check.", "context": "<issue_start><issue_comment>Title: [v1.2.0] test_matmul_small_brute_force_2d_Nd failed in Windows builds\nusername_0: ```pytb\r\n======================================================================\r\nFAIL: test_matmul_small_brute_force_2d_Nd (__main__.TestTorch)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"test_torch.py\", line 8960, in test_matmul_small_brute_force_2d_Nd\r\n    self.check_single_matmul(x, y, (o, n, p))\r\n  File \"test_torch.py\", line 8895, in check_single_matmul\r\n    self.assertTrue(expected.flags['C_CONTIGUOUS'])\r\nAssertionError: False is not true\r\n```\n<issue_comment>username_0: @username_2 thought it maybe just because the numpy version is old.\n<issue_comment>username_1: I wrote that test, actually the line is only there to check that numpy and pytorch do the same thing (and the test has always passed so far).\r\n\r\nBut if some numpy versions (or MKL?) don't return a c-contiguous result there's no reason to keep this check.\n<issue_comment>username_2: yea it looks like on numpy v1.1 on Windows (probably a particular install) is failing that assert. we can remove it\n<issue_comment>username_0: @username_1 Would you please delete them in both master and v1.2.0?\n<issue_comment>username_2: i've just patched v1.2.0\n<issue_comment>username_1: @username_0 Yes, for master there's #24012 now."}
{"repo": "pytorch/pytorch", "issue_id": 478262268, "issue_number": 24001, "timestamp": "2019-08-08 12:35:45+00:00", "text": "yea it looks like on numpy v1.1 on Windows (probably a particular install) is failing that assert. we can remove it", "context": "<issue_start><issue_comment>Title: [v1.2.0] test_matmul_small_brute_force_2d_Nd failed in Windows builds\nusername_0: ```pytb\r\n======================================================================\r\nFAIL: test_matmul_small_brute_force_2d_Nd (__main__.TestTorch)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"test_torch.py\", line 8960, in test_matmul_small_brute_force_2d_Nd\r\n    self.check_single_matmul(x, y, (o, n, p))\r\n  File \"test_torch.py\", line 8895, in check_single_matmul\r\n    self.assertTrue(expected.flags['C_CONTIGUOUS'])\r\nAssertionError: False is not true\r\n```\n<issue_comment>username_0: @username_2 thought it maybe just because the numpy version is old.\n<issue_comment>username_1: I wrote that test, actually the line is only there to check that numpy and pytorch do the same thing (and the test has always passed so far).\r\n\r\nBut if some numpy versions (or MKL?) don't return a c-contiguous result there's no reason to keep this check.\n<issue_comment>username_2: yea it looks like on numpy v1.1 on Windows (probably a particular install) is failing that assert. we can remove it\n<issue_comment>username_0: @username_1 Would you please delete them in both master and v1.2.0?\n<issue_comment>username_2: i've just patched v1.2.0\n<issue_comment>username_1: @username_0 Yes, for master there's #24012 now."}
{"repo": "pytorch/pytorch", "issue_id": 478262268, "issue_number": 24001, "timestamp": "2019-08-08 12:51:34+00:00", "text": "@username_1 Would you please delete them in both master and v1.2.0?", "context": "<issue_start><issue_comment>Title: [v1.2.0] test_matmul_small_brute_force_2d_Nd failed in Windows builds\nusername_0: ```pytb\r\n======================================================================\r\nFAIL: test_matmul_small_brute_force_2d_Nd (__main__.TestTorch)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"test_torch.py\", line 8960, in test_matmul_small_brute_force_2d_Nd\r\n    self.check_single_matmul(x, y, (o, n, p))\r\n  File \"test_torch.py\", line 8895, in check_single_matmul\r\n    self.assertTrue(expected.flags['C_CONTIGUOUS'])\r\nAssertionError: False is not true\r\n```\n<issue_comment>username_0: @username_2 thought it maybe just because the numpy version is old.\n<issue_comment>username_1: I wrote that test, actually the line is only there to check that numpy and pytorch do the same thing (and the test has always passed so far).\r\n\r\nBut if some numpy versions (or MKL?) don't return a c-contiguous result there's no reason to keep this check.\n<issue_comment>username_2: yea it looks like on numpy v1.1 on Windows (probably a particular install) is failing that assert. we can remove it\n<issue_comment>username_0: @username_1 Would you please delete them in both master and v1.2.0?\n<issue_comment>username_2: i've just patched v1.2.0\n<issue_comment>username_1: @username_0 Yes, for master there's #24012 now."}
{"repo": "pytorch/pytorch", "issue_id": 478262268, "issue_number": 24001, "timestamp": "2019-08-08 12:54:45+00:00", "text": "i've just patched v1.2.0", "context": "<issue_start><issue_comment>Title: [v1.2.0] test_matmul_small_brute_force_2d_Nd failed in Windows builds\nusername_0: ```pytb\r\n======================================================================\r\nFAIL: test_matmul_small_brute_force_2d_Nd (__main__.TestTorch)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"test_torch.py\", line 8960, in test_matmul_small_brute_force_2d_Nd\r\n    self.check_single_matmul(x, y, (o, n, p))\r\n  File \"test_torch.py\", line 8895, in check_single_matmul\r\n    self.assertTrue(expected.flags['C_CONTIGUOUS'])\r\nAssertionError: False is not true\r\n```\n<issue_comment>username_0: @username_2 thought it maybe just because the numpy version is old.\n<issue_comment>username_1: I wrote that test, actually the line is only there to check that numpy and pytorch do the same thing (and the test has always passed so far).\r\n\r\nBut if some numpy versions (or MKL?) don't return a c-contiguous result there's no reason to keep this check.\n<issue_comment>username_2: yea it looks like on numpy v1.1 on Windows (probably a particular install) is failing that assert. we can remove it\n<issue_comment>username_0: @username_1 Would you please delete them in both master and v1.2.0?\n<issue_comment>username_2: i've just patched v1.2.0\n<issue_comment>username_1: @username_0 Yes, for master there's #24012 now."}
{"repo": "pytorch/pytorch", "issue_id": 478262268, "issue_number": 24001, "timestamp": "2019-08-08 13:20:47+00:00", "text": "@username_0 Yes, for master there's #24012 now.", "context": "<issue_start><issue_comment>Title: [v1.2.0] test_matmul_small_brute_force_2d_Nd failed in Windows builds\nusername_0: ```pytb\r\n======================================================================\r\nFAIL: test_matmul_small_brute_force_2d_Nd (__main__.TestTorch)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"test_torch.py\", line 8960, in test_matmul_small_brute_force_2d_Nd\r\n    self.check_single_matmul(x, y, (o, n, p))\r\n  File \"test_torch.py\", line 8895, in check_single_matmul\r\n    self.assertTrue(expected.flags['C_CONTIGUOUS'])\r\nAssertionError: False is not true\r\n```\n<issue_comment>username_0: @username_2 thought it maybe just because the numpy version is old.\n<issue_comment>username_1: I wrote that test, actually the line is only there to check that numpy and pytorch do the same thing (and the test has always passed so far).\r\n\r\nBut if some numpy versions (or MKL?) don't return a c-contiguous result there's no reason to keep this check.\n<issue_comment>username_2: yea it looks like on numpy v1.1 on Windows (probably a particular install) is failing that assert. we can remove it\n<issue_comment>username_0: @username_1 Would you please delete them in both master and v1.2.0?\n<issue_comment>username_2: i've just patched v1.2.0\n<issue_comment>username_1: @username_0 Yes, for master there's #24012 now."}
{"repo": "pytorch/pytorch", "issue_id": 524706993, "issue_number": 30060, "timestamp": "2019-11-19T00:54:18Z", "text": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#30060 Mobile module forward() pass input by value.**\nMobile forward() passed inputs by reference, which is different from JIT's script::module. To make it consistent, change it pass by value.\n\nDifferential Revision: [D18587786](https://our.internmc.facebook.com/intern/diff/D18587786)", "context": "<issue_start><issue_comment>Title: Mobile module forward() pass input by value.\nusername_0: Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#30060 Mobile module forward() pass input by value.**\nMobile forward() passed inputs by reference, which is different from JIT's script::module. To make it consistent, change it pass by value.\n\nDifferential Revision: [D18587786](https://our.internmc.facebook.com/intern/diff/D18587786)"}
{"repo": "pytorch/pytorch", "issue_id": 524787350, "issue_number": 30068, "timestamp": "2019-11-19T05:41:55Z", "text": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* #30069 Polish rpc docstring.\n* **#30068 Add distributed optimizer section to distributed autograd design doc.**\n\nDifferential Revision: [D18556536](https://our.internmc.facebook.com/intern/diff/D18556536/)", "context": "<issue_start><issue_comment>Title: Add distributed optimizer section to distributed autograd design doc.\nusername_0: Stack from [ghstack](https://github.com/ezyang/ghstack):\n* #30069 Polish rpc docstring.\n* **#30068 Add distributed optimizer section to distributed autograd design doc.**\n\nDifferential Revision: [D18556536](https://our.internmc.facebook.com/intern/diff/D18556536/)"}
{"repo": "pytorch/pytorch", "issue_id": 527461484, "issue_number": 30348, "timestamp": "2019-11-22 23:25:09+00:00", "text": "## \ud83d\ude80 Feature Speedup Qparam calculation for observers\r\n\r\n## Pitch\r\nCalculateQparams gets called every forward with QAT. Speeding this up speeds up quantization aware training. \r\nDetails are at:\r\nhttps://discuss.pytorch.org/t/quantization-aware-training-extremely-slow-on-gpu/58894/5\r\n\r\nSpecifically, the task is to remove https://github.com/pytorch/pytorch/blob/master/torch/quantization/observer.py#L119 and instead enhance https://github.com/pytorch/pytorch/blob/master/torch/quantization/observer.py#L157 to process per-channel calculations directly without needing a for loop.", "context": "<issue_start><issue_comment>Title: Speed up calculateQparams for per-channel observers\nusername_0: ## \ud83d\ude80 Feature Speedup Qparam calculation for observers\r\n\r\n## Pitch\r\nCalculateQparams gets called every forward with QAT. Speeding this up speeds up quantization aware training. \r\nDetails are at:\r\nhttps://discuss.pytorch.org/t/quantization-aware-training-extremely-slow-on-gpu/58894/5\r\n\r\nSpecifically, the task is to remove https://github.com/pytorch/pytorch/blob/master/torch/quantization/observer.py#L119 and instead enhance https://github.com/pytorch/pytorch/blob/master/torch/quantization/observer.py#L157 to process per-channel calculations directly without needing a for loop.\n<issue_comment>username_1: PR put up: https://github.com/pytorch/pytorch/pull/30485\n<issue_comment>username_2: thanks for fixing this, @username_1!<issue_closed>"}
{"repo": "pytorch/pytorch", "issue_id": 527461484, "issue_number": 30348, "timestamp": "2019-11-26 22:06:02+00:00", "text": "PR put up: https://github.com/pytorch/pytorch/pull/30485", "context": "<issue_start><issue_comment>Title: Speed up calculateQparams for per-channel observers\nusername_0: ## \ud83d\ude80 Feature Speedup Qparam calculation for observers\r\n\r\n## Pitch\r\nCalculateQparams gets called every forward with QAT. Speeding this up speeds up quantization aware training. \r\nDetails are at:\r\nhttps://discuss.pytorch.org/t/quantization-aware-training-extremely-slow-on-gpu/58894/5\r\n\r\nSpecifically, the task is to remove https://github.com/pytorch/pytorch/blob/master/torch/quantization/observer.py#L119 and instead enhance https://github.com/pytorch/pytorch/blob/master/torch/quantization/observer.py#L157 to process per-channel calculations directly without needing a for loop.\n<issue_comment>username_1: PR put up: https://github.com/pytorch/pytorch/pull/30485\n<issue_comment>username_2: thanks for fixing this, @username_1!<issue_closed>"}
{"repo": "pytorch/pytorch", "issue_id": 527461484, "issue_number": 30348, "timestamp": "2020-07-08 03:44:37+00:00", "text": "thanks for fixing this, @username_1!", "context": "<issue_start><issue_comment>Title: Speed up calculateQparams for per-channel observers\nusername_0: ## \ud83d\ude80 Feature Speedup Qparam calculation for observers\r\n\r\n## Pitch\r\nCalculateQparams gets called every forward with QAT. Speeding this up speeds up quantization aware training. \r\nDetails are at:\r\nhttps://discuss.pytorch.org/t/quantization-aware-training-extremely-slow-on-gpu/58894/5\r\n\r\nSpecifically, the task is to remove https://github.com/pytorch/pytorch/blob/master/torch/quantization/observer.py#L119 and instead enhance https://github.com/pytorch/pytorch/blob/master/torch/quantization/observer.py#L157 to process per-channel calculations directly without needing a for loop.\n<issue_comment>username_1: PR put up: https://github.com/pytorch/pytorch/pull/30485\n<issue_comment>username_2: thanks for fixing this, @username_1!<issue_closed>"}
{"repo": "pytorch/pytorch", "issue_id": 527461484, "issue_number": 30348, "timestamp": "2020-07-08 03:44:38+00:00", "text": "", "context": "<issue_start><issue_comment>Title: Speed up calculateQparams for per-channel observers\nusername_0: ## \ud83d\ude80 Feature Speedup Qparam calculation for observers\r\n\r\n## Pitch\r\nCalculateQparams gets called every forward with QAT. Speeding this up speeds up quantization aware training. \r\nDetails are at:\r\nhttps://discuss.pytorch.org/t/quantization-aware-training-extremely-slow-on-gpu/58894/5\r\n\r\nSpecifically, the task is to remove https://github.com/pytorch/pytorch/blob/master/torch/quantization/observer.py#L119 and instead enhance https://github.com/pytorch/pytorch/blob/master/torch/quantization/observer.py#L157 to process per-channel calculations directly without needing a for loop.\n<issue_comment>username_1: PR put up: https://github.com/pytorch/pytorch/pull/30485\n<issue_comment>username_2: thanks for fixing this, @username_1!<issue_closed>"}
{"repo": "facebook/react", "issue_id": 571641815, "issue_number": 18143, "timestamp": "2020-02-26T20:32:43Z", "text": "also includes a bugfix when downloading error codes from circleci. \r\n\r\n[merging this right now, tagging reviewers just in case]", "context": "<issue_start><issue_comment>Title: update version numbers for 16.13\nusername_0: also includes a bugfix when downloading error codes from circleci. \r\n\r\n[merging this right now, tagging reviewers just in case]\n<issue_comment>username_0: probably a cache issue, since 0.19.0 is definitely up on npm https://www.npmjs.com/package/scheduler"}
{"repo": "facebook/react", "issue_id": 571641815, "issue_number": 18143, "timestamp": "2020-02-27 16:46:46+00:00", "text": "probably a cache issue, since 0.19.0 is definitely up on npm https://www.npmjs.com/package/scheduler", "context": "<issue_start><issue_comment>Title: update version numbers for 16.13\nusername_0: also includes a bugfix when downloading error codes from circleci. \r\n\r\n[merging this right now, tagging reviewers just in case]\n<issue_comment>username_0: probably a cache issue, since 0.19.0 is definitely up on npm https://www.npmjs.com/package/scheduler"}
{"repo": "pytorch/pytorch", "issue_id": 605788984, "issue_number": 37166, "timestamp": "2020-04-23T19:07:44Z", "text": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* #37178 recompute_scale_factor default True (half-baked)\n* #37177 Update interpolate (half-baked)\n* #37176 Add interpolate-style overloads to aten::upsample* ops\n* #37175 Add support for float[]? arguments in native_functions.yaml\n* #37174 Add support for int[]? arguments in native_functions.yaml\n* #37173 In interpolate, inline the call to _interp_output_size\n* #37172 In interpolate, move exceptional cases to the bottom\n* #37171 In interpolate, use if instead of elif\n* #37170 In interpolate, join short lines\n* #37169 In interpolate, give a short name to scale_factor_list\n* #37168 In interpolate, only call _interp_output_size in one place\n* **#37166 Clean up formatting in upsample ops**\n* #37165 Whitespace cleanup\n\nDifferential Revision: [D21210001](https://our.internmc.facebook.com/intern/diff/D21210001/)", "context": "<issue_start><issue_comment>Title: Clean up formatting in upsample ops\nusername_0: Stack from [ghstack](https://github.com/ezyang/ghstack):\n* #37178 recompute_scale_factor default True (half-baked)\n* #37177 Update interpolate (half-baked)\n* #37176 Add interpolate-style overloads to aten::upsample* ops\n* #37175 Add support for float[]? arguments in native_functions.yaml\n* #37174 Add support for int[]? arguments in native_functions.yaml\n* #37173 In interpolate, inline the call to _interp_output_size\n* #37172 In interpolate, move exceptional cases to the bottom\n* #37171 In interpolate, use if instead of elif\n* #37170 In interpolate, join short lines\n* #37169 In interpolate, give a short name to scale_factor_list\n* #37168 In interpolate, only call _interp_output_size in one place\n* **#37166 Clean up formatting in upsample ops**\n* #37165 Whitespace cleanup\n\nDifferential Revision: [D21210001](https://our.internmc.facebook.com/intern/diff/D21210001/)"}
{"repo": "pytorch/pytorch", "issue_id": 721853822, "issue_number": 46358, "timestamp": "2020-10-14T23:12:15Z", "text": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* (to be filled)\n\ntorch.vmap is a prototype feature and should not be in the stable\nbinary. This PR:\n- Removes the `torch.vmap` API\n- Removes the documentation entry for `torch.vmap`\n- Changes the vmap tests to use an internal API instead of `torch.vmap`.\n\nTest Plan:\n- Tested locally (test_torch, test_type_hints, test_vmap), but also wait\nfor CI.", "context": "<issue_start><issue_comment>Title: [1.7] Remove torch.vmap (#45571)\nusername_0: Stack from [ghstack](https://github.com/ezyang/ghstack):\n* (to be filled)\n\ntorch.vmap is a prototype feature and should not be in the stable\nbinary. This PR:\n- Removes the `torch.vmap` API\n- Removes the documentation entry for `torch.vmap`\n- Changes the vmap tests to use an internal API instead of `torch.vmap`.\n\nTest Plan:\n- Tested locally (test_torch, test_type_hints, test_vmap), but also wait\nfor CI."}
{"repo": "pytorch/pytorch", "issue_id": 835484070, "issue_number": 54303, "timestamp": "2021-03-19T02:41:00Z", "text": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#54303 [package] Make exporters write to buffer in fbcode**\n\n**Summary**\nCreating temporary files can cause problem in fbcode. This commit\nupdates the packaging tests so that exporters write to a memory\nbuffer when tests run in fbcode.\n\n**Test Plan**\nContinuous integration.", "context": "<issue_start><issue_comment>Title: [package] Make exporters write to buffer in fbcode\nusername_0: Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#54303 [package] Make exporters write to buffer in fbcode**\n\n**Summary**\nCreating temporary files can cause problem in fbcode. This commit\nupdates the packaging tests so that exporters write to a memory\nbuffer when tests run in fbcode.\n\n**Test Plan**\nContinuous integration."}
{"repo": "facebook/react", "issue_id": 865542002, "issue_number": 21337, "timestamp": "2021-04-22T22:35:01Z", "text": "We don't have either Browser nor Node streams at FB WWW. This adds a specific ServerStreamConfig and exported API for this purpose. It's not necessarily optimal. E.g. this doesn't take advantage of back pressure and doesn't support TypedArrays. It's just closest to what we already have but we can iterate on this independently.\r\n\r\nWe also happen to import React with custom builds instead of npm. So I build it separately without an entry point.\r\n\r\nIt's kinds of similar to what we do with Flight for Relay so I just added it under that folder and it's flow typed under the \"dom-relay\" flag to avoid having too many Flow passes.\r\n\r\nI made a new build that builds to ReactDOMServer.js but only in EXPERIMENTAL which is actually \"modern\" builds. So the net effect will be that ReactDOMServer.modern.js is Fizz.\r\n\r\nI plan on moving Fizz onto the main `react-dom/server` export but it's difficult because we need it to export multiple builds which goes against the grain of our system atm. But the idea is to export both and then remove legacy. So this is just jumping ahead to the last step for FB.\r\n\r\nThis structure probably doesn't make much sense overall but it is what it is.", "context": "<issue_start><issue_comment>Title: [Fizz] Add FB specific streaming API and build\nusername_0: We don't have either Browser nor Node streams at FB WWW. This adds a specific ServerStreamConfig and exported API for this purpose. It's not necessarily optimal. E.g. this doesn't take advantage of back pressure and doesn't support TypedArrays. It's just closest to what we already have but we can iterate on this independently.\r\n\r\nWe also happen to import React with custom builds instead of npm. So I build it separately without an entry point.\r\n\r\nIt's kinds of similar to what we do with Flight for Relay so I just added it under that folder and it's flow typed under the \"dom-relay\" flag to avoid having too many Flow passes.\r\n\r\nI made a new build that builds to ReactDOMServer.js but only in EXPERIMENTAL which is actually \"modern\" builds. So the net effect will be that ReactDOMServer.modern.js is Fizz.\r\n\r\nI plan on moving Fizz onto the main `react-dom/server` export but it's difficult because we need it to export multiple builds which goes against the grain of our system atm. But the idea is to export both and then remove legacy. So this is just jumping ahead to the last step for FB.\r\n\r\nThis structure probably doesn't make much sense overall but it is what it is.\n<issue_comment>username_1: I wonder how this PR was able to land given that `packages/react-server-dom-relay/package.json` doesn't define a `files` field and the build scripts seem to enforce this:\r\nhttps://github.com/facebook/react/blob/50263d3273b6fc983acc5b0fd52e670399b248b1/scripts/rollup/packaging.js#L149-L152\r\n\r\nThe build script also throws if there's no `README` file, since it tries to copy unconditionally:\r\nhttps://github.com/facebook/react/blob/50263d3273b6fc983acc5b0fd52e670399b248b1/scripts/rollup/packaging.js#L193-L196\n<issue_comment>username_0: Seems we don\u2019t get there if there wasn\u2019t a node_modules built which there wouldn\u2019t be for FB-only builds.\r\n\r\nhttps://github.com/facebook/react/blob/cae635054e17a6f107a39d328649137b83f25972/scripts/rollup/packaging.js#L213\n<issue_comment>username_1: Hm. When I build all packages locally, `build/node_module` includes 'react-server-dom-relay' (which is why I noticed this in the first place)."}
{"repo": "facebook/react", "issue_id": 865542002, "issue_number": 21337, "timestamp": "2021-09-14 21:30:02+00:00", "text": "I wonder how this PR was able to land given that `packages/react-server-dom-relay/package.json` doesn't define a `files` field and the build scripts seem to enforce this:\r\nhttps://github.com/facebook/react/blob/50263d3273b6fc983acc5b0fd52e670399b248b1/scripts/rollup/packaging.js#L149-L152\r\n\r\nThe build script also throws if there's no `README` file, since it tries to copy unconditionally:\r\nhttps://github.com/facebook/react/blob/50263d3273b6fc983acc5b0fd52e670399b248b1/scripts/rollup/packaging.js#L193-L196", "context": "<issue_start><issue_comment>Title: [Fizz] Add FB specific streaming API and build\nusername_0: We don't have either Browser nor Node streams at FB WWW. This adds a specific ServerStreamConfig and exported API for this purpose. It's not necessarily optimal. E.g. this doesn't take advantage of back pressure and doesn't support TypedArrays. It's just closest to what we already have but we can iterate on this independently.\r\n\r\nWe also happen to import React with custom builds instead of npm. So I build it separately without an entry point.\r\n\r\nIt's kinds of similar to what we do with Flight for Relay so I just added it under that folder and it's flow typed under the \"dom-relay\" flag to avoid having too many Flow passes.\r\n\r\nI made a new build that builds to ReactDOMServer.js but only in EXPERIMENTAL which is actually \"modern\" builds. So the net effect will be that ReactDOMServer.modern.js is Fizz.\r\n\r\nI plan on moving Fizz onto the main `react-dom/server` export but it's difficult because we need it to export multiple builds which goes against the grain of our system atm. But the idea is to export both and then remove legacy. So this is just jumping ahead to the last step for FB.\r\n\r\nThis structure probably doesn't make much sense overall but it is what it is.\n<issue_comment>username_1: I wonder how this PR was able to land given that `packages/react-server-dom-relay/package.json` doesn't define a `files` field and the build scripts seem to enforce this:\r\nhttps://github.com/facebook/react/blob/50263d3273b6fc983acc5b0fd52e670399b248b1/scripts/rollup/packaging.js#L149-L152\r\n\r\nThe build script also throws if there's no `README` file, since it tries to copy unconditionally:\r\nhttps://github.com/facebook/react/blob/50263d3273b6fc983acc5b0fd52e670399b248b1/scripts/rollup/packaging.js#L193-L196\n<issue_comment>username_0: Seems we don\u2019t get there if there wasn\u2019t a node_modules built which there wouldn\u2019t be for FB-only builds.\r\n\r\nhttps://github.com/facebook/react/blob/cae635054e17a6f107a39d328649137b83f25972/scripts/rollup/packaging.js#L213\n<issue_comment>username_1: Hm. When I build all packages locally, `build/node_module` includes 'react-server-dom-relay' (which is why I noticed this in the first place)."}
{"repo": "facebook/react", "issue_id": 865542002, "issue_number": 21337, "timestamp": "2021-09-14 21:35:00+00:00", "text": "Seems we don\u2019t get there if there wasn\u2019t a node_modules built which there wouldn\u2019t be for FB-only builds.\r\n\r\nhttps://github.com/facebook/react/blob/cae635054e17a6f107a39d328649137b83f25972/scripts/rollup/packaging.js#L213", "context": "<issue_start><issue_comment>Title: [Fizz] Add FB specific streaming API and build\nusername_0: We don't have either Browser nor Node streams at FB WWW. This adds a specific ServerStreamConfig and exported API for this purpose. It's not necessarily optimal. E.g. this doesn't take advantage of back pressure and doesn't support TypedArrays. It's just closest to what we already have but we can iterate on this independently.\r\n\r\nWe also happen to import React with custom builds instead of npm. So I build it separately without an entry point.\r\n\r\nIt's kinds of similar to what we do with Flight for Relay so I just added it under that folder and it's flow typed under the \"dom-relay\" flag to avoid having too many Flow passes.\r\n\r\nI made a new build that builds to ReactDOMServer.js but only in EXPERIMENTAL which is actually \"modern\" builds. So the net effect will be that ReactDOMServer.modern.js is Fizz.\r\n\r\nI plan on moving Fizz onto the main `react-dom/server` export but it's difficult because we need it to export multiple builds which goes against the grain of our system atm. But the idea is to export both and then remove legacy. So this is just jumping ahead to the last step for FB.\r\n\r\nThis structure probably doesn't make much sense overall but it is what it is.\n<issue_comment>username_1: I wonder how this PR was able to land given that `packages/react-server-dom-relay/package.json` doesn't define a `files` field and the build scripts seem to enforce this:\r\nhttps://github.com/facebook/react/blob/50263d3273b6fc983acc5b0fd52e670399b248b1/scripts/rollup/packaging.js#L149-L152\r\n\r\nThe build script also throws if there's no `README` file, since it tries to copy unconditionally:\r\nhttps://github.com/facebook/react/blob/50263d3273b6fc983acc5b0fd52e670399b248b1/scripts/rollup/packaging.js#L193-L196\n<issue_comment>username_0: Seems we don\u2019t get there if there wasn\u2019t a node_modules built which there wouldn\u2019t be for FB-only builds.\r\n\r\nhttps://github.com/facebook/react/blob/cae635054e17a6f107a39d328649137b83f25972/scripts/rollup/packaging.js#L213\n<issue_comment>username_1: Hm. When I build all packages locally, `build/node_module` includes 'react-server-dom-relay' (which is why I noticed this in the first place)."}
{"repo": "facebook/react", "issue_id": 865542002, "issue_number": 21337, "timestamp": "2021-09-15 13:40:32+00:00", "text": "Hm. When I build all packages locally, `build/node_module` includes 'react-server-dom-relay' (which is why I noticed this in the first place).", "context": "<issue_start><issue_comment>Title: [Fizz] Add FB specific streaming API and build\nusername_0: We don't have either Browser nor Node streams at FB WWW. This adds a specific ServerStreamConfig and exported API for this purpose. It's not necessarily optimal. E.g. this doesn't take advantage of back pressure and doesn't support TypedArrays. It's just closest to what we already have but we can iterate on this independently.\r\n\r\nWe also happen to import React with custom builds instead of npm. So I build it separately without an entry point.\r\n\r\nIt's kinds of similar to what we do with Flight for Relay so I just added it under that folder and it's flow typed under the \"dom-relay\" flag to avoid having too many Flow passes.\r\n\r\nI made a new build that builds to ReactDOMServer.js but only in EXPERIMENTAL which is actually \"modern\" builds. So the net effect will be that ReactDOMServer.modern.js is Fizz.\r\n\r\nI plan on moving Fizz onto the main `react-dom/server` export but it's difficult because we need it to export multiple builds which goes against the grain of our system atm. But the idea is to export both and then remove legacy. So this is just jumping ahead to the last step for FB.\r\n\r\nThis structure probably doesn't make much sense overall but it is what it is.\n<issue_comment>username_1: I wonder how this PR was able to land given that `packages/react-server-dom-relay/package.json` doesn't define a `files` field and the build scripts seem to enforce this:\r\nhttps://github.com/facebook/react/blob/50263d3273b6fc983acc5b0fd52e670399b248b1/scripts/rollup/packaging.js#L149-L152\r\n\r\nThe build script also throws if there's no `README` file, since it tries to copy unconditionally:\r\nhttps://github.com/facebook/react/blob/50263d3273b6fc983acc5b0fd52e670399b248b1/scripts/rollup/packaging.js#L193-L196\n<issue_comment>username_0: Seems we don\u2019t get there if there wasn\u2019t a node_modules built which there wouldn\u2019t be for FB-only builds.\r\n\r\nhttps://github.com/facebook/react/blob/cae635054e17a6f107a39d328649137b83f25972/scripts/rollup/packaging.js#L213\n<issue_comment>username_1: Hm. When I build all packages locally, `build/node_module` includes 'react-server-dom-relay' (which is why I noticed this in the first place)."}
{"repo": "pytorch/pytorch", "issue_id": 897336101, "issue_number": 58686, "timestamp": "2021-05-20T19:31:21Z", "text": "Summary: added more statistic info for static runtime\n\nTest Plan:\ncaffe2/benchmarks/static_runtime:static_runtime_cpptest\n\nExpected output example:\n\nStatic runtime ms per iter: 0.939483. Iters per second: 1064.41\nNode #0: 0.195671 ms/iter, %wide_offset.1 : Tensor = aten::add(%wide.1, %self._mu, %4)\nNode #1: 0.169457 ms/iter, %wide_normalized.1 : Tensor = aten::mul(%wide_offset.1, %self._sigma)\nNode #2: 0.118218 ms/iter, %wide_preproc.1 : Tensor = aten::clamp(%wide_normalized.1, %5, %6)\nNode #3: 0.038814 ms/iter, %user_emb_t.1 : Tensor = aten::transpose(%user_emb.1, %4, %7)\nNode #4: 0.0860747 ms/iter, %dp_unflatten.1 : Tensor = aten::bmm(%ad_emb_packed.1, %user_emb_t.1)\nNode #5: 0.0102666 ms/iter, %31 : Tensor = static_runtime::flatten_copy(%dp_unflatten.1, %4, %8)\nNode #6: 0.000476333 ms/iter, %19 : Tensor[] = prim::ListConstruct(%31, %wide_preproc.1)\nNode #7: 0.0707332 ms/iter, %input.1 : Tensor = aten::cat(%19, %4)\nNode #8: 0.123695 ms/iter, %fc1.1 : Tensor = aten::addmm(%self._fc_b, %input.1, %29, %4, %4)\nNode #9: 0.0309244 ms/iter, %23 : Tensor = aten::sigmoid(%fc1.1)\nNode #10: 0.0046297 ms/iter, %24 : (Tensor) = prim::TupleConstruct(%23)\nTime per node type:\n       0.195671 ms.    23.0483%. aten::add (1 nodes)\n       0.169457 ms.    19.9605%. aten::mul (1 nodes, out variant)\n       0.123695 ms.    14.5702%. aten::addmm (1 nodes, out variant)\n       0.118218 ms.     13.925%. aten::clamp (1 nodes, out variant)\n      0.0860747 ms.    10.1388%. aten::bmm (1 nodes, out variant)\n      0.0707332 ms.    8.33175%. aten::cat (1 nodes, out variant)\n       0.038814 ms.    4.57195%. aten::transpose (1 nodes)\n      0.0309244 ms.    3.64263%. aten::sigmoid (1 nodes, out variant)\n      0.0102666 ms.    1.20932%. static_runtime::flatten_copy (1 nodes, out variant)\n      0.0046297 ms.   0.545338%. prim::TupleConstruct (1 nodes, out variant)\n    0.000476333 ms.  0.0561079%. prim::ListConstruct (1 nodes, out variant)\n       0.848959 ms. in Total\nStaticRuntime setup time: 0.018925 ms\nMemory allocation time: 0.019808 ms\nMemory deallocation time: 0.0120445 ms\nOutputs deallocation time: 0.0864947 ms\nTotal memory managed: 19328 bytes\nTotal number of reused tensors: 3\nTotal number of 'out' variant nodes/total number of nodes: 9/11 (81.8182%)\n\nReviewed By: hlu1\n\nDifferential Revision: D28553029", "context": "<issue_start><issue_comment>Title: Added statistic related to out variant nodes\nusername_0: Summary: added more statistic info for static runtime\n\nTest Plan:\ncaffe2/benchmarks/static_runtime:static_runtime_cpptest\n\nExpected output example:\n\nStatic runtime ms per iter: 0.939483. Iters per second: 1064.41\nNode #0: 0.195671 ms/iter, %wide_offset.1 : Tensor = aten::add(%wide.1, %self._mu, %4)\nNode #1: 0.169457 ms/iter, %wide_normalized.1 : Tensor = aten::mul(%wide_offset.1, %self._sigma)\nNode #2: 0.118218 ms/iter, %wide_preproc.1 : Tensor = aten::clamp(%wide_normalized.1, %5, %6)\nNode #3: 0.038814 ms/iter, %user_emb_t.1 : Tensor = aten::transpose(%user_emb.1, %4, %7)\nNode #4: 0.0860747 ms/iter, %dp_unflatten.1 : Tensor = aten::bmm(%ad_emb_packed.1, %user_emb_t.1)\nNode #5: 0.0102666 ms/iter, %31 : Tensor = static_runtime::flatten_copy(%dp_unflatten.1, %4, %8)\nNode #6: 0.000476333 ms/iter, %19 : Tensor[] = prim::ListConstruct(%31, %wide_preproc.1)\nNode #7: 0.0707332 ms/iter, %input.1 : Tensor = aten::cat(%19, %4)\nNode #8: 0.123695 ms/iter, %fc1.1 : Tensor = aten::addmm(%self._fc_b, %input.1, %29, %4, %4)\nNode #9: 0.0309244 ms/iter, %23 : Tensor = aten::sigmoid(%fc1.1)\nNode #10: 0.0046297 ms/iter, %24 : (Tensor) = prim::TupleConstruct(%23)\nTime per node type:\n       0.195671 ms.    23.0483%. aten::add (1 nodes)\n       0.169457 ms.    19.9605%. aten::mul (1 nodes, out variant)\n       0.123695 ms.    14.5702%. aten::addmm (1 nodes, out variant)\n       0.118218 ms.     13.925%. aten::clamp (1 nodes, out variant)\n      0.0860747 ms.    10.1388%. aten::bmm (1 nodes, out variant)\n      0.0707332 ms.    8.33175%. aten::cat (1 nodes, out variant)\n       0.038814 ms.    4.57195%. aten::transpose (1 nodes)\n      0.0309244 ms.    3.64263%. aten::sigmoid (1 nodes, out variant)\n      0.0102666 ms.    1.20932%. static_runtime::flatten_copy (1 nodes, out variant)\n      0.0046297 ms.   0.545338%. prim::TupleConstruct (1 nodes, out variant)\n    0.000476333 ms.  0.0561079%. prim::ListConstruct (1 nodes, out variant)\n       0.848959 ms. in Total\nStaticRuntime setup time: 0.018925 ms\nMemory allocation time: 0.019808 ms\nMemory deallocation time: 0.0120445 ms\nOutputs deallocation time: 0.0864947 ms\nTotal memory managed: 19328 bytes\nTotal number of reused tensors: 3\nTotal number of 'out' variant nodes/total number of nodes: 9/11 (81.8182%)\n\nReviewed By: hlu1\n\nDifferential Revision: D28553029"}
{"repo": "pytorch/pytorch", "issue_id": 958487800, "issue_number": 62605, "timestamp": "2021-08-02T20:35:44Z", "text": "**Overview:**\r\nThis removes the preceding `_` from `_Join`, `_Joinable`, and `_JoinHook` in preparation for adding the generic join context manager tutorial (see [here](https://github.com/pytorch/tutorials/pull/1610)). This also adds a docs page, which can be linked from the tutorial. [Here](https://github.com/pytorch/pytorch/files/6919475/render.pdf) is a render of the docs page.\r\n\r\n\r\n**Test Plan:**\r\n`DistributedDataParallel.join()`:\r\n```\r\ntouch /tmp/barrier && TEMP_DIR=\"/tmp\" BACKEND=\"nccl\" WORLD_SIZE=\"2\" gpurun python test/distributed/test_distributed_fork.py -- TestDistBackendWithFork.test_ddp_uneven_inputs TestDistBackendWithFork.test_ddp_uneven_inputs_stop_iteration_sync_bn TestDistBackendWithFork.test_ddp_grad_div_uneven_inputs TestDistBackendWithFork.test_ddp_uneven_input_join_disable TestDistBackendWithFork.test_ddp_uneven_input_exception\r\n```\r\n\r\n`ZeroRedundancyOptimizer`:\r\n```\r\ngpurun4 python test/distributed/optim/test_zero_redundancy_optimizer.py\r\n```\r\nNOTE: DDP overlap tests are failing due to a landing race. See https://github.com/pytorch/pytorch/pull/62592. Once the fix is landed, I will rebase, and tests should be passing.\r\n\r\n`Join`:\r\n```\r\ngpurun4 python test/distributed/algorithms/test_join.py\r\n```", "context": "<issue_start><issue_comment>Title: Make _Join, _Joinable, _JoinHook public\nusername_0: **Overview:**\r\nThis removes the preceding `_` from `_Join`, `_Joinable`, and `_JoinHook` in preparation for adding the generic join context manager tutorial (see [here](https://github.com/pytorch/tutorials/pull/1610)). This also adds a docs page, which can be linked from the tutorial. [Here](https://github.com/pytorch/pytorch/files/6919475/render.pdf) is a render of the docs page.\r\n\r\n\r\n**Test Plan:**\r\n`DistributedDataParallel.join()`:\r\n```\r\ntouch /tmp/barrier && TEMP_DIR=\"/tmp\" BACKEND=\"nccl\" WORLD_SIZE=\"2\" gpurun python test/distributed/test_distributed_fork.py -- TestDistBackendWithFork.test_ddp_uneven_inputs TestDistBackendWithFork.test_ddp_uneven_inputs_stop_iteration_sync_bn TestDistBackendWithFork.test_ddp_grad_div_uneven_inputs TestDistBackendWithFork.test_ddp_uneven_input_join_disable TestDistBackendWithFork.test_ddp_uneven_input_exception\r\n```\r\n\r\n`ZeroRedundancyOptimizer`:\r\n```\r\ngpurun4 python test/distributed/optim/test_zero_redundancy_optimizer.py\r\n```\r\nNOTE: DDP overlap tests are failing due to a landing race. See https://github.com/pytorch/pytorch/pull/62592. Once the fix is landed, I will rebase, and tests should be passing.\r\n\r\n`Join`:\r\n```\r\ngpurun4 python test/distributed/algorithms/test_join.py\r\n```\n<issue_comment>username_0: Should `Joinable._join_hook()`, `Joinable._join_device()`, and `Joinable._join_process_group()` be made public as well (i.e. have their preceding `_` removed)?"}
{"repo": "pytorch/pytorch", "issue_id": 958487800, "issue_number": 62605, "timestamp": "2021-08-02 20:38:02+00:00", "text": "Should `Joinable._join_hook()`, `Joinable._join_device()`, and `Joinable._join_process_group()` be made public as well (i.e. have their preceding `_` removed)?", "context": "<issue_start><issue_comment>Title: Make _Join, _Joinable, _JoinHook public\nusername_0: **Overview:**\r\nThis removes the preceding `_` from `_Join`, `_Joinable`, and `_JoinHook` in preparation for adding the generic join context manager tutorial (see [here](https://github.com/pytorch/tutorials/pull/1610)). This also adds a docs page, which can be linked from the tutorial. [Here](https://github.com/pytorch/pytorch/files/6919475/render.pdf) is a render of the docs page.\r\n\r\n\r\n**Test Plan:**\r\n`DistributedDataParallel.join()`:\r\n```\r\ntouch /tmp/barrier && TEMP_DIR=\"/tmp\" BACKEND=\"nccl\" WORLD_SIZE=\"2\" gpurun python test/distributed/test_distributed_fork.py -- TestDistBackendWithFork.test_ddp_uneven_inputs TestDistBackendWithFork.test_ddp_uneven_inputs_stop_iteration_sync_bn TestDistBackendWithFork.test_ddp_grad_div_uneven_inputs TestDistBackendWithFork.test_ddp_uneven_input_join_disable TestDistBackendWithFork.test_ddp_uneven_input_exception\r\n```\r\n\r\n`ZeroRedundancyOptimizer`:\r\n```\r\ngpurun4 python test/distributed/optim/test_zero_redundancy_optimizer.py\r\n```\r\nNOTE: DDP overlap tests are failing due to a landing race. See https://github.com/pytorch/pytorch/pull/62592. Once the fix is landed, I will rebase, and tests should be passing.\r\n\r\n`Join`:\r\n```\r\ngpurun4 python test/distributed/algorithms/test_join.py\r\n```\n<issue_comment>username_0: Should `Joinable._join_hook()`, `Joinable._join_device()`, and `Joinable._join_process_group()` be made public as well (i.e. have their preceding `_` removed)?"}
{"repo": "pytorch/pytorch", "issue_id": 962267603, "issue_number": 62862, "timestamp": "2021-08-05 23:25:15+00:00", "text": "https://github.com/pytorch/pytorch/pull/62770 refactored some code around timing methods in Reducer, which are primarily used for logging purposes to understand performance issues. [This comment](https://github.com/pytorch/pytorch/pull/62770/files#r683666092) mentions that to keep conventions consistent, instead of doing something like:\r\n\r\n```\r\nint64_t forward_start_time = kUnsetTime;\r\n```\r\n\r\nwe should prefer\r\n```\r\nc10::optional<int64_t> forward_start_time = c10::nullopt;\r\n```\r\n\r\nThe goal here is to make all necessary changes to enable this conversion.", "context": "<issue_start><issue_comment>Title: Convert unset variables in `Reducer::Timer` to use c10::optional\nusername_0: https://github.com/pytorch/pytorch/pull/62770 refactored some code around timing methods in Reducer, which are primarily used for logging purposes to understand performance issues. [This comment](https://github.com/pytorch/pytorch/pull/62770/files#r683666092) mentions that to keep conventions consistent, instead of doing something like:\r\n\r\n```\r\nint64_t forward_start_time = kUnsetTime;\r\n```\r\n\r\nwe should prefer\r\n```\r\nc10::optional<int64_t> forward_start_time = c10::nullopt;\r\n```\r\n\r\nThe goal here is to make all necessary changes to enable this conversion."}
{"repo": "pytorch/pytorch", "issue_id": 992543198, "issue_number": 64773, "timestamp": "2021-09-09T19:09:46Z", "text": "Summary: Remove loose `#pragma warning ( pop )` in TensorBase.h.\n\nReviewed By: ezyang\n\nDifferential Revision: D30846958", "context": "<issue_start><issue_comment>Title: [caffe2/aten] Remove loose #pragma warning ( pop ) in TensorBase.h\nusername_0: Summary: Remove loose `#pragma warning ( pop )` in TensorBase.h.\n\nReviewed By: ezyang\n\nDifferential Revision: D30846958\n<issue_comment>username_1: Reverting due to broken `test_reductions` in https://github.com/pytorch/pytorch/runs/3564783665"}
{"repo": "pytorch/pytorch", "issue_id": 992543198, "issue_number": 64773, "timestamp": "2021-09-10 18:47:58+00:00", "text": "Reverting due to broken `test_reductions` in https://github.com/pytorch/pytorch/runs/3564783665", "context": "<issue_start><issue_comment>Title: [caffe2/aten] Remove loose #pragma warning ( pop ) in TensorBase.h\nusername_0: Summary: Remove loose `#pragma warning ( pop )` in TensorBase.h.\n\nReviewed By: ezyang\n\nDifferential Revision: D30846958\n<issue_comment>username_1: Reverting due to broken `test_reductions` in https://github.com/pytorch/pytorch/runs/3564783665"}
{"repo": "pytorch/pytorch", "issue_id": 1012360542, "issue_number": 65924, "timestamp": "2021-09-30T16:05:24Z", "text": "This PR fixes a bug from a newly introduced feature in PR #64951\r\n\r\n\r\nSummary:\r\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65220\r\n\r\nFixes #65221\r\n\r\n- Remove deepcopy from Mapper to support file handles\r\n- Convert `IterableWrapper` to deepcopy iterable instance within each iterator to prevent in-place modification (different data per epoch)\r\n- Convert `IDP` to `IterableWrapper` in test_datapipe.py\r\n- Refine the variable names (prevent using `dp` that is module reference)\r\n\r\nTest Plan: Imported from OSS\r\n\r\nReviewed By: malfet\r\n\r\nDifferential Revision: D31021886\r\n\r\nPulled By: username_0\r\n\r\nfbshipit-source-id: 72a9eee66c758e2717d591cd0942892bddedc223\r\n\r\nFixes #{issue number}", "context": "<issue_start><issue_comment>Title: [DataPipe] Fix deepcopy for Mapper and in-place modification for IterableWrapper\nusername_0: This PR fixes a bug from a newly introduced feature in PR #64951\r\n\r\n\r\nSummary:\r\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65220\r\n\r\nFixes #65221\r\n\r\n- Remove deepcopy from Mapper to support file handles\r\n- Convert `IterableWrapper` to deepcopy iterable instance within each iterator to prevent in-place modification (different data per epoch)\r\n- Convert `IDP` to `IterableWrapper` in test_datapipe.py\r\n- Refine the variable names (prevent using `dp` that is module reference)\r\n\r\nTest Plan: Imported from OSS\r\n\r\nReviewed By: malfet\r\n\r\nDifferential Revision: D31021886\r\n\r\nPulled By: username_0\r\n\r\nfbshipit-source-id: 72a9eee66c758e2717d591cd0942892bddedc223\r\n\r\nFixes #{issue number}"}
{"repo": "pytorch/pytorch", "issue_id": 1032118962, "issue_number": 67003, "timestamp": "2021-10-21T06:43:36Z", "text": "Summary: Currently Torchbind classes arent selective. This makes is a rough granularity pass that will remove entire classes if they arent selected. If we need finer granularity in the future we can make individual methods within classes Selective. Theres also more classes that can be marked selective. Current linux list of all custom classes is {P463389669}\n\nTest Plan: CI, model unit tests, test models in prod apps\n\nDifferential Revision: D31092564", "context": "<issue_start><issue_comment>Title: [Pytorch Edge] Make some torchbind classes selective\nusername_0: Summary: Currently Torchbind classes arent selective. This makes is a rough granularity pass that will remove entire classes if they arent selected. If we need finer granularity in the future we can make individual methods within classes Selective. Theres also more classes that can be marked selective. Current linux list of all custom classes is {P463389669}\n\nTest Plan: CI, model unit tests, test models in prod apps\n\nDifferential Revision: D31092564"}
{"repo": "facebook/react", "issue_id": 1086594300, "issue_number": 23016, "timestamp": "2021-12-22T08:49:36Z", "text": "<!--\r\n  Thanks for submitting a pull request!\r\n  We appreciate you spending the time to work on these changes. Please provide enough information so that others can review your pull request. The three fields below are mandatory.\r\n\r\n  Before submitting a pull request, please make sure the following is done:\r\n\r\n  1. Fork [the repository](https://github.com/facebook/react) and create your branch from `main`.\r\n  2. Run `yarn` in the repository root.\r\n  3. If you've fixed a bug or added code that should be tested, add tests!\r\n  4. Ensure the test suite passes (`yarn test`). Tip: `yarn test --watch TestName` is helpful in development.\r\n  5. Run `yarn test --prod` to test in the production environment. It supports the same options as `yarn test`.\r\n  6. If you need a debugger, run `yarn debug-test --watch TestName`, open `chrome://inspect`, and press \"Inspect\".\r\n  7. Format your code with [prettier](https://github.com/prettier/prettier) (`yarn prettier`).\r\n  8. Make sure your code lints (`yarn lint`). Tip: `yarn linc` to only check changed files.\r\n  9. Run the [Flow](https://flowtype.org/) type checks (`yarn flow`).\r\n  10. If you haven't already, complete the CLA.\r\n\r\n  Learn more about contributing: https://reactjs.org/docs/how-to-contribute.html\r\n-->\r\n\r\n## Summary\r\nGenerate flow config for specific renderer instead of generating all configs while running `yarn flow` for the given renderer.\r\nSee https://github.com/facebook/react/issues/22941\r\n<!--\r\n Explain the **motivation** for making this change. What existing problem does the pull request solve?\r\n-->\r\n\r\n## How did you test this change?\r\nNo test suites/tests are added for this change.\r\n_**Before:**_\r\n![image](https://user-images.githubusercontent.com/1722495/147063864-88ec2f4b-1e5d-4479-8bbb-5afba9bbfd96.png)\r\n\r\n**_After:_**\r\n![image](https://user-images.githubusercontent.com/1722495/147063803-632ab1ab-ea23-4294-906b-10b5baf8e7d2.png)\r\n<!--\r\n  Demonstrate the code is solid. Example: The exact commands you ran and their output, screenshots / videos if the pull request changes the user interface.\r\n  How exactly did you verify that your PR solves the issue you wanted to solve?\r\n  If you leave this empty, your PR will very likely be closed.\r\n-->", "context": "<issue_start><issue_comment>Title: feat: generate flow config for specific renderer instead of generating all configs\nusername_0: <!--\r\n  Thanks for submitting a pull request!\r\n  We appreciate you spending the time to work on these changes. Please provide enough information so that others can review your pull request. The three fields below are mandatory.\r\n\r\n  Before submitting a pull request, please make sure the following is done:\r\n\r\n  1. Fork [the repository](https://github.com/facebook/react) and create your branch from `main`.\r\n  2. Run `yarn` in the repository root.\r\n  3. If you've fixed a bug or added code that should be tested, add tests!\r\n  4. Ensure the test suite passes (`yarn test`). Tip: `yarn test --watch TestName` is helpful in development.\r\n  5. Run `yarn test --prod` to test in the production environment. It supports the same options as `yarn test`.\r\n  6. If you need a debugger, run `yarn debug-test --watch TestName`, open `chrome://inspect`, and press \"Inspect\".\r\n  7. Format your code with [prettier](https://github.com/prettier/prettier) (`yarn prettier`).\r\n  8. Make sure your code lints (`yarn lint`). Tip: `yarn linc` to only check changed files.\r\n  9. Run the [Flow](https://flowtype.org/) type checks (`yarn flow`).\r\n  10. If you haven't already, complete the CLA.\r\n\r\n  Learn more about contributing: https://reactjs.org/docs/how-to-contribute.html\r\n-->\r\n\r\n## Summary\r\nGenerate flow config for specific renderer instead of generating all configs while running `yarn flow` for the given renderer.\r\nSee https://github.com/facebook/react/issues/22941\r\n<!--\r\n Explain the **motivation** for making this change. What existing problem does the pull request solve?\r\n-->\r\n\r\n## How did you test this change?\r\nNo test suites/tests are added for this change.\r\n_**Before:**_\r\n![image](https://user-images.githubusercontent.com/1722495/147063864-88ec2f4b-1e5d-4479-8bbb-5afba9bbfd96.png)\r\n\r\n**_After:_**\r\n![image](https://user-images.githubusercontent.com/1722495/147063803-632ab1ab-ea23-4294-906b-10b5baf8e7d2.png)\r\n<!--\r\n  Demonstrate the code is solid. Example: The exact commands you ran and their output, screenshots / videos if the pull request changes the user interface.\r\n  How exactly did you verify that your PR solves the issue you wanted to solve?\r\n  If you leave this empty, your PR will very likely be closed.\r\n-->"}
{"repo": "pytorch/pytorch", "issue_id": 1182536553, "issue_number": 74808, "timestamp": "2022-03-27 13:50:58+00:00", "text": "### \ud83d\udc1b Describe the bug\n\ngithub: skipping check (offline), for updates see [https://github.com/ultralytics/yolov5]()\r\nOutput exceeds the [size limit](command:workbench.action.openSettings?[%22notebook.output.textLineLimit%22]). Open the full output data[ in a text editor](command:workbench.action.openLargeOutput?a5828476-4b57-44eb-8623-b95587e6a784)\r\ntrain: weights=yolov5s.pt, cfg=, data=e:/2022/yolov5/yolov5/data.yaml, hyp=data\\hyps\\hyp.scratch-low.yaml, epochs=50, batch_size=8, imgsz=416, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, evolve=None, bucket=, cache=ram, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=runs\\train, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\r\nYOLOv5  v6.1-69-g7830e91 torch 1.11.0+cpu CPU\r\n\r\nhyperparameters: lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\r\nWeights & Biases: run 'pip install wandb' to automatically track and visualize YOLOv5  runs (RECOMMENDED)\r\nTensorBoard: Start with 'tensorboard --logdir runs\\train', view at [http://localhost:6006/]()\r\nOverriding model.yaml nc=80 with nc=1\r\n\r\n                 from  n    params  module                                  arguments                     \r\n  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \r\n  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \r\n  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \r\n  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \r\n  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \r\n  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \r\n  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \r\n  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \r\n  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \r\n  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \r\n 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \r\n 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \r\n 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \r\n 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \r\n 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \r\n 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \r\n...\r\n    exitcode = _main(fd, parent_sentinel)\r\n  File \"G:\\Anaconda\\envs\\yolov5\\lib\\multiprocessing\\spawn.py\", line 126, in _main\r\n    self = reduction.pickle.load(from_parent)\r\nEOFError: Ran out of input\n\n### Versions\n\ni am a fresh to yolo,I follow a yolov5 teach blog:https://blog.roboflow.com/how-to-train-yolov5-on-a-custom-dataset/\uff0cand it is run in the colab, i train my own dataset in the jupyter, about running for 3mins, the error cames, i don't how the error come, is about the setting or the code?is there someone can help me fix it", "context": "<issue_start><issue_comment>Title: yolov5 in jupyter error: self = reduction.pickle.load(from_parent) EOFError: Ran out of input\nusername_0: ### \ud83d\udc1b Describe the bug\n\ngithub: skipping check (offline), for updates see [https://github.com/ultralytics/yolov5]()\r\nOutput exceeds the [size limit](command:workbench.action.openSettings?[%22notebook.output.textLineLimit%22]). Open the full output data[ in a text editor](command:workbench.action.openLargeOutput?a5828476-4b57-44eb-8623-b95587e6a784)\r\ntrain: weights=yolov5s.pt, cfg=, data=e:/2022/yolov5/yolov5/data.yaml, hyp=data\\hyps\\hyp.scratch-low.yaml, epochs=50, batch_size=8, imgsz=416, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, evolve=None, bucket=, cache=ram, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=runs\\train, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\r\nYOLOv5  v6.1-69-g7830e91 torch 1.11.0+cpu CPU\r\n\r\nhyperparameters: lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\r\nWeights & Biases: run 'pip install wandb' to automatically track and visualize YOLOv5  runs (RECOMMENDED)\r\nTensorBoard: Start with 'tensorboard --logdir runs\\train', view at [http://localhost:6006/]()\r\nOverriding model.yaml nc=80 with nc=1\r\n\r\n                 from  n    params  module                                  arguments                     \r\n  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \r\n  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \r\n  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \r\n  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \r\n  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \r\n  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \r\n  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \r\n  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \r\n  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \r\n  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \r\n 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \r\n 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \r\n 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \r\n 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \r\n 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \r\n 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \r\n...\r\n    exitcode = _main(fd, parent_sentinel)\r\n  File \"G:\\Anaconda\\envs\\yolov5\\lib\\multiprocessing\\spawn.py\", line 126, in _main\r\n    self = reduction.pickle.load(from_parent)\r\nEOFError: Ran out of input\n\n### Versions\n\ni am a fresh to yolo,I follow a yolov5 teach blog:https://blog.roboflow.com/how-to-train-yolov5-on-a-custom-dataset/\uff0cand it is run in the colab, i train my own dataset in the jupyter, about running for 3mins, the error cames, i don't how the error come, is about the setting or the code?is there someone can help me fix it<issue_closed>"}
{"repo": "pytorch/pytorch", "issue_id": 1182536553, "issue_number": 74808, "timestamp": "2022-03-27 14:22:10+00:00", "text": "", "context": "<issue_start><issue_comment>Title: yolov5 in jupyter error: self = reduction.pickle.load(from_parent) EOFError: Ran out of input\nusername_0: ### \ud83d\udc1b Describe the bug\n\ngithub: skipping check (offline), for updates see [https://github.com/ultralytics/yolov5]()\r\nOutput exceeds the [size limit](command:workbench.action.openSettings?[%22notebook.output.textLineLimit%22]). Open the full output data[ in a text editor](command:workbench.action.openLargeOutput?a5828476-4b57-44eb-8623-b95587e6a784)\r\ntrain: weights=yolov5s.pt, cfg=, data=e:/2022/yolov5/yolov5/data.yaml, hyp=data\\hyps\\hyp.scratch-low.yaml, epochs=50, batch_size=8, imgsz=416, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, evolve=None, bucket=, cache=ram, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=runs\\train, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\r\nYOLOv5  v6.1-69-g7830e91 torch 1.11.0+cpu CPU\r\n\r\nhyperparameters: lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\r\nWeights & Biases: run 'pip install wandb' to automatically track and visualize YOLOv5  runs (RECOMMENDED)\r\nTensorBoard: Start with 'tensorboard --logdir runs\\train', view at [http://localhost:6006/]()\r\nOverriding model.yaml nc=80 with nc=1\r\n\r\n                 from  n    params  module                                  arguments                     \r\n  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \r\n  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \r\n  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \r\n  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \r\n  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \r\n  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \r\n  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \r\n  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \r\n  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \r\n  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \r\n 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \r\n 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \r\n 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \r\n 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \r\n 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \r\n 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \r\n...\r\n    exitcode = _main(fd, parent_sentinel)\r\n  File \"G:\\Anaconda\\envs\\yolov5\\lib\\multiprocessing\\spawn.py\", line 126, in _main\r\n    self = reduction.pickle.load(from_parent)\r\nEOFError: Ran out of input\n\n### Versions\n\ni am a fresh to yolo,I follow a yolov5 teach blog:https://blog.roboflow.com/how-to-train-yolov5-on-a-custom-dataset/\uff0cand it is run in the colab, i train my own dataset in the jupyter, about running for 3mins, the error cames, i don't how the error come, is about the setting or the code?is there someone can help me fix it<issue_closed>"}
{"repo": "facebook/react", "issue_id": 69988869, "issue_number": 3717, "timestamp": "2015-04-22T00:47:05Z", "text": "I found that `ReactMultiChild.mountChildren` and the functions it calls were iterating over the children many times. This inlines those calls as it is a critical rendering path. Through testing a chat example I have found about a 10-15% performance gains through benchmark.js.\r\n\r\nTest: https://github.com/username_0/react-perf/tree/inlineMountChildren\r\nTest results: https://travis-ci.org/username_0/react-perf/builds/59480548", "context": "<issue_start><issue_comment>Title: [performance] Inline ReactMultiChild.mountChildren\nusername_0: I found that `ReactMultiChild.mountChildren` and the functions it calls were iterating over the children many times. This inlines those calls as it is a critical rendering path. Through testing a chat example I have found about a 10-15% performance gains through benchmark.js.\r\n\r\nTest: https://github.com/username_0/react-perf/tree/inlineMountChildren\r\nTest results: https://travis-ci.org/username_0/react-perf/builds/59480548\n<issue_comment>username_1: For reference, this separation was introduced intentionally in this PR:\r\n\r\nhttps://github.com/facebook/react/pull/2567\r\n\r\ncc @username_2\n<issue_comment>username_0: I seem to have broken the test right before pushing. Will investigate.\n<issue_comment>username_2: Yea, this was intentionally separated, knowing that it is a slight performance loss for now. It is a part of an on-going refactor to separate instantiation and resolution from the render strategy. That way we can reuse core React logic while having the option to optimize for different render outputs (DOM, ART, React Native, etc). It is also easier to reason about fragments (components that render multiple elements) if we can diff the resolved set.\r\n\r\nThe idea is to move the instantiation out of the DOM component.\r\n\r\nIt is likely we can remerge this kind of optimization later as more of this refactor starts coming together but I think I'd rather keep it separate for now so that we can incrementally continue the refactor instead of having to make a complete reimplementation from scratch.\r\n\r\nThis brings up an interesting point though. For renderToString, we don't really need to keep any of the internal state tree which is only used for optimizing updates. Maybe we need a whole optimized pipeline that doesn't generate the internal memoization nor the state tree?\n<issue_comment>username_0: :+1: totally agree @username_2. I think that would also help with some of the GC issues we're seeing with higher throughput in node. If you can point me in the right direction for where to start with this, I'd be more than glad to work on it.\n<issue_comment>username_0: Looks like there's still a significant gain if we inline only `ReactChildReconciler.instantiateChildren`:\r\n\r\nChanges: https://github.com/facebook/react/compare/master...username_0:inlineInstantiateChildren\r\nTest: https://github.com/username_0/react-perf/tree/inlineInstantiateChildren\r\nResult: https://travis-ci.org/username_0/react-perf/builds/59784288\n<issue_comment>username_1: Nice find. Maybe we can take advantage of ReactChildren.map here? We may want a version which skips the ReactFragment.create call for these internals but otherwise I think it should give the right effect.\n<issue_comment>username_0: I pulled the traversal function out into a named functioned rather than inline in the branch above. I will run a comparison of using `ReactChildren.map` vs `traverseAllChildren` directly.\n<issue_comment>username_0: As you said I'm running into a lot of error logs due to `ReactFragment.create` being called with nulls and other invalid arguments. I think sticking with `traverseAllChildren` in this case will be better.\n<issue_comment>username_1: Sorry, I may have been a little unclear. Can you do something like this\r\n\r\n```\r\ndiff --git a/src/utils/ReactChildren.js b/src/utils/ReactChildren.js\r\nindex e470609..84ca357 100644\r\n--- a/src/utils/ReactChildren.js\r\n+++ b/src/utils/ReactChildren.js\r\n@@ -122,7 +122,7 @@ function mapChildren(children, func, context) {\r\n   var traverseContext = MapBookKeeping.getPooled(mapResult, func, context);\r\n   traverseAllChildren(children, mapSingleChildIntoContext, traverseContext);\r\n   MapBookKeeping.release(traverseContext);\r\n-  return ReactFragment.create(mapResult);\r\n+  return mapResult;\r\n }\r\n \r\n function forEachSingleChildDummy(traverseContext, child, name, i) {\r\n@@ -142,7 +142,10 @@ function countChildren(children, context) {\r\n \r\n var ReactChildren = {\r\n   forEach: forEachChildren,\r\n-  map: mapChildren,\r\n+  map: function(children, func, context) {\r\n+    return ReactFragment.create(mapChildren(children, func, context));\r\n+  },\r\n+  mapObject: mapChildren,\r\n   count: countChildren\r\n };\r\n \r\n```\r\n\r\nand then use ReactChildren.mapObject?\n<issue_comment>username_0: Ok, so I create a branch that uses ReactChildren.mapObject: https://github.com/facebook/react/compare/master...username_0:inlineInstantiateChildrenWithMap\r\n\r\nThere are two tests that fail using this method:\r\n\r\n * core/__tests__/ReactMultiChildReconcile-test ReactMultiChildReconcile should insert non-empty children in middle where nulls were.\r\n * core/__tests__/ReactMultiChildText-test ReactMultiChildText should correctly handle all possible children for render and update.\r\n\r\nI believe this is because map doesn't handle skipping nulls.\r\n\r\nI also ran a benchmark for using `traverseAllChilden` as my per previous comment vs. using `ReactChildren.mapObject` as @username_1 suggested. Running locally, I'm seeing a significant overhead in using the pooling added by `ReactChildren.mapObject`. On some runs it shows to be less performant than not inlining. You can see the travis-ci run of the benchmark here: https://travis-ci.org/username_0/react-perf/builds/60283471\n<issue_comment>username_0: @username_2 Can you elaborate one what you mean by \"internal state tree\"? I'm trying to understand what could be removed from the rendering path to optimize server rendering. I have a feeling some of the `mountComponent` logic needs to be split up into rendering logic and other logic (event registration, state management, etc.), but I'm not familiar enough with the \"other\" logic that's happening to know what to look at.\n<issue_comment>username_1: @username_0 Interesting. If inlining a closure (like https://gist.github.com/username_1/594910785229dba07d5c) is faster we should probably switch to that \u2013 we've tried to avoid closures in favor of manually passing context objects in hopes of lowering GC pressure and improving perf but all of this feels like dogscience.\r\n\r\n![image](https://cloud.githubusercontent.com/assets/6820/7360334/68f64cd2-ecfe-11e4-8e6e-b85703c9a2de.png)\n<issue_comment>username_1: Sorry for all the back and forth.\n<issue_comment>username_0: No worries. I too often feel like I have no idea what I'm doing.\r\n\r\nI think using `traverseAllChildren` directly as in https://github.com/facebook/react/compare/master...username_0:inlineInstantiateChildren addresses your concern about GC although it does duplicate some of the code in `ReactChildren.map`. We do need [this check](https://github.com/facebook/react/compare/master...username_0:inlineInstantiateChildren#diff-89f33512519fa4d853463913eac8ef04R24) though in order for the tests to pass. Any ideas on introducing that check in to `ReactChildren.map`?\n<issue_comment>username_1: I tried to replicate these results locally and in different browsers. Here's what I did:\r\n\r\n```sh\r\n$ git checkout origin/inlineMountChildren\r\n$ npm install -g browserify\r\n$ npm install babelify envify\r\n$ NODE_ENV=production browserify -t babelify bench-chat.js >bundle.js\r\n```\r\n\r\nAnd I created bench.html:\r\n\r\n```html\r\n<meta charset=\"utf-8\">\r\n<body style=\"font: 10px Menlo; white-space: pre;\">\r\n<div id=\"container\"></div>\r\n<script>\r\nconsole.log = function(x) {\r\n  container.textContent += x.replace(/\\033\\[[0-9;]*m/g, '') + '\\n';\r\n};\r\n</script>\r\n<script src=\"bundle.js\"></script>\r\n```\r\n\r\nOpening bench.html in Safari (JavaScriptCore) gave me:\r\n\r\n```\r\nRenderChat for inputs Optimized x 2,094 ops/sec \u00b10.85% (66 runs sampled)\r\nRenderChat for inputs Stock x 2,081 ops/sec \u00b10.78% (66 runs sampled)\r\nThe Fastest test suite is RenderChat for inputs Optimized,RenderChat for inputs Stock\r\n\r\n+------------+---------------+---------------+\r\n|            \u00e2\u201d\u201a Optimized     \u00e2\u201d\u201a Stock         |\r\n+------------+---------------+---------------+\r\n| RenderChat \u00e2\u201d\u201a 2,094 ops/sec \u00e2\u201d\u201a 2,081 ops/sec |\r\n+------------+---------------+---------------+\r\n```\r\n\r\nwhere they're the same speed (not sure what's up with the encoding there), and in Chrome I got:\r\n\r\n```\r\nRenderChat for inputs Optimized x 1,534 ops/sec \u00b12.38% (61 runs sampled)\r\nRenderChat for inputs Stock x 1,775 ops/sec \u00b11.56% (65 runs sampled)\r\nThe Fastest test suite is RenderChat for inputs Stock\r\n\r\n+------------+---------------+---------------+\r\n|            \u2502 Optimized     \u2502 Stock         |\r\n+------------+---------------+---------------+\r\n| RenderChat \u2502 1,534 ops/sec \u2502 1,775 ops/sec |\r\n+------------+---------------+---------------+\r\n```\r\n\r\nwhere the old version is 15% faster. Running `node bench-chat.js` gave me\r\n\r\n```\r\nRenderChat for inputs Optimized x 368 ops/sec \u00b13.18% (73 runs sampled)\r\nRenderChat for inputs Stock x 348 ops/sec \u00b12.99% (72 runs sampled)\r\nThe Fastest test suite is RenderChat for inputs Optimized\r\n\r\n+------------+-------------+-------------+\r\n|            \u2502 Optimized   \u2502 Stock       |\r\n+------------+-------------+-------------+\r\n| RenderChat \u2502 368 ops/sec \u2502 348 ops/sec |\r\n+------------+-------------+-------------+\r\n```\r\n\r\nwhere your version is maybe 5% faster, but not the 10\u201315% you were seeing originally. Any ideas if I'm doing something wrong or if this is just wildly different between engines? I'd have expected Chrome and Node to be more similar, at the very least.\n<issue_comment>username_0: Are the symlinks working correctly for you? browserify does not work for me locally to create the bundle.js as you did. I'm not sure why your results are so different from my local tests and travis-ci.\n<issue_comment>username_0: It's also worth noting, that even with two versions of stock flux, the benchmark for the item that is tested first is usually ~5% lower than the second which is why I moved \"optimized\" as the first test case. Consistency seems to be a consistent problem =/\n<issue_comment>username_0: Here's travis with 10 runs of bench-chat: https://travis-ci.org/username_0/react-perf/builds/60620846\n<issue_comment>username_0: So it looks like I didn't have the node_modules for each of the apps pushed, so the components may have been using the wrong React. I have pushed the node_modules folder for each app and used symlinks to the react versions such that they are completely isolated. I'm hoping this will make the tests more consistent for you.\r\n\r\nhttps://github.com/username_0/react-perf/tree/inlineMountChildren\n<issue_comment>username_0: Closing this since it undoes a purposeful refactor. I will open a new PR to discuss inlining just `ReactChildReconciler.instantiateChildren`."}
{"repo": "facebook/react", "issue_id": 69988869, "issue_number": 3717, "timestamp": "2015-04-22 00:49:53+00:00", "text": "For reference, this separation was introduced intentionally in this PR:\r\n\r\nhttps://github.com/facebook/react/pull/2567\r\n\r\ncc @username_2", "context": "<issue_start><issue_comment>Title: [performance] Inline ReactMultiChild.mountChildren\nusername_0: I found that `ReactMultiChild.mountChildren` and the functions it calls were iterating over the children many times. This inlines those calls as it is a critical rendering path. Through testing a chat example I have found about a 10-15% performance gains through benchmark.js.\r\n\r\nTest: https://github.com/username_0/react-perf/tree/inlineMountChildren\r\nTest results: https://travis-ci.org/username_0/react-perf/builds/59480548\n<issue_comment>username_1: For reference, this separation was introduced intentionally in this PR:\r\n\r\nhttps://github.com/facebook/react/pull/2567\r\n\r\ncc @username_2\n<issue_comment>username_0: I seem to have broken the test right before pushing. Will investigate.\n<issue_comment>username_2: Yea, this was intentionally separated, knowing that it is a slight performance loss for now. It is a part of an on-going refactor to separate instantiation and resolution from the render strategy. That way we can reuse core React logic while having the option to optimize for different render outputs (DOM, ART, React Native, etc). It is also easier to reason about fragments (components that render multiple elements) if we can diff the resolved set.\r\n\r\nThe idea is to move the instantiation out of the DOM component.\r\n\r\nIt is likely we can remerge this kind of optimization later as more of this refactor starts coming together but I think I'd rather keep it separate for now so that we can incrementally continue the refactor instead of having to make a complete reimplementation from scratch.\r\n\r\nThis brings up an interesting point though. For renderToString, we don't really need to keep any of the internal state tree which is only used for optimizing updates. Maybe we need a whole optimized pipeline that doesn't generate the internal memoization nor the state tree?\n<issue_comment>username_0: :+1: totally agree @username_2. I think that would also help with some of the GC issues we're seeing with higher throughput in node. If you can point me in the right direction for where to start with this, I'd be more than glad to work on it.\n<issue_comment>username_0: Looks like there's still a significant gain if we inline only `ReactChildReconciler.instantiateChildren`:\r\n\r\nChanges: https://github.com/facebook/react/compare/master...username_0:inlineInstantiateChildren\r\nTest: https://github.com/username_0/react-perf/tree/inlineInstantiateChildren\r\nResult: https://travis-ci.org/username_0/react-perf/builds/59784288\n<issue_comment>username_1: Nice find. Maybe we can take advantage of ReactChildren.map here? We may want a version which skips the ReactFragment.create call for these internals but otherwise I think it should give the right effect.\n<issue_comment>username_0: I pulled the traversal function out into a named functioned rather than inline in the branch above. I will run a comparison of using `ReactChildren.map` vs `traverseAllChildren` directly.\n<issue_comment>username_0: As you said I'm running into a lot of error logs due to `ReactFragment.create` being called with nulls and other invalid arguments. I think sticking with `traverseAllChildren` in this case will be better.\n<issue_comment>username_1: Sorry, I may have been a little unclear. Can you do something like this\r\n\r\n```\r\ndiff --git a/src/utils/ReactChildren.js b/src/utils/ReactChildren.js\r\nindex e470609..84ca357 100644\r\n--- a/src/utils/ReactChildren.js\r\n+++ b/src/utils/ReactChildren.js\r\n@@ -122,7 +122,7 @@ function mapChildren(children, func, context) {\r\n   var traverseContext = MapBookKeeping.getPooled(mapResult, func, context);\r\n   traverseAllChildren(children, mapSingleChildIntoContext, traverseContext);\r\n   MapBookKeeping.release(traverseContext);\r\n-  return ReactFragment.create(mapResult);\r\n+  return mapResult;\r\n }\r\n \r\n function forEachSingleChildDummy(traverseContext, child, name, i) {\r\n@@ -142,7 +142,10 @@ function countChildren(children, context) {\r\n \r\n var ReactChildren = {\r\n   forEach: forEachChildren,\r\n-  map: mapChildren,\r\n+  map: function(children, func, context) {\r\n+    return ReactFragment.create(mapChildren(children, func, context));\r\n+  },\r\n+  mapObject: mapChildren,\r\n   count: countChildren\r\n };\r\n \r\n```\r\n\r\nand then use ReactChildren.mapObject?\n<issue_comment>username_0: Ok, so I create a branch that uses ReactChildren.mapObject: https://github.com/facebook/react/compare/master...username_0:inlineInstantiateChildrenWithMap\r\n\r\nThere are two tests that fail using this method:\r\n\r\n * core/__tests__/ReactMultiChildReconcile-test ReactMultiChildReconcile should insert non-empty children in middle where nulls were.\r\n * core/__tests__/ReactMultiChildText-test ReactMultiChildText should correctly handle all possible children for render and update.\r\n\r\nI believe this is because map doesn't handle skipping nulls.\r\n\r\nI also ran a benchmark for using `traverseAllChilden` as my per previous comment vs. using `ReactChildren.mapObject` as @username_1 suggested. Running locally, I'm seeing a significant overhead in using the pooling added by `ReactChildren.mapObject`. On some runs it shows to be less performant than not inlining. You can see the travis-ci run of the benchmark here: https://travis-ci.org/username_0/react-perf/builds/60283471\n<issue_comment>username_0: @username_2 Can you elaborate one what you mean by \"internal state tree\"? I'm trying to understand what could be removed from the rendering path to optimize server rendering. I have a feeling some of the `mountComponent` logic needs to be split up into rendering logic and other logic (event registration, state management, etc.), but I'm not familiar enough with the \"other\" logic that's happening to know what to look at.\n<issue_comment>username_1: @username_0 Interesting. If inlining a closure (like https://gist.github.com/username_1/594910785229dba07d5c) is faster we should probably switch to that \u2013 we've tried to avoid closures in favor of manually passing context objects in hopes of lowering GC pressure and improving perf but all of this feels like dogscience.\r\n\r\n![image](https://cloud.githubusercontent.com/assets/6820/7360334/68f64cd2-ecfe-11e4-8e6e-b85703c9a2de.png)\n<issue_comment>username_1: Sorry for all the back and forth.\n<issue_comment>username_0: No worries. I too often feel like I have no idea what I'm doing.\r\n\r\nI think using `traverseAllChildren` directly as in https://github.com/facebook/react/compare/master...username_0:inlineInstantiateChildren addresses your concern about GC although it does duplicate some of the code in `ReactChildren.map`. We do need [this check](https://github.com/facebook/react/compare/master...username_0:inlineInstantiateChildren#diff-89f33512519fa4d853463913eac8ef04R24) though in order for the tests to pass. Any ideas on introducing that check in to `ReactChildren.map`?\n<issue_comment>username_1: I tried to replicate these results locally and in different browsers. Here's what I did:\r\n\r\n```sh\r\n$ git checkout origin/inlineMountChildren\r\n$ npm install -g browserify\r\n$ npm install babelify envify\r\n$ NODE_ENV=production browserify -t babelify bench-chat.js >bundle.js\r\n```\r\n\r\nAnd I created bench.html:\r\n\r\n```html\r\n<meta charset=\"utf-8\">\r\n<body style=\"font: 10px Menlo; white-space: pre;\">\r\n<div id=\"container\"></div>\r\n<script>\r\nconsole.log = function(x) {\r\n  container.textContent += x.replace(/\\033\\[[0-9;]*m/g, '') + '\\n';\r\n};\r\n</script>\r\n<script src=\"bundle.js\"></script>\r\n```\r\n\r\nOpening bench.html in Safari (JavaScriptCore) gave me:\r\n\r\n```\r\nRenderChat for inputs Optimized x 2,094 ops/sec \u00b10.85% (66 runs sampled)\r\nRenderChat for inputs Stock x 2,081 ops/sec \u00b10.78% (66 runs sampled)\r\nThe Fastest test suite is RenderChat for inputs Optimized,RenderChat for inputs Stock\r\n\r\n+------------+---------------+---------------+\r\n|            \u00e2\u201d\u201a Optimized     \u00e2\u201d\u201a Stock         |\r\n+------------+---------------+---------------+\r\n| RenderChat \u00e2\u201d\u201a 2,094 ops/sec \u00e2\u201d\u201a 2,081 ops/sec |\r\n+------------+---------------+---------------+\r\n```\r\n\r\nwhere they're the same speed (not sure what's up with the encoding there), and in Chrome I got:\r\n\r\n```\r\nRenderChat for inputs Optimized x 1,534 ops/sec \u00b12.38% (61 runs sampled)\r\nRenderChat for inputs Stock x 1,775 ops/sec \u00b11.56% (65 runs sampled)\r\nThe Fastest test suite is RenderChat for inputs Stock\r\n\r\n+------------+---------------+---------------+\r\n|            \u2502 Optimized     \u2502 Stock         |\r\n+------------+---------------+---------------+\r\n| RenderChat \u2502 1,534 ops/sec \u2502 1,775 ops/sec |\r\n+------------+---------------+---------------+\r\n```\r\n\r\nwhere the old version is 15% faster. Running `node bench-chat.js` gave me\r\n\r\n```\r\nRenderChat for inputs Optimized x 368 ops/sec \u00b13.18% (73 runs sampled)\r\nRenderChat for inputs Stock x 348 ops/sec \u00b12.99% (72 runs sampled)\r\nThe Fastest test suite is RenderChat for inputs Optimized\r\n\r\n+------------+-------------+-------------+\r\n|            \u2502 Optimized   \u2502 Stock       |\r\n+------------+-------------+-------------+\r\n| RenderChat \u2502 368 ops/sec \u2502 348 ops/sec |\r\n+------------+-------------+-------------+\r\n```\r\n\r\nwhere your version is maybe 5% faster, but not the 10\u201315% you were seeing originally. Any ideas if I'm doing something wrong or if this is just wildly different between engines? I'd have expected Chrome and Node to be more similar, at the very least.\n<issue_comment>username_0: Are the symlinks working correctly for you? browserify does not work for me locally to create the bundle.js as you did. I'm not sure why your results are so different from my local tests and travis-ci.\n<issue_comment>username_0: It's also worth noting, that even with two versions of stock flux, the benchmark for the item that is tested first is usually ~5% lower than the second which is why I moved \"optimized\" as the first test case. Consistency seems to be a consistent problem =/\n<issue_comment>username_0: Here's travis with 10 runs of bench-chat: https://travis-ci.org/username_0/react-perf/builds/60620846\n<issue_comment>username_0: So it looks like I didn't have the node_modules for each of the apps pushed, so the components may have been using the wrong React. I have pushed the node_modules folder for each app and used symlinks to the react versions such that they are completely isolated. I'm hoping this will make the tests more consistent for you.\r\n\r\nhttps://github.com/username_0/react-perf/tree/inlineMountChildren\n<issue_comment>username_0: Closing this since it undoes a purposeful refactor. I will open a new PR to discuss inlining just `ReactChildReconciler.instantiateChildren`."}
{"repo": "facebook/react", "issue_id": 69988869, "issue_number": 3717, "timestamp": "2015-04-22 01:04:27+00:00", "text": "I seem to have broken the test right before pushing. Will investigate.", "context": "<issue_start><issue_comment>Title: [performance] Inline ReactMultiChild.mountChildren\nusername_0: I found that `ReactMultiChild.mountChildren` and the functions it calls were iterating over the children many times. This inlines those calls as it is a critical rendering path. Through testing a chat example I have found about a 10-15% performance gains through benchmark.js.\r\n\r\nTest: https://github.com/username_0/react-perf/tree/inlineMountChildren\r\nTest results: https://travis-ci.org/username_0/react-perf/builds/59480548\n<issue_comment>username_1: For reference, this separation was introduced intentionally in this PR:\r\n\r\nhttps://github.com/facebook/react/pull/2567\r\n\r\ncc @username_2\n<issue_comment>username_0: I seem to have broken the test right before pushing. Will investigate.\n<issue_comment>username_2: Yea, this was intentionally separated, knowing that it is a slight performance loss for now. It is a part of an on-going refactor to separate instantiation and resolution from the render strategy. That way we can reuse core React logic while having the option to optimize for different render outputs (DOM, ART, React Native, etc). It is also easier to reason about fragments (components that render multiple elements) if we can diff the resolved set.\r\n\r\nThe idea is to move the instantiation out of the DOM component.\r\n\r\nIt is likely we can remerge this kind of optimization later as more of this refactor starts coming together but I think I'd rather keep it separate for now so that we can incrementally continue the refactor instead of having to make a complete reimplementation from scratch.\r\n\r\nThis brings up an interesting point though. For renderToString, we don't really need to keep any of the internal state tree which is only used for optimizing updates. Maybe we need a whole optimized pipeline that doesn't generate the internal memoization nor the state tree?\n<issue_comment>username_0: :+1: totally agree @username_2. I think that would also help with some of the GC issues we're seeing with higher throughput in node. If you can point me in the right direction for where to start with this, I'd be more than glad to work on it.\n<issue_comment>username_0: Looks like there's still a significant gain if we inline only `ReactChildReconciler.instantiateChildren`:\r\n\r\nChanges: https://github.com/facebook/react/compare/master...username_0:inlineInstantiateChildren\r\nTest: https://github.com/username_0/react-perf/tree/inlineInstantiateChildren\r\nResult: https://travis-ci.org/username_0/react-perf/builds/59784288\n<issue_comment>username_1: Nice find. Maybe we can take advantage of ReactChildren.map here? We may want a version which skips the ReactFragment.create call for these internals but otherwise I think it should give the right effect.\n<issue_comment>username_0: I pulled the traversal function out into a named functioned rather than inline in the branch above. I will run a comparison of using `ReactChildren.map` vs `traverseAllChildren` directly.\n<issue_comment>username_0: As you said I'm running into a lot of error logs due to `ReactFragment.create` being called with nulls and other invalid arguments. I think sticking with `traverseAllChildren` in this case will be better.\n<issue_comment>username_1: Sorry, I may have been a little unclear. Can you do something like this\r\n\r\n```\r\ndiff --git a/src/utils/ReactChildren.js b/src/utils/ReactChildren.js\r\nindex e470609..84ca357 100644\r\n--- a/src/utils/ReactChildren.js\r\n+++ b/src/utils/ReactChildren.js\r\n@@ -122,7 +122,7 @@ function mapChildren(children, func, context) {\r\n   var traverseContext = MapBookKeeping.getPooled(mapResult, func, context);\r\n   traverseAllChildren(children, mapSingleChildIntoContext, traverseContext);\r\n   MapBookKeeping.release(traverseContext);\r\n-  return ReactFragment.create(mapResult);\r\n+  return mapResult;\r\n }\r\n \r\n function forEachSingleChildDummy(traverseContext, child, name, i) {\r\n@@ -142,7 +142,10 @@ function countChildren(children, context) {\r\n \r\n var ReactChildren = {\r\n   forEach: forEachChildren,\r\n-  map: mapChildren,\r\n+  map: function(children, func, context) {\r\n+    return ReactFragment.create(mapChildren(children, func, context));\r\n+  },\r\n+  mapObject: mapChildren,\r\n   count: countChildren\r\n };\r\n \r\n```\r\n\r\nand then use ReactChildren.mapObject?\n<issue_comment>username_0: Ok, so I create a branch that uses ReactChildren.mapObject: https://github.com/facebook/react/compare/master...username_0:inlineInstantiateChildrenWithMap\r\n\r\nThere are two tests that fail using this method:\r\n\r\n * core/__tests__/ReactMultiChildReconcile-test ReactMultiChildReconcile should insert non-empty children in middle where nulls were.\r\n * core/__tests__/ReactMultiChildText-test ReactMultiChildText should correctly handle all possible children for render and update.\r\n\r\nI believe this is because map doesn't handle skipping nulls.\r\n\r\nI also ran a benchmark for using `traverseAllChilden` as my per previous comment vs. using `ReactChildren.mapObject` as @username_1 suggested. Running locally, I'm seeing a significant overhead in using the pooling added by `ReactChildren.mapObject`. On some runs it shows to be less performant than not inlining. You can see the travis-ci run of the benchmark here: https://travis-ci.org/username_0/react-perf/builds/60283471\n<issue_comment>username_0: @username_2 Can you elaborate one what you mean by \"internal state tree\"? I'm trying to understand what could be removed from the rendering path to optimize server rendering. I have a feeling some of the `mountComponent` logic needs to be split up into rendering logic and other logic (event registration, state management, etc.), but I'm not familiar enough with the \"other\" logic that's happening to know what to look at.\n<issue_comment>username_1: @username_0 Interesting. If inlining a closure (like https://gist.github.com/username_1/594910785229dba07d5c) is faster we should probably switch to that \u2013 we've tried to avoid closures in favor of manually passing context objects in hopes of lowering GC pressure and improving perf but all of this feels like dogscience.\r\n\r\n![image](https://cloud.githubusercontent.com/assets/6820/7360334/68f64cd2-ecfe-11e4-8e6e-b85703c9a2de.png)\n<issue_comment>username_1: Sorry for all the back and forth.\n<issue_comment>username_0: No worries. I too often feel like I have no idea what I'm doing.\r\n\r\nI think using `traverseAllChildren` directly as in https://github.com/facebook/react/compare/master...username_0:inlineInstantiateChildren addresses your concern about GC although it does duplicate some of the code in `ReactChildren.map`. We do need [this check](https://github.com/facebook/react/compare/master...username_0:inlineInstantiateChildren#diff-89f33512519fa4d853463913eac8ef04R24) though in order for the tests to pass. Any ideas on introducing that check in to `ReactChildren.map`?\n<issue_comment>username_1: I tried to replicate these results locally and in different browsers. Here's what I did:\r\n\r\n```sh\r\n$ git checkout origin/inlineMountChildren\r\n$ npm install -g browserify\r\n$ npm install babelify envify\r\n$ NODE_ENV=production browserify -t babelify bench-chat.js >bundle.js\r\n```\r\n\r\nAnd I created bench.html:\r\n\r\n```html\r\n<meta charset=\"utf-8\">\r\n<body style=\"font: 10px Menlo; white-space: pre;\">\r\n<div id=\"container\"></div>\r\n<script>\r\nconsole.log = function(x) {\r\n  container.textContent += x.replace(/\\033\\[[0-9;]*m/g, '') + '\\n';\r\n};\r\n</script>\r\n<script src=\"bundle.js\"></script>\r\n```\r\n\r\nOpening bench.html in Safari (JavaScriptCore) gave me:\r\n\r\n```\r\nRenderChat for inputs Optimized x 2,094 ops/sec \u00b10.85% (66 runs sampled)\r\nRenderChat for inputs Stock x 2,081 ops/sec \u00b10.78% (66 runs sampled)\r\nThe Fastest test suite is RenderChat for inputs Optimized,RenderChat for inputs Stock\r\n\r\n+------------+---------------+---------------+\r\n|            \u00e2\u201d\u201a Optimized     \u00e2\u201d\u201a Stock         |\r\n+------------+---------------+---------------+\r\n| RenderChat \u00e2\u201d\u201a 2,094 ops/sec \u00e2\u201d\u201a 2,081 ops/sec |\r\n+------------+---------------+---------------+\r\n```\r\n\r\nwhere they're the same speed (not sure what's up with the encoding there), and in Chrome I got:\r\n\r\n```\r\nRenderChat for inputs Optimized x 1,534 ops/sec \u00b12.38% (61 runs sampled)\r\nRenderChat for inputs Stock x 1,775 ops/sec \u00b11.56% (65 runs sampled)\r\nThe Fastest test suite is RenderChat for inputs Stock\r\n\r\n+------------+---------------+---------------+\r\n|            \u2502 Optimized     \u2502 Stock         |\r\n+------------+---------------+---------------+\r\n| RenderChat \u2502 1,534 ops/sec \u2502 1,775 ops/sec |\r\n+------------+---------------+---------------+\r\n```\r\n\r\nwhere the old version is 15% faster. Running `node bench-chat.js` gave me\r\n\r\n```\r\nRenderChat for inputs Optimized x 368 ops/sec \u00b13.18% (73 runs sampled)\r\nRenderChat for inputs Stock x 348 ops/sec \u00b12.99% (72 runs sampled)\r\nThe Fastest test suite is RenderChat for inputs Optimized\r\n\r\n+------------+-------------+-------------+\r\n|            \u2502 Optimized   \u2502 Stock       |\r\n+------------+-------------+-------------+\r\n| RenderChat \u2502 368 ops/sec \u2502 348 ops/sec |\r\n+------------+-------------+-------------+\r\n```\r\n\r\nwhere your version is maybe 5% faster, but not the 10\u201315% you were seeing originally. Any ideas if I'm doing something wrong or if this is just wildly different between engines? I'd have expected Chrome and Node to be more similar, at the very least.\n<issue_comment>username_0: Are the symlinks working correctly for you? browserify does not work for me locally to create the bundle.js as you did. I'm not sure why your results are so different from my local tests and travis-ci.\n<issue_comment>username_0: It's also worth noting, that even with two versions of stock flux, the benchmark for the item that is tested first is usually ~5% lower than the second which is why I moved \"optimized\" as the first test case. Consistency seems to be a consistent problem =/\n<issue_comment>username_0: Here's travis with 10 runs of bench-chat: https://travis-ci.org/username_0/react-perf/builds/60620846\n<issue_comment>username_0: So it looks like I didn't have the node_modules for each of the apps pushed, so the components may have been using the wrong React. I have pushed the node_modules folder for each app and used symlinks to the react versions such that they are completely isolated. I'm hoping this will make the tests more consistent for you.\r\n\r\nhttps://github.com/username_0/react-perf/tree/inlineMountChildren\n<issue_comment>username_0: Closing this since it undoes a purposeful refactor. I will open a new PR to discuss inlining just `ReactChildReconciler.instantiateChildren`."}
{"repo": "facebook/react", "issue_id": 69988869, "issue_number": 3717, "timestamp": "2015-04-23 06:53:13+00:00", "text": "Yea, this was intentionally separated, knowing that it is a slight performance loss for now. It is a part of an on-going refactor to separate instantiation and resolution from the render strategy. That way we can reuse core React logic while having the option to optimize for different render outputs (DOM, ART, React Native, etc). It is also easier to reason about fragments (components that render multiple elements) if we can diff the resolved set.\r\n\r\nThe idea is to move the instantiation out of the DOM component.\r\n\r\nIt is likely we can remerge this kind of optimization later as more of this refactor starts coming together but I think I'd rather keep it separate for now so that we can incrementally continue the refactor instead of having to make a complete reimplementation from scratch.\r\n\r\nThis brings up an interesting point though. For renderToString, we don't really need to keep any of the internal state tree which is only used for optimizing updates. Maybe we need a whole optimized pipeline that doesn't generate the internal memoization nor the state tree?", "context": "<issue_start><issue_comment>Title: [performance] Inline ReactMultiChild.mountChildren\nusername_0: I found that `ReactMultiChild.mountChildren` and the functions it calls were iterating over the children many times. This inlines those calls as it is a critical rendering path. Through testing a chat example I have found about a 10-15% performance gains through benchmark.js.\r\n\r\nTest: https://github.com/username_0/react-perf/tree/inlineMountChildren\r\nTest results: https://travis-ci.org/username_0/react-perf/builds/59480548\n<issue_comment>username_1: For reference, this separation was introduced intentionally in this PR:\r\n\r\nhttps://github.com/facebook/react/pull/2567\r\n\r\ncc @username_2\n<issue_comment>username_0: I seem to have broken the test right before pushing. Will investigate.\n<issue_comment>username_2: Yea, this was intentionally separated, knowing that it is a slight performance loss for now. It is a part of an on-going refactor to separate instantiation and resolution from the render strategy. That way we can reuse core React logic while having the option to optimize for different render outputs (DOM, ART, React Native, etc). It is also easier to reason about fragments (components that render multiple elements) if we can diff the resolved set.\r\n\r\nThe idea is to move the instantiation out of the DOM component.\r\n\r\nIt is likely we can remerge this kind of optimization later as more of this refactor starts coming together but I think I'd rather keep it separate for now so that we can incrementally continue the refactor instead of having to make a complete reimplementation from scratch.\r\n\r\nThis brings up an interesting point though. For renderToString, we don't really need to keep any of the internal state tree which is only used for optimizing updates. Maybe we need a whole optimized pipeline that doesn't generate the internal memoization nor the state tree?\n<issue_comment>username_0: :+1: totally agree @username_2. I think that would also help with some of the GC issues we're seeing with higher throughput in node. If you can point me in the right direction for where to start with this, I'd be more than glad to work on it.\n<issue_comment>username_0: Looks like there's still a significant gain if we inline only `ReactChildReconciler.instantiateChildren`:\r\n\r\nChanges: https://github.com/facebook/react/compare/master...username_0:inlineInstantiateChildren\r\nTest: https://github.com/username_0/react-perf/tree/inlineInstantiateChildren\r\nResult: https://travis-ci.org/username_0/react-perf/builds/59784288\n<issue_comment>username_1: Nice find. Maybe we can take advantage of ReactChildren.map here? We may want a version which skips the ReactFragment.create call for these internals but otherwise I think it should give the right effect.\n<issue_comment>username_0: I pulled the traversal function out into a named functioned rather than inline in the branch above. I will run a comparison of using `ReactChildren.map` vs `traverseAllChildren` directly.\n<issue_comment>username_0: As you said I'm running into a lot of error logs due to `ReactFragment.create` being called with nulls and other invalid arguments. I think sticking with `traverseAllChildren` in this case will be better.\n<issue_comment>username_1: Sorry, I may have been a little unclear. Can you do something like this\r\n\r\n```\r\ndiff --git a/src/utils/ReactChildren.js b/src/utils/ReactChildren.js\r\nindex e470609..84ca357 100644\r\n--- a/src/utils/ReactChildren.js\r\n+++ b/src/utils/ReactChildren.js\r\n@@ -122,7 +122,7 @@ function mapChildren(children, func, context) {\r\n   var traverseContext = MapBookKeeping.getPooled(mapResult, func, context);\r\n   traverseAllChildren(children, mapSingleChildIntoContext, traverseContext);\r\n   MapBookKeeping.release(traverseContext);\r\n-  return ReactFragment.create(mapResult);\r\n+  return mapResult;\r\n }\r\n \r\n function forEachSingleChildDummy(traverseContext, child, name, i) {\r\n@@ -142,7 +142,10 @@ function countChildren(children, context) {\r\n \r\n var ReactChildren = {\r\n   forEach: forEachChildren,\r\n-  map: mapChildren,\r\n+  map: function(children, func, context) {\r\n+    return ReactFragment.create(mapChildren(children, func, context));\r\n+  },\r\n+  mapObject: mapChildren,\r\n   count: countChildren\r\n };\r\n \r\n```\r\n\r\nand then use ReactChildren.mapObject?\n<issue_comment>username_0: Ok, so I create a branch that uses ReactChildren.mapObject: https://github.com/facebook/react/compare/master...username_0:inlineInstantiateChildrenWithMap\r\n\r\nThere are two tests that fail using this method:\r\n\r\n * core/__tests__/ReactMultiChildReconcile-test ReactMultiChildReconcile should insert non-empty children in middle where nulls were.\r\n * core/__tests__/ReactMultiChildText-test ReactMultiChildText should correctly handle all possible children for render and update.\r\n\r\nI believe this is because map doesn't handle skipping nulls.\r\n\r\nI also ran a benchmark for using `traverseAllChilden` as my per previous comment vs. using `ReactChildren.mapObject` as @username_1 suggested. Running locally, I'm seeing a significant overhead in using the pooling added by `ReactChildren.mapObject`. On some runs it shows to be less performant than not inlining. You can see the travis-ci run of the benchmark here: https://travis-ci.org/username_0/react-perf/builds/60283471\n<issue_comment>username_0: @username_2 Can you elaborate one what you mean by \"internal state tree\"? I'm trying to understand what could be removed from the rendering path to optimize server rendering. I have a feeling some of the `mountComponent` logic needs to be split up into rendering logic and other logic (event registration, state management, etc.), but I'm not familiar enough with the \"other\" logic that's happening to know what to look at.\n<issue_comment>username_1: @username_0 Interesting. If inlining a closure (like https://gist.github.com/username_1/594910785229dba07d5c) is faster we should probably switch to that \u2013 we've tried to avoid closures in favor of manually passing context objects in hopes of lowering GC pressure and improving perf but all of this feels like dogscience.\r\n\r\n![image](https://cloud.githubusercontent.com/assets/6820/7360334/68f64cd2-ecfe-11e4-8e6e-b85703c9a2de.png)\n<issue_comment>username_1: Sorry for all the back and forth.\n<issue_comment>username_0: No worries. I too often feel like I have no idea what I'm doing.\r\n\r\nI think using `traverseAllChildren` directly as in https://github.com/facebook/react/compare/master...username_0:inlineInstantiateChildren addresses your concern about GC although it does duplicate some of the code in `ReactChildren.map`. We do need [this check](https://github.com/facebook/react/compare/master...username_0:inlineInstantiateChildren#diff-89f33512519fa4d853463913eac8ef04R24) though in order for the tests to pass. Any ideas on introducing that check in to `ReactChildren.map`?\n<issue_comment>username_1: I tried to replicate these results locally and in different browsers. Here's what I did:\r\n\r\n```sh\r\n$ git checkout origin/inlineMountChildren\r\n$ npm install -g browserify\r\n$ npm install babelify envify\r\n$ NODE_ENV=production browserify -t babelify bench-chat.js >bundle.js\r\n```\r\n\r\nAnd I created bench.html:\r\n\r\n```html\r\n<meta charset=\"utf-8\">\r\n<body style=\"font: 10px Menlo; white-space: pre;\">\r\n<div id=\"container\"></div>\r\n<script>\r\nconsole.log = function(x) {\r\n  container.textContent += x.replace(/\\033\\[[0-9;]*m/g, '') + '\\n';\r\n};\r\n</script>\r\n<script src=\"bundle.js\"></script>\r\n```\r\n\r\nOpening bench.html in Safari (JavaScriptCore) gave me:\r\n\r\n```\r\nRenderChat for inputs Optimized x 2,094 ops/sec \u00b10.85% (66 runs sampled)\r\nRenderChat for inputs Stock x 2,081 ops/sec \u00b10.78% (66 runs sampled)\r\nThe Fastest test suite is RenderChat for inputs Optimized,RenderChat for inputs Stock\r\n\r\n+------------+---------------+---------------+\r\n|            \u00e2\u201d\u201a Optimized     \u00e2\u201d\u201a Stock         |\r\n+------------+---------------+---------------+\r\n| RenderChat \u00e2\u201d\u201a 2,094 ops/sec \u00e2\u201d\u201a 2,081 ops/sec |\r\n+------------+---------------+---------------+\r\n```\r\n\r\nwhere they're the same speed (not sure what's up with the encoding there), and in Chrome I got:\r\n\r\n```\r\nRenderChat for inputs Optimized x 1,534 ops/sec \u00b12.38% (61 runs sampled)\r\nRenderChat for inputs Stock x 1,775 ops/sec \u00b11.56% (65 runs sampled)\r\nThe Fastest test suite is RenderChat for inputs Stock\r\n\r\n+------------+---------------+---------------+\r\n|            \u2502 Optimized     \u2502 Stock         |\r\n+------------+---------------+---------------+\r\n| RenderChat \u2502 1,534 ops/sec \u2502 1,775 ops/sec |\r\n+------------+---------------+---------------+\r\n```\r\n\r\nwhere the old version is 15% faster. Running `node bench-chat.js` gave me\r\n\r\n```\r\nRenderChat for inputs Optimized x 368 ops/sec \u00b13.18% (73 runs sampled)\r\nRenderChat for inputs Stock x 348 ops/sec \u00b12.99% (72 runs sampled)\r\nThe Fastest test suite is RenderChat for inputs Optimized\r\n\r\n+------------+-------------+-------------+\r\n|            \u2502 Optimized   \u2502 Stock       |\r\n+------------+-------------+-------------+\r\n| RenderChat \u2502 368 ops/sec \u2502 348 ops/sec |\r\n+------------+-------------+-------------+\r\n```\r\n\r\nwhere your version is maybe 5% faster, but not the 10\u201315% you were seeing originally. Any ideas if I'm doing something wrong or if this is just wildly different between engines? I'd have expected Chrome and Node to be more similar, at the very least.\n<issue_comment>username_0: Are the symlinks working correctly for you? browserify does not work for me locally to create the bundle.js as you did. I'm not sure why your results are so different from my local tests and travis-ci.\n<issue_comment>username_0: It's also worth noting, that even with two versions of stock flux, the benchmark for the item that is tested first is usually ~5% lower than the second which is why I moved \"optimized\" as the first test case. Consistency seems to be a consistent problem =/\n<issue_comment>username_0: Here's travis with 10 runs of bench-chat: https://travis-ci.org/username_0/react-perf/builds/60620846\n<issue_comment>username_0: So it looks like I didn't have the node_modules for each of the apps pushed, so the components may have been using the wrong React. I have pushed the node_modules folder for each app and used symlinks to the react versions such that they are completely isolated. I'm hoping this will make the tests more consistent for you.\r\n\r\nhttps://github.com/username_0/react-perf/tree/inlineMountChildren\n<issue_comment>username_0: Closing this since it undoes a purposeful refactor. I will open a new PR to discuss inlining just `ReactChildReconciler.instantiateChildren`."}
{"repo": "facebook/react", "issue_id": 69988869, "issue_number": 3717, "timestamp": "2015-04-23 07:26:02+00:00", "text": ":+1: totally agree @username_2. I think that would also help with some of the GC issues we're seeing with higher throughput in node. If you can point me in the right direction for where to start with this, I'd be more than glad to work on it.", "context": "<issue_start><issue_comment>Title: [performance] Inline ReactMultiChild.mountChildren\nusername_0: I found that `ReactMultiChild.mountChildren` and the functions it calls were iterating over the children many times. This inlines those calls as it is a critical rendering path. Through testing a chat example I have found about a 10-15% performance gains through benchmark.js.\r\n\r\nTest: https://github.com/username_0/react-perf/tree/inlineMountChildren\r\nTest results: https://travis-ci.org/username_0/react-perf/builds/59480548\n<issue_comment>username_1: For reference, this separation was introduced intentionally in this PR:\r\n\r\nhttps://github.com/facebook/react/pull/2567\r\n\r\ncc @username_2\n<issue_comment>username_0: I seem to have broken the test right before pushing. Will investigate.\n<issue_comment>username_2: Yea, this was intentionally separated, knowing that it is a slight performance loss for now. It is a part of an on-going refactor to separate instantiation and resolution from the render strategy. That way we can reuse core React logic while having the option to optimize for different render outputs (DOM, ART, React Native, etc). It is also easier to reason about fragments (components that render multiple elements) if we can diff the resolved set.\r\n\r\nThe idea is to move the instantiation out of the DOM component.\r\n\r\nIt is likely we can remerge this kind of optimization later as more of this refactor starts coming together but I think I'd rather keep it separate for now so that we can incrementally continue the refactor instead of having to make a complete reimplementation from scratch.\r\n\r\nThis brings up an interesting point though. For renderToString, we don't really need to keep any of the internal state tree which is only used for optimizing updates. Maybe we need a whole optimized pipeline that doesn't generate the internal memoization nor the state tree?\n<issue_comment>username_0: :+1: totally agree @username_2. I think that would also help with some of the GC issues we're seeing with higher throughput in node. If you can point me in the right direction for where to start with this, I'd be more than glad to work on it.\n<issue_comment>username_0: Looks like there's still a significant gain if we inline only `ReactChildReconciler.instantiateChildren`:\r\n\r\nChanges: https://github.com/facebook/react/compare/master...username_0:inlineInstantiateChildren\r\nTest: https://github.com/username_0/react-perf/tree/inlineInstantiateChildren\r\nResult: https://travis-ci.org/username_0/react-perf/builds/59784288\n<issue_comment>username_1: Nice find. Maybe we can take advantage of ReactChildren.map here? We may want a version which skips the ReactFragment.create call for these internals but otherwise I think it should give the right effect.\n<issue_comment>username_0: I pulled the traversal function out into a named functioned rather than inline in the branch above. I will run a comparison of using `ReactChildren.map` vs `traverseAllChildren` directly.\n<issue_comment>username_0: As you said I'm running into a lot of error logs due to `ReactFragment.create` being called with nulls and other invalid arguments. I think sticking with `traverseAllChildren` in this case will be better.\n<issue_comment>username_1: Sorry, I may have been a little unclear. Can you do something like this\r\n\r\n```\r\ndiff --git a/src/utils/ReactChildren.js b/src/utils/ReactChildren.js\r\nindex e470609..84ca357 100644\r\n--- a/src/utils/ReactChildren.js\r\n+++ b/src/utils/ReactChildren.js\r\n@@ -122,7 +122,7 @@ function mapChildren(children, func, context) {\r\n   var traverseContext = MapBookKeeping.getPooled(mapResult, func, context);\r\n   traverseAllChildren(children, mapSingleChildIntoContext, traverseContext);\r\n   MapBookKeeping.release(traverseContext);\r\n-  return ReactFragment.create(mapResult);\r\n+  return mapResult;\r\n }\r\n \r\n function forEachSingleChildDummy(traverseContext, child, name, i) {\r\n@@ -142,7 +142,10 @@ function countChildren(children, context) {\r\n \r\n var ReactChildren = {\r\n   forEach: forEachChildren,\r\n-  map: mapChildren,\r\n+  map: function(children, func, context) {\r\n+    return ReactFragment.create(mapChildren(children, func, context));\r\n+  },\r\n+  mapObject: mapChildren,\r\n   count: countChildren\r\n };\r\n \r\n```\r\n\r\nand then use ReactChildren.mapObject?\n<issue_comment>username_0: Ok, so I create a branch that uses ReactChildren.mapObject: https://github.com/facebook/react/compare/master...username_0:inlineInstantiateChildrenWithMap\r\n\r\nThere are two tests that fail using this method:\r\n\r\n * core/__tests__/ReactMultiChildReconcile-test ReactMultiChildReconcile should insert non-empty children in middle where nulls were.\r\n * core/__tests__/ReactMultiChildText-test ReactMultiChildText should correctly handle all possible children for render and update.\r\n\r\nI believe this is because map doesn't handle skipping nulls.\r\n\r\nI also ran a benchmark for using `traverseAllChilden` as my per previous comment vs. using `ReactChildren.mapObject` as @username_1 suggested. Running locally, I'm seeing a significant overhead in using the pooling added by `ReactChildren.mapObject`. On some runs it shows to be less performant than not inlining. You can see the travis-ci run of the benchmark here: https://travis-ci.org/username_0/react-perf/builds/60283471\n<issue_comment>username_0: @username_2 Can you elaborate one what you mean by \"internal state tree\"? I'm trying to understand what could be removed from the rendering path to optimize server rendering. I have a feeling some of the `mountComponent` logic needs to be split up into rendering logic and other logic (event registration, state management, etc.), but I'm not familiar enough with the \"other\" logic that's happening to know what to look at.\n<issue_comment>username_1: @username_0 Interesting. If inlining a closure (like https://gist.github.com/username_1/594910785229dba07d5c) is faster we should probably switch to that \u2013 we've tried to avoid closures in favor of manually passing context objects in hopes of lowering GC pressure and improving perf but all of this feels like dogscience.\r\n\r\n![image](https://cloud.githubusercontent.com/assets/6820/7360334/68f64cd2-ecfe-11e4-8e6e-b85703c9a2de.png)\n<issue_comment>username_1: Sorry for all the back and forth.\n<issue_comment>username_0: No worries. I too often feel like I have no idea what I'm doing.\r\n\r\nI think using `traverseAllChildren` directly as in https://github.com/facebook/react/compare/master...username_0:inlineInstantiateChildren addresses your concern about GC although it does duplicate some of the code in `ReactChildren.map`. We do need [this check](https://github.com/facebook/react/compare/master...username_0:inlineInstantiateChildren#diff-89f33512519fa4d853463913eac8ef04R24) though in order for the tests to pass. Any ideas on introducing that check in to `ReactChildren.map`?\n<issue_comment>username_1: I tried to replicate these results locally and in different browsers. Here's what I did:\r\n\r\n```sh\r\n$ git checkout origin/inlineMountChildren\r\n$ npm install -g browserify\r\n$ npm install babelify envify\r\n$ NODE_ENV=production browserify -t babelify bench-chat.js >bundle.js\r\n```\r\n\r\nAnd I created bench.html:\r\n\r\n```html\r\n<meta charset=\"utf-8\">\r\n<body style=\"font: 10px Menlo; white-space: pre;\">\r\n<div id=\"container\"></div>\r\n<script>\r\nconsole.log = function(x) {\r\n  container.textContent += x.replace(/\\033\\[[0-9;]*m/g, '') + '\\n';\r\n};\r\n</script>\r\n<script src=\"bundle.js\"></script>\r\n```\r\n\r\nOpening bench.html in Safari (JavaScriptCore) gave me:\r\n\r\n```\r\nRenderChat for inputs Optimized x 2,094 ops/sec \u00b10.85% (66 runs sampled)\r\nRenderChat for inputs Stock x 2,081 ops/sec \u00b10.78% (66 runs sampled)\r\nThe Fastest test suite is RenderChat for inputs Optimized,RenderChat for inputs Stock\r\n\r\n+------------+---------------+---------------+\r\n|            \u00e2\u201d\u201a Optimized     \u00e2\u201d\u201a Stock         |\r\n+------------+---------------+---------------+\r\n| RenderChat \u00e2\u201d\u201a 2,094 ops/sec \u00e2\u201d\u201a 2,081 ops/sec |\r\n+------------+---------------+---------------+\r\n```\r\n\r\nwhere they're the same speed (not sure what's up with the encoding there), and in Chrome I got:\r\n\r\n```\r\nRenderChat for inputs Optimized x 1,534 ops/sec \u00b12.38% (61 runs sampled)\r\nRenderChat for inputs Stock x 1,775 ops/sec \u00b11.56% (65 runs sampled)\r\nThe Fastest test suite is RenderChat for inputs Stock\r\n\r\n+------------+---------------+---------------+\r\n|            \u2502 Optimized     \u2502 Stock         |\r\n+------------+---------------+---------------+\r\n| RenderChat \u2502 1,534 ops/sec \u2502 1,775 ops/sec |\r\n+------------+---------------+---------------+\r\n```\r\n\r\nwhere the old version is 15% faster. Running `node bench-chat.js` gave me\r\n\r\n```\r\nRenderChat for inputs Optimized x 368 ops/sec \u00b13.18% (73 runs sampled)\r\nRenderChat for inputs Stock x 348 ops/sec \u00b12.99% (72 runs sampled)\r\nThe Fastest test suite is RenderChat for inputs Optimized\r\n\r\n+------------+-------------+-------------+\r\n|            \u2502 Optimized   \u2502 Stock       |\r\n+------------+-------------+-------------+\r\n| RenderChat \u2502 368 ops/sec \u2502 348 ops/sec |\r\n+------------+-------------+-------------+\r\n```\r\n\r\nwhere your version is maybe 5% faster, but not the 10\u201315% you were seeing originally. Any ideas if I'm doing something wrong or if this is just wildly different between engines? I'd have expected Chrome and Node to be more similar, at the very least.\n<issue_comment>username_0: Are the symlinks working correctly for you? browserify does not work for me locally to create the bundle.js as you did. I'm not sure why your results are so different from my local tests and travis-ci.\n<issue_comment>username_0: It's also worth noting, that even with two versions of stock flux, the benchmark for the item that is tested first is usually ~5% lower than the second which is why I moved \"optimized\" as the first test case. Consistency seems to be a consistent problem =/\n<issue_comment>username_0: Here's travis with 10 runs of bench-chat: https://travis-ci.org/username_0/react-perf/builds/60620846\n<issue_comment>username_0: So it looks like I didn't have the node_modules for each of the apps pushed, so the components may have been using the wrong React. I have pushed the node_modules folder for each app and used symlinks to the react versions such that they are completely isolated. I'm hoping this will make the tests more consistent for you.\r\n\r\nhttps://github.com/username_0/react-perf/tree/inlineMountChildren\n<issue_comment>username_0: Closing this since it undoes a purposeful refactor. I will open a new PR to discuss inlining just `ReactChildReconciler.instantiateChildren`."}
{"repo": "facebook/react", "issue_id": 69988869, "issue_number": 3717, "timestamp": "2015-04-23 20:44:22+00:00", "text": "Looks like there's still a significant gain if we inline only `ReactChildReconciler.instantiateChildren`:\r\n\r\nChanges: https://github.com/facebook/react/compare/master...username_0:inlineInstantiateChildren\r\nTest: https://github.com/username_0/react-perf/tree/inlineInstantiateChildren\r\nResult: https://travis-ci.org/username_0/react-perf/builds/59784288", "context": "<issue_start><issue_comment>Title: [performance] Inline ReactMultiChild.mountChildren\nusername_0: I found that `ReactMultiChild.mountChildren` and the functions it calls were iterating over the children many times. This inlines those calls as it is a critical rendering path. Through testing a chat example I have found about a 10-15% performance gains through benchmark.js.\r\n\r\nTest: https://github.com/username_0/react-perf/tree/inlineMountChildren\r\nTest results: https://travis-ci.org/username_0/react-perf/builds/59480548\n<issue_comment>username_1: For reference, this separation was introduced intentionally in this PR:\r\n\r\nhttps://github.com/facebook/react/pull/2567\r\n\r\ncc @username_2\n<issue_comment>username_0: I seem to have broken the test right before pushing. Will investigate.\n<issue_comment>username_2: Yea, this was intentionally separated, knowing that it is a slight performance loss for now. It is a part of an on-going refactor to separate instantiation and resolution from the render strategy. That way we can reuse core React logic while having the option to optimize for different render outputs (DOM, ART, React Native, etc). It is also easier to reason about fragments (components that render multiple elements) if we can diff the resolved set.\r\n\r\nThe idea is to move the instantiation out of the DOM component.\r\n\r\nIt is likely we can remerge this kind of optimization later as more of this refactor starts coming together but I think I'd rather keep it separate for now so that we can incrementally continue the refactor instead of having to make a complete reimplementation from scratch.\r\n\r\nThis brings up an interesting point though. For renderToString, we don't really need to keep any of the internal state tree which is only used for optimizing updates. Maybe we need a whole optimized pipeline that doesn't generate the internal memoization nor the state tree?\n<issue_comment>username_0: :+1: totally agree @username_2. I think that would also help with some of the GC issues we're seeing with higher throughput in node. If you can point me in the right direction for where to start with this, I'd be more than glad to work on it.\n<issue_comment>username_0: Looks like there's still a significant gain if we inline only `ReactChildReconciler.instantiateChildren`:\r\n\r\nChanges: https://github.com/facebook/react/compare/master...username_0:inlineInstantiateChildren\r\nTest: https://github.com/username_0/react-perf/tree/inlineInstantiateChildren\r\nResult: https://travis-ci.org/username_0/react-perf/builds/59784288\n<issue_comment>username_1: Nice find. Maybe we can take advantage of ReactChildren.map here? We may want a version which skips the ReactFragment.create call for these internals but otherwise I think it should give the right effect.\n<issue_comment>username_0: I pulled the traversal function out into a named functioned rather than inline in the branch above. I will run a comparison of using `ReactChildren.map` vs `traverseAllChildren` directly.\n<issue_comment>username_0: As you said I'm running into a lot of error logs due to `ReactFragment.create` being called with nulls and other invalid arguments. I think sticking with `traverseAllChildren` in this case will be better.\n<issue_comment>username_1: Sorry, I may have been a little unclear. Can you do something like this\r\n\r\n```\r\ndiff --git a/src/utils/ReactChildren.js b/src/utils/ReactChildren.js\r\nindex e470609..84ca357 100644\r\n--- a/src/utils/ReactChildren.js\r\n+++ b/src/utils/ReactChildren.js\r\n@@ -122,7 +122,7 @@ function mapChildren(children, func, context) {\r\n   var traverseContext = MapBookKeeping.getPooled(mapResult, func, context);\r\n   traverseAllChildren(children, mapSingleChildIntoContext, traverseContext);\r\n   MapBookKeeping.release(traverseContext);\r\n-  return ReactFragment.create(mapResult);\r\n+  return mapResult;\r\n }\r\n \r\n function forEachSingleChildDummy(traverseContext, child, name, i) {\r\n@@ -142,7 +142,10 @@ function countChildren(children, context) {\r\n \r\n var ReactChildren = {\r\n   forEach: forEachChildren,\r\n-  map: mapChildren,\r\n+  map: function(children, func, context) {\r\n+    return ReactFragment.create(mapChildren(children, func, context));\r\n+  },\r\n+  mapObject: mapChildren,\r\n   count: countChildren\r\n };\r\n \r\n```\r\n\r\nand then use ReactChildren.mapObject?\n<issue_comment>username_0: Ok, so I create a branch that uses ReactChildren.mapObject: https://github.com/facebook/react/compare/master...username_0:inlineInstantiateChildrenWithMap\r\n\r\nThere are two tests that fail using this method:\r\n\r\n * core/__tests__/ReactMultiChildReconcile-test ReactMultiChildReconcile should insert non-empty children in middle where nulls were.\r\n * core/__tests__/ReactMultiChildText-test ReactMultiChildText should correctly handle all possible children for render and update.\r\n\r\nI believe this is because map doesn't handle skipping nulls.\r\n\r\nI also ran a benchmark for using `traverseAllChilden` as my per previous comment vs. using `ReactChildren.mapObject` as @username_1 suggested. Running locally, I'm seeing a significant overhead in using the pooling added by `ReactChildren.mapObject`. On some runs it shows to be less performant than not inlining. You can see the travis-ci run of the benchmark here: https://travis-ci.org/username_0/react-perf/builds/60283471\n<issue_comment>username_0: @username_2 Can you elaborate one what you mean by \"internal state tree\"? I'm trying to understand what could be removed from the rendering path to optimize server rendering. I have a feeling some of the `mountComponent` logic needs to be split up into rendering logic and other logic (event registration, state management, etc.), but I'm not familiar enough with the \"other\" logic that's happening to know what to look at.\n<issue_comment>username_1: @username_0 Interesting. If inlining a closure (like https://gist.github.com/username_1/594910785229dba07d5c) is faster we should probably switch to that \u2013 we've tried to avoid closures in favor of manually passing context objects in hopes of lowering GC pressure and improving perf but all of this feels like dogscience.\r\n\r\n![image](https://cloud.githubusercontent.com/assets/6820/7360334/68f64cd2-ecfe-11e4-8e6e-b85703c9a2de.png)\n<issue_comment>username_1: Sorry for all the back and forth.\n<issue_comment>username_0: No worries. I too often feel like I have no idea what I'm doing.\r\n\r\nI think using `traverseAllChildren` directly as in https://github.com/facebook/react/compare/master...username_0:inlineInstantiateChildren addresses your concern about GC although it does duplicate some of the code in `ReactChildren.map`. We do need [this check](https://github.com/facebook/react/compare/master...username_0:inlineInstantiateChildren#diff-89f33512519fa4d853463913eac8ef04R24) though in order for the tests to pass. Any ideas on introducing that check in to `ReactChildren.map`?\n<issue_comment>username_1: I tried to replicate these results locally and in different browsers. Here's what I did:\r\n\r\n```sh\r\n$ git checkout origin/inlineMountChildren\r\n$ npm install -g browserify\r\n$ npm install babelify envify\r\n$ NODE_ENV=production browserify -t babelify bench-chat.js >bundle.js\r\n```\r\n\r\nAnd I created bench.html:\r\n\r\n```html\r\n<meta charset=\"utf-8\">\r\n<body style=\"font: 10px Menlo; white-space: pre;\">\r\n<div id=\"container\"></div>\r\n<script>\r\nconsole.log = function(x) {\r\n  container.textContent += x.replace(/\\033\\[[0-9;]*m/g, '') + '\\n';\r\n};\r\n</script>\r\n<script src=\"bundle.js\"></script>\r\n```\r\n\r\nOpening bench.html in Safari (JavaScriptCore) gave me:\r\n\r\n```\r\nRenderChat for inputs Optimized x 2,094 ops/sec \u00b10.85% (66 runs sampled)\r\nRenderChat for inputs Stock x 2,081 ops/sec \u00b10.78% (66 runs sampled)\r\nThe Fastest test suite is RenderChat for inputs Optimized,RenderChat for inputs Stock\r\n\r\n+------------+---------------+---------------+\r\n|            \u00e2\u201d\u201a Optimized     \u00e2\u201d\u201a Stock         |\r\n+------------+---------------+---------------+\r\n| RenderChat \u00e2\u201d\u201a 2,094 ops/sec \u00e2\u201d\u201a 2,081 ops/sec |\r\n+------------+---------------+---------------+\r\n```\r\n\r\nwhere they're the same speed (not sure what's up with the encoding there), and in Chrome I got:\r\n\r\n```\r\nRenderChat for inputs Optimized x 1,534 ops/sec \u00b12.38% (61 runs sampled)\r\nRenderChat for inputs Stock x 1,775 ops/sec \u00b11.56% (65 runs sampled)\r\nThe Fastest test suite is RenderChat for inputs Stock\r\n\r\n+------------+---------------+---------------+\r\n|            \u2502 Optimized     \u2502 Stock         |\r\n+------------+---------------+---------------+\r\n| RenderChat \u2502 1,534 ops/sec \u2502 1,775 ops/sec |\r\n+------------+---------------+---------------+\r\n```\r\n\r\nwhere the old version is 15% faster. Running `node bench-chat.js` gave me\r\n\r\n```\r\nRenderChat for inputs Optimized x 368 ops/sec \u00b13.18% (73 runs sampled)\r\nRenderChat for inputs Stock x 348 ops/sec \u00b12.99% (72 runs sampled)\r\nThe Fastest test suite is RenderChat for inputs Optimized\r\n\r\n+------------+-------------+-------------+\r\n|            \u2502 Optimized   \u2502 Stock       |\r\n+------------+-------------+-------------+\r\n| RenderChat \u2502 368 ops/sec \u2502 348 ops/sec |\r\n+------------+-------------+-------------+\r\n```\r\n\r\nwhere your version is maybe 5% faster, but not the 10\u201315% you were seeing originally. Any ideas if I'm doing something wrong or if this is just wildly different between engines? I'd have expected Chrome and Node to be more similar, at the very least.\n<issue_comment>username_0: Are the symlinks working correctly for you? browserify does not work for me locally to create the bundle.js as you did. I'm not sure why your results are so different from my local tests and travis-ci.\n<issue_comment>username_0: It's also worth noting, that even with two versions of stock flux, the benchmark for the item that is tested first is usually ~5% lower than the second which is why I moved \"optimized\" as the first test case. Consistency seems to be a consistent problem =/\n<issue_comment>username_0: Here's travis with 10 runs of bench-chat: https://travis-ci.org/username_0/react-perf/builds/60620846\n<issue_comment>username_0: So it looks like I didn't have the node_modules for each of the apps pushed, so the components may have been using the wrong React. I have pushed the node_modules folder for each app and used symlinks to the react versions such that they are completely isolated. I'm hoping this will make the tests more consistent for you.\r\n\r\nhttps://github.com/username_0/react-perf/tree/inlineMountChildren\n<issue_comment>username_0: Closing this since it undoes a purposeful refactor. I will open a new PR to discuss inlining just `ReactChildReconciler.instantiateChildren`."}
{"repo": "facebook/react", "issue_id": 69988869, "issue_number": 3717, "timestamp": "2015-04-23 20:52:48+00:00", "text": "Nice find. Maybe we can take advantage of ReactChildren.map here? We may want a version which skips the ReactFragment.create call for these internals but otherwise I think it should give the right effect.", "context": "<issue_start><issue_comment>Title: [performance] Inline ReactMultiChild.mountChildren\nusername_0: I found that `ReactMultiChild.mountChildren` and the functions it calls were iterating over the children many times. This inlines those calls as it is a critical rendering path. Through testing a chat example I have found about a 10-15% performance gains through benchmark.js.\r\n\r\nTest: https://github.com/username_0/react-perf/tree/inlineMountChildren\r\nTest results: https://travis-ci.org/username_0/react-perf/builds/59480548\n<issue_comment>username_1: For reference, this separation was introduced intentionally in this PR:\r\n\r\nhttps://github.com/facebook/react/pull/2567\r\n\r\ncc @username_2\n<issue_comment>username_0: I seem to have broken the test right before pushing. Will investigate.\n<issue_comment>username_2: Yea, this was intentionally separated, knowing that it is a slight performance loss for now. It is a part of an on-going refactor to separate instantiation and resolution from the render strategy. That way we can reuse core React logic while having the option to optimize for different render outputs (DOM, ART, React Native, etc). It is also easier to reason about fragments (components that render multiple elements) if we can diff the resolved set.\r\n\r\nThe idea is to move the instantiation out of the DOM component.\r\n\r\nIt is likely we can remerge this kind of optimization later as more of this refactor starts coming together but I think I'd rather keep it separate for now so that we can incrementally continue the refactor instead of having to make a complete reimplementation from scratch.\r\n\r\nThis brings up an interesting point though. For renderToString, we don't really need to keep any of the internal state tree which is only used for optimizing updates. Maybe we need a whole optimized pipeline that doesn't generate the internal memoization nor the state tree?\n<issue_comment>username_0: :+1: totally agree @username_2. I think that would also help with some of the GC issues we're seeing with higher throughput in node. If you can point me in the right direction for where to start with this, I'd be more than glad to work on it.\n<issue_comment>username_0: Looks like there's still a significant gain if we inline only `ReactChildReconciler.instantiateChildren`:\r\n\r\nChanges: https://github.com/facebook/react/compare/master...username_0:inlineInstantiateChildren\r\nTest: https://github.com/username_0/react-perf/tree/inlineInstantiateChildren\r\nResult: https://travis-ci.org/username_0/react-perf/builds/59784288\n<issue_comment>username_1: Nice find. Maybe we can take advantage of ReactChildren.map here? We may want a version which skips the ReactFragment.create call for these internals but otherwise I think it should give the right effect.\n<issue_comment>username_0: I pulled the traversal function out into a named functioned rather than inline in the branch above. I will run a comparison of using `ReactChildren.map` vs `traverseAllChildren` directly.\n<issue_comment>username_0: As you said I'm running into a lot of error logs due to `ReactFragment.create` being called with nulls and other invalid arguments. I think sticking with `traverseAllChildren` in this case will be better.\n<issue_comment>username_1: Sorry, I may have been a little unclear. Can you do something like this\r\n\r\n```\r\ndiff --git a/src/utils/ReactChildren.js b/src/utils/ReactChildren.js\r\nindex e470609..84ca357 100644\r\n--- a/src/utils/ReactChildren.js\r\n+++ b/src/utils/ReactChildren.js\r\n@@ -122,7 +122,7 @@ function mapChildren(children, func, context) {\r\n   var traverseContext = MapBookKeeping.getPooled(mapResult, func, context);\r\n   traverseAllChildren(children, mapSingleChildIntoContext, traverseContext);\r\n   MapBookKeeping.release(traverseContext);\r\n-  return ReactFragment.create(mapResult);\r\n+  return mapResult;\r\n }\r\n \r\n function forEachSingleChildDummy(traverseContext, child, name, i) {\r\n@@ -142,7 +142,10 @@ function countChildren(children, context) {\r\n \r\n var ReactChildren = {\r\n   forEach: forEachChildren,\r\n-  map: mapChildren,\r\n+  map: function(children, func, context) {\r\n+    return ReactFragment.create(mapChildren(children, func, context));\r\n+  },\r\n+  mapObject: mapChildren,\r\n   count: countChildren\r\n };\r\n \r\n```\r\n\r\nand then use ReactChildren.mapObject?\n<issue_comment>username_0: Ok, so I create a branch that uses ReactChildren.mapObject: https://github.com/facebook/react/compare/master...username_0:inlineInstantiateChildrenWithMap\r\n\r\nThere are two tests that fail using this method:\r\n\r\n * core/__tests__/ReactMultiChildReconcile-test ReactMultiChildReconcile should insert non-empty children in middle where nulls were.\r\n * core/__tests__/ReactMultiChildText-test ReactMultiChildText should correctly handle all possible children for render and update.\r\n\r\nI believe this is because map doesn't handle skipping nulls.\r\n\r\nI also ran a benchmark for using `traverseAllChilden` as my per previous comment vs. using `ReactChildren.mapObject` as @username_1 suggested. Running locally, I'm seeing a significant overhead in using the pooling added by `ReactChildren.mapObject`. On some runs it shows to be less performant than not inlining. You can see the travis-ci run of the benchmark here: https://travis-ci.org/username_0/react-perf/builds/60283471\n<issue_comment>username_0: @username_2 Can you elaborate one what you mean by \"internal state tree\"? I'm trying to understand what could be removed from the rendering path to optimize server rendering. I have a feeling some of the `mountComponent` logic needs to be split up into rendering logic and other logic (event registration, state management, etc.), but I'm not familiar enough with the \"other\" logic that's happening to know what to look at.\n<issue_comment>username_1: @username_0 Interesting. If inlining a closure (like https://gist.github.com/username_1/594910785229dba07d5c) is faster we should probably switch to that \u2013 we've tried to avoid closures in favor of manually passing context objects in hopes of lowering GC pressure and improving perf but all of this feels like dogscience.\r\n\r\n![image](https://cloud.githubusercontent.com/assets/6820/7360334/68f64cd2-ecfe-11e4-8e6e-b85703c9a2de.png)\n<issue_comment>username_1: Sorry for all the back and forth.\n<issue_comment>username_0: No worries. I too often feel like I have no idea what I'm doing.\r\n\r\nI think using `traverseAllChildren` directly as in https://github.com/facebook/react/compare/master...username_0:inlineInstantiateChildren addresses your concern about GC although it does duplicate some of the code in `ReactChildren.map`. We do need [this check](https://github.com/facebook/react/compare/master...username_0:inlineInstantiateChildren#diff-89f33512519fa4d853463913eac8ef04R24) though in order for the tests to pass. Any ideas on introducing that check in to `ReactChildren.map`?\n<issue_comment>username_1: I tried to replicate these results locally and in different browsers. Here's what I did:\r\n\r\n```sh\r\n$ git checkout origin/inlineMountChildren\r\n$ npm install -g browserify\r\n$ npm install babelify envify\r\n$ NODE_ENV=production browserify -t babelify bench-chat.js >bundle.js\r\n```\r\n\r\nAnd I created bench.html:\r\n\r\n```html\r\n<meta charset=\"utf-8\">\r\n<body style=\"font: 10px Menlo; white-space: pre;\">\r\n<div id=\"container\"></div>\r\n<script>\r\nconsole.log = function(x) {\r\n  container.textContent += x.replace(/\\033\\[[0-9;]*m/g, '') + '\\n';\r\n};\r\n</script>\r\n<script src=\"bundle.js\"></script>\r\n```\r\n\r\nOpening bench.html in Safari (JavaScriptCore) gave me:\r\n\r\n```\r\nRenderChat for inputs Optimized x 2,094 ops/sec \u00b10.85% (66 runs sampled)\r\nRenderChat for inputs Stock x 2,081 ops/sec \u00b10.78% (66 runs sampled)\r\nThe Fastest test suite is RenderChat for inputs Optimized,RenderChat for inputs Stock\r\n\r\n+------------+---------------+---------------+\r\n|            \u00e2\u201d\u201a Optimized     \u00e2\u201d\u201a Stock         |\r\n+------------+---------------+---------------+\r\n| RenderChat \u00e2\u201d\u201a 2,094 ops/sec \u00e2\u201d\u201a 2,081 ops/sec |\r\n+------------+---------------+---------------+\r\n```\r\n\r\nwhere they're the same speed (not sure what's up with the encoding there), and in Chrome I got:\r\n\r\n```\r\nRenderChat for inputs Optimized x 1,534 ops/sec \u00b12.38% (61 runs sampled)\r\nRenderChat for inputs Stock x 1,775 ops/sec \u00b11.56% (65 runs sampled)\r\nThe Fastest test suite is RenderChat for inputs Stock\r\n\r\n+------------+---------------+---------------+\r\n|            \u2502 Optimized     \u2502 Stock         |\r\n+------------+---------------+---------------+\r\n| RenderChat \u2502 1,534 ops/sec \u2502 1,775 ops/sec |\r\n+------------+---------------+---------------+\r\n```\r\n\r\nwhere the old version is 15% faster. Running `node bench-chat.js` gave me\r\n\r\n```\r\nRenderChat for inputs Optimized x 368 ops/sec \u00b13.18% (73 runs sampled)\r\nRenderChat for inputs Stock x 348 ops/sec \u00b12.99% (72 runs sampled)\r\nThe Fastest test suite is RenderChat for inputs Optimized\r\n\r\n+------------+-------------+-------------+\r\n|            \u2502 Optimized   \u2502 Stock       |\r\n+------------+-------------+-------------+\r\n| RenderChat \u2502 368 ops/sec \u2502 348 ops/sec |\r\n+------------+-------------+-------------+\r\n```\r\n\r\nwhere your version is maybe 5% faster, but not the 10\u201315% you were seeing originally. Any ideas if I'm doing something wrong or if this is just wildly different between engines? I'd have expected Chrome and Node to be more similar, at the very least.\n<issue_comment>username_0: Are the symlinks working correctly for you? browserify does not work for me locally to create the bundle.js as you did. I'm not sure why your results are so different from my local tests and travis-ci.\n<issue_comment>username_0: It's also worth noting, that even with two versions of stock flux, the benchmark for the item that is tested first is usually ~5% lower than the second which is why I moved \"optimized\" as the first test case. Consistency seems to be a consistent problem =/\n<issue_comment>username_0: Here's travis with 10 runs of bench-chat: https://travis-ci.org/username_0/react-perf/builds/60620846\n<issue_comment>username_0: So it looks like I didn't have the node_modules for each of the apps pushed, so the components may have been using the wrong React. I have pushed the node_modules folder for each app and used symlinks to the react versions such that they are completely isolated. I'm hoping this will make the tests more consistent for you.\r\n\r\nhttps://github.com/username_0/react-perf/tree/inlineMountChildren\n<issue_comment>username_0: Closing this since it undoes a purposeful refactor. I will open a new PR to discuss inlining just `ReactChildReconciler.instantiateChildren`."}
{"repo": "facebook/react", "issue_id": 69988869, "issue_number": 3717, "timestamp": "2015-04-23 21:01:23+00:00", "text": "I pulled the traversal function out into a named functioned rather than inline in the branch above. I will run a comparison of using `ReactChildren.map` vs `traverseAllChildren` directly.", "context": "<issue_start><issue_comment>Title: [performance] Inline ReactMultiChild.mountChildren\nusername_0: I found that `ReactMultiChild.mountChildren` and the functions it calls were iterating over the children many times. This inlines those calls as it is a critical rendering path. Through testing a chat example I have found about a 10-15% performance gains through benchmark.js.\r\n\r\nTest: https://github.com/username_0/react-perf/tree/inlineMountChildren\r\nTest results: https://travis-ci.org/username_0/react-perf/builds/59480548\n<issue_comment>username_1: For reference, this separation was introduced intentionally in this PR:\r\n\r\nhttps://github.com/facebook/react/pull/2567\r\n\r\ncc @username_2\n<issue_comment>username_0: I seem to have broken the test right before pushing. Will investigate.\n<issue_comment>username_2: Yea, this was intentionally separated, knowing that it is a slight performance loss for now. It is a part of an on-going refactor to separate instantiation and resolution from the render strategy. That way we can reuse core React logic while having the option to optimize for different render outputs (DOM, ART, React Native, etc). It is also easier to reason about fragments (components that render multiple elements) if we can diff the resolved set.\r\n\r\nThe idea is to move the instantiation out of the DOM component.\r\n\r\nIt is likely we can remerge this kind of optimization later as more of this refactor starts coming together but I think I'd rather keep it separate for now so that we can incrementally continue the refactor instead of having to make a complete reimplementation from scratch.\r\n\r\nThis brings up an interesting point though. For renderToString, we don't really need to keep any of the internal state tree which is only used for optimizing updates. Maybe we need a whole optimized pipeline that doesn't generate the internal memoization nor the state tree?\n<issue_comment>username_0: :+1: totally agree @username_2. I think that would also help with some of the GC issues we're seeing with higher throughput in node. If you can point me in the right direction for where to start with this, I'd be more than glad to work on it.\n<issue_comment>username_0: Looks like there's still a significant gain if we inline only `ReactChildReconciler.instantiateChildren`:\r\n\r\nChanges: https://github.com/facebook/react/compare/master...username_0:inlineInstantiateChildren\r\nTest: https://github.com/username_0/react-perf/tree/inlineInstantiateChildren\r\nResult: https://travis-ci.org/username_0/react-perf/builds/59784288\n<issue_comment>username_1: Nice find. Maybe we can take advantage of ReactChildren.map here? We may want a version which skips the ReactFragment.create call for these internals but otherwise I think it should give the right effect.\n<issue_comment>username_0: I pulled the traversal function out into a named functioned rather than inline in the branch above. I will run a comparison of using `ReactChildren.map` vs `traverseAllChildren` directly.\n<issue_comment>username_0: As you said I'm running into a lot of error logs due to `ReactFragment.create` being called with nulls and other invalid arguments. I think sticking with `traverseAllChildren` in this case will be better.\n<issue_comment>username_1: Sorry, I may have been a little unclear. Can you do something like this\r\n\r\n```\r\ndiff --git a/src/utils/ReactChildren.js b/src/utils/ReactChildren.js\r\nindex e470609..84ca357 100644\r\n--- a/src/utils/ReactChildren.js\r\n+++ b/src/utils/ReactChildren.js\r\n@@ -122,7 +122,7 @@ function mapChildren(children, func, context) {\r\n   var traverseContext = MapBookKeeping.getPooled(mapResult, func, context);\r\n   traverseAllChildren(children, mapSingleChildIntoContext, traverseContext);\r\n   MapBookKeeping.release(traverseContext);\r\n-  return ReactFragment.create(mapResult);\r\n+  return mapResult;\r\n }\r\n \r\n function forEachSingleChildDummy(traverseContext, child, name, i) {\r\n@@ -142,7 +142,10 @@ function countChildren(children, context) {\r\n \r\n var ReactChildren = {\r\n   forEach: forEachChildren,\r\n-  map: mapChildren,\r\n+  map: function(children, func, context) {\r\n+    return ReactFragment.create(mapChildren(children, func, context));\r\n+  },\r\n+  mapObject: mapChildren,\r\n   count: countChildren\r\n };\r\n \r\n```\r\n\r\nand then use ReactChildren.mapObject?\n<issue_comment>username_0: Ok, so I create a branch that uses ReactChildren.mapObject: https://github.com/facebook/react/compare/master...username_0:inlineInstantiateChildrenWithMap\r\n\r\nThere are two tests that fail using this method:\r\n\r\n * core/__tests__/ReactMultiChildReconcile-test ReactMultiChildReconcile should insert non-empty children in middle where nulls were.\r\n * core/__tests__/ReactMultiChildText-test ReactMultiChildText should correctly handle all possible children for render and update.\r\n\r\nI believe this is because map doesn't handle skipping nulls.\r\n\r\nI also ran a benchmark for using `traverseAllChilden` as my per previous comment vs. using `ReactChildren.mapObject` as @username_1 suggested. Running locally, I'm seeing a significant overhead in using the pooling added by `ReactChildren.mapObject`. On some runs it shows to be less performant than not inlining. You can see the travis-ci run of the benchmark here: https://travis-ci.org/username_0/react-perf/builds/60283471\n<issue_comment>username_0: @username_2 Can you elaborate one what you mean by \"internal state tree\"? I'm trying to understand what could be removed from the rendering path to optimize server rendering. I have a feeling some of the `mountComponent` logic needs to be split up into rendering logic and other logic (event registration, state management, etc.), but I'm not familiar enough with the \"other\" logic that's happening to know what to look at.\n<issue_comment>username_1: @username_0 Interesting. If inlining a closure (like https://gist.github.com/username_1/594910785229dba07d5c) is faster we should probably switch to that \u2013 we've tried to avoid closures in favor of manually passing context objects in hopes of lowering GC pressure and improving perf but all of this feels like dogscience.\r\n\r\n![image](https://cloud.githubusercontent.com/assets/6820/7360334/68f64cd2-ecfe-11e4-8e6e-b85703c9a2de.png)\n<issue_comment>username_1: Sorry for all the back and forth.\n<issue_comment>username_0: No worries. I too often feel like I have no idea what I'm doing.\r\n\r\nI think using `traverseAllChildren` directly as in https://github.com/facebook/react/compare/master...username_0:inlineInstantiateChildren addresses your concern about GC although it does duplicate some of the code in `ReactChildren.map`. We do need [this check](https://github.com/facebook/react/compare/master...username_0:inlineInstantiateChildren#diff-89f33512519fa4d853463913eac8ef04R24) though in order for the tests to pass. Any ideas on introducing that check in to `ReactChildren.map`?\n<issue_comment>username_1: I tried to replicate these results locally and in different browsers. Here's what I did:\r\n\r\n```sh\r\n$ git checkout origin/inlineMountChildren\r\n$ npm install -g browserify\r\n$ npm install babelify envify\r\n$ NODE_ENV=production browserify -t babelify bench-chat.js >bundle.js\r\n```\r\n\r\nAnd I created bench.html:\r\n\r\n```html\r\n<meta charset=\"utf-8\">\r\n<body style=\"font: 10px Menlo; white-space: pre;\">\r\n<div id=\"container\"></div>\r\n<script>\r\nconsole.log = function(x) {\r\n  container.textContent += x.replace(/\\033\\[[0-9;]*m/g, '') + '\\n';\r\n};\r\n</script>\r\n<script src=\"bundle.js\"></script>\r\n```\r\n\r\nOpening bench.html in Safari (JavaScriptCore) gave me:\r\n\r\n```\r\nRenderChat for inputs Optimized x 2,094 ops/sec \u00b10.85% (66 runs sampled)\r\nRenderChat for inputs Stock x 2,081 ops/sec \u00b10.78% (66 runs sampled)\r\nThe Fastest test suite is RenderChat for inputs Optimized,RenderChat for inputs Stock\r\n\r\n+------------+---------------+---------------+\r\n|            \u00e2\u201d\u201a Optimized     \u00e2\u201d\u201a Stock         |\r\n+------------+---------------+---------------+\r\n| RenderChat \u00e2\u201d\u201a 2,094 ops/sec \u00e2\u201d\u201a 2,081 ops/sec |\r\n+------------+---------------+---------------+\r\n```\r\n\r\nwhere they're the same speed (not sure what's up with the encoding there), and in Chrome I got:\r\n\r\n```\r\nRenderChat for inputs Optimized x 1,534 ops/sec \u00b12.38% (61 runs sampled)\r\nRenderChat for inputs Stock x 1,775 ops/sec \u00b11.56% (65 runs sampled)\r\nThe Fastest test suite is RenderChat for inputs Stock\r\n\r\n+------------+---------------+---------------+\r\n|            \u2502 Optimized     \u2502 Stock         |\r\n+------------+---------------+---------------+\r\n| RenderChat \u2502 1,534 ops/sec \u2502 1,775 ops/sec |\r\n+------------+---------------+---------------+\r\n```\r\n\r\nwhere the old version is 15% faster. Running `node bench-chat.js` gave me\r\n\r\n```\r\nRenderChat for inputs Optimized x 368 ops/sec \u00b13.18% (73 runs sampled)\r\nRenderChat for inputs Stock x 348 ops/sec \u00b12.99% (72 runs sampled)\r\nThe Fastest test suite is RenderChat for inputs Optimized\r\n\r\n+------------+-------------+-------------+\r\n|            \u2502 Optimized   \u2502 Stock       |\r\n+------------+-------------+-------------+\r\n| RenderChat \u2502 368 ops/sec \u2502 348 ops/sec |\r\n+------------+-------------+-------------+\r\n```\r\n\r\nwhere your version is maybe 5% faster, but not the 10\u201315% you were seeing originally. Any ideas if I'm doing something wrong or if this is just wildly different between engines? I'd have expected Chrome and Node to be more similar, at the very least.\n<issue_comment>username_0: Are the symlinks working correctly for you? browserify does not work for me locally to create the bundle.js as you did. I'm not sure why your results are so different from my local tests and travis-ci.\n<issue_comment>username_0: It's also worth noting, that even with two versions of stock flux, the benchmark for the item that is tested first is usually ~5% lower than the second which is why I moved \"optimized\" as the first test case. Consistency seems to be a consistent problem =/\n<issue_comment>username_0: Here's travis with 10 runs of bench-chat: https://travis-ci.org/username_0/react-perf/builds/60620846\n<issue_comment>username_0: So it looks like I didn't have the node_modules for each of the apps pushed, so the components may have been using the wrong React. I have pushed the node_modules folder for each app and used symlinks to the react versions such that they are completely isolated. I'm hoping this will make the tests more consistent for you.\r\n\r\nhttps://github.com/username_0/react-perf/tree/inlineMountChildren\n<issue_comment>username_0: Closing this since it undoes a purposeful refactor. I will open a new PR to discuss inlining just `ReactChildReconciler.instantiateChildren`."}
{"repo": "facebook/react", "issue_id": 69988869, "issue_number": 3717, "timestamp": "2015-04-24 18:19:32+00:00", "text": "As you said I'm running into a lot of error logs due to `ReactFragment.create` being called with nulls and other invalid arguments. I think sticking with `traverseAllChildren` in this case will be better.", "context": "<issue_start><issue_comment>Title: [performance] Inline ReactMultiChild.mountChildren\nusername_0: I found that `ReactMultiChild.mountChildren` and the functions it calls were iterating over the children many times. This inlines those calls as it is a critical rendering path. Through testing a chat example I have found about a 10-15% performance gains through benchmark.js.\r\n\r\nTest: https://github.com/username_0/react-perf/tree/inlineMountChildren\r\nTest results: https://travis-ci.org/username_0/react-perf/builds/59480548\n<issue_comment>username_1: For reference, this separation was introduced intentionally in this PR:\r\n\r\nhttps://github.com/facebook/react/pull/2567\r\n\r\ncc @username_2\n<issue_comment>username_0: I seem to have broken the test right before pushing. Will investigate.\n<issue_comment>username_2: Yea, this was intentionally separated, knowing that it is a slight performance loss for now. It is a part of an on-going refactor to separate instantiation and resolution from the render strategy. That way we can reuse core React logic while having the option to optimize for different render outputs (DOM, ART, React Native, etc). It is also easier to reason about fragments (components that render multiple elements) if we can diff the resolved set.\r\n\r\nThe idea is to move the instantiation out of the DOM component.\r\n\r\nIt is likely we can remerge this kind of optimization later as more of this refactor starts coming together but I think I'd rather keep it separate for now so that we can incrementally continue the refactor instead of having to make a complete reimplementation from scratch.\r\n\r\nThis brings up an interesting point though. For renderToString, we don't really need to keep any of the internal state tree which is only used for optimizing updates. Maybe we need a whole optimized pipeline that doesn't generate the internal memoization nor the state tree?\n<issue_comment>username_0: :+1: totally agree @username_2. I think that would also help with some of the GC issues we're seeing with higher throughput in node. If you can point me in the right direction for where to start with this, I'd be more than glad to work on it.\n<issue_comment>username_0: Looks like there's still a significant gain if we inline only `ReactChildReconciler.instantiateChildren`:\r\n\r\nChanges: https://github.com/facebook/react/compare/master...username_0:inlineInstantiateChildren\r\nTest: https://github.com/username_0/react-perf/tree/inlineInstantiateChildren\r\nResult: https://travis-ci.org/username_0/react-perf/builds/59784288\n<issue_comment>username_1: Nice find. Maybe we can take advantage of ReactChildren.map here? We may want a version which skips the ReactFragment.create call for these internals but otherwise I think it should give the right effect.\n<issue_comment>username_0: I pulled the traversal function out into a named functioned rather than inline in the branch above. I will run a comparison of using `ReactChildren.map` vs `traverseAllChildren` directly.\n<issue_comment>username_0: As you said I'm running into a lot of error logs due to `ReactFragment.create` being called with nulls and other invalid arguments. I think sticking with `traverseAllChildren` in this case will be better.\n<issue_comment>username_1: Sorry, I may have been a little unclear. Can you do something like this\r\n\r\n```\r\ndiff --git a/src/utils/ReactChildren.js b/src/utils/ReactChildren.js\r\nindex e470609..84ca357 100644\r\n--- a/src/utils/ReactChildren.js\r\n+++ b/src/utils/ReactChildren.js\r\n@@ -122,7 +122,7 @@ function mapChildren(children, func, context) {\r\n   var traverseContext = MapBookKeeping.getPooled(mapResult, func, context);\r\n   traverseAllChildren(children, mapSingleChildIntoContext, traverseContext);\r\n   MapBookKeeping.release(traverseContext);\r\n-  return ReactFragment.create(mapResult);\r\n+  return mapResult;\r\n }\r\n \r\n function forEachSingleChildDummy(traverseContext, child, name, i) {\r\n@@ -142,7 +142,10 @@ function countChildren(children, context) {\r\n \r\n var ReactChildren = {\r\n   forEach: forEachChildren,\r\n-  map: mapChildren,\r\n+  map: function(children, func, context) {\r\n+    return ReactFragment.create(mapChildren(children, func, context));\r\n+  },\r\n+  mapObject: mapChildren,\r\n   count: countChildren\r\n };\r\n \r\n```\r\n\r\nand then use ReactChildren.mapObject?\n<issue_comment>username_0: Ok, so I create a branch that uses ReactChildren.mapObject: https://github.com/facebook/react/compare/master...username_0:inlineInstantiateChildrenWithMap\r\n\r\nThere are two tests that fail using this method:\r\n\r\n * core/__tests__/ReactMultiChildReconcile-test ReactMultiChildReconcile should insert non-empty children in middle where nulls were.\r\n * core/__tests__/ReactMultiChildText-test ReactMultiChildText should correctly handle all possible children for render and update.\r\n\r\nI believe this is because map doesn't handle skipping nulls.\r\n\r\nI also ran a benchmark for using `traverseAllChilden` as my per previous comment vs. using `ReactChildren.mapObject` as @username_1 suggested. Running locally, I'm seeing a significant overhead in using the pooling added by `ReactChildren.mapObject`. On some runs it shows to be less performant than not inlining. You can see the travis-ci run of the benchmark here: https://travis-ci.org/username_0/react-perf/builds/60283471\n<issue_comment>username_0: @username_2 Can you elaborate one what you mean by \"internal state tree\"? I'm trying to understand what could be removed from the rendering path to optimize server rendering. I have a feeling some of the `mountComponent` logic needs to be split up into rendering logic and other logic (event registration, state management, etc.), but I'm not familiar enough with the \"other\" logic that's happening to know what to look at.\n<issue_comment>username_1: @username_0 Interesting. If inlining a closure (like https://gist.github.com/username_1/594910785229dba07d5c) is faster we should probably switch to that \u2013 we've tried to avoid closures in favor of manually passing context objects in hopes of lowering GC pressure and improving perf but all of this feels like dogscience.\r\n\r\n![image](https://cloud.githubusercontent.com/assets/6820/7360334/68f64cd2-ecfe-11e4-8e6e-b85703c9a2de.png)\n<issue_comment>username_1: Sorry for all the back and forth.\n<issue_comment>username_0: No worries. I too often feel like I have no idea what I'm doing.\r\n\r\nI think using `traverseAllChildren` directly as in https://github.com/facebook/react/compare/master...username_0:inlineInstantiateChildren addresses your concern about GC although it does duplicate some of the code in `ReactChildren.map`. We do need [this check](https://github.com/facebook/react/compare/master...username_0:inlineInstantiateChildren#diff-89f33512519fa4d853463913eac8ef04R24) though in order for the tests to pass. Any ideas on introducing that check in to `ReactChildren.map`?\n<issue_comment>username_1: I tried to replicate these results locally and in different browsers. Here's what I did:\r\n\r\n```sh\r\n$ git checkout origin/inlineMountChildren\r\n$ npm install -g browserify\r\n$ npm install babelify envify\r\n$ NODE_ENV=production browserify -t babelify bench-chat.js >bundle.js\r\n```\r\n\r\nAnd I created bench.html:\r\n\r\n```html\r\n<meta charset=\"utf-8\">\r\n<body style=\"font: 10px Menlo; white-space: pre;\">\r\n<div id=\"container\"></div>\r\n<script>\r\nconsole.log = function(x) {\r\n  container.textContent += x.replace(/\\033\\[[0-9;]*m/g, '') + '\\n';\r\n};\r\n</script>\r\n<script src=\"bundle.js\"></script>\r\n```\r\n\r\nOpening bench.html in Safari (JavaScriptCore) gave me:\r\n\r\n```\r\nRenderChat for inputs Optimized x 2,094 ops/sec \u00b10.85% (66 runs sampled)\r\nRenderChat for inputs Stock x 2,081 ops/sec \u00b10.78% (66 runs sampled)\r\nThe Fastest test suite is RenderChat for inputs Optimized,RenderChat for inputs Stock\r\n\r\n+------------+---------------+---------------+\r\n|            \u00e2\u201d\u201a Optimized     \u00e2\u201d\u201a Stock         |\r\n+------------+---------------+---------------+\r\n| RenderChat \u00e2\u201d\u201a 2,094 ops/sec \u00e2\u201d\u201a 2,081 ops/sec |\r\n+------------+---------------+---------------+\r\n```\r\n\r\nwhere they're the same speed (not sure what's up with the encoding there), and in Chrome I got:\r\n\r\n```\r\nRenderChat for inputs Optimized x 1,534 ops/sec \u00b12.38% (61 runs sampled)\r\nRenderChat for inputs Stock x 1,775 ops/sec \u00b11.56% (65 runs sampled)\r\nThe Fastest test suite is RenderChat for inputs Stock\r\n\r\n+------------+---------------+---------------+\r\n|            \u2502 Optimized     \u2502 Stock         |\r\n+------------+---------------+---------------+\r\n| RenderChat \u2502 1,534 ops/sec \u2502 1,775 ops/sec |\r\n+------------+---------------+---------------+\r\n```\r\n\r\nwhere the old version is 15% faster. Running `node bench-chat.js` gave me\r\n\r\n```\r\nRenderChat for inputs Optimized x 368 ops/sec \u00b13.18% (73 runs sampled)\r\nRenderChat for inputs Stock x 348 ops/sec \u00b12.99% (72 runs sampled)\r\nThe Fastest test suite is RenderChat for inputs Optimized\r\n\r\n+------------+-------------+-------------+\r\n|            \u2502 Optimized   \u2502 Stock       |\r\n+------------+-------------+-------------+\r\n| RenderChat \u2502 368 ops/sec \u2502 348 ops/sec |\r\n+------------+-------------+-------------+\r\n```\r\n\r\nwhere your version is maybe 5% faster, but not the 10\u201315% you were seeing originally. Any ideas if I'm doing something wrong or if this is just wildly different between engines? I'd have expected Chrome and Node to be more similar, at the very least.\n<issue_comment>username_0: Are the symlinks working correctly for you? browserify does not work for me locally to create the bundle.js as you did. I'm not sure why your results are so different from my local tests and travis-ci.\n<issue_comment>username_0: It's also worth noting, that even with two versions of stock flux, the benchmark for the item that is tested first is usually ~5% lower than the second which is why I moved \"optimized\" as the first test case. Consistency seems to be a consistent problem =/\n<issue_comment>username_0: Here's travis with 10 runs of bench-chat: https://travis-ci.org/username_0/react-perf/builds/60620846\n<issue_comment>username_0: So it looks like I didn't have the node_modules for each of the apps pushed, so the components may have been using the wrong React. I have pushed the node_modules folder for each app and used symlinks to the react versions such that they are completely isolated. I'm hoping this will make the tests more consistent for you.\r\n\r\nhttps://github.com/username_0/react-perf/tree/inlineMountChildren\n<issue_comment>username_0: Closing this since it undoes a purposeful refactor. I will open a new PR to discuss inlining just `ReactChildReconciler.instantiateChildren`."}
{"repo": "facebook/react", "issue_id": 69988869, "issue_number": 3717, "timestamp": "2015-04-24 18:28:08+00:00", "text": "Sorry, I may have been a little unclear. Can you do something like this\r\n\r\n```\r\ndiff --git a/src/utils/ReactChildren.js b/src/utils/ReactChildren.js\r\nindex e470609..84ca357 100644\r\n--- a/src/utils/ReactChildren.js\r\n+++ b/src/utils/ReactChildren.js\r\n@@ -122,7 +122,7 @@ function mapChildren(children, func, context) {\r\n   var traverseContext = MapBookKeeping.getPooled(mapResult, func, context);\r\n   traverseAllChildren(children, mapSingleChildIntoContext, traverseContext);\r\n   MapBookKeeping.release(traverseContext);\r\n-  return ReactFragment.create(mapResult);\r\n+  return mapResult;\r\n }\r\n \r\n function forEachSingleChildDummy(traverseContext, child, name, i) {\r\n@@ -142,7 +142,10 @@ function countChildren(children, context) {\r\n \r\n var ReactChildren = {\r\n   forEach: forEachChildren,\r\n-  map: mapChildren,\r\n+  map: function(children, func, context) {\r\n+    return ReactFragment.create(mapChildren(children, func, context));\r\n+  },\r\n+  mapObject: mapChildren,\r\n   count: countChildren\r\n };\r\n \r\n```\r\n\r\nand then use ReactChildren.mapObject?", "context": "<issue_start><issue_comment>Title: [performance] Inline ReactMultiChild.mountChildren\nusername_0: I found that `ReactMultiChild.mountChildren` and the functions it calls were iterating over the children many times. This inlines those calls as it is a critical rendering path. Through testing a chat example I have found about a 10-15% performance gains through benchmark.js.\r\n\r\nTest: https://github.com/username_0/react-perf/tree/inlineMountChildren\r\nTest results: https://travis-ci.org/username_0/react-perf/builds/59480548\n<issue_comment>username_1: For reference, this separation was introduced intentionally in this PR:\r\n\r\nhttps://github.com/facebook/react/pull/2567\r\n\r\ncc @username_2\n<issue_comment>username_0: I seem to have broken the test right before pushing. Will investigate.\n<issue_comment>username_2: Yea, this was intentionally separated, knowing that it is a slight performance loss for now. It is a part of an on-going refactor to separate instantiation and resolution from the render strategy. That way we can reuse core React logic while having the option to optimize for different render outputs (DOM, ART, React Native, etc). It is also easier to reason about fragments (components that render multiple elements) if we can diff the resolved set.\r\n\r\nThe idea is to move the instantiation out of the DOM component.\r\n\r\nIt is likely we can remerge this kind of optimization later as more of this refactor starts coming together but I think I'd rather keep it separate for now so that we can incrementally continue the refactor instead of having to make a complete reimplementation from scratch.\r\n\r\nThis brings up an interesting point though. For renderToString, we don't really need to keep any of the internal state tree which is only used for optimizing updates. Maybe we need a whole optimized pipeline that doesn't generate the internal memoization nor the state tree?\n<issue_comment>username_0: :+1: totally agree @username_2. I think that would also help with some of the GC issues we're seeing with higher throughput in node. If you can point me in the right direction for where to start with this, I'd be more than glad to work on it.\n<issue_comment>username_0: Looks like there's still a significant gain if we inline only `ReactChildReconciler.instantiateChildren`:\r\n\r\nChanges: https://github.com/facebook/react/compare/master...username_0:inlineInstantiateChildren\r\nTest: https://github.com/username_0/react-perf/tree/inlineInstantiateChildren\r\nResult: https://travis-ci.org/username_0/react-perf/builds/59784288\n<issue_comment>username_1: Nice find. Maybe we can take advantage of ReactChildren.map here? We may want a version which skips the ReactFragment.create call for these internals but otherwise I think it should give the right effect.\n<issue_comment>username_0: I pulled the traversal function out into a named functioned rather than inline in the branch above. I will run a comparison of using `ReactChildren.map` vs `traverseAllChildren` directly.\n<issue_comment>username_0: As you said I'm running into a lot of error logs due to `ReactFragment.create` being called with nulls and other invalid arguments. I think sticking with `traverseAllChildren` in this case will be better.\n<issue_comment>username_1: Sorry, I may have been a little unclear. Can you do something like this\r\n\r\n```\r\ndiff --git a/src/utils/ReactChildren.js b/src/utils/ReactChildren.js\r\nindex e470609..84ca357 100644\r\n--- a/src/utils/ReactChildren.js\r\n+++ b/src/utils/ReactChildren.js\r\n@@ -122,7 +122,7 @@ function mapChildren(children, func, context) {\r\n   var traverseContext = MapBookKeeping.getPooled(mapResult, func, context);\r\n   traverseAllChildren(children, mapSingleChildIntoContext, traverseContext);\r\n   MapBookKeeping.release(traverseContext);\r\n-  return ReactFragment.create(mapResult);\r\n+  return mapResult;\r\n }\r\n \r\n function forEachSingleChildDummy(traverseContext, child, name, i) {\r\n@@ -142,7 +142,10 @@ function countChildren(children, context) {\r\n \r\n var ReactChildren = {\r\n   forEach: forEachChildren,\r\n-  map: mapChildren,\r\n+  map: function(children, func, context) {\r\n+    return ReactFragment.create(mapChildren(children, func, context));\r\n+  },\r\n+  mapObject: mapChildren,\r\n   count: countChildren\r\n };\r\n \r\n```\r\n\r\nand then use ReactChildren.mapObject?\n<issue_comment>username_0: Ok, so I create a branch that uses ReactChildren.mapObject: https://github.com/facebook/react/compare/master...username_0:inlineInstantiateChildrenWithMap\r\n\r\nThere are two tests that fail using this method:\r\n\r\n * core/__tests__/ReactMultiChildReconcile-test ReactMultiChildReconcile should insert non-empty children in middle where nulls were.\r\n * core/__tests__/ReactMultiChildText-test ReactMultiChildText should correctly handle all possible children for render and update.\r\n\r\nI believe this is because map doesn't handle skipping nulls.\r\n\r\nI also ran a benchmark for using `traverseAllChilden` as my per previous comment vs. using `ReactChildren.mapObject` as @username_1 suggested. Running locally, I'm seeing a significant overhead in using the pooling added by `ReactChildren.mapObject`. On some runs it shows to be less performant than not inlining. You can see the travis-ci run of the benchmark here: https://travis-ci.org/username_0/react-perf/builds/60283471\n<issue_comment>username_0: @username_2 Can you elaborate one what you mean by \"internal state tree\"? I'm trying to understand what could be removed from the rendering path to optimize server rendering. I have a feeling some of the `mountComponent` logic needs to be split up into rendering logic and other logic (event registration, state management, etc.), but I'm not familiar enough with the \"other\" logic that's happening to know what to look at.\n<issue_comment>username_1: @username_0 Interesting. If inlining a closure (like https://gist.github.com/username_1/594910785229dba07d5c) is faster we should probably switch to that \u2013 we've tried to avoid closures in favor of manually passing context objects in hopes of lowering GC pressure and improving perf but all of this feels like dogscience.\r\n\r\n![image](https://cloud.githubusercontent.com/assets/6820/7360334/68f64cd2-ecfe-11e4-8e6e-b85703c9a2de.png)\n<issue_comment>username_1: Sorry for all the back and forth.\n<issue_comment>username_0: No worries. I too often feel like I have no idea what I'm doing.\r\n\r\nI think using `traverseAllChildren` directly as in https://github.com/facebook/react/compare/master...username_0:inlineInstantiateChildren addresses your concern about GC although it does duplicate some of the code in `ReactChildren.map`. We do need [this check](https://github.com/facebook/react/compare/master...username_0:inlineInstantiateChildren#diff-89f33512519fa4d853463913eac8ef04R24) though in order for the tests to pass. Any ideas on introducing that check in to `ReactChildren.map`?\n<issue_comment>username_1: I tried to replicate these results locally and in different browsers. Here's what I did:\r\n\r\n```sh\r\n$ git checkout origin/inlineMountChildren\r\n$ npm install -g browserify\r\n$ npm install babelify envify\r\n$ NODE_ENV=production browserify -t babelify bench-chat.js >bundle.js\r\n```\r\n\r\nAnd I created bench.html:\r\n\r\n```html\r\n<meta charset=\"utf-8\">\r\n<body style=\"font: 10px Menlo; white-space: pre;\">\r\n<div id=\"container\"></div>\r\n<script>\r\nconsole.log = function(x) {\r\n  container.textContent += x.replace(/\\033\\[[0-9;]*m/g, '') + '\\n';\r\n};\r\n</script>\r\n<script src=\"bundle.js\"></script>\r\n```\r\n\r\nOpening bench.html in Safari (JavaScriptCore) gave me:\r\n\r\n```\r\nRenderChat for inputs Optimized x 2,094 ops/sec \u00b10.85% (66 runs sampled)\r\nRenderChat for inputs Stock x 2,081 ops/sec \u00b10.78% (66 runs sampled)\r\nThe Fastest test suite is RenderChat for inputs Optimized,RenderChat for inputs Stock\r\n\r\n+------------+---------------+---------------+\r\n|            \u00e2\u201d\u201a Optimized     \u00e2\u201d\u201a Stock         |\r\n+------------+---------------+---------------+\r\n| RenderChat \u00e2\u201d\u201a 2,094 ops/sec \u00e2\u201d\u201a 2,081 ops/sec |\r\n+------------+---------------+---------------+\r\n```\r\n\r\nwhere they're the same speed (not sure what's up with the encoding there), and in Chrome I got:\r\n\r\n```\r\nRenderChat for inputs Optimized x 1,534 ops/sec \u00b12.38% (61 runs sampled)\r\nRenderChat for inputs Stock x 1,775 ops/sec \u00b11.56% (65 runs sampled)\r\nThe Fastest test suite is RenderChat for inputs Stock\r\n\r\n+------------+---------------+---------------+\r\n|            \u2502 Optimized     \u2502 Stock         |\r\n+------------+---------------+---------------+\r\n| RenderChat \u2502 1,534 ops/sec \u2502 1,775 ops/sec |\r\n+------------+---------------+---------------+\r\n```\r\n\r\nwhere the old version is 15% faster. Running `node bench-chat.js` gave me\r\n\r\n```\r\nRenderChat for inputs Optimized x 368 ops/sec \u00b13.18% (73 runs sampled)\r\nRenderChat for inputs Stock x 348 ops/sec \u00b12.99% (72 runs sampled)\r\nThe Fastest test suite is RenderChat for inputs Optimized\r\n\r\n+------------+-------------+-------------+\r\n|            \u2502 Optimized   \u2502 Stock       |\r\n+------------+-------------+-------------+\r\n| RenderChat \u2502 368 ops/sec \u2502 348 ops/sec |\r\n+------------+-------------+-------------+\r\n```\r\n\r\nwhere your version is maybe 5% faster, but not the 10\u201315% you were seeing originally. Any ideas if I'm doing something wrong or if this is just wildly different between engines? I'd have expected Chrome and Node to be more similar, at the very least.\n<issue_comment>username_0: Are the symlinks working correctly for you? browserify does not work for me locally to create the bundle.js as you did. I'm not sure why your results are so different from my local tests and travis-ci.\n<issue_comment>username_0: It's also worth noting, that even with two versions of stock flux, the benchmark for the item that is tested first is usually ~5% lower than the second which is why I moved \"optimized\" as the first test case. Consistency seems to be a consistent problem =/\n<issue_comment>username_0: Here's travis with 10 runs of bench-chat: https://travis-ci.org/username_0/react-perf/builds/60620846\n<issue_comment>username_0: So it looks like I didn't have the node_modules for each of the apps pushed, so the components may have been using the wrong React. I have pushed the node_modules folder for each app and used symlinks to the react versions such that they are completely isolated. I'm hoping this will make the tests more consistent for you.\r\n\r\nhttps://github.com/username_0/react-perf/tree/inlineMountChildren\n<issue_comment>username_0: Closing this since it undoes a purposeful refactor. I will open a new PR to discuss inlining just `ReactChildReconciler.instantiateChildren`."}
{"repo": "facebook/react", "issue_id": 69988869, "issue_number": 3717, "timestamp": "2015-04-27 21:25:23+00:00", "text": "Ok, so I create a branch that uses ReactChildren.mapObject: https://github.com/facebook/react/compare/master...username_0:inlineInstantiateChildrenWithMap\r\n\r\nThere are two tests that fail using this method:\r\n\r\n * core/__tests__/ReactMultiChildReconcile-test ReactMultiChildReconcile should insert non-empty children in middle where nulls were.\r\n * core/__tests__/ReactMultiChildText-test ReactMultiChildText should correctly handle all possible children for render and update.\r\n\r\nI believe this is because map doesn't handle skipping nulls.\r\n\r\nI also ran a benchmark for using `traverseAllChilden` as my per previous comment vs. using `ReactChildren.mapObject` as @username_1 suggested. Running locally, I'm seeing a significant overhead in using the pooling added by `ReactChildren.mapObject`. On some runs it shows to be less performant than not inlining. You can see the travis-ci run of the benchmark here: https://travis-ci.org/username_0/react-perf/builds/60283471", "context": "<issue_start><issue_comment>Title: [performance] Inline ReactMultiChild.mountChildren\nusername_0: I found that `ReactMultiChild.mountChildren` and the functions it calls were iterating over the children many times. This inlines those calls as it is a critical rendering path. Through testing a chat example I have found about a 10-15% performance gains through benchmark.js.\r\n\r\nTest: https://github.com/username_0/react-perf/tree/inlineMountChildren\r\nTest results: https://travis-ci.org/username_0/react-perf/builds/59480548\n<issue_comment>username_1: For reference, this separation was introduced intentionally in this PR:\r\n\r\nhttps://github.com/facebook/react/pull/2567\r\n\r\ncc @username_2\n<issue_comment>username_0: I seem to have broken the test right before pushing. Will investigate.\n<issue_comment>username_2: Yea, this was intentionally separated, knowing that it is a slight performance loss for now. It is a part of an on-going refactor to separate instantiation and resolution from the render strategy. That way we can reuse core React logic while having the option to optimize for different render outputs (DOM, ART, React Native, etc). It is also easier to reason about fragments (components that render multiple elements) if we can diff the resolved set.\r\n\r\nThe idea is to move the instantiation out of the DOM component.\r\n\r\nIt is likely we can remerge this kind of optimization later as more of this refactor starts coming together but I think I'd rather keep it separate for now so that we can incrementally continue the refactor instead of having to make a complete reimplementation from scratch.\r\n\r\nThis brings up an interesting point though. For renderToString, we don't really need to keep any of the internal state tree which is only used for optimizing updates. Maybe we need a whole optimized pipeline that doesn't generate the internal memoization nor the state tree?\n<issue_comment>username_0: :+1: totally agree @username_2. I think that would also help with some of the GC issues we're seeing with higher throughput in node. If you can point me in the right direction for where to start with this, I'd be more than glad to work on it.\n<issue_comment>username_0: Looks like there's still a significant gain if we inline only `ReactChildReconciler.instantiateChildren`:\r\n\r\nChanges: https://github.com/facebook/react/compare/master...username_0:inlineInstantiateChildren\r\nTest: https://github.com/username_0/react-perf/tree/inlineInstantiateChildren\r\nResult: https://travis-ci.org/username_0/react-perf/builds/59784288\n<issue_comment>username_1: Nice find. Maybe we can take advantage of ReactChildren.map here? We may want a version which skips the ReactFragment.create call for these internals but otherwise I think it should give the right effect.\n<issue_comment>username_0: I pulled the traversal function out into a named functioned rather than inline in the branch above. I will run a comparison of using `ReactChildren.map` vs `traverseAllChildren` directly.\n<issue_comment>username_0: As you said I'm running into a lot of error logs due to `ReactFragment.create` being called with nulls and other invalid arguments. I think sticking with `traverseAllChildren` in this case will be better.\n<issue_comment>username_1: Sorry, I may have been a little unclear. Can you do something like this\r\n\r\n```\r\ndiff --git a/src/utils/ReactChildren.js b/src/utils/ReactChildren.js\r\nindex e470609..84ca357 100644\r\n--- a/src/utils/ReactChildren.js\r\n+++ b/src/utils/ReactChildren.js\r\n@@ -122,7 +122,7 @@ function mapChildren(children, func, context) {\r\n   var traverseContext = MapBookKeeping.getPooled(mapResult, func, context);\r\n   traverseAllChildren(children, mapSingleChildIntoContext, traverseContext);\r\n   MapBookKeeping.release(traverseContext);\r\n-  return ReactFragment.create(mapResult);\r\n+  return mapResult;\r\n }\r\n \r\n function forEachSingleChildDummy(traverseContext, child, name, i) {\r\n@@ -142,7 +142,10 @@ function countChildren(children, context) {\r\n \r\n var ReactChildren = {\r\n   forEach: forEachChildren,\r\n-  map: mapChildren,\r\n+  map: function(children, func, context) {\r\n+    return ReactFragment.create(mapChildren(children, func, context));\r\n+  },\r\n+  mapObject: mapChildren,\r\n   count: countChildren\r\n };\r\n \r\n```\r\n\r\nand then use ReactChildren.mapObject?\n<issue_comment>username_0: Ok, so I create a branch that uses ReactChildren.mapObject: https://github.com/facebook/react/compare/master...username_0:inlineInstantiateChildrenWithMap\r\n\r\nThere are two tests that fail using this method:\r\n\r\n * core/__tests__/ReactMultiChildReconcile-test ReactMultiChildReconcile should insert non-empty children in middle where nulls were.\r\n * core/__tests__/ReactMultiChildText-test ReactMultiChildText should correctly handle all possible children for render and update.\r\n\r\nI believe this is because map doesn't handle skipping nulls.\r\n\r\nI also ran a benchmark for using `traverseAllChilden` as my per previous comment vs. using `ReactChildren.mapObject` as @username_1 suggested. Running locally, I'm seeing a significant overhead in using the pooling added by `ReactChildren.mapObject`. On some runs it shows to be less performant than not inlining. You can see the travis-ci run of the benchmark here: https://travis-ci.org/username_0/react-perf/builds/60283471\n<issue_comment>username_0: @username_2 Can you elaborate one what you mean by \"internal state tree\"? I'm trying to understand what could be removed from the rendering path to optimize server rendering. I have a feeling some of the `mountComponent` logic needs to be split up into rendering logic and other logic (event registration, state management, etc.), but I'm not familiar enough with the \"other\" logic that's happening to know what to look at.\n<issue_comment>username_1: @username_0 Interesting. If inlining a closure (like https://gist.github.com/username_1/594910785229dba07d5c) is faster we should probably switch to that \u2013 we've tried to avoid closures in favor of manually passing context objects in hopes of lowering GC pressure and improving perf but all of this feels like dogscience.\r\n\r\n![image](https://cloud.githubusercontent.com/assets/6820/7360334/68f64cd2-ecfe-11e4-8e6e-b85703c9a2de.png)\n<issue_comment>username_1: Sorry for all the back and forth.\n<issue_comment>username_0: No worries. I too often feel like I have no idea what I'm doing.\r\n\r\nI think using `traverseAllChildren` directly as in https://github.com/facebook/react/compare/master...username_0:inlineInstantiateChildren addresses your concern about GC although it does duplicate some of the code in `ReactChildren.map`. We do need [this check](https://github.com/facebook/react/compare/master...username_0:inlineInstantiateChildren#diff-89f33512519fa4d853463913eac8ef04R24) though in order for the tests to pass. Any ideas on introducing that check in to `ReactChildren.map`?\n<issue_comment>username_1: I tried to replicate these results locally and in different browsers. Here's what I did:\r\n\r\n```sh\r\n$ git checkout origin/inlineMountChildren\r\n$ npm install -g browserify\r\n$ npm install babelify envify\r\n$ NODE_ENV=production browserify -t babelify bench-chat.js >bundle.js\r\n```\r\n\r\nAnd I created bench.html:\r\n\r\n```html\r\n<meta charset=\"utf-8\">\r\n<body style=\"font: 10px Menlo; white-space: pre;\">\r\n<div id=\"container\"></div>\r\n<script>\r\nconsole.log = function(x) {\r\n  container.textContent += x.replace(/\\033\\[[0-9;]*m/g, '') + '\\n';\r\n};\r\n</script>\r\n<script src=\"bundle.js\"></script>\r\n```\r\n\r\nOpening bench.html in Safari (JavaScriptCore) gave me:\r\n\r\n```\r\nRenderChat for inputs Optimized x 2,094 ops/sec \u00b10.85% (66 runs sampled)\r\nRenderChat for inputs Stock x 2,081 ops/sec \u00b10.78% (66 runs sampled)\r\nThe Fastest test suite is RenderChat for inputs Optimized,RenderChat for inputs Stock\r\n\r\n+------------+---------------+---------------+\r\n|            \u00e2\u201d\u201a Optimized     \u00e2\u201d\u201a Stock         |\r\n+------------+---------------+---------------+\r\n| RenderChat \u00e2\u201d\u201a 2,094 ops/sec \u00e2\u201d\u201a 2,081 ops/sec |\r\n+------------+---------------+---------------+\r\n```\r\n\r\nwhere they're the same speed (not sure what's up with the encoding there), and in Chrome I got:\r\n\r\n```\r\nRenderChat for inputs Optimized x 1,534 ops/sec \u00b12.38% (61 runs sampled)\r\nRenderChat for inputs Stock x 1,775 ops/sec \u00b11.56% (65 runs sampled)\r\nThe Fastest test suite is RenderChat for inputs Stock\r\n\r\n+------------+---------------+---------------+\r\n|            \u2502 Optimized     \u2502 Stock         |\r\n+------------+---------------+---------------+\r\n| RenderChat \u2502 1,534 ops/sec \u2502 1,775 ops/sec |\r\n+------------+---------------+---------------+\r\n```\r\n\r\nwhere the old version is 15% faster. Running `node bench-chat.js` gave me\r\n\r\n```\r\nRenderChat for inputs Optimized x 368 ops/sec \u00b13.18% (73 runs sampled)\r\nRenderChat for inputs Stock x 348 ops/sec \u00b12.99% (72 runs sampled)\r\nThe Fastest test suite is RenderChat for inputs Optimized\r\n\r\n+------------+-------------+-------------+\r\n|            \u2502 Optimized   \u2502 Stock       |\r\n+------------+-------------+-------------+\r\n| RenderChat \u2502 368 ops/sec \u2502 348 ops/sec |\r\n+------------+-------------+-------------+\r\n```\r\n\r\nwhere your version is maybe 5% faster, but not the 10\u201315% you were seeing originally. Any ideas if I'm doing something wrong or if this is just wildly different between engines? I'd have expected Chrome and Node to be more similar, at the very least.\n<issue_comment>username_0: Are the symlinks working correctly for you? browserify does not work for me locally to create the bundle.js as you did. I'm not sure why your results are so different from my local tests and travis-ci.\n<issue_comment>username_0: It's also worth noting, that even with two versions of stock flux, the benchmark for the item that is tested first is usually ~5% lower than the second which is why I moved \"optimized\" as the first test case. Consistency seems to be a consistent problem =/\n<issue_comment>username_0: Here's travis with 10 runs of bench-chat: https://travis-ci.org/username_0/react-perf/builds/60620846\n<issue_comment>username_0: So it looks like I didn't have the node_modules for each of the apps pushed, so the components may have been using the wrong React. I have pushed the node_modules folder for each app and used symlinks to the react versions such that they are completely isolated. I'm hoping this will make the tests more consistent for you.\r\n\r\nhttps://github.com/username_0/react-perf/tree/inlineMountChildren\n<issue_comment>username_0: Closing this since it undoes a purposeful refactor. I will open a new PR to discuss inlining just `ReactChildReconciler.instantiateChildren`."}
{"repo": "facebook/react", "issue_id": 69988869, "issue_number": 3717, "timestamp": "2015-04-27 23:44:13+00:00", "text": "@username_2 Can you elaborate one what you mean by \"internal state tree\"? I'm trying to understand what could be removed from the rendering path to optimize server rendering. I have a feeling some of the `mountComponent` logic needs to be split up into rendering logic and other logic (event registration, state management, etc.), but I'm not familiar enough with the \"other\" logic that's happening to know what to look at.", "context": "<issue_start><issue_comment>Title: [performance] Inline ReactMultiChild.mountChildren\nusername_0: I found that `ReactMultiChild.mountChildren` and the functions it calls were iterating over the children many times. This inlines those calls as it is a critical rendering path. Through testing a chat example I have found about a 10-15% performance gains through benchmark.js.\r\n\r\nTest: https://github.com/username_0/react-perf/tree/inlineMountChildren\r\nTest results: https://travis-ci.org/username_0/react-perf/builds/59480548\n<issue_comment>username_1: For reference, this separation was introduced intentionally in this PR:\r\n\r\nhttps://github.com/facebook/react/pull/2567\r\n\r\ncc @username_2\n<issue_comment>username_0: I seem to have broken the test right before pushing. Will investigate.\n<issue_comment>username_2: Yea, this was intentionally separated, knowing that it is a slight performance loss for now. It is a part of an on-going refactor to separate instantiation and resolution from the render strategy. That way we can reuse core React logic while having the option to optimize for different render outputs (DOM, ART, React Native, etc). It is also easier to reason about fragments (components that render multiple elements) if we can diff the resolved set.\r\n\r\nThe idea is to move the instantiation out of the DOM component.\r\n\r\nIt is likely we can remerge this kind of optimization later as more of this refactor starts coming together but I think I'd rather keep it separate for now so that we can incrementally continue the refactor instead of having to make a complete reimplementation from scratch.\r\n\r\nThis brings up an interesting point though. For renderToString, we don't really need to keep any of the internal state tree which is only used for optimizing updates. Maybe we need a whole optimized pipeline that doesn't generate the internal memoization nor the state tree?\n<issue_comment>username_0: :+1: totally agree @username_2. I think that would also help with some of the GC issues we're seeing with higher throughput in node. If you can point me in the right direction for where to start with this, I'd be more than glad to work on it.\n<issue_comment>username_0: Looks like there's still a significant gain if we inline only `ReactChildReconciler.instantiateChildren`:\r\n\r\nChanges: https://github.com/facebook/react/compare/master...username_0:inlineInstantiateChildren\r\nTest: https://github.com/username_0/react-perf/tree/inlineInstantiateChildren\r\nResult: https://travis-ci.org/username_0/react-perf/builds/59784288\n<issue_comment>username_1: Nice find. Maybe we can take advantage of ReactChildren.map here? We may want a version which skips the ReactFragment.create call for these internals but otherwise I think it should give the right effect.\n<issue_comment>username_0: I pulled the traversal function out into a named functioned rather than inline in the branch above. I will run a comparison of using `ReactChildren.map` vs `traverseAllChildren` directly.\n<issue_comment>username_0: As you said I'm running into a lot of error logs due to `ReactFragment.create` being called with nulls and other invalid arguments. I think sticking with `traverseAllChildren` in this case will be better.\n<issue_comment>username_1: Sorry, I may have been a little unclear. Can you do something like this\r\n\r\n```\r\ndiff --git a/src/utils/ReactChildren.js b/src/utils/ReactChildren.js\r\nindex e470609..84ca357 100644\r\n--- a/src/utils/ReactChildren.js\r\n+++ b/src/utils/ReactChildren.js\r\n@@ -122,7 +122,7 @@ function mapChildren(children, func, context) {\r\n   var traverseContext = MapBookKeeping.getPooled(mapResult, func, context);\r\n   traverseAllChildren(children, mapSingleChildIntoContext, traverseContext);\r\n   MapBookKeeping.release(traverseContext);\r\n-  return ReactFragment.create(mapResult);\r\n+  return mapResult;\r\n }\r\n \r\n function forEachSingleChildDummy(traverseContext, child, name, i) {\r\n@@ -142,7 +142,10 @@ function countChildren(children, context) {\r\n \r\n var ReactChildren = {\r\n   forEach: forEachChildren,\r\n-  map: mapChildren,\r\n+  map: function(children, func, context) {\r\n+    return ReactFragment.create(mapChildren(children, func, context));\r\n+  },\r\n+  mapObject: mapChildren,\r\n   count: countChildren\r\n };\r\n \r\n```\r\n\r\nand then use ReactChildren.mapObject?\n<issue_comment>username_0: Ok, so I create a branch that uses ReactChildren.mapObject: https://github.com/facebook/react/compare/master...username_0:inlineInstantiateChildrenWithMap\r\n\r\nThere are two tests that fail using this method:\r\n\r\n * core/__tests__/ReactMultiChildReconcile-test ReactMultiChildReconcile should insert non-empty children in middle where nulls were.\r\n * core/__tests__/ReactMultiChildText-test ReactMultiChildText should correctly handle all possible children for render and update.\r\n\r\nI believe this is because map doesn't handle skipping nulls.\r\n\r\nI also ran a benchmark for using `traverseAllChilden` as my per previous comment vs. using `ReactChildren.mapObject` as @username_1 suggested. Running locally, I'm seeing a significant overhead in using the pooling added by `ReactChildren.mapObject`. On some runs it shows to be less performant than not inlining. You can see the travis-ci run of the benchmark here: https://travis-ci.org/username_0/react-perf/builds/60283471\n<issue_comment>username_0: @username_2 Can you elaborate one what you mean by \"internal state tree\"? I'm trying to understand what could be removed from the rendering path to optimize server rendering. I have a feeling some of the `mountComponent` logic needs to be split up into rendering logic and other logic (event registration, state management, etc.), but I'm not familiar enough with the \"other\" logic that's happening to know what to look at.\n<issue_comment>username_1: @username_0 Interesting. If inlining a closure (like https://gist.github.com/username_1/594910785229dba07d5c) is faster we should probably switch to that \u2013 we've tried to avoid closures in favor of manually passing context objects in hopes of lowering GC pressure and improving perf but all of this feels like dogscience.\r\n\r\n![image](https://cloud.githubusercontent.com/assets/6820/7360334/68f64cd2-ecfe-11e4-8e6e-b85703c9a2de.png)\n<issue_comment>username_1: Sorry for all the back and forth.\n<issue_comment>username_0: No worries. I too often feel like I have no idea what I'm doing.\r\n\r\nI think using `traverseAllChildren` directly as in https://github.com/facebook/react/compare/master...username_0:inlineInstantiateChildren addresses your concern about GC although it does duplicate some of the code in `ReactChildren.map`. We do need [this check](https://github.com/facebook/react/compare/master...username_0:inlineInstantiateChildren#diff-89f33512519fa4d853463913eac8ef04R24) though in order for the tests to pass. Any ideas on introducing that check in to `ReactChildren.map`?\n<issue_comment>username_1: I tried to replicate these results locally and in different browsers. Here's what I did:\r\n\r\n```sh\r\n$ git checkout origin/inlineMountChildren\r\n$ npm install -g browserify\r\n$ npm install babelify envify\r\n$ NODE_ENV=production browserify -t babelify bench-chat.js >bundle.js\r\n```\r\n\r\nAnd I created bench.html:\r\n\r\n```html\r\n<meta charset=\"utf-8\">\r\n<body style=\"font: 10px Menlo; white-space: pre;\">\r\n<div id=\"container\"></div>\r\n<script>\r\nconsole.log = function(x) {\r\n  container.textContent += x.replace(/\\033\\[[0-9;]*m/g, '') + '\\n';\r\n};\r\n</script>\r\n<script src=\"bundle.js\"></script>\r\n```\r\n\r\nOpening bench.html in Safari (JavaScriptCore) gave me:\r\n\r\n```\r\nRenderChat for inputs Optimized x 2,094 ops/sec \u00b10.85% (66 runs sampled)\r\nRenderChat for inputs Stock x 2,081 ops/sec \u00b10.78% (66 runs sampled)\r\nThe Fastest test suite is RenderChat for inputs Optimized,RenderChat for inputs Stock\r\n\r\n+------------+---------------+---------------+\r\n|            \u00e2\u201d\u201a Optimized     \u00e2\u201d\u201a Stock         |\r\n+------------+---------------+---------------+\r\n| RenderChat \u00e2\u201d\u201a 2,094 ops/sec \u00e2\u201d\u201a 2,081 ops/sec |\r\n+------------+---------------+---------------+\r\n```\r\n\r\nwhere they're the same speed (not sure what's up with the encoding there), and in Chrome I got:\r\n\r\n```\r\nRenderChat for inputs Optimized x 1,534 ops/sec \u00b12.38% (61 runs sampled)\r\nRenderChat for inputs Stock x 1,775 ops/sec \u00b11.56% (65 runs sampled)\r\nThe Fastest test suite is RenderChat for inputs Stock\r\n\r\n+------------+---------------+---------------+\r\n|            \u2502 Optimized     \u2502 Stock         |\r\n+------------+---------------+---------------+\r\n| RenderChat \u2502 1,534 ops/sec \u2502 1,775 ops/sec |\r\n+------------+---------------+---------------+\r\n```\r\n\r\nwhere the old version is 15% faster. Running `node bench-chat.js` gave me\r\n\r\n```\r\nRenderChat for inputs Optimized x 368 ops/sec \u00b13.18% (73 runs sampled)\r\nRenderChat for inputs Stock x 348 ops/sec \u00b12.99% (72 runs sampled)\r\nThe Fastest test suite is RenderChat for inputs Optimized\r\n\r\n+------------+-------------+-------------+\r\n|            \u2502 Optimized   \u2502 Stock       |\r\n+------------+-------------+-------------+\r\n| RenderChat \u2502 368 ops/sec \u2502 348 ops/sec |\r\n+------------+-------------+-------------+\r\n```\r\n\r\nwhere your version is maybe 5% faster, but not the 10\u201315% you were seeing originally. Any ideas if I'm doing something wrong or if this is just wildly different between engines? I'd have expected Chrome and Node to be more similar, at the very least.\n<issue_comment>username_0: Are the symlinks working correctly for you? browserify does not work for me locally to create the bundle.js as you did. I'm not sure why your results are so different from my local tests and travis-ci.\n<issue_comment>username_0: It's also worth noting, that even with two versions of stock flux, the benchmark for the item that is tested first is usually ~5% lower than the second which is why I moved \"optimized\" as the first test case. Consistency seems to be a consistent problem =/\n<issue_comment>username_0: Here's travis with 10 runs of bench-chat: https://travis-ci.org/username_0/react-perf/builds/60620846\n<issue_comment>username_0: So it looks like I didn't have the node_modules for each of the apps pushed, so the components may have been using the wrong React. I have pushed the node_modules folder for each app and used symlinks to the react versions such that they are completely isolated. I'm hoping this will make the tests more consistent for you.\r\n\r\nhttps://github.com/username_0/react-perf/tree/inlineMountChildren\n<issue_comment>username_0: Closing this since it undoes a purposeful refactor. I will open a new PR to discuss inlining just `ReactChildReconciler.instantiateChildren`."}
{"repo": "facebook/react", "issue_id": 69988869, "issue_number": 3717, "timestamp": "2015-04-27 23:57:00+00:00", "text": "@username_0 Interesting. If inlining a closure (like https://gist.github.com/username_1/594910785229dba07d5c) is faster we should probably switch to that \u2013 we've tried to avoid closures in favor of manually passing context objects in hopes of lowering GC pressure and improving perf but all of this feels like dogscience.\r\n\r\n![image](https://cloud.githubusercontent.com/assets/6820/7360334/68f64cd2-ecfe-11e4-8e6e-b85703c9a2de.png)", "context": "<issue_start><issue_comment>Title: [performance] Inline ReactMultiChild.mountChildren\nusername_0: I found that `ReactMultiChild.mountChildren` and the functions it calls were iterating over the children many times. This inlines those calls as it is a critical rendering path. Through testing a chat example I have found about a 10-15% performance gains through benchmark.js.\r\n\r\nTest: https://github.com/username_0/react-perf/tree/inlineMountChildren\r\nTest results: https://travis-ci.org/username_0/react-perf/builds/59480548\n<issue_comment>username_1: For reference, this separation was introduced intentionally in this PR:\r\n\r\nhttps://github.com/facebook/react/pull/2567\r\n\r\ncc @username_2\n<issue_comment>username_0: I seem to have broken the test right before pushing. Will investigate.\n<issue_comment>username_2: Yea, this was intentionally separated, knowing that it is a slight performance loss for now. It is a part of an on-going refactor to separate instantiation and resolution from the render strategy. That way we can reuse core React logic while having the option to optimize for different render outputs (DOM, ART, React Native, etc). It is also easier to reason about fragments (components that render multiple elements) if we can diff the resolved set.\r\n\r\nThe idea is to move the instantiation out of the DOM component.\r\n\r\nIt is likely we can remerge this kind of optimization later as more of this refactor starts coming together but I think I'd rather keep it separate for now so that we can incrementally continue the refactor instead of having to make a complete reimplementation from scratch.\r\n\r\nThis brings up an interesting point though. For renderToString, we don't really need to keep any of the internal state tree which is only used for optimizing updates. Maybe we need a whole optimized pipeline that doesn't generate the internal memoization nor the state tree?\n<issue_comment>username_0: :+1: totally agree @username_2. I think that would also help with some of the GC issues we're seeing with higher throughput in node. If you can point me in the right direction for where to start with this, I'd be more than glad to work on it.\n<issue_comment>username_0: Looks like there's still a significant gain if we inline only `ReactChildReconciler.instantiateChildren`:\r\n\r\nChanges: https://github.com/facebook/react/compare/master...username_0:inlineInstantiateChildren\r\nTest: https://github.com/username_0/react-perf/tree/inlineInstantiateChildren\r\nResult: https://travis-ci.org/username_0/react-perf/builds/59784288\n<issue_comment>username_1: Nice find. Maybe we can take advantage of ReactChildren.map here? We may want a version which skips the ReactFragment.create call for these internals but otherwise I think it should give the right effect.\n<issue_comment>username_0: I pulled the traversal function out into a named functioned rather than inline in the branch above. I will run a comparison of using `ReactChildren.map` vs `traverseAllChildren` directly.\n<issue_comment>username_0: As you said I'm running into a lot of error logs due to `ReactFragment.create` being called with nulls and other invalid arguments. I think sticking with `traverseAllChildren` in this case will be better.\n<issue_comment>username_1: Sorry, I may have been a little unclear. Can you do something like this\r\n\r\n```\r\ndiff --git a/src/utils/ReactChildren.js b/src/utils/ReactChildren.js\r\nindex e470609..84ca357 100644\r\n--- a/src/utils/ReactChildren.js\r\n+++ b/src/utils/ReactChildren.js\r\n@@ -122,7 +122,7 @@ function mapChildren(children, func, context) {\r\n   var traverseContext = MapBookKeeping.getPooled(mapResult, func, context);\r\n   traverseAllChildren(children, mapSingleChildIntoContext, traverseContext);\r\n   MapBookKeeping.release(traverseContext);\r\n-  return ReactFragment.create(mapResult);\r\n+  return mapResult;\r\n }\r\n \r\n function forEachSingleChildDummy(traverseContext, child, name, i) {\r\n@@ -142,7 +142,10 @@ function countChildren(children, context) {\r\n \r\n var ReactChildren = {\r\n   forEach: forEachChildren,\r\n-  map: mapChildren,\r\n+  map: function(children, func, context) {\r\n+    return ReactFragment.create(mapChildren(children, func, context));\r\n+  },\r\n+  mapObject: mapChildren,\r\n   count: countChildren\r\n };\r\n \r\n```\r\n\r\nand then use ReactChildren.mapObject?\n<issue_comment>username_0: Ok, so I create a branch that uses ReactChildren.mapObject: https://github.com/facebook/react/compare/master...username_0:inlineInstantiateChildrenWithMap\r\n\r\nThere are two tests that fail using this method:\r\n\r\n * core/__tests__/ReactMultiChildReconcile-test ReactMultiChildReconcile should insert non-empty children in middle where nulls were.\r\n * core/__tests__/ReactMultiChildText-test ReactMultiChildText should correctly handle all possible children for render and update.\r\n\r\nI believe this is because map doesn't handle skipping nulls.\r\n\r\nI also ran a benchmark for using `traverseAllChilden` as my per previous comment vs. using `ReactChildren.mapObject` as @username_1 suggested. Running locally, I'm seeing a significant overhead in using the pooling added by `ReactChildren.mapObject`. On some runs it shows to be less performant than not inlining. You can see the travis-ci run of the benchmark here: https://travis-ci.org/username_0/react-perf/builds/60283471\n<issue_comment>username_0: @username_2 Can you elaborate one what you mean by \"internal state tree\"? I'm trying to understand what could be removed from the rendering path to optimize server rendering. I have a feeling some of the `mountComponent` logic needs to be split up into rendering logic and other logic (event registration, state management, etc.), but I'm not familiar enough with the \"other\" logic that's happening to know what to look at.\n<issue_comment>username_1: @username_0 Interesting. If inlining a closure (like https://gist.github.com/username_1/594910785229dba07d5c) is faster we should probably switch to that \u2013 we've tried to avoid closures in favor of manually passing context objects in hopes of lowering GC pressure and improving perf but all of this feels like dogscience.\r\n\r\n![image](https://cloud.githubusercontent.com/assets/6820/7360334/68f64cd2-ecfe-11e4-8e6e-b85703c9a2de.png)\n<issue_comment>username_1: Sorry for all the back and forth.\n<issue_comment>username_0: No worries. I too often feel like I have no idea what I'm doing.\r\n\r\nI think using `traverseAllChildren` directly as in https://github.com/facebook/react/compare/master...username_0:inlineInstantiateChildren addresses your concern about GC although it does duplicate some of the code in `ReactChildren.map`. We do need [this check](https://github.com/facebook/react/compare/master...username_0:inlineInstantiateChildren#diff-89f33512519fa4d853463913eac8ef04R24) though in order for the tests to pass. Any ideas on introducing that check in to `ReactChildren.map`?\n<issue_comment>username_1: I tried to replicate these results locally and in different browsers. Here's what I did:\r\n\r\n```sh\r\n$ git checkout origin/inlineMountChildren\r\n$ npm install -g browserify\r\n$ npm install babelify envify\r\n$ NODE_ENV=production browserify -t babelify bench-chat.js >bundle.js\r\n```\r\n\r\nAnd I created bench.html:\r\n\r\n```html\r\n<meta charset=\"utf-8\">\r\n<body style=\"font: 10px Menlo; white-space: pre;\">\r\n<div id=\"container\"></div>\r\n<script>\r\nconsole.log = function(x) {\r\n  container.textContent += x.replace(/\\033\\[[0-9;]*m/g, '') + '\\n';\r\n};\r\n</script>\r\n<script src=\"bundle.js\"></script>\r\n```\r\n\r\nOpening bench.html in Safari (JavaScriptCore) gave me:\r\n\r\n```\r\nRenderChat for inputs Optimized x 2,094 ops/sec \u00b10.85% (66 runs sampled)\r\nRenderChat for inputs Stock x 2,081 ops/sec \u00b10.78% (66 runs sampled)\r\nThe Fastest test suite is RenderChat for inputs Optimized,RenderChat for inputs Stock\r\n\r\n+------------+---------------+---------------+\r\n|            \u00e2\u201d\u201a Optimized     \u00e2\u201d\u201a Stock         |\r\n+------------+---------------+---------------+\r\n| RenderChat \u00e2\u201d\u201a 2,094 ops/sec \u00e2\u201d\u201a 2,081 ops/sec |\r\n+------------+---------------+---------------+\r\n```\r\n\r\nwhere they're the same speed (not sure what's up with the encoding there), and in Chrome I got:\r\n\r\n```\r\nRenderChat for inputs Optimized x 1,534 ops/sec \u00b12.38% (61 runs sampled)\r\nRenderChat for inputs Stock x 1,775 ops/sec \u00b11.56% (65 runs sampled)\r\nThe Fastest test suite is RenderChat for inputs Stock\r\n\r\n+------------+---------------+---------------+\r\n|            \u2502 Optimized     \u2502 Stock         |\r\n+------------+---------------+---------------+\r\n| RenderChat \u2502 1,534 ops/sec \u2502 1,775 ops/sec |\r\n+------------+---------------+---------------+\r\n```\r\n\r\nwhere the old version is 15% faster. Running `node bench-chat.js` gave me\r\n\r\n```\r\nRenderChat for inputs Optimized x 368 ops/sec \u00b13.18% (73 runs sampled)\r\nRenderChat for inputs Stock x 348 ops/sec \u00b12.99% (72 runs sampled)\r\nThe Fastest test suite is RenderChat for inputs Optimized\r\n\r\n+------------+-------------+-------------+\r\n|            \u2502 Optimized   \u2502 Stock       |\r\n+------------+-------------+-------------+\r\n| RenderChat \u2502 368 ops/sec \u2502 348 ops/sec |\r\n+------------+-------------+-------------+\r\n```\r\n\r\nwhere your version is maybe 5% faster, but not the 10\u201315% you were seeing originally. Any ideas if I'm doing something wrong or if this is just wildly different between engines? I'd have expected Chrome and Node to be more similar, at the very least.\n<issue_comment>username_0: Are the symlinks working correctly for you? browserify does not work for me locally to create the bundle.js as you did. I'm not sure why your results are so different from my local tests and travis-ci.\n<issue_comment>username_0: It's also worth noting, that even with two versions of stock flux, the benchmark for the item that is tested first is usually ~5% lower than the second which is why I moved \"optimized\" as the first test case. Consistency seems to be a consistent problem =/\n<issue_comment>username_0: Here's travis with 10 runs of bench-chat: https://travis-ci.org/username_0/react-perf/builds/60620846\n<issue_comment>username_0: So it looks like I didn't have the node_modules for each of the apps pushed, so the components may have been using the wrong React. I have pushed the node_modules folder for each app and used symlinks to the react versions such that they are completely isolated. I'm hoping this will make the tests more consistent for you.\r\n\r\nhttps://github.com/username_0/react-perf/tree/inlineMountChildren\n<issue_comment>username_0: Closing this since it undoes a purposeful refactor. I will open a new PR to discuss inlining just `ReactChildReconciler.instantiateChildren`."}
{"repo": "facebook/react", "issue_id": 69988869, "issue_number": 3717, "timestamp": "2015-04-27 23:58:07+00:00", "text": "Sorry for all the back and forth.", "context": "<issue_start><issue_comment>Title: [performance] Inline ReactMultiChild.mountChildren\nusername_0: I found that `ReactMultiChild.mountChildren` and the functions it calls were iterating over the children many times. This inlines those calls as it is a critical rendering path. Through testing a chat example I have found about a 10-15% performance gains through benchmark.js.\r\n\r\nTest: https://github.com/username_0/react-perf/tree/inlineMountChildren\r\nTest results: https://travis-ci.org/username_0/react-perf/builds/59480548\n<issue_comment>username_1: For reference, this separation was introduced intentionally in this PR:\r\n\r\nhttps://github.com/facebook/react/pull/2567\r\n\r\ncc @username_2\n<issue_comment>username_0: I seem to have broken the test right before pushing. Will investigate.\n<issue_comment>username_2: Yea, this was intentionally separated, knowing that it is a slight performance loss for now. It is a part of an on-going refactor to separate instantiation and resolution from the render strategy. That way we can reuse core React logic while having the option to optimize for different render outputs (DOM, ART, React Native, etc). It is also easier to reason about fragments (components that render multiple elements) if we can diff the resolved set.\r\n\r\nThe idea is to move the instantiation out of the DOM component.\r\n\r\nIt is likely we can remerge this kind of optimization later as more of this refactor starts coming together but I think I'd rather keep it separate for now so that we can incrementally continue the refactor instead of having to make a complete reimplementation from scratch.\r\n\r\nThis brings up an interesting point though. For renderToString, we don't really need to keep any of the internal state tree which is only used for optimizing updates. Maybe we need a whole optimized pipeline that doesn't generate the internal memoization nor the state tree?\n<issue_comment>username_0: :+1: totally agree @username_2. I think that would also help with some of the GC issues we're seeing with higher throughput in node. If you can point me in the right direction for where to start with this, I'd be more than glad to work on it.\n<issue_comment>username_0: Looks like there's still a significant gain if we inline only `ReactChildReconciler.instantiateChildren`:\r\n\r\nChanges: https://github.com/facebook/react/compare/master...username_0:inlineInstantiateChildren\r\nTest: https://github.com/username_0/react-perf/tree/inlineInstantiateChildren\r\nResult: https://travis-ci.org/username_0/react-perf/builds/59784288\n<issue_comment>username_1: Nice find. Maybe we can take advantage of ReactChildren.map here? We may want a version which skips the ReactFragment.create call for these internals but otherwise I think it should give the right effect.\n<issue_comment>username_0: I pulled the traversal function out into a named functioned rather than inline in the branch above. I will run a comparison of using `ReactChildren.map` vs `traverseAllChildren` directly.\n<issue_comment>username_0: As you said I'm running into a lot of error logs due to `ReactFragment.create` being called with nulls and other invalid arguments. I think sticking with `traverseAllChildren` in this case will be better.\n<issue_comment>username_1: Sorry, I may have been a little unclear. Can you do something like this\r\n\r\n```\r\ndiff --git a/src/utils/ReactChildren.js b/src/utils/ReactChildren.js\r\nindex e470609..84ca357 100644\r\n--- a/src/utils/ReactChildren.js\r\n+++ b/src/utils/ReactChildren.js\r\n@@ -122,7 +122,7 @@ function mapChildren(children, func, context) {\r\n   var traverseContext = MapBookKeeping.getPooled(mapResult, func, context);\r\n   traverseAllChildren(children, mapSingleChildIntoContext, traverseContext);\r\n   MapBookKeeping.release(traverseContext);\r\n-  return ReactFragment.create(mapResult);\r\n+  return mapResult;\r\n }\r\n \r\n function forEachSingleChildDummy(traverseContext, child, name, i) {\r\n@@ -142,7 +142,10 @@ function countChildren(children, context) {\r\n \r\n var ReactChildren = {\r\n   forEach: forEachChildren,\r\n-  map: mapChildren,\r\n+  map: function(children, func, context) {\r\n+    return ReactFragment.create(mapChildren(children, func, context));\r\n+  },\r\n+  mapObject: mapChildren,\r\n   count: countChildren\r\n };\r\n \r\n```\r\n\r\nand then use ReactChildren.mapObject?\n<issue_comment>username_0: Ok, so I create a branch that uses ReactChildren.mapObject: https://github.com/facebook/react/compare/master...username_0:inlineInstantiateChildrenWithMap\r\n\r\nThere are two tests that fail using this method:\r\n\r\n * core/__tests__/ReactMultiChildReconcile-test ReactMultiChildReconcile should insert non-empty children in middle where nulls were.\r\n * core/__tests__/ReactMultiChildText-test ReactMultiChildText should correctly handle all possible children for render and update.\r\n\r\nI believe this is because map doesn't handle skipping nulls.\r\n\r\nI also ran a benchmark for using `traverseAllChilden` as my per previous comment vs. using `ReactChildren.mapObject` as @username_1 suggested. Running locally, I'm seeing a significant overhead in using the pooling added by `ReactChildren.mapObject`. On some runs it shows to be less performant than not inlining. You can see the travis-ci run of the benchmark here: https://travis-ci.org/username_0/react-perf/builds/60283471\n<issue_comment>username_0: @username_2 Can you elaborate one what you mean by \"internal state tree\"? I'm trying to understand what could be removed from the rendering path to optimize server rendering. I have a feeling some of the `mountComponent` logic needs to be split up into rendering logic and other logic (event registration, state management, etc.), but I'm not familiar enough with the \"other\" logic that's happening to know what to look at.\n<issue_comment>username_1: @username_0 Interesting. If inlining a closure (like https://gist.github.com/username_1/594910785229dba07d5c) is faster we should probably switch to that \u2013 we've tried to avoid closures in favor of manually passing context objects in hopes of lowering GC pressure and improving perf but all of this feels like dogscience.\r\n\r\n![image](https://cloud.githubusercontent.com/assets/6820/7360334/68f64cd2-ecfe-11e4-8e6e-b85703c9a2de.png)\n<issue_comment>username_1: Sorry for all the back and forth.\n<issue_comment>username_0: No worries. I too often feel like I have no idea what I'm doing.\r\n\r\nI think using `traverseAllChildren` directly as in https://github.com/facebook/react/compare/master...username_0:inlineInstantiateChildren addresses your concern about GC although it does duplicate some of the code in `ReactChildren.map`. We do need [this check](https://github.com/facebook/react/compare/master...username_0:inlineInstantiateChildren#diff-89f33512519fa4d853463913eac8ef04R24) though in order for the tests to pass. Any ideas on introducing that check in to `ReactChildren.map`?\n<issue_comment>username_1: I tried to replicate these results locally and in different browsers. Here's what I did:\r\n\r\n```sh\r\n$ git checkout origin/inlineMountChildren\r\n$ npm install -g browserify\r\n$ npm install babelify envify\r\n$ NODE_ENV=production browserify -t babelify bench-chat.js >bundle.js\r\n```\r\n\r\nAnd I created bench.html:\r\n\r\n```html\r\n<meta charset=\"utf-8\">\r\n<body style=\"font: 10px Menlo; white-space: pre;\">\r\n<div id=\"container\"></div>\r\n<script>\r\nconsole.log = function(x) {\r\n  container.textContent += x.replace(/\\033\\[[0-9;]*m/g, '') + '\\n';\r\n};\r\n</script>\r\n<script src=\"bundle.js\"></script>\r\n```\r\n\r\nOpening bench.html in Safari (JavaScriptCore) gave me:\r\n\r\n```\r\nRenderChat for inputs Optimized x 2,094 ops/sec \u00b10.85% (66 runs sampled)\r\nRenderChat for inputs Stock x 2,081 ops/sec \u00b10.78% (66 runs sampled)\r\nThe Fastest test suite is RenderChat for inputs Optimized,RenderChat for inputs Stock\r\n\r\n+------------+---------------+---------------+\r\n|            \u00e2\u201d\u201a Optimized     \u00e2\u201d\u201a Stock         |\r\n+------------+---------------+---------------+\r\n| RenderChat \u00e2\u201d\u201a 2,094 ops/sec \u00e2\u201d\u201a 2,081 ops/sec |\r\n+------------+---------------+---------------+\r\n```\r\n\r\nwhere they're the same speed (not sure what's up with the encoding there), and in Chrome I got:\r\n\r\n```\r\nRenderChat for inputs Optimized x 1,534 ops/sec \u00b12.38% (61 runs sampled)\r\nRenderChat for inputs Stock x 1,775 ops/sec \u00b11.56% (65 runs sampled)\r\nThe Fastest test suite is RenderChat for inputs Stock\r\n\r\n+------------+---------------+---------------+\r\n|            \u2502 Optimized     \u2502 Stock         |\r\n+------------+---------------+---------------+\r\n| RenderChat \u2502 1,534 ops/sec \u2502 1,775 ops/sec |\r\n+------------+---------------+---------------+\r\n```\r\n\r\nwhere the old version is 15% faster. Running `node bench-chat.js` gave me\r\n\r\n```\r\nRenderChat for inputs Optimized x 368 ops/sec \u00b13.18% (73 runs sampled)\r\nRenderChat for inputs Stock x 348 ops/sec \u00b12.99% (72 runs sampled)\r\nThe Fastest test suite is RenderChat for inputs Optimized\r\n\r\n+------------+-------------+-------------+\r\n|            \u2502 Optimized   \u2502 Stock       |\r\n+------------+-------------+-------------+\r\n| RenderChat \u2502 368 ops/sec \u2502 348 ops/sec |\r\n+------------+-------------+-------------+\r\n```\r\n\r\nwhere your version is maybe 5% faster, but not the 10\u201315% you were seeing originally. Any ideas if I'm doing something wrong or if this is just wildly different between engines? I'd have expected Chrome and Node to be more similar, at the very least.\n<issue_comment>username_0: Are the symlinks working correctly for you? browserify does not work for me locally to create the bundle.js as you did. I'm not sure why your results are so different from my local tests and travis-ci.\n<issue_comment>username_0: It's also worth noting, that even with two versions of stock flux, the benchmark for the item that is tested first is usually ~5% lower than the second which is why I moved \"optimized\" as the first test case. Consistency seems to be a consistent problem =/\n<issue_comment>username_0: Here's travis with 10 runs of bench-chat: https://travis-ci.org/username_0/react-perf/builds/60620846\n<issue_comment>username_0: So it looks like I didn't have the node_modules for each of the apps pushed, so the components may have been using the wrong React. I have pushed the node_modules folder for each app and used symlinks to the react versions such that they are completely isolated. I'm hoping this will make the tests more consistent for you.\r\n\r\nhttps://github.com/username_0/react-perf/tree/inlineMountChildren\n<issue_comment>username_0: Closing this since it undoes a purposeful refactor. I will open a new PR to discuss inlining just `ReactChildReconciler.instantiateChildren`."}
{"repo": "facebook/react", "issue_id": 69988869, "issue_number": 3717, "timestamp": "2015-04-28 00:29:51+00:00", "text": "No worries. I too often feel like I have no idea what I'm doing.\r\n\r\nI think using `traverseAllChildren` directly as in https://github.com/facebook/react/compare/master...username_0:inlineInstantiateChildren addresses your concern about GC although it does duplicate some of the code in `ReactChildren.map`. We do need [this check](https://github.com/facebook/react/compare/master...username_0:inlineInstantiateChildren#diff-89f33512519fa4d853463913eac8ef04R24) though in order for the tests to pass. Any ideas on introducing that check in to `ReactChildren.map`?", "context": "<issue_start><issue_comment>Title: [performance] Inline ReactMultiChild.mountChildren\nusername_0: I found that `ReactMultiChild.mountChildren` and the functions it calls were iterating over the children many times. This inlines those calls as it is a critical rendering path. Through testing a chat example I have found about a 10-15% performance gains through benchmark.js.\r\n\r\nTest: https://github.com/username_0/react-perf/tree/inlineMountChildren\r\nTest results: https://travis-ci.org/username_0/react-perf/builds/59480548\n<issue_comment>username_1: For reference, this separation was introduced intentionally in this PR:\r\n\r\nhttps://github.com/facebook/react/pull/2567\r\n\r\ncc @username_2\n<issue_comment>username_0: I seem to have broken the test right before pushing. Will investigate.\n<issue_comment>username_2: Yea, this was intentionally separated, knowing that it is a slight performance loss for now. It is a part of an on-going refactor to separate instantiation and resolution from the render strategy. That way we can reuse core React logic while having the option to optimize for different render outputs (DOM, ART, React Native, etc). It is also easier to reason about fragments (components that render multiple elements) if we can diff the resolved set.\r\n\r\nThe idea is to move the instantiation out of the DOM component.\r\n\r\nIt is likely we can remerge this kind of optimization later as more of this refactor starts coming together but I think I'd rather keep it separate for now so that we can incrementally continue the refactor instead of having to make a complete reimplementation from scratch.\r\n\r\nThis brings up an interesting point though. For renderToString, we don't really need to keep any of the internal state tree which is only used for optimizing updates. Maybe we need a whole optimized pipeline that doesn't generate the internal memoization nor the state tree?\n<issue_comment>username_0: :+1: totally agree @username_2. I think that would also help with some of the GC issues we're seeing with higher throughput in node. If you can point me in the right direction for where to start with this, I'd be more than glad to work on it.\n<issue_comment>username_0: Looks like there's still a significant gain if we inline only `ReactChildReconciler.instantiateChildren`:\r\n\r\nChanges: https://github.com/facebook/react/compare/master...username_0:inlineInstantiateChildren\r\nTest: https://github.com/username_0/react-perf/tree/inlineInstantiateChildren\r\nResult: https://travis-ci.org/username_0/react-perf/builds/59784288\n<issue_comment>username_1: Nice find. Maybe we can take advantage of ReactChildren.map here? We may want a version which skips the ReactFragment.create call for these internals but otherwise I think it should give the right effect.\n<issue_comment>username_0: I pulled the traversal function out into a named functioned rather than inline in the branch above. I will run a comparison of using `ReactChildren.map` vs `traverseAllChildren` directly.\n<issue_comment>username_0: As you said I'm running into a lot of error logs due to `ReactFragment.create` being called with nulls and other invalid arguments. I think sticking with `traverseAllChildren` in this case will be better.\n<issue_comment>username_1: Sorry, I may have been a little unclear. Can you do something like this\r\n\r\n```\r\ndiff --git a/src/utils/ReactChildren.js b/src/utils/ReactChildren.js\r\nindex e470609..84ca357 100644\r\n--- a/src/utils/ReactChildren.js\r\n+++ b/src/utils/ReactChildren.js\r\n@@ -122,7 +122,7 @@ function mapChildren(children, func, context) {\r\n   var traverseContext = MapBookKeeping.getPooled(mapResult, func, context);\r\n   traverseAllChildren(children, mapSingleChildIntoContext, traverseContext);\r\n   MapBookKeeping.release(traverseContext);\r\n-  return ReactFragment.create(mapResult);\r\n+  return mapResult;\r\n }\r\n \r\n function forEachSingleChildDummy(traverseContext, child, name, i) {\r\n@@ -142,7 +142,10 @@ function countChildren(children, context) {\r\n \r\n var ReactChildren = {\r\n   forEach: forEachChildren,\r\n-  map: mapChildren,\r\n+  map: function(children, func, context) {\r\n+    return ReactFragment.create(mapChildren(children, func, context));\r\n+  },\r\n+  mapObject: mapChildren,\r\n   count: countChildren\r\n };\r\n \r\n```\r\n\r\nand then use ReactChildren.mapObject?\n<issue_comment>username_0: Ok, so I create a branch that uses ReactChildren.mapObject: https://github.com/facebook/react/compare/master...username_0:inlineInstantiateChildrenWithMap\r\n\r\nThere are two tests that fail using this method:\r\n\r\n * core/__tests__/ReactMultiChildReconcile-test ReactMultiChildReconcile should insert non-empty children in middle where nulls were.\r\n * core/__tests__/ReactMultiChildText-test ReactMultiChildText should correctly handle all possible children for render and update.\r\n\r\nI believe this is because map doesn't handle skipping nulls.\r\n\r\nI also ran a benchmark for using `traverseAllChilden` as my per previous comment vs. using `ReactChildren.mapObject` as @username_1 suggested. Running locally, I'm seeing a significant overhead in using the pooling added by `ReactChildren.mapObject`. On some runs it shows to be less performant than not inlining. You can see the travis-ci run of the benchmark here: https://travis-ci.org/username_0/react-perf/builds/60283471\n<issue_comment>username_0: @username_2 Can you elaborate one what you mean by \"internal state tree\"? I'm trying to understand what could be removed from the rendering path to optimize server rendering. I have a feeling some of the `mountComponent` logic needs to be split up into rendering logic and other logic (event registration, state management, etc.), but I'm not familiar enough with the \"other\" logic that's happening to know what to look at.\n<issue_comment>username_1: @username_0 Interesting. If inlining a closure (like https://gist.github.com/username_1/594910785229dba07d5c) is faster we should probably switch to that \u2013 we've tried to avoid closures in favor of manually passing context objects in hopes of lowering GC pressure and improving perf but all of this feels like dogscience.\r\n\r\n![image](https://cloud.githubusercontent.com/assets/6820/7360334/68f64cd2-ecfe-11e4-8e6e-b85703c9a2de.png)\n<issue_comment>username_1: Sorry for all the back and forth.\n<issue_comment>username_0: No worries. I too often feel like I have no idea what I'm doing.\r\n\r\nI think using `traverseAllChildren` directly as in https://github.com/facebook/react/compare/master...username_0:inlineInstantiateChildren addresses your concern about GC although it does duplicate some of the code in `ReactChildren.map`. We do need [this check](https://github.com/facebook/react/compare/master...username_0:inlineInstantiateChildren#diff-89f33512519fa4d853463913eac8ef04R24) though in order for the tests to pass. Any ideas on introducing that check in to `ReactChildren.map`?\n<issue_comment>username_1: I tried to replicate these results locally and in different browsers. Here's what I did:\r\n\r\n```sh\r\n$ git checkout origin/inlineMountChildren\r\n$ npm install -g browserify\r\n$ npm install babelify envify\r\n$ NODE_ENV=production browserify -t babelify bench-chat.js >bundle.js\r\n```\r\n\r\nAnd I created bench.html:\r\n\r\n```html\r\n<meta charset=\"utf-8\">\r\n<body style=\"font: 10px Menlo; white-space: pre;\">\r\n<div id=\"container\"></div>\r\n<script>\r\nconsole.log = function(x) {\r\n  container.textContent += x.replace(/\\033\\[[0-9;]*m/g, '') + '\\n';\r\n};\r\n</script>\r\n<script src=\"bundle.js\"></script>\r\n```\r\n\r\nOpening bench.html in Safari (JavaScriptCore) gave me:\r\n\r\n```\r\nRenderChat for inputs Optimized x 2,094 ops/sec \u00b10.85% (66 runs sampled)\r\nRenderChat for inputs Stock x 2,081 ops/sec \u00b10.78% (66 runs sampled)\r\nThe Fastest test suite is RenderChat for inputs Optimized,RenderChat for inputs Stock\r\n\r\n+------------+---------------+---------------+\r\n|            \u00e2\u201d\u201a Optimized     \u00e2\u201d\u201a Stock         |\r\n+------------+---------------+---------------+\r\n| RenderChat \u00e2\u201d\u201a 2,094 ops/sec \u00e2\u201d\u201a 2,081 ops/sec |\r\n+------------+---------------+---------------+\r\n```\r\n\r\nwhere they're the same speed (not sure what's up with the encoding there), and in Chrome I got:\r\n\r\n```\r\nRenderChat for inputs Optimized x 1,534 ops/sec \u00b12.38% (61 runs sampled)\r\nRenderChat for inputs Stock x 1,775 ops/sec \u00b11.56% (65 runs sampled)\r\nThe Fastest test suite is RenderChat for inputs Stock\r\n\r\n+------------+---------------+---------------+\r\n|            \u2502 Optimized     \u2502 Stock         |\r\n+------------+---------------+---------------+\r\n| RenderChat \u2502 1,534 ops/sec \u2502 1,775 ops/sec |\r\n+------------+---------------+---------------+\r\n```\r\n\r\nwhere the old version is 15% faster. Running `node bench-chat.js` gave me\r\n\r\n```\r\nRenderChat for inputs Optimized x 368 ops/sec \u00b13.18% (73 runs sampled)\r\nRenderChat for inputs Stock x 348 ops/sec \u00b12.99% (72 runs sampled)\r\nThe Fastest test suite is RenderChat for inputs Optimized\r\n\r\n+------------+-------------+-------------+\r\n|            \u2502 Optimized   \u2502 Stock       |\r\n+------------+-------------+-------------+\r\n| RenderChat \u2502 368 ops/sec \u2502 348 ops/sec |\r\n+------------+-------------+-------------+\r\n```\r\n\r\nwhere your version is maybe 5% faster, but not the 10\u201315% you were seeing originally. Any ideas if I'm doing something wrong or if this is just wildly different between engines? I'd have expected Chrome and Node to be more similar, at the very least.\n<issue_comment>username_0: Are the symlinks working correctly for you? browserify does not work for me locally to create the bundle.js as you did. I'm not sure why your results are so different from my local tests and travis-ci.\n<issue_comment>username_0: It's also worth noting, that even with two versions of stock flux, the benchmark for the item that is tested first is usually ~5% lower than the second which is why I moved \"optimized\" as the first test case. Consistency seems to be a consistent problem =/\n<issue_comment>username_0: Here's travis with 10 runs of bench-chat: https://travis-ci.org/username_0/react-perf/builds/60620846\n<issue_comment>username_0: So it looks like I didn't have the node_modules for each of the apps pushed, so the components may have been using the wrong React. I have pushed the node_modules folder for each app and used symlinks to the react versions such that they are completely isolated. I'm hoping this will make the tests more consistent for you.\r\n\r\nhttps://github.com/username_0/react-perf/tree/inlineMountChildren\n<issue_comment>username_0: Closing this since it undoes a purposeful refactor. I will open a new PR to discuss inlining just `ReactChildReconciler.instantiateChildren`."}
{"repo": "facebook/react", "issue_id": 69988869, "issue_number": 3717, "timestamp": "2015-04-29 21:55:11+00:00", "text": "I tried to replicate these results locally and in different browsers. Here's what I did:\r\n\r\n```sh\r\n$ git checkout origin/inlineMountChildren\r\n$ npm install -g browserify\r\n$ npm install babelify envify\r\n$ NODE_ENV=production browserify -t babelify bench-chat.js >bundle.js\r\n```\r\n\r\nAnd I created bench.html:\r\n\r\n```html\r\n<meta charset=\"utf-8\">\r\n<body style=\"font: 10px Menlo; white-space: pre;\">\r\n<div id=\"container\"></div>\r\n<script>\r\nconsole.log = function(x) {\r\n  container.textContent += x.replace(/\\033\\[[0-9;]*m/g, '') + '\\n';\r\n};\r\n</script>\r\n<script src=\"bundle.js\"></script>\r\n```\r\n\r\nOpening bench.html in Safari (JavaScriptCore) gave me:\r\n\r\n```\r\nRenderChat for inputs Optimized x 2,094 ops/sec \u00b10.85% (66 runs sampled)\r\nRenderChat for inputs Stock x 2,081 ops/sec \u00b10.78% (66 runs sampled)\r\nThe Fastest test suite is RenderChat for inputs Optimized,RenderChat for inputs Stock\r\n\r\n+------------+---------------+---------------+\r\n|            \u00e2\u201d\u201a Optimized     \u00e2\u201d\u201a Stock         |\r\n+------------+---------------+---------------+\r\n| RenderChat \u00e2\u201d\u201a 2,094 ops/sec \u00e2\u201d\u201a 2,081 ops/sec |\r\n+------------+---------------+---------------+\r\n```\r\n\r\nwhere they're the same speed (not sure what's up with the encoding there), and in Chrome I got:\r\n\r\n```\r\nRenderChat for inputs Optimized x 1,534 ops/sec \u00b12.38% (61 runs sampled)\r\nRenderChat for inputs Stock x 1,775 ops/sec \u00b11.56% (65 runs sampled)\r\nThe Fastest test suite is RenderChat for inputs Stock\r\n\r\n+------------+---------------+---------------+\r\n|            \u2502 Optimized     \u2502 Stock         |\r\n+------------+---------------+---------------+\r\n| RenderChat \u2502 1,534 ops/sec \u2502 1,775 ops/sec |\r\n+------------+---------------+---------------+\r\n```\r\n\r\nwhere the old version is 15% faster. Running `node bench-chat.js` gave me\r\n\r\n```\r\nRenderChat for inputs Optimized x 368 ops/sec \u00b13.18% (73 runs sampled)\r\nRenderChat for inputs Stock x 348 ops/sec \u00b12.99% (72 runs sampled)\r\nThe Fastest test suite is RenderChat for inputs Optimized\r\n\r\n+------------+-------------+-------------+\r\n|            \u2502 Optimized   \u2502 Stock       |\r\n+------------+-------------+-------------+\r\n| RenderChat \u2502 368 ops/sec \u2502 348 ops/sec |\r\n+------------+-------------+-------------+\r\n```\r\n\r\nwhere your version is maybe 5% faster, but not the 10\u201315% you were seeing originally. Any ideas if I'm doing something wrong or if this is just wildly different between engines? I'd have expected Chrome and Node to be more similar, at the very least.", "context": "<issue_start><issue_comment>Title: [performance] Inline ReactMultiChild.mountChildren\nusername_0: I found that `ReactMultiChild.mountChildren` and the functions it calls were iterating over the children many times. This inlines those calls as it is a critical rendering path. Through testing a chat example I have found about a 10-15% performance gains through benchmark.js.\r\n\r\nTest: https://github.com/username_0/react-perf/tree/inlineMountChildren\r\nTest results: https://travis-ci.org/username_0/react-perf/builds/59480548\n<issue_comment>username_1: For reference, this separation was introduced intentionally in this PR:\r\n\r\nhttps://github.com/facebook/react/pull/2567\r\n\r\ncc @username_2\n<issue_comment>username_0: I seem to have broken the test right before pushing. Will investigate.\n<issue_comment>username_2: Yea, this was intentionally separated, knowing that it is a slight performance loss for now. It is a part of an on-going refactor to separate instantiation and resolution from the render strategy. That way we can reuse core React logic while having the option to optimize for different render outputs (DOM, ART, React Native, etc). It is also easier to reason about fragments (components that render multiple elements) if we can diff the resolved set.\r\n\r\nThe idea is to move the instantiation out of the DOM component.\r\n\r\nIt is likely we can remerge this kind of optimization later as more of this refactor starts coming together but I think I'd rather keep it separate for now so that we can incrementally continue the refactor instead of having to make a complete reimplementation from scratch.\r\n\r\nThis brings up an interesting point though. For renderToString, we don't really need to keep any of the internal state tree which is only used for optimizing updates. Maybe we need a whole optimized pipeline that doesn't generate the internal memoization nor the state tree?\n<issue_comment>username_0: :+1: totally agree @username_2. I think that would also help with some of the GC issues we're seeing with higher throughput in node. If you can point me in the right direction for where to start with this, I'd be more than glad to work on it.\n<issue_comment>username_0: Looks like there's still a significant gain if we inline only `ReactChildReconciler.instantiateChildren`:\r\n\r\nChanges: https://github.com/facebook/react/compare/master...username_0:inlineInstantiateChildren\r\nTest: https://github.com/username_0/react-perf/tree/inlineInstantiateChildren\r\nResult: https://travis-ci.org/username_0/react-perf/builds/59784288\n<issue_comment>username_1: Nice find. Maybe we can take advantage of ReactChildren.map here? We may want a version which skips the ReactFragment.create call for these internals but otherwise I think it should give the right effect.\n<issue_comment>username_0: I pulled the traversal function out into a named functioned rather than inline in the branch above. I will run a comparison of using `ReactChildren.map` vs `traverseAllChildren` directly.\n<issue_comment>username_0: As you said I'm running into a lot of error logs due to `ReactFragment.create` being called with nulls and other invalid arguments. I think sticking with `traverseAllChildren` in this case will be better.\n<issue_comment>username_1: Sorry, I may have been a little unclear. Can you do something like this\r\n\r\n```\r\ndiff --git a/src/utils/ReactChildren.js b/src/utils/ReactChildren.js\r\nindex e470609..84ca357 100644\r\n--- a/src/utils/ReactChildren.js\r\n+++ b/src/utils/ReactChildren.js\r\n@@ -122,7 +122,7 @@ function mapChildren(children, func, context) {\r\n   var traverseContext = MapBookKeeping.getPooled(mapResult, func, context);\r\n   traverseAllChildren(children, mapSingleChildIntoContext, traverseContext);\r\n   MapBookKeeping.release(traverseContext);\r\n-  return ReactFragment.create(mapResult);\r\n+  return mapResult;\r\n }\r\n \r\n function forEachSingleChildDummy(traverseContext, child, name, i) {\r\n@@ -142,7 +142,10 @@ function countChildren(children, context) {\r\n \r\n var ReactChildren = {\r\n   forEach: forEachChildren,\r\n-  map: mapChildren,\r\n+  map: function(children, func, context) {\r\n+    return ReactFragment.create(mapChildren(children, func, context));\r\n+  },\r\n+  mapObject: mapChildren,\r\n   count: countChildren\r\n };\r\n \r\n```\r\n\r\nand then use ReactChildren.mapObject?\n<issue_comment>username_0: Ok, so I create a branch that uses ReactChildren.mapObject: https://github.com/facebook/react/compare/master...username_0:inlineInstantiateChildrenWithMap\r\n\r\nThere are two tests that fail using this method:\r\n\r\n * core/__tests__/ReactMultiChildReconcile-test ReactMultiChildReconcile should insert non-empty children in middle where nulls were.\r\n * core/__tests__/ReactMultiChildText-test ReactMultiChildText should correctly handle all possible children for render and update.\r\n\r\nI believe this is because map doesn't handle skipping nulls.\r\n\r\nI also ran a benchmark for using `traverseAllChilden` as my per previous comment vs. using `ReactChildren.mapObject` as @username_1 suggested. Running locally, I'm seeing a significant overhead in using the pooling added by `ReactChildren.mapObject`. On some runs it shows to be less performant than not inlining. You can see the travis-ci run of the benchmark here: https://travis-ci.org/username_0/react-perf/builds/60283471\n<issue_comment>username_0: @username_2 Can you elaborate one what you mean by \"internal state tree\"? I'm trying to understand what could be removed from the rendering path to optimize server rendering. I have a feeling some of the `mountComponent` logic needs to be split up into rendering logic and other logic (event registration, state management, etc.), but I'm not familiar enough with the \"other\" logic that's happening to know what to look at.\n<issue_comment>username_1: @username_0 Interesting. If inlining a closure (like https://gist.github.com/username_1/594910785229dba07d5c) is faster we should probably switch to that \u2013 we've tried to avoid closures in favor of manually passing context objects in hopes of lowering GC pressure and improving perf but all of this feels like dogscience.\r\n\r\n![image](https://cloud.githubusercontent.com/assets/6820/7360334/68f64cd2-ecfe-11e4-8e6e-b85703c9a2de.png)\n<issue_comment>username_1: Sorry for all the back and forth.\n<issue_comment>username_0: No worries. I too often feel like I have no idea what I'm doing.\r\n\r\nI think using `traverseAllChildren` directly as in https://github.com/facebook/react/compare/master...username_0:inlineInstantiateChildren addresses your concern about GC although it does duplicate some of the code in `ReactChildren.map`. We do need [this check](https://github.com/facebook/react/compare/master...username_0:inlineInstantiateChildren#diff-89f33512519fa4d853463913eac8ef04R24) though in order for the tests to pass. Any ideas on introducing that check in to `ReactChildren.map`?\n<issue_comment>username_1: I tried to replicate these results locally and in different browsers. Here's what I did:\r\n\r\n```sh\r\n$ git checkout origin/inlineMountChildren\r\n$ npm install -g browserify\r\n$ npm install babelify envify\r\n$ NODE_ENV=production browserify -t babelify bench-chat.js >bundle.js\r\n```\r\n\r\nAnd I created bench.html:\r\n\r\n```html\r\n<meta charset=\"utf-8\">\r\n<body style=\"font: 10px Menlo; white-space: pre;\">\r\n<div id=\"container\"></div>\r\n<script>\r\nconsole.log = function(x) {\r\n  container.textContent += x.replace(/\\033\\[[0-9;]*m/g, '') + '\\n';\r\n};\r\n</script>\r\n<script src=\"bundle.js\"></script>\r\n```\r\n\r\nOpening bench.html in Safari (JavaScriptCore) gave me:\r\n\r\n```\r\nRenderChat for inputs Optimized x 2,094 ops/sec \u00b10.85% (66 runs sampled)\r\nRenderChat for inputs Stock x 2,081 ops/sec \u00b10.78% (66 runs sampled)\r\nThe Fastest test suite is RenderChat for inputs Optimized,RenderChat for inputs Stock\r\n\r\n+------------+---------------+---------------+\r\n|            \u00e2\u201d\u201a Optimized     \u00e2\u201d\u201a Stock         |\r\n+------------+---------------+---------------+\r\n| RenderChat \u00e2\u201d\u201a 2,094 ops/sec \u00e2\u201d\u201a 2,081 ops/sec |\r\n+------------+---------------+---------------+\r\n```\r\n\r\nwhere they're the same speed (not sure what's up with the encoding there), and in Chrome I got:\r\n\r\n```\r\nRenderChat for inputs Optimized x 1,534 ops/sec \u00b12.38% (61 runs sampled)\r\nRenderChat for inputs Stock x 1,775 ops/sec \u00b11.56% (65 runs sampled)\r\nThe Fastest test suite is RenderChat for inputs Stock\r\n\r\n+------------+---------------+---------------+\r\n|            \u2502 Optimized     \u2502 Stock         |\r\n+------------+---------------+---------------+\r\n| RenderChat \u2502 1,534 ops/sec \u2502 1,775 ops/sec |\r\n+------------+---------------+---------------+\r\n```\r\n\r\nwhere the old version is 15% faster. Running `node bench-chat.js` gave me\r\n\r\n```\r\nRenderChat for inputs Optimized x 368 ops/sec \u00b13.18% (73 runs sampled)\r\nRenderChat for inputs Stock x 348 ops/sec \u00b12.99% (72 runs sampled)\r\nThe Fastest test suite is RenderChat for inputs Optimized\r\n\r\n+------------+-------------+-------------+\r\n|            \u2502 Optimized   \u2502 Stock       |\r\n+------------+-------------+-------------+\r\n| RenderChat \u2502 368 ops/sec \u2502 348 ops/sec |\r\n+------------+-------------+-------------+\r\n```\r\n\r\nwhere your version is maybe 5% faster, but not the 10\u201315% you were seeing originally. Any ideas if I'm doing something wrong or if this is just wildly different between engines? I'd have expected Chrome and Node to be more similar, at the very least.\n<issue_comment>username_0: Are the symlinks working correctly for you? browserify does not work for me locally to create the bundle.js as you did. I'm not sure why your results are so different from my local tests and travis-ci.\n<issue_comment>username_0: It's also worth noting, that even with two versions of stock flux, the benchmark for the item that is tested first is usually ~5% lower than the second which is why I moved \"optimized\" as the first test case. Consistency seems to be a consistent problem =/\n<issue_comment>username_0: Here's travis with 10 runs of bench-chat: https://travis-ci.org/username_0/react-perf/builds/60620846\n<issue_comment>username_0: So it looks like I didn't have the node_modules for each of the apps pushed, so the components may have been using the wrong React. I have pushed the node_modules folder for each app and used symlinks to the react versions such that they are completely isolated. I'm hoping this will make the tests more consistent for you.\r\n\r\nhttps://github.com/username_0/react-perf/tree/inlineMountChildren\n<issue_comment>username_0: Closing this since it undoes a purposeful refactor. I will open a new PR to discuss inlining just `ReactChildReconciler.instantiateChildren`."}
{"repo": "facebook/react", "issue_id": 69988869, "issue_number": 3717, "timestamp": "2015-04-29 23:16:40+00:00", "text": "Are the symlinks working correctly for you? browserify does not work for me locally to create the bundle.js as you did. I'm not sure why your results are so different from my local tests and travis-ci.", "context": "<issue_start><issue_comment>Title: [performance] Inline ReactMultiChild.mountChildren\nusername_0: I found that `ReactMultiChild.mountChildren` and the functions it calls were iterating over the children many times. This inlines those calls as it is a critical rendering path. Through testing a chat example I have found about a 10-15% performance gains through benchmark.js.\r\n\r\nTest: https://github.com/username_0/react-perf/tree/inlineMountChildren\r\nTest results: https://travis-ci.org/username_0/react-perf/builds/59480548\n<issue_comment>username_1: For reference, this separation was introduced intentionally in this PR:\r\n\r\nhttps://github.com/facebook/react/pull/2567\r\n\r\ncc @username_2\n<issue_comment>username_0: I seem to have broken the test right before pushing. Will investigate.\n<issue_comment>username_2: Yea, this was intentionally separated, knowing that it is a slight performance loss for now. It is a part of an on-going refactor to separate instantiation and resolution from the render strategy. That way we can reuse core React logic while having the option to optimize for different render outputs (DOM, ART, React Native, etc). It is also easier to reason about fragments (components that render multiple elements) if we can diff the resolved set.\r\n\r\nThe idea is to move the instantiation out of the DOM component.\r\n\r\nIt is likely we can remerge this kind of optimization later as more of this refactor starts coming together but I think I'd rather keep it separate for now so that we can incrementally continue the refactor instead of having to make a complete reimplementation from scratch.\r\n\r\nThis brings up an interesting point though. For renderToString, we don't really need to keep any of the internal state tree which is only used for optimizing updates. Maybe we need a whole optimized pipeline that doesn't generate the internal memoization nor the state tree?\n<issue_comment>username_0: :+1: totally agree @username_2. I think that would also help with some of the GC issues we're seeing with higher throughput in node. If you can point me in the right direction for where to start with this, I'd be more than glad to work on it.\n<issue_comment>username_0: Looks like there's still a significant gain if we inline only `ReactChildReconciler.instantiateChildren`:\r\n\r\nChanges: https://github.com/facebook/react/compare/master...username_0:inlineInstantiateChildren\r\nTest: https://github.com/username_0/react-perf/tree/inlineInstantiateChildren\r\nResult: https://travis-ci.org/username_0/react-perf/builds/59784288\n<issue_comment>username_1: Nice find. Maybe we can take advantage of ReactChildren.map here? We may want a version which skips the ReactFragment.create call for these internals but otherwise I think it should give the right effect.\n<issue_comment>username_0: I pulled the traversal function out into a named functioned rather than inline in the branch above. I will run a comparison of using `ReactChildren.map` vs `traverseAllChildren` directly.\n<issue_comment>username_0: As you said I'm running into a lot of error logs due to `ReactFragment.create` being called with nulls and other invalid arguments. I think sticking with `traverseAllChildren` in this case will be better.\n<issue_comment>username_1: Sorry, I may have been a little unclear. Can you do something like this\r\n\r\n```\r\ndiff --git a/src/utils/ReactChildren.js b/src/utils/ReactChildren.js\r\nindex e470609..84ca357 100644\r\n--- a/src/utils/ReactChildren.js\r\n+++ b/src/utils/ReactChildren.js\r\n@@ -122,7 +122,7 @@ function mapChildren(children, func, context) {\r\n   var traverseContext = MapBookKeeping.getPooled(mapResult, func, context);\r\n   traverseAllChildren(children, mapSingleChildIntoContext, traverseContext);\r\n   MapBookKeeping.release(traverseContext);\r\n-  return ReactFragment.create(mapResult);\r\n+  return mapResult;\r\n }\r\n \r\n function forEachSingleChildDummy(traverseContext, child, name, i) {\r\n@@ -142,7 +142,10 @@ function countChildren(children, context) {\r\n \r\n var ReactChildren = {\r\n   forEach: forEachChildren,\r\n-  map: mapChildren,\r\n+  map: function(children, func, context) {\r\n+    return ReactFragment.create(mapChildren(children, func, context));\r\n+  },\r\n+  mapObject: mapChildren,\r\n   count: countChildren\r\n };\r\n \r\n```\r\n\r\nand then use ReactChildren.mapObject?\n<issue_comment>username_0: Ok, so I create a branch that uses ReactChildren.mapObject: https://github.com/facebook/react/compare/master...username_0:inlineInstantiateChildrenWithMap\r\n\r\nThere are two tests that fail using this method:\r\n\r\n * core/__tests__/ReactMultiChildReconcile-test ReactMultiChildReconcile should insert non-empty children in middle where nulls were.\r\n * core/__tests__/ReactMultiChildText-test ReactMultiChildText should correctly handle all possible children for render and update.\r\n\r\nI believe this is because map doesn't handle skipping nulls.\r\n\r\nI also ran a benchmark for using `traverseAllChilden` as my per previous comment vs. using `ReactChildren.mapObject` as @username_1 suggested. Running locally, I'm seeing a significant overhead in using the pooling added by `ReactChildren.mapObject`. On some runs it shows to be less performant than not inlining. You can see the travis-ci run of the benchmark here: https://travis-ci.org/username_0/react-perf/builds/60283471\n<issue_comment>username_0: @username_2 Can you elaborate one what you mean by \"internal state tree\"? I'm trying to understand what could be removed from the rendering path to optimize server rendering. I have a feeling some of the `mountComponent` logic needs to be split up into rendering logic and other logic (event registration, state management, etc.), but I'm not familiar enough with the \"other\" logic that's happening to know what to look at.\n<issue_comment>username_1: @username_0 Interesting. If inlining a closure (like https://gist.github.com/username_1/594910785229dba07d5c) is faster we should probably switch to that \u2013 we've tried to avoid closures in favor of manually passing context objects in hopes of lowering GC pressure and improving perf but all of this feels like dogscience.\r\n\r\n![image](https://cloud.githubusercontent.com/assets/6820/7360334/68f64cd2-ecfe-11e4-8e6e-b85703c9a2de.png)\n<issue_comment>username_1: Sorry for all the back and forth.\n<issue_comment>username_0: No worries. I too often feel like I have no idea what I'm doing.\r\n\r\nI think using `traverseAllChildren` directly as in https://github.com/facebook/react/compare/master...username_0:inlineInstantiateChildren addresses your concern about GC although it does duplicate some of the code in `ReactChildren.map`. We do need [this check](https://github.com/facebook/react/compare/master...username_0:inlineInstantiateChildren#diff-89f33512519fa4d853463913eac8ef04R24) though in order for the tests to pass. Any ideas on introducing that check in to `ReactChildren.map`?\n<issue_comment>username_1: I tried to replicate these results locally and in different browsers. Here's what I did:\r\n\r\n```sh\r\n$ git checkout origin/inlineMountChildren\r\n$ npm install -g browserify\r\n$ npm install babelify envify\r\n$ NODE_ENV=production browserify -t babelify bench-chat.js >bundle.js\r\n```\r\n\r\nAnd I created bench.html:\r\n\r\n```html\r\n<meta charset=\"utf-8\">\r\n<body style=\"font: 10px Menlo; white-space: pre;\">\r\n<div id=\"container\"></div>\r\n<script>\r\nconsole.log = function(x) {\r\n  container.textContent += x.replace(/\\033\\[[0-9;]*m/g, '') + '\\n';\r\n};\r\n</script>\r\n<script src=\"bundle.js\"></script>\r\n```\r\n\r\nOpening bench.html in Safari (JavaScriptCore) gave me:\r\n\r\n```\r\nRenderChat for inputs Optimized x 2,094 ops/sec \u00b10.85% (66 runs sampled)\r\nRenderChat for inputs Stock x 2,081 ops/sec \u00b10.78% (66 runs sampled)\r\nThe Fastest test suite is RenderChat for inputs Optimized,RenderChat for inputs Stock\r\n\r\n+------------+---------------+---------------+\r\n|            \u00e2\u201d\u201a Optimized     \u00e2\u201d\u201a Stock         |\r\n+------------+---------------+---------------+\r\n| RenderChat \u00e2\u201d\u201a 2,094 ops/sec \u00e2\u201d\u201a 2,081 ops/sec |\r\n+------------+---------------+---------------+\r\n```\r\n\r\nwhere they're the same speed (not sure what's up with the encoding there), and in Chrome I got:\r\n\r\n```\r\nRenderChat for inputs Optimized x 1,534 ops/sec \u00b12.38% (61 runs sampled)\r\nRenderChat for inputs Stock x 1,775 ops/sec \u00b11.56% (65 runs sampled)\r\nThe Fastest test suite is RenderChat for inputs Stock\r\n\r\n+------------+---------------+---------------+\r\n|            \u2502 Optimized     \u2502 Stock         |\r\n+------------+---------------+---------------+\r\n| RenderChat \u2502 1,534 ops/sec \u2502 1,775 ops/sec |\r\n+------------+---------------+---------------+\r\n```\r\n\r\nwhere the old version is 15% faster. Running `node bench-chat.js` gave me\r\n\r\n```\r\nRenderChat for inputs Optimized x 368 ops/sec \u00b13.18% (73 runs sampled)\r\nRenderChat for inputs Stock x 348 ops/sec \u00b12.99% (72 runs sampled)\r\nThe Fastest test suite is RenderChat for inputs Optimized\r\n\r\n+------------+-------------+-------------+\r\n|            \u2502 Optimized   \u2502 Stock       |\r\n+------------+-------------+-------------+\r\n| RenderChat \u2502 368 ops/sec \u2502 348 ops/sec |\r\n+------------+-------------+-------------+\r\n```\r\n\r\nwhere your version is maybe 5% faster, but not the 10\u201315% you were seeing originally. Any ideas if I'm doing something wrong or if this is just wildly different between engines? I'd have expected Chrome and Node to be more similar, at the very least.\n<issue_comment>username_0: Are the symlinks working correctly for you? browserify does not work for me locally to create the bundle.js as you did. I'm not sure why your results are so different from my local tests and travis-ci.\n<issue_comment>username_0: It's also worth noting, that even with two versions of stock flux, the benchmark for the item that is tested first is usually ~5% lower than the second which is why I moved \"optimized\" as the first test case. Consistency seems to be a consistent problem =/\n<issue_comment>username_0: Here's travis with 10 runs of bench-chat: https://travis-ci.org/username_0/react-perf/builds/60620846\n<issue_comment>username_0: So it looks like I didn't have the node_modules for each of the apps pushed, so the components may have been using the wrong React. I have pushed the node_modules folder for each app and used symlinks to the react versions such that they are completely isolated. I'm hoping this will make the tests more consistent for you.\r\n\r\nhttps://github.com/username_0/react-perf/tree/inlineMountChildren\n<issue_comment>username_0: Closing this since it undoes a purposeful refactor. I will open a new PR to discuss inlining just `ReactChildReconciler.instantiateChildren`."}
{"repo": "facebook/react", "issue_id": 69988869, "issue_number": 3717, "timestamp": "2015-04-29 23:18:49+00:00", "text": "It's also worth noting, that even with two versions of stock flux, the benchmark for the item that is tested first is usually ~5% lower than the second which is why I moved \"optimized\" as the first test case. Consistency seems to be a consistent problem =/", "context": "<issue_start><issue_comment>Title: [performance] Inline ReactMultiChild.mountChildren\nusername_0: I found that `ReactMultiChild.mountChildren` and the functions it calls were iterating over the children many times. This inlines those calls as it is a critical rendering path. Through testing a chat example I have found about a 10-15% performance gains through benchmark.js.\r\n\r\nTest: https://github.com/username_0/react-perf/tree/inlineMountChildren\r\nTest results: https://travis-ci.org/username_0/react-perf/builds/59480548\n<issue_comment>username_1: For reference, this separation was introduced intentionally in this PR:\r\n\r\nhttps://github.com/facebook/react/pull/2567\r\n\r\ncc @username_2\n<issue_comment>username_0: I seem to have broken the test right before pushing. Will investigate.\n<issue_comment>username_2: Yea, this was intentionally separated, knowing that it is a slight performance loss for now. It is a part of an on-going refactor to separate instantiation and resolution from the render strategy. That way we can reuse core React logic while having the option to optimize for different render outputs (DOM, ART, React Native, etc). It is also easier to reason about fragments (components that render multiple elements) if we can diff the resolved set.\r\n\r\nThe idea is to move the instantiation out of the DOM component.\r\n\r\nIt is likely we can remerge this kind of optimization later as more of this refactor starts coming together but I think I'd rather keep it separate for now so that we can incrementally continue the refactor instead of having to make a complete reimplementation from scratch.\r\n\r\nThis brings up an interesting point though. For renderToString, we don't really need to keep any of the internal state tree which is only used for optimizing updates. Maybe we need a whole optimized pipeline that doesn't generate the internal memoization nor the state tree?\n<issue_comment>username_0: :+1: totally agree @username_2. I think that would also help with some of the GC issues we're seeing with higher throughput in node. If you can point me in the right direction for where to start with this, I'd be more than glad to work on it.\n<issue_comment>username_0: Looks like there's still a significant gain if we inline only `ReactChildReconciler.instantiateChildren`:\r\n\r\nChanges: https://github.com/facebook/react/compare/master...username_0:inlineInstantiateChildren\r\nTest: https://github.com/username_0/react-perf/tree/inlineInstantiateChildren\r\nResult: https://travis-ci.org/username_0/react-perf/builds/59784288\n<issue_comment>username_1: Nice find. Maybe we can take advantage of ReactChildren.map here? We may want a version which skips the ReactFragment.create call for these internals but otherwise I think it should give the right effect.\n<issue_comment>username_0: I pulled the traversal function out into a named functioned rather than inline in the branch above. I will run a comparison of using `ReactChildren.map` vs `traverseAllChildren` directly.\n<issue_comment>username_0: As you said I'm running into a lot of error logs due to `ReactFragment.create` being called with nulls and other invalid arguments. I think sticking with `traverseAllChildren` in this case will be better.\n<issue_comment>username_1: Sorry, I may have been a little unclear. Can you do something like this\r\n\r\n```\r\ndiff --git a/src/utils/ReactChildren.js b/src/utils/ReactChildren.js\r\nindex e470609..84ca357 100644\r\n--- a/src/utils/ReactChildren.js\r\n+++ b/src/utils/ReactChildren.js\r\n@@ -122,7 +122,7 @@ function mapChildren(children, func, context) {\r\n   var traverseContext = MapBookKeeping.getPooled(mapResult, func, context);\r\n   traverseAllChildren(children, mapSingleChildIntoContext, traverseContext);\r\n   MapBookKeeping.release(traverseContext);\r\n-  return ReactFragment.create(mapResult);\r\n+  return mapResult;\r\n }\r\n \r\n function forEachSingleChildDummy(traverseContext, child, name, i) {\r\n@@ -142,7 +142,10 @@ function countChildren(children, context) {\r\n \r\n var ReactChildren = {\r\n   forEach: forEachChildren,\r\n-  map: mapChildren,\r\n+  map: function(children, func, context) {\r\n+    return ReactFragment.create(mapChildren(children, func, context));\r\n+  },\r\n+  mapObject: mapChildren,\r\n   count: countChildren\r\n };\r\n \r\n```\r\n\r\nand then use ReactChildren.mapObject?\n<issue_comment>username_0: Ok, so I create a branch that uses ReactChildren.mapObject: https://github.com/facebook/react/compare/master...username_0:inlineInstantiateChildrenWithMap\r\n\r\nThere are two tests that fail using this method:\r\n\r\n * core/__tests__/ReactMultiChildReconcile-test ReactMultiChildReconcile should insert non-empty children in middle where nulls were.\r\n * core/__tests__/ReactMultiChildText-test ReactMultiChildText should correctly handle all possible children for render and update.\r\n\r\nI believe this is because map doesn't handle skipping nulls.\r\n\r\nI also ran a benchmark for using `traverseAllChilden` as my per previous comment vs. using `ReactChildren.mapObject` as @username_1 suggested. Running locally, I'm seeing a significant overhead in using the pooling added by `ReactChildren.mapObject`. On some runs it shows to be less performant than not inlining. You can see the travis-ci run of the benchmark here: https://travis-ci.org/username_0/react-perf/builds/60283471\n<issue_comment>username_0: @username_2 Can you elaborate one what you mean by \"internal state tree\"? I'm trying to understand what could be removed from the rendering path to optimize server rendering. I have a feeling some of the `mountComponent` logic needs to be split up into rendering logic and other logic (event registration, state management, etc.), but I'm not familiar enough with the \"other\" logic that's happening to know what to look at.\n<issue_comment>username_1: @username_0 Interesting. If inlining a closure (like https://gist.github.com/username_1/594910785229dba07d5c) is faster we should probably switch to that \u2013 we've tried to avoid closures in favor of manually passing context objects in hopes of lowering GC pressure and improving perf but all of this feels like dogscience.\r\n\r\n![image](https://cloud.githubusercontent.com/assets/6820/7360334/68f64cd2-ecfe-11e4-8e6e-b85703c9a2de.png)\n<issue_comment>username_1: Sorry for all the back and forth.\n<issue_comment>username_0: No worries. I too often feel like I have no idea what I'm doing.\r\n\r\nI think using `traverseAllChildren` directly as in https://github.com/facebook/react/compare/master...username_0:inlineInstantiateChildren addresses your concern about GC although it does duplicate some of the code in `ReactChildren.map`. We do need [this check](https://github.com/facebook/react/compare/master...username_0:inlineInstantiateChildren#diff-89f33512519fa4d853463913eac8ef04R24) though in order for the tests to pass. Any ideas on introducing that check in to `ReactChildren.map`?\n<issue_comment>username_1: I tried to replicate these results locally and in different browsers. Here's what I did:\r\n\r\n```sh\r\n$ git checkout origin/inlineMountChildren\r\n$ npm install -g browserify\r\n$ npm install babelify envify\r\n$ NODE_ENV=production browserify -t babelify bench-chat.js >bundle.js\r\n```\r\n\r\nAnd I created bench.html:\r\n\r\n```html\r\n<meta charset=\"utf-8\">\r\n<body style=\"font: 10px Menlo; white-space: pre;\">\r\n<div id=\"container\"></div>\r\n<script>\r\nconsole.log = function(x) {\r\n  container.textContent += x.replace(/\\033\\[[0-9;]*m/g, '') + '\\n';\r\n};\r\n</script>\r\n<script src=\"bundle.js\"></script>\r\n```\r\n\r\nOpening bench.html in Safari (JavaScriptCore) gave me:\r\n\r\n```\r\nRenderChat for inputs Optimized x 2,094 ops/sec \u00b10.85% (66 runs sampled)\r\nRenderChat for inputs Stock x 2,081 ops/sec \u00b10.78% (66 runs sampled)\r\nThe Fastest test suite is RenderChat for inputs Optimized,RenderChat for inputs Stock\r\n\r\n+------------+---------------+---------------+\r\n|            \u00e2\u201d\u201a Optimized     \u00e2\u201d\u201a Stock         |\r\n+------------+---------------+---------------+\r\n| RenderChat \u00e2\u201d\u201a 2,094 ops/sec \u00e2\u201d\u201a 2,081 ops/sec |\r\n+------------+---------------+---------------+\r\n```\r\n\r\nwhere they're the same speed (not sure what's up with the encoding there), and in Chrome I got:\r\n\r\n```\r\nRenderChat for inputs Optimized x 1,534 ops/sec \u00b12.38% (61 runs sampled)\r\nRenderChat for inputs Stock x 1,775 ops/sec \u00b11.56% (65 runs sampled)\r\nThe Fastest test suite is RenderChat for inputs Stock\r\n\r\n+------------+---------------+---------------+\r\n|            \u2502 Optimized     \u2502 Stock         |\r\n+------------+---------------+---------------+\r\n| RenderChat \u2502 1,534 ops/sec \u2502 1,775 ops/sec |\r\n+------------+---------------+---------------+\r\n```\r\n\r\nwhere the old version is 15% faster. Running `node bench-chat.js` gave me\r\n\r\n```\r\nRenderChat for inputs Optimized x 368 ops/sec \u00b13.18% (73 runs sampled)\r\nRenderChat for inputs Stock x 348 ops/sec \u00b12.99% (72 runs sampled)\r\nThe Fastest test suite is RenderChat for inputs Optimized\r\n\r\n+------------+-------------+-------------+\r\n|            \u2502 Optimized   \u2502 Stock       |\r\n+------------+-------------+-------------+\r\n| RenderChat \u2502 368 ops/sec \u2502 348 ops/sec |\r\n+------------+-------------+-------------+\r\n```\r\n\r\nwhere your version is maybe 5% faster, but not the 10\u201315% you were seeing originally. Any ideas if I'm doing something wrong or if this is just wildly different between engines? I'd have expected Chrome and Node to be more similar, at the very least.\n<issue_comment>username_0: Are the symlinks working correctly for you? browserify does not work for me locally to create the bundle.js as you did. I'm not sure why your results are so different from my local tests and travis-ci.\n<issue_comment>username_0: It's also worth noting, that even with two versions of stock flux, the benchmark for the item that is tested first is usually ~5% lower than the second which is why I moved \"optimized\" as the first test case. Consistency seems to be a consistent problem =/\n<issue_comment>username_0: Here's travis with 10 runs of bench-chat: https://travis-ci.org/username_0/react-perf/builds/60620846\n<issue_comment>username_0: So it looks like I didn't have the node_modules for each of the apps pushed, so the components may have been using the wrong React. I have pushed the node_modules folder for each app and used symlinks to the react versions such that they are completely isolated. I'm hoping this will make the tests more consistent for you.\r\n\r\nhttps://github.com/username_0/react-perf/tree/inlineMountChildren\n<issue_comment>username_0: Closing this since it undoes a purposeful refactor. I will open a new PR to discuss inlining just `ReactChildReconciler.instantiateChildren`."}
{"repo": "facebook/react", "issue_id": 69988869, "issue_number": 3717, "timestamp": "2015-04-29 23:29:42+00:00", "text": "Here's travis with 10 runs of bench-chat: https://travis-ci.org/username_0/react-perf/builds/60620846", "context": "<issue_start><issue_comment>Title: [performance] Inline ReactMultiChild.mountChildren\nusername_0: I found that `ReactMultiChild.mountChildren` and the functions it calls were iterating over the children many times. This inlines those calls as it is a critical rendering path. Through testing a chat example I have found about a 10-15% performance gains through benchmark.js.\r\n\r\nTest: https://github.com/username_0/react-perf/tree/inlineMountChildren\r\nTest results: https://travis-ci.org/username_0/react-perf/builds/59480548\n<issue_comment>username_1: For reference, this separation was introduced intentionally in this PR:\r\n\r\nhttps://github.com/facebook/react/pull/2567\r\n\r\ncc @username_2\n<issue_comment>username_0: I seem to have broken the test right before pushing. Will investigate.\n<issue_comment>username_2: Yea, this was intentionally separated, knowing that it is a slight performance loss for now. It is a part of an on-going refactor to separate instantiation and resolution from the render strategy. That way we can reuse core React logic while having the option to optimize for different render outputs (DOM, ART, React Native, etc). It is also easier to reason about fragments (components that render multiple elements) if we can diff the resolved set.\r\n\r\nThe idea is to move the instantiation out of the DOM component.\r\n\r\nIt is likely we can remerge this kind of optimization later as more of this refactor starts coming together but I think I'd rather keep it separate for now so that we can incrementally continue the refactor instead of having to make a complete reimplementation from scratch.\r\n\r\nThis brings up an interesting point though. For renderToString, we don't really need to keep any of the internal state tree which is only used for optimizing updates. Maybe we need a whole optimized pipeline that doesn't generate the internal memoization nor the state tree?\n<issue_comment>username_0: :+1: totally agree @username_2. I think that would also help with some of the GC issues we're seeing with higher throughput in node. If you can point me in the right direction for where to start with this, I'd be more than glad to work on it.\n<issue_comment>username_0: Looks like there's still a significant gain if we inline only `ReactChildReconciler.instantiateChildren`:\r\n\r\nChanges: https://github.com/facebook/react/compare/master...username_0:inlineInstantiateChildren\r\nTest: https://github.com/username_0/react-perf/tree/inlineInstantiateChildren\r\nResult: https://travis-ci.org/username_0/react-perf/builds/59784288\n<issue_comment>username_1: Nice find. Maybe we can take advantage of ReactChildren.map here? We may want a version which skips the ReactFragment.create call for these internals but otherwise I think it should give the right effect.\n<issue_comment>username_0: I pulled the traversal function out into a named functioned rather than inline in the branch above. I will run a comparison of using `ReactChildren.map` vs `traverseAllChildren` directly.\n<issue_comment>username_0: As you said I'm running into a lot of error logs due to `ReactFragment.create` being called with nulls and other invalid arguments. I think sticking with `traverseAllChildren` in this case will be better.\n<issue_comment>username_1: Sorry, I may have been a little unclear. Can you do something like this\r\n\r\n```\r\ndiff --git a/src/utils/ReactChildren.js b/src/utils/ReactChildren.js\r\nindex e470609..84ca357 100644\r\n--- a/src/utils/ReactChildren.js\r\n+++ b/src/utils/ReactChildren.js\r\n@@ -122,7 +122,7 @@ function mapChildren(children, func, context) {\r\n   var traverseContext = MapBookKeeping.getPooled(mapResult, func, context);\r\n   traverseAllChildren(children, mapSingleChildIntoContext, traverseContext);\r\n   MapBookKeeping.release(traverseContext);\r\n-  return ReactFragment.create(mapResult);\r\n+  return mapResult;\r\n }\r\n \r\n function forEachSingleChildDummy(traverseContext, child, name, i) {\r\n@@ -142,7 +142,10 @@ function countChildren(children, context) {\r\n \r\n var ReactChildren = {\r\n   forEach: forEachChildren,\r\n-  map: mapChildren,\r\n+  map: function(children, func, context) {\r\n+    return ReactFragment.create(mapChildren(children, func, context));\r\n+  },\r\n+  mapObject: mapChildren,\r\n   count: countChildren\r\n };\r\n \r\n```\r\n\r\nand then use ReactChildren.mapObject?\n<issue_comment>username_0: Ok, so I create a branch that uses ReactChildren.mapObject: https://github.com/facebook/react/compare/master...username_0:inlineInstantiateChildrenWithMap\r\n\r\nThere are two tests that fail using this method:\r\n\r\n * core/__tests__/ReactMultiChildReconcile-test ReactMultiChildReconcile should insert non-empty children in middle where nulls were.\r\n * core/__tests__/ReactMultiChildText-test ReactMultiChildText should correctly handle all possible children for render and update.\r\n\r\nI believe this is because map doesn't handle skipping nulls.\r\n\r\nI also ran a benchmark for using `traverseAllChilden` as my per previous comment vs. using `ReactChildren.mapObject` as @username_1 suggested. Running locally, I'm seeing a significant overhead in using the pooling added by `ReactChildren.mapObject`. On some runs it shows to be less performant than not inlining. You can see the travis-ci run of the benchmark here: https://travis-ci.org/username_0/react-perf/builds/60283471\n<issue_comment>username_0: @username_2 Can you elaborate one what you mean by \"internal state tree\"? I'm trying to understand what could be removed from the rendering path to optimize server rendering. I have a feeling some of the `mountComponent` logic needs to be split up into rendering logic and other logic (event registration, state management, etc.), but I'm not familiar enough with the \"other\" logic that's happening to know what to look at.\n<issue_comment>username_1: @username_0 Interesting. If inlining a closure (like https://gist.github.com/username_1/594910785229dba07d5c) is faster we should probably switch to that \u2013 we've tried to avoid closures in favor of manually passing context objects in hopes of lowering GC pressure and improving perf but all of this feels like dogscience.\r\n\r\n![image](https://cloud.githubusercontent.com/assets/6820/7360334/68f64cd2-ecfe-11e4-8e6e-b85703c9a2de.png)\n<issue_comment>username_1: Sorry for all the back and forth.\n<issue_comment>username_0: No worries. I too often feel like I have no idea what I'm doing.\r\n\r\nI think using `traverseAllChildren` directly as in https://github.com/facebook/react/compare/master...username_0:inlineInstantiateChildren addresses your concern about GC although it does duplicate some of the code in `ReactChildren.map`. We do need [this check](https://github.com/facebook/react/compare/master...username_0:inlineInstantiateChildren#diff-89f33512519fa4d853463913eac8ef04R24) though in order for the tests to pass. Any ideas on introducing that check in to `ReactChildren.map`?\n<issue_comment>username_1: I tried to replicate these results locally and in different browsers. Here's what I did:\r\n\r\n```sh\r\n$ git checkout origin/inlineMountChildren\r\n$ npm install -g browserify\r\n$ npm install babelify envify\r\n$ NODE_ENV=production browserify -t babelify bench-chat.js >bundle.js\r\n```\r\n\r\nAnd I created bench.html:\r\n\r\n```html\r\n<meta charset=\"utf-8\">\r\n<body style=\"font: 10px Menlo; white-space: pre;\">\r\n<div id=\"container\"></div>\r\n<script>\r\nconsole.log = function(x) {\r\n  container.textContent += x.replace(/\\033\\[[0-9;]*m/g, '') + '\\n';\r\n};\r\n</script>\r\n<script src=\"bundle.js\"></script>\r\n```\r\n\r\nOpening bench.html in Safari (JavaScriptCore) gave me:\r\n\r\n```\r\nRenderChat for inputs Optimized x 2,094 ops/sec \u00b10.85% (66 runs sampled)\r\nRenderChat for inputs Stock x 2,081 ops/sec \u00b10.78% (66 runs sampled)\r\nThe Fastest test suite is RenderChat for inputs Optimized,RenderChat for inputs Stock\r\n\r\n+------------+---------------+---------------+\r\n|            \u00e2\u201d\u201a Optimized     \u00e2\u201d\u201a Stock         |\r\n+------------+---------------+---------------+\r\n| RenderChat \u00e2\u201d\u201a 2,094 ops/sec \u00e2\u201d\u201a 2,081 ops/sec |\r\n+------------+---------------+---------------+\r\n```\r\n\r\nwhere they're the same speed (not sure what's up with the encoding there), and in Chrome I got:\r\n\r\n```\r\nRenderChat for inputs Optimized x 1,534 ops/sec \u00b12.38% (61 runs sampled)\r\nRenderChat for inputs Stock x 1,775 ops/sec \u00b11.56% (65 runs sampled)\r\nThe Fastest test suite is RenderChat for inputs Stock\r\n\r\n+------------+---------------+---------------+\r\n|            \u2502 Optimized     \u2502 Stock         |\r\n+------------+---------------+---------------+\r\n| RenderChat \u2502 1,534 ops/sec \u2502 1,775 ops/sec |\r\n+------------+---------------+---------------+\r\n```\r\n\r\nwhere the old version is 15% faster. Running `node bench-chat.js` gave me\r\n\r\n```\r\nRenderChat for inputs Optimized x 368 ops/sec \u00b13.18% (73 runs sampled)\r\nRenderChat for inputs Stock x 348 ops/sec \u00b12.99% (72 runs sampled)\r\nThe Fastest test suite is RenderChat for inputs Optimized\r\n\r\n+------------+-------------+-------------+\r\n|            \u2502 Optimized   \u2502 Stock       |\r\n+------------+-------------+-------------+\r\n| RenderChat \u2502 368 ops/sec \u2502 348 ops/sec |\r\n+------------+-------------+-------------+\r\n```\r\n\r\nwhere your version is maybe 5% faster, but not the 10\u201315% you were seeing originally. Any ideas if I'm doing something wrong or if this is just wildly different between engines? I'd have expected Chrome and Node to be more similar, at the very least.\n<issue_comment>username_0: Are the symlinks working correctly for you? browserify does not work for me locally to create the bundle.js as you did. I'm not sure why your results are so different from my local tests and travis-ci.\n<issue_comment>username_0: It's also worth noting, that even with two versions of stock flux, the benchmark for the item that is tested first is usually ~5% lower than the second which is why I moved \"optimized\" as the first test case. Consistency seems to be a consistent problem =/\n<issue_comment>username_0: Here's travis with 10 runs of bench-chat: https://travis-ci.org/username_0/react-perf/builds/60620846\n<issue_comment>username_0: So it looks like I didn't have the node_modules for each of the apps pushed, so the components may have been using the wrong React. I have pushed the node_modules folder for each app and used symlinks to the react versions such that they are completely isolated. I'm hoping this will make the tests more consistent for you.\r\n\r\nhttps://github.com/username_0/react-perf/tree/inlineMountChildren\n<issue_comment>username_0: Closing this since it undoes a purposeful refactor. I will open a new PR to discuss inlining just `ReactChildReconciler.instantiateChildren`."}
{"repo": "facebook/react", "issue_id": 69988869, "issue_number": 3717, "timestamp": "2015-05-05 01:44:42+00:00", "text": "So it looks like I didn't have the node_modules for each of the apps pushed, so the components may have been using the wrong React. I have pushed the node_modules folder for each app and used symlinks to the react versions such that they are completely isolated. I'm hoping this will make the tests more consistent for you.\r\n\r\nhttps://github.com/username_0/react-perf/tree/inlineMountChildren", "context": "<issue_start><issue_comment>Title: [performance] Inline ReactMultiChild.mountChildren\nusername_0: I found that `ReactMultiChild.mountChildren` and the functions it calls were iterating over the children many times. This inlines those calls as it is a critical rendering path. Through testing a chat example I have found about a 10-15% performance gains through benchmark.js.\r\n\r\nTest: https://github.com/username_0/react-perf/tree/inlineMountChildren\r\nTest results: https://travis-ci.org/username_0/react-perf/builds/59480548\n<issue_comment>username_1: For reference, this separation was introduced intentionally in this PR:\r\n\r\nhttps://github.com/facebook/react/pull/2567\r\n\r\ncc @username_2\n<issue_comment>username_0: I seem to have broken the test right before pushing. Will investigate.\n<issue_comment>username_2: Yea, this was intentionally separated, knowing that it is a slight performance loss for now. It is a part of an on-going refactor to separate instantiation and resolution from the render strategy. That way we can reuse core React logic while having the option to optimize for different render outputs (DOM, ART, React Native, etc). It is also easier to reason about fragments (components that render multiple elements) if we can diff the resolved set.\r\n\r\nThe idea is to move the instantiation out of the DOM component.\r\n\r\nIt is likely we can remerge this kind of optimization later as more of this refactor starts coming together but I think I'd rather keep it separate for now so that we can incrementally continue the refactor instead of having to make a complete reimplementation from scratch.\r\n\r\nThis brings up an interesting point though. For renderToString, we don't really need to keep any of the internal state tree which is only used for optimizing updates. Maybe we need a whole optimized pipeline that doesn't generate the internal memoization nor the state tree?\n<issue_comment>username_0: :+1: totally agree @username_2. I think that would also help with some of the GC issues we're seeing with higher throughput in node. If you can point me in the right direction for where to start with this, I'd be more than glad to work on it.\n<issue_comment>username_0: Looks like there's still a significant gain if we inline only `ReactChildReconciler.instantiateChildren`:\r\n\r\nChanges: https://github.com/facebook/react/compare/master...username_0:inlineInstantiateChildren\r\nTest: https://github.com/username_0/react-perf/tree/inlineInstantiateChildren\r\nResult: https://travis-ci.org/username_0/react-perf/builds/59784288\n<issue_comment>username_1: Nice find. Maybe we can take advantage of ReactChildren.map here? We may want a version which skips the ReactFragment.create call for these internals but otherwise I think it should give the right effect.\n<issue_comment>username_0: I pulled the traversal function out into a named functioned rather than inline in the branch above. I will run a comparison of using `ReactChildren.map` vs `traverseAllChildren` directly.\n<issue_comment>username_0: As you said I'm running into a lot of error logs due to `ReactFragment.create` being called with nulls and other invalid arguments. I think sticking with `traverseAllChildren` in this case will be better.\n<issue_comment>username_1: Sorry, I may have been a little unclear. Can you do something like this\r\n\r\n```\r\ndiff --git a/src/utils/ReactChildren.js b/src/utils/ReactChildren.js\r\nindex e470609..84ca357 100644\r\n--- a/src/utils/ReactChildren.js\r\n+++ b/src/utils/ReactChildren.js\r\n@@ -122,7 +122,7 @@ function mapChildren(children, func, context) {\r\n   var traverseContext = MapBookKeeping.getPooled(mapResult, func, context);\r\n   traverseAllChildren(children, mapSingleChildIntoContext, traverseContext);\r\n   MapBookKeeping.release(traverseContext);\r\n-  return ReactFragment.create(mapResult);\r\n+  return mapResult;\r\n }\r\n \r\n function forEachSingleChildDummy(traverseContext, child, name, i) {\r\n@@ -142,7 +142,10 @@ function countChildren(children, context) {\r\n \r\n var ReactChildren = {\r\n   forEach: forEachChildren,\r\n-  map: mapChildren,\r\n+  map: function(children, func, context) {\r\n+    return ReactFragment.create(mapChildren(children, func, context));\r\n+  },\r\n+  mapObject: mapChildren,\r\n   count: countChildren\r\n };\r\n \r\n```\r\n\r\nand then use ReactChildren.mapObject?\n<issue_comment>username_0: Ok, so I create a branch that uses ReactChildren.mapObject: https://github.com/facebook/react/compare/master...username_0:inlineInstantiateChildrenWithMap\r\n\r\nThere are two tests that fail using this method:\r\n\r\n * core/__tests__/ReactMultiChildReconcile-test ReactMultiChildReconcile should insert non-empty children in middle where nulls were.\r\n * core/__tests__/ReactMultiChildText-test ReactMultiChildText should correctly handle all possible children for render and update.\r\n\r\nI believe this is because map doesn't handle skipping nulls.\r\n\r\nI also ran a benchmark for using `traverseAllChilden` as my per previous comment vs. using `ReactChildren.mapObject` as @username_1 suggested. Running locally, I'm seeing a significant overhead in using the pooling added by `ReactChildren.mapObject`. On some runs it shows to be less performant than not inlining. You can see the travis-ci run of the benchmark here: https://travis-ci.org/username_0/react-perf/builds/60283471\n<issue_comment>username_0: @username_2 Can you elaborate one what you mean by \"internal state tree\"? I'm trying to understand what could be removed from the rendering path to optimize server rendering. I have a feeling some of the `mountComponent` logic needs to be split up into rendering logic and other logic (event registration, state management, etc.), but I'm not familiar enough with the \"other\" logic that's happening to know what to look at.\n<issue_comment>username_1: @username_0 Interesting. If inlining a closure (like https://gist.github.com/username_1/594910785229dba07d5c) is faster we should probably switch to that \u2013 we've tried to avoid closures in favor of manually passing context objects in hopes of lowering GC pressure and improving perf but all of this feels like dogscience.\r\n\r\n![image](https://cloud.githubusercontent.com/assets/6820/7360334/68f64cd2-ecfe-11e4-8e6e-b85703c9a2de.png)\n<issue_comment>username_1: Sorry for all the back and forth.\n<issue_comment>username_0: No worries. I too often feel like I have no idea what I'm doing.\r\n\r\nI think using `traverseAllChildren` directly as in https://github.com/facebook/react/compare/master...username_0:inlineInstantiateChildren addresses your concern about GC although it does duplicate some of the code in `ReactChildren.map`. We do need [this check](https://github.com/facebook/react/compare/master...username_0:inlineInstantiateChildren#diff-89f33512519fa4d853463913eac8ef04R24) though in order for the tests to pass. Any ideas on introducing that check in to `ReactChildren.map`?\n<issue_comment>username_1: I tried to replicate these results locally and in different browsers. Here's what I did:\r\n\r\n```sh\r\n$ git checkout origin/inlineMountChildren\r\n$ npm install -g browserify\r\n$ npm install babelify envify\r\n$ NODE_ENV=production browserify -t babelify bench-chat.js >bundle.js\r\n```\r\n\r\nAnd I created bench.html:\r\n\r\n```html\r\n<meta charset=\"utf-8\">\r\n<body style=\"font: 10px Menlo; white-space: pre;\">\r\n<div id=\"container\"></div>\r\n<script>\r\nconsole.log = function(x) {\r\n  container.textContent += x.replace(/\\033\\[[0-9;]*m/g, '') + '\\n';\r\n};\r\n</script>\r\n<script src=\"bundle.js\"></script>\r\n```\r\n\r\nOpening bench.html in Safari (JavaScriptCore) gave me:\r\n\r\n```\r\nRenderChat for inputs Optimized x 2,094 ops/sec \u00b10.85% (66 runs sampled)\r\nRenderChat for inputs Stock x 2,081 ops/sec \u00b10.78% (66 runs sampled)\r\nThe Fastest test suite is RenderChat for inputs Optimized,RenderChat for inputs Stock\r\n\r\n+------------+---------------+---------------+\r\n|            \u00e2\u201d\u201a Optimized     \u00e2\u201d\u201a Stock         |\r\n+------------+---------------+---------------+\r\n| RenderChat \u00e2\u201d\u201a 2,094 ops/sec \u00e2\u201d\u201a 2,081 ops/sec |\r\n+------------+---------------+---------------+\r\n```\r\n\r\nwhere they're the same speed (not sure what's up with the encoding there), and in Chrome I got:\r\n\r\n```\r\nRenderChat for inputs Optimized x 1,534 ops/sec \u00b12.38% (61 runs sampled)\r\nRenderChat for inputs Stock x 1,775 ops/sec \u00b11.56% (65 runs sampled)\r\nThe Fastest test suite is RenderChat for inputs Stock\r\n\r\n+------------+---------------+---------------+\r\n|            \u2502 Optimized     \u2502 Stock         |\r\n+------------+---------------+---------------+\r\n| RenderChat \u2502 1,534 ops/sec \u2502 1,775 ops/sec |\r\n+------------+---------------+---------------+\r\n```\r\n\r\nwhere the old version is 15% faster. Running `node bench-chat.js` gave me\r\n\r\n```\r\nRenderChat for inputs Optimized x 368 ops/sec \u00b13.18% (73 runs sampled)\r\nRenderChat for inputs Stock x 348 ops/sec \u00b12.99% (72 runs sampled)\r\nThe Fastest test suite is RenderChat for inputs Optimized\r\n\r\n+------------+-------------+-------------+\r\n|            \u2502 Optimized   \u2502 Stock       |\r\n+------------+-------------+-------------+\r\n| RenderChat \u2502 368 ops/sec \u2502 348 ops/sec |\r\n+------------+-------------+-------------+\r\n```\r\n\r\nwhere your version is maybe 5% faster, but not the 10\u201315% you were seeing originally. Any ideas if I'm doing something wrong or if this is just wildly different between engines? I'd have expected Chrome and Node to be more similar, at the very least.\n<issue_comment>username_0: Are the symlinks working correctly for you? browserify does not work for me locally to create the bundle.js as you did. I'm not sure why your results are so different from my local tests and travis-ci.\n<issue_comment>username_0: It's also worth noting, that even with two versions of stock flux, the benchmark for the item that is tested first is usually ~5% lower than the second which is why I moved \"optimized\" as the first test case. Consistency seems to be a consistent problem =/\n<issue_comment>username_0: Here's travis with 10 runs of bench-chat: https://travis-ci.org/username_0/react-perf/builds/60620846\n<issue_comment>username_0: So it looks like I didn't have the node_modules for each of the apps pushed, so the components may have been using the wrong React. I have pushed the node_modules folder for each app and used symlinks to the react versions such that they are completely isolated. I'm hoping this will make the tests more consistent for you.\r\n\r\nhttps://github.com/username_0/react-perf/tree/inlineMountChildren\n<issue_comment>username_0: Closing this since it undoes a purposeful refactor. I will open a new PR to discuss inlining just `ReactChildReconciler.instantiateChildren`."}
{"repo": "facebook/react", "issue_id": 69988869, "issue_number": 3717, "timestamp": "2015-05-20 21:48:29+00:00", "text": "Closing this since it undoes a purposeful refactor. I will open a new PR to discuss inlining just `ReactChildReconciler.instantiateChildren`.", "context": "<issue_start><issue_comment>Title: [performance] Inline ReactMultiChild.mountChildren\nusername_0: I found that `ReactMultiChild.mountChildren` and the functions it calls were iterating over the children many times. This inlines those calls as it is a critical rendering path. Through testing a chat example I have found about a 10-15% performance gains through benchmark.js.\r\n\r\nTest: https://github.com/username_0/react-perf/tree/inlineMountChildren\r\nTest results: https://travis-ci.org/username_0/react-perf/builds/59480548\n<issue_comment>username_1: For reference, this separation was introduced intentionally in this PR:\r\n\r\nhttps://github.com/facebook/react/pull/2567\r\n\r\ncc @username_2\n<issue_comment>username_0: I seem to have broken the test right before pushing. Will investigate.\n<issue_comment>username_2: Yea, this was intentionally separated, knowing that it is a slight performance loss for now. It is a part of an on-going refactor to separate instantiation and resolution from the render strategy. That way we can reuse core React logic while having the option to optimize for different render outputs (DOM, ART, React Native, etc). It is also easier to reason about fragments (components that render multiple elements) if we can diff the resolved set.\r\n\r\nThe idea is to move the instantiation out of the DOM component.\r\n\r\nIt is likely we can remerge this kind of optimization later as more of this refactor starts coming together but I think I'd rather keep it separate for now so that we can incrementally continue the refactor instead of having to make a complete reimplementation from scratch.\r\n\r\nThis brings up an interesting point though. For renderToString, we don't really need to keep any of the internal state tree which is only used for optimizing updates. Maybe we need a whole optimized pipeline that doesn't generate the internal memoization nor the state tree?\n<issue_comment>username_0: :+1: totally agree @username_2. I think that would also help with some of the GC issues we're seeing with higher throughput in node. If you can point me in the right direction for where to start with this, I'd be more than glad to work on it.\n<issue_comment>username_0: Looks like there's still a significant gain if we inline only `ReactChildReconciler.instantiateChildren`:\r\n\r\nChanges: https://github.com/facebook/react/compare/master...username_0:inlineInstantiateChildren\r\nTest: https://github.com/username_0/react-perf/tree/inlineInstantiateChildren\r\nResult: https://travis-ci.org/username_0/react-perf/builds/59784288\n<issue_comment>username_1: Nice find. Maybe we can take advantage of ReactChildren.map here? We may want a version which skips the ReactFragment.create call for these internals but otherwise I think it should give the right effect.\n<issue_comment>username_0: I pulled the traversal function out into a named functioned rather than inline in the branch above. I will run a comparison of using `ReactChildren.map` vs `traverseAllChildren` directly.\n<issue_comment>username_0: As you said I'm running into a lot of error logs due to `ReactFragment.create` being called with nulls and other invalid arguments. I think sticking with `traverseAllChildren` in this case will be better.\n<issue_comment>username_1: Sorry, I may have been a little unclear. Can you do something like this\r\n\r\n```\r\ndiff --git a/src/utils/ReactChildren.js b/src/utils/ReactChildren.js\r\nindex e470609..84ca357 100644\r\n--- a/src/utils/ReactChildren.js\r\n+++ b/src/utils/ReactChildren.js\r\n@@ -122,7 +122,7 @@ function mapChildren(children, func, context) {\r\n   var traverseContext = MapBookKeeping.getPooled(mapResult, func, context);\r\n   traverseAllChildren(children, mapSingleChildIntoContext, traverseContext);\r\n   MapBookKeeping.release(traverseContext);\r\n-  return ReactFragment.create(mapResult);\r\n+  return mapResult;\r\n }\r\n \r\n function forEachSingleChildDummy(traverseContext, child, name, i) {\r\n@@ -142,7 +142,10 @@ function countChildren(children, context) {\r\n \r\n var ReactChildren = {\r\n   forEach: forEachChildren,\r\n-  map: mapChildren,\r\n+  map: function(children, func, context) {\r\n+    return ReactFragment.create(mapChildren(children, func, context));\r\n+  },\r\n+  mapObject: mapChildren,\r\n   count: countChildren\r\n };\r\n \r\n```\r\n\r\nand then use ReactChildren.mapObject?\n<issue_comment>username_0: Ok, so I create a branch that uses ReactChildren.mapObject: https://github.com/facebook/react/compare/master...username_0:inlineInstantiateChildrenWithMap\r\n\r\nThere are two tests that fail using this method:\r\n\r\n * core/__tests__/ReactMultiChildReconcile-test ReactMultiChildReconcile should insert non-empty children in middle where nulls were.\r\n * core/__tests__/ReactMultiChildText-test ReactMultiChildText should correctly handle all possible children for render and update.\r\n\r\nI believe this is because map doesn't handle skipping nulls.\r\n\r\nI also ran a benchmark for using `traverseAllChilden` as my per previous comment vs. using `ReactChildren.mapObject` as @username_1 suggested. Running locally, I'm seeing a significant overhead in using the pooling added by `ReactChildren.mapObject`. On some runs it shows to be less performant than not inlining. You can see the travis-ci run of the benchmark here: https://travis-ci.org/username_0/react-perf/builds/60283471\n<issue_comment>username_0: @username_2 Can you elaborate one what you mean by \"internal state tree\"? I'm trying to understand what could be removed from the rendering path to optimize server rendering. I have a feeling some of the `mountComponent` logic needs to be split up into rendering logic and other logic (event registration, state management, etc.), but I'm not familiar enough with the \"other\" logic that's happening to know what to look at.\n<issue_comment>username_1: @username_0 Interesting. If inlining a closure (like https://gist.github.com/username_1/594910785229dba07d5c) is faster we should probably switch to that \u2013 we've tried to avoid closures in favor of manually passing context objects in hopes of lowering GC pressure and improving perf but all of this feels like dogscience.\r\n\r\n![image](https://cloud.githubusercontent.com/assets/6820/7360334/68f64cd2-ecfe-11e4-8e6e-b85703c9a2de.png)\n<issue_comment>username_1: Sorry for all the back and forth.\n<issue_comment>username_0: No worries. I too often feel like I have no idea what I'm doing.\r\n\r\nI think using `traverseAllChildren` directly as in https://github.com/facebook/react/compare/master...username_0:inlineInstantiateChildren addresses your concern about GC although it does duplicate some of the code in `ReactChildren.map`. We do need [this check](https://github.com/facebook/react/compare/master...username_0:inlineInstantiateChildren#diff-89f33512519fa4d853463913eac8ef04R24) though in order for the tests to pass. Any ideas on introducing that check in to `ReactChildren.map`?\n<issue_comment>username_1: I tried to replicate these results locally and in different browsers. Here's what I did:\r\n\r\n```sh\r\n$ git checkout origin/inlineMountChildren\r\n$ npm install -g browserify\r\n$ npm install babelify envify\r\n$ NODE_ENV=production browserify -t babelify bench-chat.js >bundle.js\r\n```\r\n\r\nAnd I created bench.html:\r\n\r\n```html\r\n<meta charset=\"utf-8\">\r\n<body style=\"font: 10px Menlo; white-space: pre;\">\r\n<div id=\"container\"></div>\r\n<script>\r\nconsole.log = function(x) {\r\n  container.textContent += x.replace(/\\033\\[[0-9;]*m/g, '') + '\\n';\r\n};\r\n</script>\r\n<script src=\"bundle.js\"></script>\r\n```\r\n\r\nOpening bench.html in Safari (JavaScriptCore) gave me:\r\n\r\n```\r\nRenderChat for inputs Optimized x 2,094 ops/sec \u00b10.85% (66 runs sampled)\r\nRenderChat for inputs Stock x 2,081 ops/sec \u00b10.78% (66 runs sampled)\r\nThe Fastest test suite is RenderChat for inputs Optimized,RenderChat for inputs Stock\r\n\r\n+------------+---------------+---------------+\r\n|            \u00e2\u201d\u201a Optimized     \u00e2\u201d\u201a Stock         |\r\n+------------+---------------+---------------+\r\n| RenderChat \u00e2\u201d\u201a 2,094 ops/sec \u00e2\u201d\u201a 2,081 ops/sec |\r\n+------------+---------------+---------------+\r\n```\r\n\r\nwhere they're the same speed (not sure what's up with the encoding there), and in Chrome I got:\r\n\r\n```\r\nRenderChat for inputs Optimized x 1,534 ops/sec \u00b12.38% (61 runs sampled)\r\nRenderChat for inputs Stock x 1,775 ops/sec \u00b11.56% (65 runs sampled)\r\nThe Fastest test suite is RenderChat for inputs Stock\r\n\r\n+------------+---------------+---------------+\r\n|            \u2502 Optimized     \u2502 Stock         |\r\n+------------+---------------+---------------+\r\n| RenderChat \u2502 1,534 ops/sec \u2502 1,775 ops/sec |\r\n+------------+---------------+---------------+\r\n```\r\n\r\nwhere the old version is 15% faster. Running `node bench-chat.js` gave me\r\n\r\n```\r\nRenderChat for inputs Optimized x 368 ops/sec \u00b13.18% (73 runs sampled)\r\nRenderChat for inputs Stock x 348 ops/sec \u00b12.99% (72 runs sampled)\r\nThe Fastest test suite is RenderChat for inputs Optimized\r\n\r\n+------------+-------------+-------------+\r\n|            \u2502 Optimized   \u2502 Stock       |\r\n+------------+-------------+-------------+\r\n| RenderChat \u2502 368 ops/sec \u2502 348 ops/sec |\r\n+------------+-------------+-------------+\r\n```\r\n\r\nwhere your version is maybe 5% faster, but not the 10\u201315% you were seeing originally. Any ideas if I'm doing something wrong or if this is just wildly different between engines? I'd have expected Chrome and Node to be more similar, at the very least.\n<issue_comment>username_0: Are the symlinks working correctly for you? browserify does not work for me locally to create the bundle.js as you did. I'm not sure why your results are so different from my local tests and travis-ci.\n<issue_comment>username_0: It's also worth noting, that even with two versions of stock flux, the benchmark for the item that is tested first is usually ~5% lower than the second which is why I moved \"optimized\" as the first test case. Consistency seems to be a consistent problem =/\n<issue_comment>username_0: Here's travis with 10 runs of bench-chat: https://travis-ci.org/username_0/react-perf/builds/60620846\n<issue_comment>username_0: So it looks like I didn't have the node_modules for each of the apps pushed, so the components may have been using the wrong React. I have pushed the node_modules folder for each app and used symlinks to the react versions such that they are completely isolated. I'm hoping this will make the tests more consistent for you.\r\n\r\nhttps://github.com/username_0/react-perf/tree/inlineMountChildren\n<issue_comment>username_0: Closing this since it undoes a purposeful refactor. I will open a new PR to discuss inlining just `ReactChildReconciler.instantiateChildren`."}
{"repo": "facebook/react", "issue_id": 82446120, "issue_number": 3983, "timestamp": "2015-05-29 14:15:35+00:00", "text": "The standard way to do `if` for something in the middle of a block of jsx is with a ternary or boolean expression. (ie: `{this.props.condition && <span>foo</span>}`)\r\n\r\nIf you need to do a conditional for multiple elements the natural method I expect would be:\r\n```jsx\r\n{this.props.condition && [\r\n\t<p>Foo</p>,\r\n\t<p>Bar</p>]}\r\n```\r\n\r\nHowever doing so generates warnings like:\r\n\"Warning: Each child in an array or iterator should have a unique \"key\" prop. Check the React.render call using <p>. See https://fb.me/react-warning-keys for more information.\"\r\n\r\nThis array isn't a list of things that'll be reordered or anything, it's just a set of elements that are all conditional together.\r\n\r\nNone of the alternatives to avoid the warning are very good.\r\n\r\nA) Add key=\"\" to each of the element. For a few elements, which might even be inline elements like a `<b>` next to two strings, being forced to add a key=\"\" is really overboard.\r\n\r\nB) Do the condition for each element separately:\r\n```jsx\r\n{condition && <p>Foo</p>}\r\n{condition && <p>Bar</p>}\r\n```\r\n...I don't think I need to explain why repeating the same code over and over is a bad thing.\r\n\r\nC) Wrap the elements in one element:\r\n```jsx\r\n{this.props.condition && <div>\r\n\t<p>Foo</p>,\r\n\t<p>Bar</p>\r\n</div>}\r\n```\r\n\r\nAdding extra elements into code should not be required when those elements only exist to satisfy React quirks and are not actually needed inside the DOM.", "context": "<issue_start><issue_comment>Title: Multi element conditionals trap the developer with key warnings\nusername_0: The standard way to do `if` for something in the middle of a block of jsx is with a ternary or boolean expression. (ie: `{this.props.condition && <span>foo</span>}`)\r\n\r\nIf you need to do a conditional for multiple elements the natural method I expect would be:\r\n```jsx\r\n{this.props.condition && [\r\n\t<p>Foo</p>,\r\n\t<p>Bar</p>]}\r\n```\r\n\r\nHowever doing so generates warnings like:\r\n\"Warning: Each child in an array or iterator should have a unique \"key\" prop. Check the React.render call using <p>. See https://fb.me/react-warning-keys for more information.\"\r\n\r\nThis array isn't a list of things that'll be reordered or anything, it's just a set of elements that are all conditional together.\r\n\r\nNone of the alternatives to avoid the warning are very good.\r\n\r\nA) Add key=\"\" to each of the element. For a few elements, which might even be inline elements like a `<b>` next to two strings, being forced to add a key=\"\" is really overboard.\r\n\r\nB) Do the condition for each element separately:\r\n```jsx\r\n{condition && <p>Foo</p>}\r\n{condition && <p>Bar</p>}\r\n```\r\n...I don't think I need to explain why repeating the same code over and over is a bad thing.\r\n\r\nC) Wrap the elements in one element:\r\n```jsx\r\n{this.props.condition && <div>\r\n\t<p>Foo</p>,\r\n\t<p>Bar</p>\r\n</div>}\r\n```\r\n\r\nAdding extra elements into code should not be required when those elements only exist to satisfy React quirks and are not actually needed inside the DOM.<issue_closed>\n<issue_comment>username_1: You listed basically all the options. In the future we might support a syntax like\r\n\r\n```\r\n{this.props.condition && <frag>\r\n    <p>Foo</p>\r\n    <p>Bar</p>\r\n</frag>}\r\n```\r\n\r\nwhich wouldn't result in an added HTML element, but there's nothing else to do here now."}
{"repo": "facebook/react", "issue_id": 82446120, "issue_number": 3983, "timestamp": "2015-05-29 18:04:27+00:00", "text": "", "context": "<issue_start><issue_comment>Title: Multi element conditionals trap the developer with key warnings\nusername_0: The standard way to do `if` for something in the middle of a block of jsx is with a ternary or boolean expression. (ie: `{this.props.condition && <span>foo</span>}`)\r\n\r\nIf you need to do a conditional for multiple elements the natural method I expect would be:\r\n```jsx\r\n{this.props.condition && [\r\n\t<p>Foo</p>,\r\n\t<p>Bar</p>]}\r\n```\r\n\r\nHowever doing so generates warnings like:\r\n\"Warning: Each child in an array or iterator should have a unique \"key\" prop. Check the React.render call using <p>. See https://fb.me/react-warning-keys for more information.\"\r\n\r\nThis array isn't a list of things that'll be reordered or anything, it's just a set of elements that are all conditional together.\r\n\r\nNone of the alternatives to avoid the warning are very good.\r\n\r\nA) Add key=\"\" to each of the element. For a few elements, which might even be inline elements like a `<b>` next to two strings, being forced to add a key=\"\" is really overboard.\r\n\r\nB) Do the condition for each element separately:\r\n```jsx\r\n{condition && <p>Foo</p>}\r\n{condition && <p>Bar</p>}\r\n```\r\n...I don't think I need to explain why repeating the same code over and over is a bad thing.\r\n\r\nC) Wrap the elements in one element:\r\n```jsx\r\n{this.props.condition && <div>\r\n\t<p>Foo</p>,\r\n\t<p>Bar</p>\r\n</div>}\r\n```\r\n\r\nAdding extra elements into code should not be required when those elements only exist to satisfy React quirks and are not actually needed inside the DOM.<issue_closed>\n<issue_comment>username_1: You listed basically all the options. In the future we might support a syntax like\r\n\r\n```\r\n{this.props.condition && <frag>\r\n    <p>Foo</p>\r\n    <p>Bar</p>\r\n</frag>}\r\n```\r\n\r\nwhich wouldn't result in an added HTML element, but there's nothing else to do here now."}
{"repo": "facebook/react", "issue_id": 82446120, "issue_number": 3983, "timestamp": "2015-05-29 18:04:27+00:00", "text": "You listed basically all the options. In the future we might support a syntax like\r\n\r\n```\r\n{this.props.condition && <frag>\r\n    <p>Foo</p>\r\n    <p>Bar</p>\r\n</frag>}\r\n```\r\n\r\nwhich wouldn't result in an added HTML element, but there's nothing else to do here now.", "context": "<issue_start><issue_comment>Title: Multi element conditionals trap the developer with key warnings\nusername_0: The standard way to do `if` for something in the middle of a block of jsx is with a ternary or boolean expression. (ie: `{this.props.condition && <span>foo</span>}`)\r\n\r\nIf you need to do a conditional for multiple elements the natural method I expect would be:\r\n```jsx\r\n{this.props.condition && [\r\n\t<p>Foo</p>,\r\n\t<p>Bar</p>]}\r\n```\r\n\r\nHowever doing so generates warnings like:\r\n\"Warning: Each child in an array or iterator should have a unique \"key\" prop. Check the React.render call using <p>. See https://fb.me/react-warning-keys for more information.\"\r\n\r\nThis array isn't a list of things that'll be reordered or anything, it's just a set of elements that are all conditional together.\r\n\r\nNone of the alternatives to avoid the warning are very good.\r\n\r\nA) Add key=\"\" to each of the element. For a few elements, which might even be inline elements like a `<b>` next to two strings, being forced to add a key=\"\" is really overboard.\r\n\r\nB) Do the condition for each element separately:\r\n```jsx\r\n{condition && <p>Foo</p>}\r\n{condition && <p>Bar</p>}\r\n```\r\n...I don't think I need to explain why repeating the same code over and over is a bad thing.\r\n\r\nC) Wrap the elements in one element:\r\n```jsx\r\n{this.props.condition && <div>\r\n\t<p>Foo</p>,\r\n\t<p>Bar</p>\r\n</div>}\r\n```\r\n\r\nAdding extra elements into code should not be required when those elements only exist to satisfy React quirks and are not actually needed inside the DOM.<issue_closed>\n<issue_comment>username_1: You listed basically all the options. In the future we might support a syntax like\r\n\r\n```\r\n{this.props.condition && <frag>\r\n    <p>Foo</p>\r\n    <p>Bar</p>\r\n</frag>}\r\n```\r\n\r\nwhich wouldn't result in an added HTML element, but there's nothing else to do here now."}
{"repo": "facebook/react", "issue_id": 127608401, "issue_number": 5887, "timestamp": "2016-01-20T05:36:16Z", "text": "an inconsistency with a missing semi-colon on a variable", "context": "<issue_start><issue_comment>Title: Fixup missing semi-colon\nusername_0: an inconsistency with a missing semi-colon on a variable\n<issue_comment>username_1: This is literally the smallest PR I've ever seen.\n<issue_comment>username_2: FYI: I'm going to revert this - sorry. This file is added automatically as part of the release process so this will just get overwritten unless we change the [source file](https://github.com/facebook/react/blob/master/vendor/react-dom.js)."}
{"repo": "facebook/react", "issue_id": 127608401, "issue_number": 5887, "timestamp": "2016-01-20 08:56:26+00:00", "text": "This is literally the smallest PR I've ever seen.", "context": "<issue_start><issue_comment>Title: Fixup missing semi-colon\nusername_0: an inconsistency with a missing semi-colon on a variable\n<issue_comment>username_1: This is literally the smallest PR I've ever seen.\n<issue_comment>username_2: FYI: I'm going to revert this - sorry. This file is added automatically as part of the release process so this will just get overwritten unless we change the [source file](https://github.com/facebook/react/blob/master/vendor/react-dom.js)."}
{"repo": "facebook/react", "issue_id": 127608401, "issue_number": 5887, "timestamp": "2016-01-20 21:31:09+00:00", "text": "FYI: I'm going to revert this - sorry. This file is added automatically as part of the release process so this will just get overwritten unless we change the [source file](https://github.com/facebook/react/blob/master/vendor/react-dom.js).", "context": "<issue_start><issue_comment>Title: Fixup missing semi-colon\nusername_0: an inconsistency with a missing semi-colon on a variable\n<issue_comment>username_1: This is literally the smallest PR I've ever seen.\n<issue_comment>username_2: FYI: I'm going to revert this - sorry. This file is added automatically as part of the release process so this will just get overwritten unless we change the [source file](https://github.com/facebook/react/blob/master/vendor/react-dom.js)."}
{"repo": "facebook/react", "issue_id": 259987639, "issue_number": 10787, "timestamp": "2017-09-23T06:33:17Z", "text": "handleClick = () => {\r\n    this.setState(prevState => ({\r\n      isToggleOn: !prevState.isToggleOn\r\n    }));\r\n  }\r\n\r\ninstead of \r\n\r\nhandleClick()  {\r\n    this.setState(prevState => ({\r\n      isToggleOn: !prevState.isToggleOn\r\n    }));\r\n  }\r\n\r\nthis.handleClick = this.handleClick.bind(this); \r\n\r\nNow we do not need above line of code\r\n\r\n**Before submitting a pull request,** please make sure the following is done:\r\n\r\n1. Fork [the repository](https://github.com/facebook/react) and create your branch from `master`.\r\n2. If you've added code that should be tested, add tests!\r\n3. If you've changed APIs, update the documentation.\r\n4. Ensure the test suite passes (`npm test`).\r\n5. Format your code with [prettier](https://github.com/prettier/prettier) (`npm run prettier`).\r\n6. Make sure your code lints (`npm run lint`).\r\n7. Run the [Flow](https://flowtype.org/) typechecks (`npm run flow`).\r\n8. If you haven't already, complete the CLA.", "context": "<issue_start><issue_comment>Title: Update handling-events.md\nusername_0: handleClick = () => {\r\n    this.setState(prevState => ({\r\n      isToggleOn: !prevState.isToggleOn\r\n    }));\r\n  }\r\n\r\ninstead of \r\n\r\nhandleClick()  {\r\n    this.setState(prevState => ({\r\n      isToggleOn: !prevState.isToggleOn\r\n    }));\r\n  }\r\n\r\nthis.handleClick = this.handleClick.bind(this); \r\n\r\nNow we do not need above line of code\r\n\r\n**Before submitting a pull request,** please make sure the following is done:\r\n\r\n1. Fork [the repository](https://github.com/facebook/react) and create your branch from `master`.\r\n2. If you've added code that should be tested, add tests!\r\n3. If you've changed APIs, update the documentation.\r\n4. Ensure the test suite passes (`npm test`).\r\n5. Format your code with [prettier](https://github.com/prettier/prettier) (`npm run prettier`).\r\n6. Make sure your code lints (`npm run lint`).\r\n7. Run the [Flow](https://flowtype.org/) typechecks (`npm run flow`).\r\n8. If you haven't already, complete the CLA.\n<issue_comment>username_1: Hey @username_0! We actually show the class fields version further down in the docs. Look for `LoggingButton`! \ud83d\ude04"}
{"repo": "facebook/react", "issue_id": 259987639, "issue_number": 10787, "timestamp": "2017-10-03 18:26:53+00:00", "text": "Hey @username_0! We actually show the class fields version further down in the docs. Look for `LoggingButton`! \ud83d\ude04", "context": "<issue_start><issue_comment>Title: Update handling-events.md\nusername_0: handleClick = () => {\r\n    this.setState(prevState => ({\r\n      isToggleOn: !prevState.isToggleOn\r\n    }));\r\n  }\r\n\r\ninstead of \r\n\r\nhandleClick()  {\r\n    this.setState(prevState => ({\r\n      isToggleOn: !prevState.isToggleOn\r\n    }));\r\n  }\r\n\r\nthis.handleClick = this.handleClick.bind(this); \r\n\r\nNow we do not need above line of code\r\n\r\n**Before submitting a pull request,** please make sure the following is done:\r\n\r\n1. Fork [the repository](https://github.com/facebook/react) and create your branch from `master`.\r\n2. If you've added code that should be tested, add tests!\r\n3. If you've changed APIs, update the documentation.\r\n4. Ensure the test suite passes (`npm test`).\r\n5. Format your code with [prettier](https://github.com/prettier/prettier) (`npm run prettier`).\r\n6. Make sure your code lints (`npm run lint`).\r\n7. Run the [Flow](https://flowtype.org/) typechecks (`npm run flow`).\r\n8. If you haven't already, complete the CLA.\n<issue_comment>username_1: Hey @username_0! We actually show the class fields version further down in the docs. Look for `LoggingButton`! \ud83d\ude04"}
{"repo": "facebook/react", "issue_id": 383541147, "issue_number": 14307, "timestamp": "2018-11-22T13:42:04Z", "text": "\"Here's\" should be changed to \"Here\" in the given sentence.\r\n\r\n**Before submitting a pull request,** please make sure the following is done:\r\n\r\n1. Fork [the repository](https://github.com/facebook/react) and create your branch from `master`.\r\n2. Run `yarn` in the repository root.\r\n3. If you've fixed a bug or added code that should be tested, add tests!\r\n4. Ensure the test suite passes (`yarn test`). Tip: `yarn test --watch TestName` is helpful in development.\r\n5. Run `yarn test-prod` to test in the production environment. It supports the same options as `yarn test`.\r\n6. If you need a debugger, run `yarn debug-test --watch TestName`, open `chrome://inspect`, and press \"Inspect\".\r\n7. Format your code with [prettier](https://github.com/prettier/prettier) (`yarn prettier`).\r\n8. Make sure your code lints (`yarn lint`). Tip: `yarn linc` to only check changed files.\r\n9. Run the [Flow](https://flowtype.org/) typechecks (`yarn flow`).\r\n10. If you haven't already, complete the CLA.\r\n\r\n**Learn more about contributing:** https://reactjs.org/docs/how-to-contribute.html", "context": "<issue_start><issue_comment>Title: fix spelling error: Here's -> Here\nusername_0: \"Here's\" should be changed to \"Here\" in the given sentence.\r\n\r\n**Before submitting a pull request,** please make sure the following is done:\r\n\r\n1. Fork [the repository](https://github.com/facebook/react) and create your branch from `master`.\r\n2. Run `yarn` in the repository root.\r\n3. If you've fixed a bug or added code that should be tested, add tests!\r\n4. Ensure the test suite passes (`yarn test`). Tip: `yarn test --watch TestName` is helpful in development.\r\n5. Run `yarn test-prod` to test in the production environment. It supports the same options as `yarn test`.\r\n6. If you need a debugger, run `yarn debug-test --watch TestName`, open `chrome://inspect`, and press \"Inspect\".\r\n7. Format your code with [prettier](https://github.com/prettier/prettier) (`yarn prettier`).\r\n8. Make sure your code lints (`yarn lint`). Tip: `yarn linc` to only check changed files.\r\n9. Run the [Flow](https://flowtype.org/) typechecks (`yarn flow`).\r\n10. If you haven't already, complete the CLA.\r\n\r\n**Learn more about contributing:** https://reactjs.org/docs/how-to-contribute.html\n<issue_comment>username_1: thanks"}
{"repo": "facebook/react", "issue_id": 383541147, "issue_number": 14307, "timestamp": "2018-11-22 14:47:24+00:00", "text": "thanks", "context": "<issue_start><issue_comment>Title: fix spelling error: Here's -> Here\nusername_0: \"Here's\" should be changed to \"Here\" in the given sentence.\r\n\r\n**Before submitting a pull request,** please make sure the following is done:\r\n\r\n1. Fork [the repository](https://github.com/facebook/react) and create your branch from `master`.\r\n2. Run `yarn` in the repository root.\r\n3. If you've fixed a bug or added code that should be tested, add tests!\r\n4. Ensure the test suite passes (`yarn test`). Tip: `yarn test --watch TestName` is helpful in development.\r\n5. Run `yarn test-prod` to test in the production environment. It supports the same options as `yarn test`.\r\n6. If you need a debugger, run `yarn debug-test --watch TestName`, open `chrome://inspect`, and press \"Inspect\".\r\n7. Format your code with [prettier](https://github.com/prettier/prettier) (`yarn prettier`).\r\n8. Make sure your code lints (`yarn lint`). Tip: `yarn linc` to only check changed files.\r\n9. Run the [Flow](https://flowtype.org/) typechecks (`yarn flow`).\r\n10. If you haven't already, complete the CLA.\r\n\r\n**Learn more about contributing:** https://reactjs.org/docs/how-to-contribute.html\n<issue_comment>username_1: thanks"}
{"repo": "pytorch/pytorch", "issue_id": 442425181, "issue_number": 20334, "timestamp": "2019-05-09T20:44:45Z", "text": "Stack:\n&nbsp;&nbsp;&nbsp;&nbsp;:black_circle:&nbsp; **#20334 Publish c10::RegisterOperators as torch::RegisterOperators**&nbsp;&nbsp;[:yellow_heart:](https://our.internmc.facebook.com/intern/diff/D15284557/)\n\n-\n\nDifferential Revision: [D15284557](https://our.internmc.facebook.com/intern/diff/D15284557/)", "context": "<issue_start><issue_comment>Title: Publish c10::RegisterOperators as torch::RegisterOperators\nusername_0: Stack:\n&nbsp;&nbsp;&nbsp;&nbsp;:black_circle:&nbsp; **#20334 Publish c10::RegisterOperators as torch::RegisterOperators**&nbsp;&nbsp;[:yellow_heart:](https://our.internmc.facebook.com/intern/diff/D15284557/)\n\n-\n\nDifferential Revision: [D15284557](https://our.internmc.facebook.com/intern/diff/D15284557/)"}
{"repo": "facebook/react", "issue_id": 482242488, "issue_number": 16453, "timestamp": "2019-08-19T10:55:13Z", "text": "We have some code that triggers suspense fallbacks when in an act() scope. This has caused a few annoying problems.\r\n- We don't want this codepath in prod because it's testing specific.\r\n- It's also on a 'hot' path, so we want to avoid code that we know will never be triggered\r\n- At this point, we have divergent behaviour between dev and prod, which sucks. it also probably means act() can't be used for prod tests (and it's not intuitive why)\r\n\r\nThis PR simply removes that feature. The longer I stare at it, I'm sure it should either not be a feature of act(), or we should bite the bullet and let it exist on the prod path. Fallbacks can be triggered in tests with act(jest.runAllTimers) if needed (and I assume someone will make a helper)", "context": "<issue_start><issue_comment>Title: Don't trigger fallbacks in act()\nusername_0: We have some code that triggers suspense fallbacks when in an act() scope. This has caused a few annoying problems.\r\n- We don't want this codepath in prod because it's testing specific.\r\n- It's also on a 'hot' path, so we want to avoid code that we know will never be triggered\r\n- At this point, we have divergent behaviour between dev and prod, which sucks. it also probably means act() can't be used for prod tests (and it's not intuitive why)\r\n\r\nThis PR simply removes that feature. The longer I stare at it, I'm sure it should either not be a feature of act(), or we should bite the bullet and let it exist on the prod path. Fallbacks can be triggered in tests with act(jest.runAllTimers) if needed (and I assume someone will make a helper)\n<issue_comment>username_0: Investigating other solutions, closing this PR"}
{"repo": "facebook/react", "issue_id": 482242488, "issue_number": 16453, "timestamp": "2020-02-28 12:03:01+00:00", "text": "Investigating other solutions, closing this PR", "context": "<issue_start><issue_comment>Title: Don't trigger fallbacks in act()\nusername_0: We have some code that triggers suspense fallbacks when in an act() scope. This has caused a few annoying problems.\r\n- We don't want this codepath in prod because it's testing specific.\r\n- It's also on a 'hot' path, so we want to avoid code that we know will never be triggered\r\n- At this point, we have divergent behaviour between dev and prod, which sucks. it also probably means act() can't be used for prod tests (and it's not intuitive why)\r\n\r\nThis PR simply removes that feature. The longer I stare at it, I'm sure it should either not be a feature of act(), or we should bite the bullet and let it exist on the prod path. Fallbacks can be triggered in tests with act(jest.runAllTimers) if needed (and I assume someone will make a helper)\n<issue_comment>username_0: Investigating other solutions, closing this PR"}
{"repo": "pytorch/pytorch", "issue_id": 502824683, "issue_number": 27401, "timestamp": "2019-10-04 20:14:05+00:00", "text": "Variable 'input' previously has type Tensor but is now being assigned to a value of type Dict[str, Tensor] : at <string>:4:16\r\n        def forward(self, input):\r\n            for m in self:\r\n                input = m(input)\r\n                ~~~~~ <--- HERE\r\n            return input\r\n\r\n## Expected behavior\r\n\r\nEither this should work or be documented as not working.", "context": "<issue_start><issue_comment>Title: [JIT] invoking nn.Sequential requires all intermediary results to be the same type\nusername_0: Variable 'input' previously has type Tensor but is now being assigned to a value of type Dict[str, Tensor] : at <string>:4:16\r\n        def forward(self, input):\r\n            for m in self:\r\n                input = m(input)\r\n                ~~~~~ <--- HERE\r\n            return input\r\n\r\n## Expected behavior\r\n\r\nEither this should work or be documented as not working.\n<issue_comment>username_1: @username_0 This seems to be already fixed? Please confirm and close.<issue_closed>\n<issue_comment>username_0: Correct, thanks for reminder."}
{"repo": "pytorch/pytorch", "issue_id": 502824683, "issue_number": 27401, "timestamp": "2019-11-08 07:59:16+00:00", "text": "@username_0 This seems to be already fixed? Please confirm and close.", "context": "<issue_start><issue_comment>Title: [JIT] invoking nn.Sequential requires all intermediary results to be the same type\nusername_0: Variable 'input' previously has type Tensor but is now being assigned to a value of type Dict[str, Tensor] : at <string>:4:16\r\n        def forward(self, input):\r\n            for m in self:\r\n                input = m(input)\r\n                ~~~~~ <--- HERE\r\n            return input\r\n\r\n## Expected behavior\r\n\r\nEither this should work or be documented as not working.\n<issue_comment>username_1: @username_0 This seems to be already fixed? Please confirm and close.<issue_closed>\n<issue_comment>username_0: Correct, thanks for reminder."}
{"repo": "pytorch/pytorch", "issue_id": 502824683, "issue_number": 27401, "timestamp": "2019-11-08 17:53:49+00:00", "text": "", "context": "<issue_start><issue_comment>Title: [JIT] invoking nn.Sequential requires all intermediary results to be the same type\nusername_0: Variable 'input' previously has type Tensor but is now being assigned to a value of type Dict[str, Tensor] : at <string>:4:16\r\n        def forward(self, input):\r\n            for m in self:\r\n                input = m(input)\r\n                ~~~~~ <--- HERE\r\n            return input\r\n\r\n## Expected behavior\r\n\r\nEither this should work or be documented as not working.\n<issue_comment>username_1: @username_0 This seems to be already fixed? Please confirm and close.<issue_closed>\n<issue_comment>username_0: Correct, thanks for reminder."}
{"repo": "pytorch/pytorch", "issue_id": 502824683, "issue_number": 27401, "timestamp": "2019-11-08 17:53:49+00:00", "text": "Correct, thanks for reminder.", "context": "<issue_start><issue_comment>Title: [JIT] invoking nn.Sequential requires all intermediary results to be the same type\nusername_0: Variable 'input' previously has type Tensor but is now being assigned to a value of type Dict[str, Tensor] : at <string>:4:16\r\n        def forward(self, input):\r\n            for m in self:\r\n                input = m(input)\r\n                ~~~~~ <--- HERE\r\n            return input\r\n\r\n## Expected behavior\r\n\r\nEither this should work or be documented as not working.\n<issue_comment>username_1: @username_0 This seems to be already fixed? Please confirm and close.<issue_closed>\n<issue_comment>username_0: Correct, thanks for reminder."}
{"repo": "pytorch/pytorch", "issue_id": 537865838, "issue_number": 31282, "timestamp": "2019-12-14T04:42:49Z", "text": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#31282 Refactor custom op tests**\n\nIntroduce a helper to easily call stack ops\n\nDifferential Revision: [D19061515](https://our.internmc.facebook.com/intern/diff/D19061515/)", "context": "<issue_start><issue_comment>Title: Refactor custom op tests\nusername_0: Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#31282 Refactor custom op tests**\n\nIntroduce a helper to easily call stack ops\n\nDifferential Revision: [D19061515](https://our.internmc.facebook.com/intern/diff/D19061515/)\n<issue_comment>username_1: ## CircleCI build failures summary\nAs of commit e49d95d:\n* **3/3** failures introduced in this PR\n\n\n\n## Detailed failure analysis\n\nOne may explore the probable reasons each build failed interactively [on the Dr. CI website](https://dr.pytorch.org/commit-details.html?sha1=e49d95d97b46a0d0f0ef58b556a1fc84a63e8836).\n\n### 3 new failures recognized by patterns\nThe following build failures don't appear to be due to upstream breakage:\n#### [![See CircleCI build](https://avatars0.githubusercontent.com/ml/7?s=12)](https://circleci.com/gh/pytorch/pytorch/3973809) pytorch_linux_xenial_py3_6_gcc5_4_build (1/3)\n**Step:** \"Build\" ([full log](https://dr.pytorch.org/api/view-log-full?build_id=75639989) | [pattern match details](https://dr.pytorch.org/build-details.html?build_id=75639989))\n<details>\n<summary>\n<code>Dec 17 02:16:34 ERROR:sccache::server: Compilation failed: Output { status: ExitStatus(ExitStatus(256)), stdout: \"\", stderr: \"/var/lib/jenkins/workspace/test/custom_operator/test_custom_ops.cpp: In function \\'void get_operator_from_registry_and_execute()\\':\\n/var/lib/jenkins/workspace/test/custom_operator/test_custom_ops.cpp:42:80: error: conversion from \\'void\\' to non-scalar type \\'std::vector<at::Tensor>\\' requested\\n     helpers::get_operator_from_registry_and_execute<std::vector<torch::Tensor>>(\\\"custom::op\\\", torch::ones(5), 2.0, 3);\\n                                                                                ^\\n/var/lib/jenkins/workspace/test/custom_operator/test_custom_ops.cpp: In function \\'void get_autograd_operator_from_registry_and_execute()\\':\\n/var/lib/jenkins/workspace/test/custom_operator/test_custom_ops.cpp:58:67: error: conversion from \\'void\\' to non-scalar type \\'at::Tensor\\' requested\\n     helpers::get_operator_from_registry_and_execute<torch::Tensor>(\\\"custom::op_with_autograd\\\", x, 2, y);\\n                                                                   ^\\n/var/lib/jenkins/workspace/test/custom_operator/test_custom_ops.cpp: In function \\'void get_autograd_operator_from_registry_and_execute_in_nograd_mode()\\':\\n/var/lib/jenkins/workspace/test/custom_operator/test_custom_ops.cpp:75:67: error: conversion from \\'void\\' to non-scalar type \\'at::Tensor\\' requested\\n     helpers::get_operator_from_registry_and_execute<torch::Tensor>(\\\"custom::op_with_autograd\\\", x, 2, y);\\n                                                                   ^\\n/var/lib/jenkins/workspace/test/custom_operator/test_custom_ops.cpp: In instantiation of \\'void helpers::get_operator_from_registry_and_execute(const char*, Args&& ...) [with Result = std::vector<at::Tensor>; Args = {at::Tensor, double, int}]\\':\\n/var/lib/jenkins/workspace/test/custom_operator/test_custom_ops.cpp:42:117:   required from here\\n/var/lib/jenkins/workspace/test/custom_operator/test_custom_ops.cpp:36:44: error: return-statement with a value, in function returning \\'void\\' [-fpermissive]\\n   return torch::jit::pop(stack).to<Result>();\\n                                            ^\\n/var/lib/jenkins/workspace/test/custom_operator/test_custom_ops.cpp: In instantiation of \\'void helpers::get_operator_from_registry_and_execute(const char*, Args&& ...) [with Result = at::Tensor; Args = {at::Tensor&, int, at::Tensor&}]\\':\\n/var/lib/jenkins/workspace/test/custom_operator/test_custom_ops.cpp:58:103:   required from here\\n/var/lib/jenkins/workspace/test/custom_operator/test_custom_ops.cpp:36:44: error: return-statement with a value, in function returning \\'void\\' [-fpermissive]\\n\" }</code>\n</summary>\n\n```\nDec 17 02:16:34 CMakeFiles/test_custom_ops.dir/build.make:62: recipe for target 'CMakeFiles/test_custom_ops.dir/test_custom_ops.cpp.o' failed \nDec 17 02:16:34 make[2]: Leaving directory '/var/lib/jenkins/custom-op-build' \nDec 17 02:16:34 CMakeFiles/Makefile2:141: recipe for target 'CMakeFiles/test_custom_ops.dir/all' failed \nDec 17 02:16:34 make[1]: Leaving directory '/var/lib/jenkins/custom-op-build' \nDec 17 02:16:34 make: *** [all] Error 2 \nDec 17 02:16:34 Makefile:83: recipe for target 'all' failed \nDec 17 02:16:34 + cleanup \nDec 17 02:16:34 + retcode=2 \nDec 17 02:16:34 + set +x \nDec 17 02:16:34 =================== sccache compilation log =================== \nar/lib/jenkins/workspace/test/custom_operator/test_custom_ops.cpp: In instantiation of \\'void helpers::get_operator_from_registry_and_execute(const char*, Args&& ...) [with Result = at::Tensor; Args = {at::Tensor&, int, at::Tensor&}]\\':\\n/var/lib/jenkins/workspace/test/custom_operator/test_custom_ops.cpp:58:103:   required from here\\n/var/lib/jenkins/workspace/test/custom_operator/test_custom_ops.cpp:36:44: error: return-statement with a value, in function returning \\'void\\' [-fpermissive]\\n\" } \nDec 17 02:16:34  \nDec 17 02:16:34 =========== If your build fails, please take a look at the log above for possible reasons =========== \nDec 17 02:16:34 Compile requests              2510 \nDec 17 02:16:34 Compile requests executed     2252 \nDec 17 02:16:34 Cache hits                    2237 \nDec 17 02:16:34 Cache misses                     0 \nDec 17 02:16:34 Cache timeouts                   0 \nDec 17 02:16:34 Cache read errors                0 \nDec 17 02:16:34 Forced recaches                  0 \nDec 17 02:16:34 Cache write errors               0 \n```\n\n</details>\n\n#### [![See CircleCI build](https://avatars0.githubusercontent.com/ml/7?s=12)](https://circleci.com/gh/pytorch/pytorch/3973801) pytorch_xla_linux_xenial_py3_6_clang7_build (2/3)\n**Step:** \"Build\" ([full log](https://dr.pytorch.org/api/view-log-full?build_id=75641320) | [pattern match details](https://dr.pytorch.org/build-details.html?build_id=75641320))\n<details>\n<summary>\n<code>Dec 17 02:17:04 ERROR:sccache::server: Compilation failed: Output { status: ExitStatus(ExitStatus(256)), stdout: \"\", stderr: \"/var/lib/jenkins/workspace/test/custom_operator/test_custom_ops.cpp:41:30: error: no viable conversion from \\'void\\' to \\'std::vector<torch::Tensor>\\'\\n  std::vector<torch::Tensor> output =\\n                             ^\\n/usr/bin/../lib/gcc/x86_64-linux-gnu/5.4.0/../../../../include/c++/5.4.0/bits/stl_vector.h:318:7: note: candidate constructor not viable: cannot convert argument of incomplete type \\'void\\' to \\'const std::vector<at::Tensor, std::allocator<at::Tensor> > &\\' for 1st argument\\n      vector(const vector& __x)\\n      ^\\n/usr/bin/../lib/gcc/x86_64-linux-gnu/5.4.0/../../../../include/c++/5.4.0/bits/stl_vector.h:335:7: note: candidate constructor not viable: cannot convert argument of incomplete type \\'void\\' to \\'std::vector<at::Tensor, std::allocator<at::Tensor> > &&\\' for 1st argument\\n      vector(vector&& __x) noexcept\\n      ^\\n/usr/bin/../lib/gcc/x86_64-linux-gnu/5.4.0/../../../../include/c++/5.4.0/bits/stl_vector.h:373:7: note: candidate constructor not viable: cannot convert argument of incomplete type \\'void\\' to \\'initializer_list<std::vector<at::Tensor, std::allocator<at::Tensor> >::value_type>\\' (aka \\'initializer_list<at::Tensor>\\') for 1st argument\\n      vector(initializer_list<value_type> __l,\\n      ^\\n/var/lib/jenkins/workspace/test/custom_operator/test_custom_ops.cpp:57:17: error: no viable conversion from \\'void\\' to \\'torch::Tensor\\'\\n  torch::Tensor output =\\n                ^\\n/opt/conda/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:87:3: note: candidate constructor not viable: cannot convert argument of incomplete type \\'void\\' to \\'const at::Tensor &\\' for 1st argument\\n  Tensor(const Tensor&) = default;\\n  ^\\n/opt/conda/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:88:3: note: candidate constructor not viable: cannot convert argument of incomplete type \\'void\\' to \\'at::Tensor &&\\' for 1st argument\\n  Tensor(Tensor&&) = default;\\n  ^\\n/var/lib/jenkins/workspace/test/custom_operator/test_custom_ops.cpp:74:17: error: no viable conversion from \\'void\\' to \\'torch::Tensor\\'\\n  torch::Tensor output =\\n                ^\\n/opt/conda/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:87:3: note: candidate constructor not viable: cannot convert argument of incomplete type \\'void\\' to \\'const at::Tensor &\\' for 1st argument\\n  Tensor(const Tensor&) = default;\\n  ^\\n/opt/conda/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:88:3: note: candidate constructor not viable: cannot convert argument of incomplete type \\'void\\' to \\'at::Tensor &&\\' for 1st argument\\n  Tensor(Tensor&&) = default;\\n  ^\\n/var/lib/jenkins/workspace/test/custom_operator/test_custom_ops.cpp:36:3: error: void function \\'get_operator_from_registry_and_execute\\' should not return a value [-Wreturn-type]\\n  return torch::jit::pop(stack).to<Result>();\\n  ^      ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\n/var/lib/jenkins/workspace/test/custom_operator/test_custom_ops.cpp:42:14: note: in instantiation of function template specialization \\'helpers::get_operator_from_registry_and_execute<std::vector<at::Tensor, std::allocator<at::Tensor> >, at::Tensor, double, int>\\' requested here\\n    helpers::get_operator_from_registry_and_execute<std::vector<torch::Tensor>>(\\\"custom::op\\\", torch::ones(5), 2.0, 3);\\n             ^\\n/var/lib/jenkins/workspace/test/custom_operator/test_custom_ops.cpp:36:3: error: void function \\'get_operator_from_registry_and_execute\\' should not return a value [-Wreturn-type]\\n  return torch::jit::pop(stack).to<Result>();\\n  ^      ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\n/var/lib/jenkins/workspace/test/custom_operator/test_custom_ops.cpp:58:14: note: in instantiation of function template specialization \\'helpers::get_operator_from_registry_and_execute<at::Tensor, at::Tensor &, int, at::Tensor &>\\' requested here\\n    helpers::get_operator_from_registry_and_execute<torch::Tensor>(\\\"custom::op_with_autograd\\\", x, 2, y);\\n             ^\\n5 errors generated.\\n\" }</code>\n</summary>\n\n```\nDec 17 02:17:04 CMakeFiles/test_custom_ops.dir/build.make:62: recipe for target 'CMakeFiles/test_custom_ops.dir/test_custom_ops.cpp.o' failed \nDec 17 02:17:04 make[2]: Leaving directory '/var/lib/jenkins/custom-op-build' \nDec 17 02:17:04 CMakeFiles/Makefile2:141: recipe for target 'CMakeFiles/test_custom_ops.dir/all' failed \nDec 17 02:17:04 make[1]: Leaving directory '/var/lib/jenkins/custom-op-build' \nDec 17 02:17:04 make: *** [all] Error 2 \nDec 17 02:17:04 Makefile:83: recipe for target 'all' failed \nDec 17 02:17:04 + cleanup \nDec 17 02:17:04 + retcode=2 \nDec 17 02:17:04 + set +x \nDec 17 02:17:04 =================== sccache compilation log =================== \nurn-type]\\n  return torch::jit::pop(stack).to<Result>();\\n  ^      ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\n/var/lib/jenkins/workspace/test/custom_operator/test_custom_ops.cpp:58:14: note: in instantiation of function template specialization \\'helpers::get_operator_from_registry_and_execute<at::Tensor, at::Tensor &, int, at::Tensor &>\\' requested here\\n    helpers::get_operator_from_registry_and_execute<torch::Tensor>(\\\"custom::op_with_autograd\\\", x, 2, y);\\n             ^\\n5 errors generated.\\n\" } \nDec 17 02:17:04  \nDec 17 02:17:04 =========== If your build fails, please take a look at the log above for possible reasons =========== \nDec 17 02:17:04 Compile requests              2518 \nDec 17 02:17:04 Compile requests executed     2260 \nDec 17 02:17:04 Cache hits                    2246 \nDec 17 02:17:04 Cache misses                     0 \nDec 17 02:17:04 Cache timeouts                   0 \nDec 17 02:17:04 Cache read errors                0 \nDec 17 02:17:04 Forced recaches                  0 \nDec 17 02:17:04 Cache write errors               0 \n```\n\n</details>\n\n#### [![See CircleCI build](https://avatars0.githubusercontent.com/ml/7?s=12)](https://circleci.com/gh/pytorch/pytorch/3973802) pytorch_linux_xenial_py2_7_9_build (3/3)\n**Step:** \"Build\" ([full log](https://dr.pytorch.org/api/view-log-full?build_id=75641755) | [pattern match details](https://dr.pytorch.org/build-details.html?build_id=75641755))\n[Truncated]\nDec 17 02:17:14 Compile requests              2519 \nDec 17 02:17:14 Compile requests executed     2260 \nDec 17 02:17:14 Cache hits                    2246 \nDec 17 02:17:14 Cache misses                     0 \nDec 17 02:17:14 Cache timeouts                   0 \nDec 17 02:17:14 Cache read errors                0 \nDec 17 02:17:14 Forced recaches                  0 \nDec 17 02:17:14 Cache write errors               0 \n```\n\n</details>\n\n\n\n---\n\n<details><summary>This comment was automatically generated by <a href=\"https://github.com/username_1/circleci-failure-tracker/tree/master/docs/from-pull-request-comment\">Dr. CI</a> (expand for details).</summary>Follow <a href=\"https://dr.pytorch.org/admin/comments-opt-out.html\">this link to opt-out</a> of these comments for your Pull Requests.\n\nPlease report bugs/suggestions on the <a href=\"https://github.com/username_1/circleci-failure-tracker/issues\">GitHub issue tracker</a>.\n</details>"}
{"repo": "pytorch/pytorch", "issue_id": 537865838, "issue_number": 31282, "timestamp": "2019-12-17 02:20:53+00:00", "text": "## CircleCI build failures summary\nAs of commit e49d95d:\n* **3/3** failures introduced in this PR\n\n\n\n## Detailed failure analysis\n\nOne may explore the probable reasons each build failed interactively [on the Dr. CI website](https://dr.pytorch.org/commit-details.html?sha1=e49d95d97b46a0d0f0ef58b556a1fc84a63e8836).\n\n### 3 new failures recognized by patterns\nThe following build failures don't appear to be due to upstream breakage:\n#### [![See CircleCI build](https://avatars0.githubusercontent.com/ml/7?s=12)](https://circleci.com/gh/pytorch/pytorch/3973809) pytorch_linux_xenial_py3_6_gcc5_4_build (1/3)\n**Step:** \"Build\" ([full log](https://dr.pytorch.org/api/view-log-full?build_id=75639989) | [pattern match details](https://dr.pytorch.org/build-details.html?build_id=75639989))\n<details>\n<summary>\n<code>Dec 17 02:16:34 ERROR:sccache::server: Compilation failed: Output { status: ExitStatus(ExitStatus(256)), stdout: \"\", stderr: \"/var/lib/jenkins/workspace/test/custom_operator/test_custom_ops.cpp: In function \\'void get_operator_from_registry_and_execute()\\':\\n/var/lib/jenkins/workspace/test/custom_operator/test_custom_ops.cpp:42:80: error: conversion from \\'void\\' to non-scalar type \\'std::vector<at::Tensor>\\' requested\\n     helpers::get_operator_from_registry_and_execute<std::vector<torch::Tensor>>(\\\"custom::op\\\", torch::ones(5), 2.0, 3);\\n                                                                                ^\\n/var/lib/jenkins/workspace/test/custom_operator/test_custom_ops.cpp: In function \\'void get_autograd_operator_from_registry_and_execute()\\':\\n/var/lib/jenkins/workspace/test/custom_operator/test_custom_ops.cpp:58:67: error: conversion from \\'void\\' to non-scalar type \\'at::Tensor\\' requested\\n     helpers::get_operator_from_registry_and_execute<torch::Tensor>(\\\"custom::op_with_autograd\\\", x, 2, y);\\n                                                                   ^\\n/var/lib/jenkins/workspace/test/custom_operator/test_custom_ops.cpp: In function \\'void get_autograd_operator_from_registry_and_execute_in_nograd_mode()\\':\\n/var/lib/jenkins/workspace/test/custom_operator/test_custom_ops.cpp:75:67: error: conversion from \\'void\\' to non-scalar type \\'at::Tensor\\' requested\\n     helpers::get_operator_from_registry_and_execute<torch::Tensor>(\\\"custom::op_with_autograd\\\", x, 2, y);\\n                                                                   ^\\n/var/lib/jenkins/workspace/test/custom_operator/test_custom_ops.cpp: In instantiation of \\'void helpers::get_operator_from_registry_and_execute(const char*, Args&& ...) [with Result = std::vector<at::Tensor>; Args = {at::Tensor, double, int}]\\':\\n/var/lib/jenkins/workspace/test/custom_operator/test_custom_ops.cpp:42:117:   required from here\\n/var/lib/jenkins/workspace/test/custom_operator/test_custom_ops.cpp:36:44: error: return-statement with a value, in function returning \\'void\\' [-fpermissive]\\n   return torch::jit::pop(stack).to<Result>();\\n                                            ^\\n/var/lib/jenkins/workspace/test/custom_operator/test_custom_ops.cpp: In instantiation of \\'void helpers::get_operator_from_registry_and_execute(const char*, Args&& ...) [with Result = at::Tensor; Args = {at::Tensor&, int, at::Tensor&}]\\':\\n/var/lib/jenkins/workspace/test/custom_operator/test_custom_ops.cpp:58:103:   required from here\\n/var/lib/jenkins/workspace/test/custom_operator/test_custom_ops.cpp:36:44: error: return-statement with a value, in function returning \\'void\\' [-fpermissive]\\n\" }</code>\n</summary>\n\n```\nDec 17 02:16:34 CMakeFiles/test_custom_ops.dir/build.make:62: recipe for target 'CMakeFiles/test_custom_ops.dir/test_custom_ops.cpp.o' failed \nDec 17 02:16:34 make[2]: Leaving directory '/var/lib/jenkins/custom-op-build' \nDec 17 02:16:34 CMakeFiles/Makefile2:141: recipe for target 'CMakeFiles/test_custom_ops.dir/all' failed \nDec 17 02:16:34 make[1]: Leaving directory '/var/lib/jenkins/custom-op-build' \nDec 17 02:16:34 make: *** [all] Error 2 \nDec 17 02:16:34 Makefile:83: recipe for target 'all' failed \nDec 17 02:16:34 + cleanup \nDec 17 02:16:34 + retcode=2 \nDec 17 02:16:34 + set +x \nDec 17 02:16:34 =================== sccache compilation log =================== \nar/lib/jenkins/workspace/test/custom_operator/test_custom_ops.cpp: In instantiation of \\'void helpers::get_operator_from_registry_and_execute(const char*, Args&& ...) [with Result = at::Tensor; Args = {at::Tensor&, int, at::Tensor&}]\\':\\n/var/lib/jenkins/workspace/test/custom_operator/test_custom_ops.cpp:58:103:   required from here\\n/var/lib/jenkins/workspace/test/custom_operator/test_custom_ops.cpp:36:44: error: return-statement with a value, in function returning \\'void\\' [-fpermissive]\\n\" } \nDec 17 02:16:34  \nDec 17 02:16:34 =========== If your build fails, please take a look at the log above for possible reasons =========== \nDec 17 02:16:34 Compile requests              2510 \nDec 17 02:16:34 Compile requests executed     2252 \nDec 17 02:16:34 Cache hits                    2237 \nDec 17 02:16:34 Cache misses                     0 \nDec 17 02:16:34 Cache timeouts                   0 \nDec 17 02:16:34 Cache read errors                0 \nDec 17 02:16:34 Forced recaches                  0 \nDec 17 02:16:34 Cache write errors               0 \n```\n\n</details>\n\n#### [![See CircleCI build](https://avatars0.githubusercontent.com/ml/7?s=12)](https://circleci.com/gh/pytorch/pytorch/3973801) pytorch_xla_linux_xenial_py3_6_clang7_build (2/3)\n**Step:** \"Build\" ([full log](https://dr.pytorch.org/api/view-log-full?build_id=75641320) | [pattern match details](https://dr.pytorch.org/build-details.html?build_id=75641320))\n<details>\n<summary>\n<code>Dec 17 02:17:04 ERROR:sccache::server: Compilation failed: Output { status: ExitStatus(ExitStatus(256)), stdout: \"\", stderr: \"/var/lib/jenkins/workspace/test/custom_operator/test_custom_ops.cpp:41:30: error: no viable conversion from \\'void\\' to \\'std::vector<torch::Tensor>\\'\\n  std::vector<torch::Tensor> output =\\n                             ^\\n/usr/bin/../lib/gcc/x86_64-linux-gnu/5.4.0/../../../../include/c++/5.4.0/bits/stl_vector.h:318:7: note: candidate constructor not viable: cannot convert argument of incomplete type \\'void\\' to \\'const std::vector<at::Tensor, std::allocator<at::Tensor> > &\\' for 1st argument\\n      vector(const vector& __x)\\n      ^\\n/usr/bin/../lib/gcc/x86_64-linux-gnu/5.4.0/../../../../include/c++/5.4.0/bits/stl_vector.h:335:7: note: candidate constructor not viable: cannot convert argument of incomplete type \\'void\\' to \\'std::vector<at::Tensor, std::allocator<at::Tensor> > &&\\' for 1st argument\\n      vector(vector&& __x) noexcept\\n      ^\\n/usr/bin/../lib/gcc/x86_64-linux-gnu/5.4.0/../../../../include/c++/5.4.0/bits/stl_vector.h:373:7: note: candidate constructor not viable: cannot convert argument of incomplete type \\'void\\' to \\'initializer_list<std::vector<at::Tensor, std::allocator<at::Tensor> >::value_type>\\' (aka \\'initializer_list<at::Tensor>\\') for 1st argument\\n      vector(initializer_list<value_type> __l,\\n      ^\\n/var/lib/jenkins/workspace/test/custom_operator/test_custom_ops.cpp:57:17: error: no viable conversion from \\'void\\' to \\'torch::Tensor\\'\\n  torch::Tensor output =\\n                ^\\n/opt/conda/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:87:3: note: candidate constructor not viable: cannot convert argument of incomplete type \\'void\\' to \\'const at::Tensor &\\' for 1st argument\\n  Tensor(const Tensor&) = default;\\n  ^\\n/opt/conda/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:88:3: note: candidate constructor not viable: cannot convert argument of incomplete type \\'void\\' to \\'at::Tensor &&\\' for 1st argument\\n  Tensor(Tensor&&) = default;\\n  ^\\n/var/lib/jenkins/workspace/test/custom_operator/test_custom_ops.cpp:74:17: error: no viable conversion from \\'void\\' to \\'torch::Tensor\\'\\n  torch::Tensor output =\\n                ^\\n/opt/conda/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:87:3: note: candidate constructor not viable: cannot convert argument of incomplete type \\'void\\' to \\'const at::Tensor &\\' for 1st argument\\n  Tensor(const Tensor&) = default;\\n  ^\\n/opt/conda/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:88:3: note: candidate constructor not viable: cannot convert argument of incomplete type \\'void\\' to \\'at::Tensor &&\\' for 1st argument\\n  Tensor(Tensor&&) = default;\\n  ^\\n/var/lib/jenkins/workspace/test/custom_operator/test_custom_ops.cpp:36:3: error: void function \\'get_operator_from_registry_and_execute\\' should not return a value [-Wreturn-type]\\n  return torch::jit::pop(stack).to<Result>();\\n  ^      ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\n/var/lib/jenkins/workspace/test/custom_operator/test_custom_ops.cpp:42:14: note: in instantiation of function template specialization \\'helpers::get_operator_from_registry_and_execute<std::vector<at::Tensor, std::allocator<at::Tensor> >, at::Tensor, double, int>\\' requested here\\n    helpers::get_operator_from_registry_and_execute<std::vector<torch::Tensor>>(\\\"custom::op\\\", torch::ones(5), 2.0, 3);\\n             ^\\n/var/lib/jenkins/workspace/test/custom_operator/test_custom_ops.cpp:36:3: error: void function \\'get_operator_from_registry_and_execute\\' should not return a value [-Wreturn-type]\\n  return torch::jit::pop(stack).to<Result>();\\n  ^      ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\n/var/lib/jenkins/workspace/test/custom_operator/test_custom_ops.cpp:58:14: note: in instantiation of function template specialization \\'helpers::get_operator_from_registry_and_execute<at::Tensor, at::Tensor &, int, at::Tensor &>\\' requested here\\n    helpers::get_operator_from_registry_and_execute<torch::Tensor>(\\\"custom::op_with_autograd\\\", x, 2, y);\\n             ^\\n5 errors generated.\\n\" }</code>\n</summary>\n\n```\nDec 17 02:17:04 CMakeFiles/test_custom_ops.dir/build.make:62: recipe for target 'CMakeFiles/test_custom_ops.dir/test_custom_ops.cpp.o' failed \nDec 17 02:17:04 make[2]: Leaving directory '/var/lib/jenkins/custom-op-build' \nDec 17 02:17:04 CMakeFiles/Makefile2:141: recipe for target 'CMakeFiles/test_custom_ops.dir/all' failed \nDec 17 02:17:04 make[1]: Leaving directory '/var/lib/jenkins/custom-op-build' \nDec 17 02:17:04 make: *** [all] Error 2 \nDec 17 02:17:04 Makefile:83: recipe for target 'all' failed \nDec 17 02:17:04 + cleanup \nDec 17 02:17:04 + retcode=2 \nDec 17 02:17:04 + set +x \nDec 17 02:17:04 =================== sccache compilation log =================== \nurn-type]\\n  return torch::jit::pop(stack).to<Result>();\\n  ^      ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\n/var/lib/jenkins/workspace/test/custom_operator/test_custom_ops.cpp:58:14: note: in instantiation of function template specialization \\'helpers::get_operator_from_registry_and_execute<at::Tensor, at::Tensor &, int, at::Tensor &>\\' requested here\\n    helpers::get_operator_from_registry_and_execute<torch::Tensor>(\\\"custom::op_with_autograd\\\", x, 2, y);\\n             ^\\n5 errors generated.\\n\" } \nDec 17 02:17:04  \nDec 17 02:17:04 =========== If your build fails, please take a look at the log above for possible reasons =========== \nDec 17 02:17:04 Compile requests              2518 \nDec 17 02:17:04 Compile requests executed     2260 \nDec 17 02:17:04 Cache hits                    2246 \nDec 17 02:17:04 Cache misses                     0 \nDec 17 02:17:04 Cache timeouts                   0 \nDec 17 02:17:04 Cache read errors                0 \nDec 17 02:17:04 Forced recaches                  0 \nDec 17 02:17:04 Cache write errors               0 \n```\n\n</details>\n\n#### [![See CircleCI build](https://avatars0.githubusercontent.com/ml/7?s=12)](https://circleci.com/gh/pytorch/pytorch/3973802) pytorch_linux_xenial_py2_7_9_build (3/3)\n**Step:** \"Build\" ([full log](https://dr.pytorch.org/api/view-log-full?build_id=75641755) | [pattern match details](https://dr.pytorch.org/build-details.html?build_id=75641755))\n[Truncated]\nDec 17 02:17:14 Compile requests              2519 \nDec 17 02:17:14 Compile requests executed     2260 \nDec 17 02:17:14 Cache hits                    2246 \nDec 17 02:17:14 Cache misses                     0 \nDec 17 02:17:14 Cache timeouts                   0 \nDec 17 02:17:14 Cache read errors                0 \nDec 17 02:17:14 Forced recaches                  0 \nDec 17 02:17:14 Cache write errors               0 \n```\n\n</details>\n\n\n\n---\n\n<details><summary>This comment was automatically generated by <a href=\"https://github.com/username_1/circleci-failure-tracker/tree/master/docs/from-pull-request-comment\">Dr. CI</a> (expand for details).</summary>Follow <a href=\"https://dr.pytorch.org/admin/comments-opt-out.html\">this link to opt-out</a> of these comments for your Pull Requests.\n\nPlease report bugs/suggestions on the <a href=\"https://github.com/username_1/circleci-failure-tracker/issues\">GitHub issue tracker</a>.\n</details>", "context": "<issue_start><issue_comment>Title: Refactor custom op tests\nusername_0: Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#31282 Refactor custom op tests**\n\nIntroduce a helper to easily call stack ops\n\nDifferential Revision: [D19061515](https://our.internmc.facebook.com/intern/diff/D19061515/)\n<issue_comment>username_1: ## CircleCI build failures summary\nAs of commit e49d95d:\n* **3/3** failures introduced in this PR\n\n\n\n## Detailed failure analysis\n\nOne may explore the probable reasons each build failed interactively [on the Dr. CI website](https://dr.pytorch.org/commit-details.html?sha1=e49d95d97b46a0d0f0ef58b556a1fc84a63e8836).\n\n### 3 new failures recognized by patterns\nThe following build failures don't appear to be due to upstream breakage:\n#### [![See CircleCI build](https://avatars0.githubusercontent.com/ml/7?s=12)](https://circleci.com/gh/pytorch/pytorch/3973809) pytorch_linux_xenial_py3_6_gcc5_4_build (1/3)\n**Step:** \"Build\" ([full log](https://dr.pytorch.org/api/view-log-full?build_id=75639989) | [pattern match details](https://dr.pytorch.org/build-details.html?build_id=75639989))\n<details>\n<summary>\n<code>Dec 17 02:16:34 ERROR:sccache::server: Compilation failed: Output { status: ExitStatus(ExitStatus(256)), stdout: \"\", stderr: \"/var/lib/jenkins/workspace/test/custom_operator/test_custom_ops.cpp: In function \\'void get_operator_from_registry_and_execute()\\':\\n/var/lib/jenkins/workspace/test/custom_operator/test_custom_ops.cpp:42:80: error: conversion from \\'void\\' to non-scalar type \\'std::vector<at::Tensor>\\' requested\\n     helpers::get_operator_from_registry_and_execute<std::vector<torch::Tensor>>(\\\"custom::op\\\", torch::ones(5), 2.0, 3);\\n                                                                                ^\\n/var/lib/jenkins/workspace/test/custom_operator/test_custom_ops.cpp: In function \\'void get_autograd_operator_from_registry_and_execute()\\':\\n/var/lib/jenkins/workspace/test/custom_operator/test_custom_ops.cpp:58:67: error: conversion from \\'void\\' to non-scalar type \\'at::Tensor\\' requested\\n     helpers::get_operator_from_registry_and_execute<torch::Tensor>(\\\"custom::op_with_autograd\\\", x, 2, y);\\n                                                                   ^\\n/var/lib/jenkins/workspace/test/custom_operator/test_custom_ops.cpp: In function \\'void get_autograd_operator_from_registry_and_execute_in_nograd_mode()\\':\\n/var/lib/jenkins/workspace/test/custom_operator/test_custom_ops.cpp:75:67: error: conversion from \\'void\\' to non-scalar type \\'at::Tensor\\' requested\\n     helpers::get_operator_from_registry_and_execute<torch::Tensor>(\\\"custom::op_with_autograd\\\", x, 2, y);\\n                                                                   ^\\n/var/lib/jenkins/workspace/test/custom_operator/test_custom_ops.cpp: In instantiation of \\'void helpers::get_operator_from_registry_and_execute(const char*, Args&& ...) [with Result = std::vector<at::Tensor>; Args = {at::Tensor, double, int}]\\':\\n/var/lib/jenkins/workspace/test/custom_operator/test_custom_ops.cpp:42:117:   required from here\\n/var/lib/jenkins/workspace/test/custom_operator/test_custom_ops.cpp:36:44: error: return-statement with a value, in function returning \\'void\\' [-fpermissive]\\n   return torch::jit::pop(stack).to<Result>();\\n                                            ^\\n/var/lib/jenkins/workspace/test/custom_operator/test_custom_ops.cpp: In instantiation of \\'void helpers::get_operator_from_registry_and_execute(const char*, Args&& ...) [with Result = at::Tensor; Args = {at::Tensor&, int, at::Tensor&}]\\':\\n/var/lib/jenkins/workspace/test/custom_operator/test_custom_ops.cpp:58:103:   required from here\\n/var/lib/jenkins/workspace/test/custom_operator/test_custom_ops.cpp:36:44: error: return-statement with a value, in function returning \\'void\\' [-fpermissive]\\n\" }</code>\n</summary>\n\n```\nDec 17 02:16:34 CMakeFiles/test_custom_ops.dir/build.make:62: recipe for target 'CMakeFiles/test_custom_ops.dir/test_custom_ops.cpp.o' failed \nDec 17 02:16:34 make[2]: Leaving directory '/var/lib/jenkins/custom-op-build' \nDec 17 02:16:34 CMakeFiles/Makefile2:141: recipe for target 'CMakeFiles/test_custom_ops.dir/all' failed \nDec 17 02:16:34 make[1]: Leaving directory '/var/lib/jenkins/custom-op-build' \nDec 17 02:16:34 make: *** [all] Error 2 \nDec 17 02:16:34 Makefile:83: recipe for target 'all' failed \nDec 17 02:16:34 + cleanup \nDec 17 02:16:34 + retcode=2 \nDec 17 02:16:34 + set +x \nDec 17 02:16:34 =================== sccache compilation log =================== \nar/lib/jenkins/workspace/test/custom_operator/test_custom_ops.cpp: In instantiation of \\'void helpers::get_operator_from_registry_and_execute(const char*, Args&& ...) [with Result = at::Tensor; Args = {at::Tensor&, int, at::Tensor&}]\\':\\n/var/lib/jenkins/workspace/test/custom_operator/test_custom_ops.cpp:58:103:   required from here\\n/var/lib/jenkins/workspace/test/custom_operator/test_custom_ops.cpp:36:44: error: return-statement with a value, in function returning \\'void\\' [-fpermissive]\\n\" } \nDec 17 02:16:34  \nDec 17 02:16:34 =========== If your build fails, please take a look at the log above for possible reasons =========== \nDec 17 02:16:34 Compile requests              2510 \nDec 17 02:16:34 Compile requests executed     2252 \nDec 17 02:16:34 Cache hits                    2237 \nDec 17 02:16:34 Cache misses                     0 \nDec 17 02:16:34 Cache timeouts                   0 \nDec 17 02:16:34 Cache read errors                0 \nDec 17 02:16:34 Forced recaches                  0 \nDec 17 02:16:34 Cache write errors               0 \n```\n\n</details>\n\n#### [![See CircleCI build](https://avatars0.githubusercontent.com/ml/7?s=12)](https://circleci.com/gh/pytorch/pytorch/3973801) pytorch_xla_linux_xenial_py3_6_clang7_build (2/3)\n**Step:** \"Build\" ([full log](https://dr.pytorch.org/api/view-log-full?build_id=75641320) | [pattern match details](https://dr.pytorch.org/build-details.html?build_id=75641320))\n<details>\n<summary>\n<code>Dec 17 02:17:04 ERROR:sccache::server: Compilation failed: Output { status: ExitStatus(ExitStatus(256)), stdout: \"\", stderr: \"/var/lib/jenkins/workspace/test/custom_operator/test_custom_ops.cpp:41:30: error: no viable conversion from \\'void\\' to \\'std::vector<torch::Tensor>\\'\\n  std::vector<torch::Tensor> output =\\n                             ^\\n/usr/bin/../lib/gcc/x86_64-linux-gnu/5.4.0/../../../../include/c++/5.4.0/bits/stl_vector.h:318:7: note: candidate constructor not viable: cannot convert argument of incomplete type \\'void\\' to \\'const std::vector<at::Tensor, std::allocator<at::Tensor> > &\\' for 1st argument\\n      vector(const vector& __x)\\n      ^\\n/usr/bin/../lib/gcc/x86_64-linux-gnu/5.4.0/../../../../include/c++/5.4.0/bits/stl_vector.h:335:7: note: candidate constructor not viable: cannot convert argument of incomplete type \\'void\\' to \\'std::vector<at::Tensor, std::allocator<at::Tensor> > &&\\' for 1st argument\\n      vector(vector&& __x) noexcept\\n      ^\\n/usr/bin/../lib/gcc/x86_64-linux-gnu/5.4.0/../../../../include/c++/5.4.0/bits/stl_vector.h:373:7: note: candidate constructor not viable: cannot convert argument of incomplete type \\'void\\' to \\'initializer_list<std::vector<at::Tensor, std::allocator<at::Tensor> >::value_type>\\' (aka \\'initializer_list<at::Tensor>\\') for 1st argument\\n      vector(initializer_list<value_type> __l,\\n      ^\\n/var/lib/jenkins/workspace/test/custom_operator/test_custom_ops.cpp:57:17: error: no viable conversion from \\'void\\' to \\'torch::Tensor\\'\\n  torch::Tensor output =\\n                ^\\n/opt/conda/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:87:3: note: candidate constructor not viable: cannot convert argument of incomplete type \\'void\\' to \\'const at::Tensor &\\' for 1st argument\\n  Tensor(const Tensor&) = default;\\n  ^\\n/opt/conda/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:88:3: note: candidate constructor not viable: cannot convert argument of incomplete type \\'void\\' to \\'at::Tensor &&\\' for 1st argument\\n  Tensor(Tensor&&) = default;\\n  ^\\n/var/lib/jenkins/workspace/test/custom_operator/test_custom_ops.cpp:74:17: error: no viable conversion from \\'void\\' to \\'torch::Tensor\\'\\n  torch::Tensor output =\\n                ^\\n/opt/conda/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:87:3: note: candidate constructor not viable: cannot convert argument of incomplete type \\'void\\' to \\'const at::Tensor &\\' for 1st argument\\n  Tensor(const Tensor&) = default;\\n  ^\\n/opt/conda/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:88:3: note: candidate constructor not viable: cannot convert argument of incomplete type \\'void\\' to \\'at::Tensor &&\\' for 1st argument\\n  Tensor(Tensor&&) = default;\\n  ^\\n/var/lib/jenkins/workspace/test/custom_operator/test_custom_ops.cpp:36:3: error: void function \\'get_operator_from_registry_and_execute\\' should not return a value [-Wreturn-type]\\n  return torch::jit::pop(stack).to<Result>();\\n  ^      ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\n/var/lib/jenkins/workspace/test/custom_operator/test_custom_ops.cpp:42:14: note: in instantiation of function template specialization \\'helpers::get_operator_from_registry_and_execute<std::vector<at::Tensor, std::allocator<at::Tensor> >, at::Tensor, double, int>\\' requested here\\n    helpers::get_operator_from_registry_and_execute<std::vector<torch::Tensor>>(\\\"custom::op\\\", torch::ones(5), 2.0, 3);\\n             ^\\n/var/lib/jenkins/workspace/test/custom_operator/test_custom_ops.cpp:36:3: error: void function \\'get_operator_from_registry_and_execute\\' should not return a value [-Wreturn-type]\\n  return torch::jit::pop(stack).to<Result>();\\n  ^      ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\n/var/lib/jenkins/workspace/test/custom_operator/test_custom_ops.cpp:58:14: note: in instantiation of function template specialization \\'helpers::get_operator_from_registry_and_execute<at::Tensor, at::Tensor &, int, at::Tensor &>\\' requested here\\n    helpers::get_operator_from_registry_and_execute<torch::Tensor>(\\\"custom::op_with_autograd\\\", x, 2, y);\\n             ^\\n5 errors generated.\\n\" }</code>\n</summary>\n\n```\nDec 17 02:17:04 CMakeFiles/test_custom_ops.dir/build.make:62: recipe for target 'CMakeFiles/test_custom_ops.dir/test_custom_ops.cpp.o' failed \nDec 17 02:17:04 make[2]: Leaving directory '/var/lib/jenkins/custom-op-build' \nDec 17 02:17:04 CMakeFiles/Makefile2:141: recipe for target 'CMakeFiles/test_custom_ops.dir/all' failed \nDec 17 02:17:04 make[1]: Leaving directory '/var/lib/jenkins/custom-op-build' \nDec 17 02:17:04 make: *** [all] Error 2 \nDec 17 02:17:04 Makefile:83: recipe for target 'all' failed \nDec 17 02:17:04 + cleanup \nDec 17 02:17:04 + retcode=2 \nDec 17 02:17:04 + set +x \nDec 17 02:17:04 =================== sccache compilation log =================== \nurn-type]\\n  return torch::jit::pop(stack).to<Result>();\\n  ^      ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\n/var/lib/jenkins/workspace/test/custom_operator/test_custom_ops.cpp:58:14: note: in instantiation of function template specialization \\'helpers::get_operator_from_registry_and_execute<at::Tensor, at::Tensor &, int, at::Tensor &>\\' requested here\\n    helpers::get_operator_from_registry_and_execute<torch::Tensor>(\\\"custom::op_with_autograd\\\", x, 2, y);\\n             ^\\n5 errors generated.\\n\" } \nDec 17 02:17:04  \nDec 17 02:17:04 =========== If your build fails, please take a look at the log above for possible reasons =========== \nDec 17 02:17:04 Compile requests              2518 \nDec 17 02:17:04 Compile requests executed     2260 \nDec 17 02:17:04 Cache hits                    2246 \nDec 17 02:17:04 Cache misses                     0 \nDec 17 02:17:04 Cache timeouts                   0 \nDec 17 02:17:04 Cache read errors                0 \nDec 17 02:17:04 Forced recaches                  0 \nDec 17 02:17:04 Cache write errors               0 \n```\n\n</details>\n\n#### [![See CircleCI build](https://avatars0.githubusercontent.com/ml/7?s=12)](https://circleci.com/gh/pytorch/pytorch/3973802) pytorch_linux_xenial_py2_7_9_build (3/3)\n**Step:** \"Build\" ([full log](https://dr.pytorch.org/api/view-log-full?build_id=75641755) | [pattern match details](https://dr.pytorch.org/build-details.html?build_id=75641755))\n[Truncated]\nDec 17 02:17:14 Compile requests              2519 \nDec 17 02:17:14 Compile requests executed     2260 \nDec 17 02:17:14 Cache hits                    2246 \nDec 17 02:17:14 Cache misses                     0 \nDec 17 02:17:14 Cache timeouts                   0 \nDec 17 02:17:14 Cache read errors                0 \nDec 17 02:17:14 Forced recaches                  0 \nDec 17 02:17:14 Cache write errors               0 \n```\n\n</details>\n\n\n\n---\n\n<details><summary>This comment was automatically generated by <a href=\"https://github.com/username_1/circleci-failure-tracker/tree/master/docs/from-pull-request-comment\">Dr. CI</a> (expand for details).</summary>Follow <a href=\"https://dr.pytorch.org/admin/comments-opt-out.html\">this link to opt-out</a> of these comments for your Pull Requests.\n\nPlease report bugs/suggestions on the <a href=\"https://github.com/username_1/circleci-failure-tracker/issues\">GitHub issue tracker</a>.\n</details>"}
{"repo": "pytorch/pytorch", "issue_id": 543015438, "issue_number": 31670, "timestamp": "2019-12-27T23:06:42Z", "text": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#31670 ONNX: Renaming scales parameter for interpolate**\n\nThis does the following:\n1) Makes the float parameters to upsample optional (float?)\n2) Renames and swaps the meaning of use_scale_factor to recompute_scale_factor as a more accurate name\n3) Clarifies some documentation\n\nDifferential Revision: [D19240239](https://our.internmc.facebook.com/intern/diff/D19240239)", "context": "<issue_start><issue_comment>Title: ONNX: Renaming scales parameter for interpolate\nusername_0: Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#31670 ONNX: Renaming scales parameter for interpolate**\n\nThis does the following:\n1) Makes the float parameters to upsample optional (float?)\n2) Renames and swaps the meaning of use_scale_factor to recompute_scale_factor as a more accurate name\n3) Clarifies some documentation\n\nDifferential Revision: [D19240239](https://our.internmc.facebook.com/intern/diff/D19240239)\n<issue_comment>username_1: ## :pill: CircleCI build failures summary and remediations\nAs of commit ad2eccf8:\n\n\n* **1/1** failures introduced in this PR\n\n\n## Detailed failure analysis\n\nOne may explore the probable reasons each build failed interactively [on the Dr. CI website](https://dr.pytorch.org/commit-details.html?sha1=ad2eccf810ba6df2c9e60a2e5941c5301041b0ec).\n\n### :detective: 1 new failure recognized by patterns\nThe following build failures do not appear to be due to upstream breakage:\n#### [![See CircleCI build](https://avatars0.githubusercontent.com/ml/7?s=12)](https://circleci.com/gh/pytorch/pytorch/4078000) pytorch_xla_linux_xenial_py3_6_clang7_build (1/1)\n**Step:** \"Build\" ([full log](https://dr.pytorch.org/api/view-log-full?build_id=78731904) | [pattern match details](https://dr.pytorch.org/build-details.html?build_id=78731904))\n<details>\n<summary>\n<code>Dec 27 23:21:25 Failed to run '['/var/lib/jenkins/workspace/xla/scripts/generate_code.sh']'</code>\n</summary>\n\n```\nDec 27 23:21:25 AtenXlaType function missed override: Tensor upsample_bilinear2d(const Tensor& self, IntArrayRef output_size, bool align_corners, double scales_1, double scales_2); // upsample_bilinear2d(Tensor,IntArrayRef,bool,double,double)->Tensor \nDec 27 23:21:25 AtenXlaType function missed override: Tensor upsample_bilinear2d_backward(const Tensor& grad_output, IntArrayRef output_size, IntArrayRef input_size, bool align_corners, double scales_1, double scales_2); // upsample_bilinear2d_backward(Tensor,IntArrayRef,IntArrayRef,bool,double,double)->Tensor \nDec 27 23:21:25 AtenXlaType function missed override: Tensor upsample_nearest2d(const Tensor& self, IntArrayRef output_size, double scales_1, double scales_2); // upsample_nearest2d(Tensor,IntArrayRef,double,double)->Tensor \nDec 27 23:21:25 AtenXlaType function missed override: Tensor upsample_nearest2d_backward(const Tensor& grad_output, IntArrayRef output_size, IntArrayRef input_size, double scales_1, double scales_2); // upsample_nearest2d_backward(Tensor,IntArrayRef,IntArrayRef,double,double)->Tensor \nDec 27 23:21:25 Traceback (most recent call last): \nDec 27 23:21:25   File \"/var/lib/jenkins/workspace/xla/scripts/gen.py\", line 1052, in <module> \nDec 27 23:21:25     generate(args) \nDec 27 23:21:25   File \"/var/lib/jenkins/workspace/xla/scripts/gen.py\", line 1022, in generate \nDec 27 23:21:25     assert check_overrides(overrides, overridden) \nDec 27 23:21:25 AssertionError \nDec 27 23:21:25 Failed to run '['/var/lib/jenkins/workspace/xla/scripts/generate_code.sh']' \nDec 27 23:21:25 Building torch_xla version: 0.8 \nDec 27 23:21:25 + cleanup \nDec 27 23:21:25 + retcode=1 \nDec 27 23:21:25 + set +x \nDec 27 23:21:25 =================== sccache compilation log =================== \nDec 27 23:21:25 =========== If your build fails, please take a look at the log above for possible reasons =========== \nDec 27 23:21:25 Compile requests              2524 \nDec 27 23:21:25 Compile requests executed     2267 \nDec 27 23:21:25 Cache hits                    2253 \nDec 27 23:21:25 Cache misses                     1 \n```\n\n</details>\n\n\n\n---\n\n<details><summary>This comment was automatically generated by <a href=\"https://github.com/username_1/circleci-failure-tracker/tree/master/docs/from-pull-request-comment\">Dr. CI</a> (expand for details).</summary>Follow <a href=\"https://dr.pytorch.org/admin/comments-opt-out.html\">this link to opt-out</a> of these comments for your Pull Requests.\n\nPlease report bugs/suggestions on the <a href=\"https://github.com/username_1/circleci-failure-tracker/issues\">GitHub issue tracker</a>.\n</details>"}
{"repo": "pytorch/pytorch", "issue_id": 543015438, "issue_number": 31670, "timestamp": "2019-12-27 23:22:23+00:00", "text": "## :pill: CircleCI build failures summary and remediations\nAs of commit ad2eccf8:\n\n\n* **1/1** failures introduced in this PR\n\n\n## Detailed failure analysis\n\nOne may explore the probable reasons each build failed interactively [on the Dr. CI website](https://dr.pytorch.org/commit-details.html?sha1=ad2eccf810ba6df2c9e60a2e5941c5301041b0ec).\n\n### :detective: 1 new failure recognized by patterns\nThe following build failures do not appear to be due to upstream breakage:\n#### [![See CircleCI build](https://avatars0.githubusercontent.com/ml/7?s=12)](https://circleci.com/gh/pytorch/pytorch/4078000) pytorch_xla_linux_xenial_py3_6_clang7_build (1/1)\n**Step:** \"Build\" ([full log](https://dr.pytorch.org/api/view-log-full?build_id=78731904) | [pattern match details](https://dr.pytorch.org/build-details.html?build_id=78731904))\n<details>\n<summary>\n<code>Dec 27 23:21:25 Failed to run '['/var/lib/jenkins/workspace/xla/scripts/generate_code.sh']'</code>\n</summary>\n\n```\nDec 27 23:21:25 AtenXlaType function missed override: Tensor upsample_bilinear2d(const Tensor& self, IntArrayRef output_size, bool align_corners, double scales_1, double scales_2); // upsample_bilinear2d(Tensor,IntArrayRef,bool,double,double)->Tensor \nDec 27 23:21:25 AtenXlaType function missed override: Tensor upsample_bilinear2d_backward(const Tensor& grad_output, IntArrayRef output_size, IntArrayRef input_size, bool align_corners, double scales_1, double scales_2); // upsample_bilinear2d_backward(Tensor,IntArrayRef,IntArrayRef,bool,double,double)->Tensor \nDec 27 23:21:25 AtenXlaType function missed override: Tensor upsample_nearest2d(const Tensor& self, IntArrayRef output_size, double scales_1, double scales_2); // upsample_nearest2d(Tensor,IntArrayRef,double,double)->Tensor \nDec 27 23:21:25 AtenXlaType function missed override: Tensor upsample_nearest2d_backward(const Tensor& grad_output, IntArrayRef output_size, IntArrayRef input_size, double scales_1, double scales_2); // upsample_nearest2d_backward(Tensor,IntArrayRef,IntArrayRef,double,double)->Tensor \nDec 27 23:21:25 Traceback (most recent call last): \nDec 27 23:21:25   File \"/var/lib/jenkins/workspace/xla/scripts/gen.py\", line 1052, in <module> \nDec 27 23:21:25     generate(args) \nDec 27 23:21:25   File \"/var/lib/jenkins/workspace/xla/scripts/gen.py\", line 1022, in generate \nDec 27 23:21:25     assert check_overrides(overrides, overridden) \nDec 27 23:21:25 AssertionError \nDec 27 23:21:25 Failed to run '['/var/lib/jenkins/workspace/xla/scripts/generate_code.sh']' \nDec 27 23:21:25 Building torch_xla version: 0.8 \nDec 27 23:21:25 + cleanup \nDec 27 23:21:25 + retcode=1 \nDec 27 23:21:25 + set +x \nDec 27 23:21:25 =================== sccache compilation log =================== \nDec 27 23:21:25 =========== If your build fails, please take a look at the log above for possible reasons =========== \nDec 27 23:21:25 Compile requests              2524 \nDec 27 23:21:25 Compile requests executed     2267 \nDec 27 23:21:25 Cache hits                    2253 \nDec 27 23:21:25 Cache misses                     1 \n```\n\n</details>\n\n\n\n---\n\n<details><summary>This comment was automatically generated by <a href=\"https://github.com/username_1/circleci-failure-tracker/tree/master/docs/from-pull-request-comment\">Dr. CI</a> (expand for details).</summary>Follow <a href=\"https://dr.pytorch.org/admin/comments-opt-out.html\">this link to opt-out</a> of these comments for your Pull Requests.\n\nPlease report bugs/suggestions on the <a href=\"https://github.com/username_1/circleci-failure-tracker/issues\">GitHub issue tracker</a>.\n</details>", "context": "<issue_start><issue_comment>Title: ONNX: Renaming scales parameter for interpolate\nusername_0: Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#31670 ONNX: Renaming scales parameter for interpolate**\n\nThis does the following:\n1) Makes the float parameters to upsample optional (float?)\n2) Renames and swaps the meaning of use_scale_factor to recompute_scale_factor as a more accurate name\n3) Clarifies some documentation\n\nDifferential Revision: [D19240239](https://our.internmc.facebook.com/intern/diff/D19240239)\n<issue_comment>username_1: ## :pill: CircleCI build failures summary and remediations\nAs of commit ad2eccf8:\n\n\n* **1/1** failures introduced in this PR\n\n\n## Detailed failure analysis\n\nOne may explore the probable reasons each build failed interactively [on the Dr. CI website](https://dr.pytorch.org/commit-details.html?sha1=ad2eccf810ba6df2c9e60a2e5941c5301041b0ec).\n\n### :detective: 1 new failure recognized by patterns\nThe following build failures do not appear to be due to upstream breakage:\n#### [![See CircleCI build](https://avatars0.githubusercontent.com/ml/7?s=12)](https://circleci.com/gh/pytorch/pytorch/4078000) pytorch_xla_linux_xenial_py3_6_clang7_build (1/1)\n**Step:** \"Build\" ([full log](https://dr.pytorch.org/api/view-log-full?build_id=78731904) | [pattern match details](https://dr.pytorch.org/build-details.html?build_id=78731904))\n<details>\n<summary>\n<code>Dec 27 23:21:25 Failed to run '['/var/lib/jenkins/workspace/xla/scripts/generate_code.sh']'</code>\n</summary>\n\n```\nDec 27 23:21:25 AtenXlaType function missed override: Tensor upsample_bilinear2d(const Tensor& self, IntArrayRef output_size, bool align_corners, double scales_1, double scales_2); // upsample_bilinear2d(Tensor,IntArrayRef,bool,double,double)->Tensor \nDec 27 23:21:25 AtenXlaType function missed override: Tensor upsample_bilinear2d_backward(const Tensor& grad_output, IntArrayRef output_size, IntArrayRef input_size, bool align_corners, double scales_1, double scales_2); // upsample_bilinear2d_backward(Tensor,IntArrayRef,IntArrayRef,bool,double,double)->Tensor \nDec 27 23:21:25 AtenXlaType function missed override: Tensor upsample_nearest2d(const Tensor& self, IntArrayRef output_size, double scales_1, double scales_2); // upsample_nearest2d(Tensor,IntArrayRef,double,double)->Tensor \nDec 27 23:21:25 AtenXlaType function missed override: Tensor upsample_nearest2d_backward(const Tensor& grad_output, IntArrayRef output_size, IntArrayRef input_size, double scales_1, double scales_2); // upsample_nearest2d_backward(Tensor,IntArrayRef,IntArrayRef,double,double)->Tensor \nDec 27 23:21:25 Traceback (most recent call last): \nDec 27 23:21:25   File \"/var/lib/jenkins/workspace/xla/scripts/gen.py\", line 1052, in <module> \nDec 27 23:21:25     generate(args) \nDec 27 23:21:25   File \"/var/lib/jenkins/workspace/xla/scripts/gen.py\", line 1022, in generate \nDec 27 23:21:25     assert check_overrides(overrides, overridden) \nDec 27 23:21:25 AssertionError \nDec 27 23:21:25 Failed to run '['/var/lib/jenkins/workspace/xla/scripts/generate_code.sh']' \nDec 27 23:21:25 Building torch_xla version: 0.8 \nDec 27 23:21:25 + cleanup \nDec 27 23:21:25 + retcode=1 \nDec 27 23:21:25 + set +x \nDec 27 23:21:25 =================== sccache compilation log =================== \nDec 27 23:21:25 =========== If your build fails, please take a look at the log above for possible reasons =========== \nDec 27 23:21:25 Compile requests              2524 \nDec 27 23:21:25 Compile requests executed     2267 \nDec 27 23:21:25 Cache hits                    2253 \nDec 27 23:21:25 Cache misses                     1 \n```\n\n</details>\n\n\n\n---\n\n<details><summary>This comment was automatically generated by <a href=\"https://github.com/username_1/circleci-failure-tracker/tree/master/docs/from-pull-request-comment\">Dr. CI</a> (expand for details).</summary>Follow <a href=\"https://dr.pytorch.org/admin/comments-opt-out.html\">this link to opt-out</a> of these comments for your Pull Requests.\n\nPlease report bugs/suggestions on the <a href=\"https://github.com/username_1/circleci-failure-tracker/issues\">GitHub issue tracker</a>.\n</details>"}
{"repo": "pytorch/pytorch", "issue_id": 560050987, "issue_number": 32993, "timestamp": "2020-02-04T23:54:17Z", "text": "Stacked PRs\n * #33578 - [jit] Unify augmented assign handling\n * **#32993 - [jit] Fix aug assign for non-tensor attributes**\n\nInstead of erroring out this de-sugars augmented assignments to class\r\nmembers from `self.a += 1` to `self.a = self.a + 1`.\r\n\r\nFixes #32973\n\nDifferential Revision: [D19737636](https://our.internmc.facebook.com/intern/diff/19737636/)", "context": "<issue_start><issue_comment>Title: [jit] Fix aug assign for non-tensor attributes\nusername_0: Stacked PRs\n * #33578 - [jit] Unify augmented assign handling\n * **#32993 - [jit] Fix aug assign for non-tensor attributes**\n\nInstead of erroring out this de-sugars augmented assignments to class\r\nmembers from `self.a += 1` to `self.a = self.a + 1`.\r\n\r\nFixes #32973\n\nDifferential Revision: [D19737636](https://our.internmc.facebook.com/intern/diff/19737636/)"}
{"repo": "pytorch/pytorch", "issue_id": 575651931, "issue_number": 34235, "timestamp": "2020-03-04T18:29:57Z", "text": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#34235 [Do not commit] Add HowToRebaseOnMaster.md doc.**\n* #34234 [WIP] Changes in test/test_jit_fuser.py test/test_jit_fuser_te.py.\n* #34233 [WIP] Changes to peephole.cpp.\n* #34231 [WIP] Guard elimination changes from bertmaher/pytorch_fusion.\n* #34230 Add tensorexpr benchmarks.\n* #34229 Add jit_get_te_* python bindings.\n* #34228 Add LLVM codegen for tensor expressions.\n* #34227 Add CUDA codegen for tensorexpressions.\n* #34226 Tensorexpr fuser implementation.\n* #34225 Remove execution_counter include from eval.h.\n* #34224 Move tensor expressions work from bertmaher/pytorch to pytorch/pytorch.\n\nDifferential Revision: [D20251834](https://our.internmc.facebook.com/intern/diff/D20251834)", "context": "<issue_start><issue_comment>Title: [Do not commit] Add HowToRebaseOnMaster.md doc.\nusername_0: Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#34235 [Do not commit] Add HowToRebaseOnMaster.md doc.**\n* #34234 [WIP] Changes in test/test_jit_fuser.py test/test_jit_fuser_te.py.\n* #34233 [WIP] Changes to peephole.cpp.\n* #34231 [WIP] Guard elimination changes from bertmaher/pytorch_fusion.\n* #34230 Add tensorexpr benchmarks.\n* #34229 Add jit_get_te_* python bindings.\n* #34228 Add LLVM codegen for tensor expressions.\n* #34227 Add CUDA codegen for tensorexpressions.\n* #34226 Tensorexpr fuser implementation.\n* #34225 Remove execution_counter include from eval.h.\n* #34224 Move tensor expressions work from bertmaher/pytorch to pytorch/pytorch.\n\nDifferential Revision: [D20251834](https://our.internmc.facebook.com/intern/diff/D20251834)"}
{"repo": "pytorch/pytorch", "issue_id": 577683373, "issue_number": 34473, "timestamp": "2020-03-09 06:17:04+00:00", "text": "I have been running this code for object detection but while running the code ,I am getting a warning \r\n\r\n\r\n..\\torch\\csrc\\autograd\\python_function.cpp:638: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\r\n..\\torch\\csrc\\autograd\\python_function.cpp:664: UserWarning: Legacy autograd function object was called twice.  You will probably get incorrect gradients from this computation, as the saved tensors from the second invocation will clobber the saved tensors from the first invocation.  Please consider rewriting your autograd function in the modern style; for information on the new format, please see: https://pytorch.org/docs/stable/notes/extending.html#extending-torch-autograd\r\n626\r\n\r\nafter every step.\r\n\r\n## Code\r\n\r\n#Importing of Libraries\r\nimport torch\r\nfrom torch.autograd import Variable\r\nimport cv2\r\nfrom data import BaseTransform, VOC_CLASSES as labelmap\r\nfrom ssd import build_ssd\r\nimport imageio\r\n\r\n# Defining a function that will do the detection\r\ndef detect(frame, net, transform):\r\n    height, width = frame.shape[:2]\r\n    frame_t = transform(frame)[0]\r\n    x = torch.from_numpy(frame_t).permute(2,0,1)\r\n    x = Variable(x.unsqueeze(0))\r\n    y = net(x)\r\n    detections = y.data # We create the detections tensor contained in the output y.\r\n    scale = torch.Tensor([width, height, width, height]) # We create a tensor object of dimensions [width, height, width, height].\r\n    for i in range(detections.size(1)): # For every class:\r\n        j = 0 # We initialize the loop variable j that will correspond to the occurrences of the class.\r\n        while detections[0, i, j, 0] >= 0.6: # We take into account all the occurrences j of the class i that have a matching score larger than 0.6.\r\n            pt = (detections[0, i, j, 1:] * scale).numpy() # We get the coordinates of the points at the upper left and the lower right of the detector rectangle.\r\n            cv2.rectangle(frame, (int(pt[0]), int(pt[1])), (int(pt[2]), int(pt[3])), (255, 0, 0), 2) # We draw a rectangle around the detected object.\r\n            cv2.putText(frame, labelmap[i - 1], (int(pt[0]), int(pt[1])), cv2.FONT_HERSHEY_SIMPLEX, 2, (255, 255, 255), 2, cv2.LINE_AA) # We put the label of the class right above the rectangle.\r\n            j += 1 # We increment j to get to the next occurrence.\r\n    return frame\r\n\r\n# Creating the SSD neural network\r\nnet = build_ssd('test')\r\nnet.load_state_dict(torch.load('ssd300_mAP_77.43_v2.pth', map_location = lambda storage, loc: storage ))\r\n\r\n# Creating the transformation\r\ntransform = BaseTransform(net.size, (104/256.0, 117/256.0, 123/256.0))\r\n \r\n\r\n# Doing some object detection on a video\r\nreader = imageio.get_reader('ghystreet.mp4')\r\nfps = reader.get_meta_data()['fps']\r\nwriter = imageio.get_writer('ghystreetout.mp4', fps = fps)\r\nfor i, frame in enumerate(reader): # We iterate on the frames of the output video:\r\n    frame = detect(frame, net.eval(), transform) # We call our detect function (defined above) to detect the object on the frame.\r\n    writer.append_data(frame) # We add the next frame in the output video.\r\n    print(i) # We print the number of the processed frame.\r\nwriter.close() # We close the process that handles the creation of the output video.", "context": "<issue_start><issue_comment>Title: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3 and UserWarning: Legacy autograd function object was called twice.  You will probably get incorrect gradients from this computation, as the saved tensors from the second invocation will clobber the saved tensors from the first invocation.  Please consider rewriting your autograd function in the modern style; for information on the new format, please see: https://pytorch.org/docs/stable/notes/extending.html#extending-torch-autograd 626\nusername_0: I have been running this code for object detection but while running the code ,I am getting a warning \r\n\r\n\r\n..\\torch\\csrc\\autograd\\python_function.cpp:638: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\r\n..\\torch\\csrc\\autograd\\python_function.cpp:664: UserWarning: Legacy autograd function object was called twice.  You will probably get incorrect gradients from this computation, as the saved tensors from the second invocation will clobber the saved tensors from the first invocation.  Please consider rewriting your autograd function in the modern style; for information on the new format, please see: https://pytorch.org/docs/stable/notes/extending.html#extending-torch-autograd\r\n626\r\n\r\nafter every step.\r\n\r\n## Code\r\n\r\n#Importing of Libraries\r\nimport torch\r\nfrom torch.autograd import Variable\r\nimport cv2\r\nfrom data import BaseTransform, VOC_CLASSES as labelmap\r\nfrom ssd import build_ssd\r\nimport imageio\r\n\r\n# Defining a function that will do the detection\r\ndef detect(frame, net, transform):\r\n    height, width = frame.shape[:2]\r\n    frame_t = transform(frame)[0]\r\n    x = torch.from_numpy(frame_t).permute(2,0,1)\r\n    x = Variable(x.unsqueeze(0))\r\n    y = net(x)\r\n    detections = y.data # We create the detections tensor contained in the output y.\r\n    scale = torch.Tensor([width, height, width, height]) # We create a tensor object of dimensions [width, height, width, height].\r\n    for i in range(detections.size(1)): # For every class:\r\n        j = 0 # We initialize the loop variable j that will correspond to the occurrences of the class.\r\n        while detections[0, i, j, 0] >= 0.6: # We take into account all the occurrences j of the class i that have a matching score larger than 0.6.\r\n            pt = (detections[0, i, j, 1:] * scale).numpy() # We get the coordinates of the points at the upper left and the lower right of the detector rectangle.\r\n            cv2.rectangle(frame, (int(pt[0]), int(pt[1])), (int(pt[2]), int(pt[3])), (255, 0, 0), 2) # We draw a rectangle around the detected object.\r\n            cv2.putText(frame, labelmap[i - 1], (int(pt[0]), int(pt[1])), cv2.FONT_HERSHEY_SIMPLEX, 2, (255, 255, 255), 2, cv2.LINE_AA) # We put the label of the class right above the rectangle.\r\n            j += 1 # We increment j to get to the next occurrence.\r\n    return frame\r\n\r\n# Creating the SSD neural network\r\nnet = build_ssd('test')\r\nnet.load_state_dict(torch.load('ssd300_mAP_77.43_v2.pth', map_location = lambda storage, loc: storage ))\r\n\r\n# Creating the transformation\r\ntransform = BaseTransform(net.size, (104/256.0, 117/256.0, 123/256.0))\r\n \r\n\r\n# Doing some object detection on a video\r\nreader = imageio.get_reader('ghystreet.mp4')\r\nfps = reader.get_meta_data()['fps']\r\nwriter = imageio.get_writer('ghystreetout.mp4', fps = fps)\r\nfor i, frame in enumerate(reader): # We iterate on the frames of the output video:\r\n    frame = detect(frame, net.eval(), transform) # We call our detect function (defined above) to detect the object on the frame.\r\n    writer.append_data(frame) # We add the next frame in the output video.\r\n    print(i) # We print the number of the processed frame.\r\nwriter.close() # We close the process that handles the creation of the output video.<issue_closed>\n<issue_comment>username_1: Please prepend the following code to ignore warnings.\r\n```\r\nimport warnings\r\nwarnings.filterwarnings(\"ignore\")\r\n```\n<issue_comment>username_0: okay...but for the  2nd warning it is also affecting the gradients,its saying to rewrite the autograd function in new style. So where to change ?\n<issue_comment>username_1: Please use `Tensor` instead of `Variable`. More details could be retrieved here. https://pytorch.org/blog/pytorch-0_4_0-migration-guide/#merging-tensor-and-variable-and-classes\n<issue_comment>username_0: I did but it could not import Tensor, i watched the blog, but couldn't identify what to change.\n<issue_comment>username_1: I have been running this code for object detection but while running the code ,I am getting a warning \r\n\r\n\r\n..\\torch\\csrc\\autograd\\python_function.cpp:638: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\r\n..\\torch\\csrc\\autograd\\python_function.cpp:664: UserWarning: Legacy autograd function object was called twice.  You will probably get incorrect gradients from this computation, as the saved tensors from the second invocation will clobber the saved tensors from the first invocation.  Please consider rewriting your autograd function in the modern style; for information on the new format, please see: https://pytorch.org/docs/stable/notes/extending.html#extending-torch-autograd\r\n626\r\n\r\nafter every step.\r\n\r\n## Code\r\n\r\n#Importing of Libraries\r\nimport torch\r\nfrom torch.autograd import Variable\r\nimport cv2\r\nfrom data import BaseTransform, VOC_CLASSES as labelmap\r\nfrom ssd import build_ssd\r\nimport imageio\r\n\r\n# Defining a function that will do the detection\r\ndef detect(frame, net, transform):\r\n    height, width = frame.shape[:2]\r\n    frame_t = transform(frame)[0]\r\n    x = torch.from_numpy(frame_t).permute(2,0,1)\r\n    x = Variable(x.unsqueeze(0))\r\n    y = net(x)\r\n    detections = y.data # We create the detections tensor contained in the output y.\r\n    scale = torch.Tensor([width, height, width, height]) # We create a tensor object of dimensions [width, height, width, height].\r\n    for i in range(detections.size(1)): # For every class:\r\n        j = 0 # We initialize the loop variable j that will correspond to the occurrences of the class.\r\n        while detections[0, i, j, 0] >= 0.6: # We take into account all the occurrences j of the class i that have a matching score larger than 0.6.\r\n            pt = (detections[0, i, j, 1:] * scale).numpy() # We get the coordinates of the points at the upper left and the lower right of the detector rectangle.\r\n            cv2.rectangle(frame, (int(pt[0]), int(pt[1])), (int(pt[2]), int(pt[3])), (255, 0, 0), 2) # We draw a rectangle around the detected object.\r\n            cv2.putText(frame, labelmap[i - 1], (int(pt[0]), int(pt[1])), cv2.FONT_HERSHEY_SIMPLEX, 2, (255, 255, 255), 2, cv2.LINE_AA) # We put the label of the class right above the rectangle.\r\n            j += 1 # We increment j to get to the next occurrence.\r\n    return frame\r\n\r\n# Creating the SSD neural network\r\nnet = build_ssd('test')\r\nnet.load_state_dict(torch.load('ssd300_mAP_77.43_v2.pth', map_location = lambda storage, loc: storage ))\r\n\r\n# Creating the transformation\r\ntransform = BaseTransform(net.size, (104/256.0, 117/256.0, 123/256.0))\r\n \r\n\r\n# Doing some object detection on a video\r\nreader = imageio.get_reader('ghystreet.mp4')\r\nfps = reader.get_meta_data()['fps']\r\nwriter = imageio.get_writer('ghystreetout.mp4', fps = fps)\r\nfor i, frame in enumerate(reader): # We iterate on the frames of the output video:\r\n    frame = detect(frame, net.eval(), transform) # We call our detect function (defined above) to detect the object on the frame.\r\n    writer.append_data(frame) # We add the next frame in the output video.\r\n    print(i) # We print the number of the processed frame.\r\nwriter.close() # We close the process that handles the creation of the output video.\n<issue_comment>username_2: You have to rewrite your custom autograd functions with @staticmethod forward and backward methods, as the warning suggests.\n<issue_comment>username_0: Can you specify on which file the changes have to be made.\n<issue_comment>username_3: In your net, you use a custom autograd.Function that is an old style one.  As mentionned in the warning, you should follow the instructions in this doc on how to write a proper custom Function: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function\r\n\r\nYou have to make the change wherever you define the new `autograd.Function`.\r\n\r\nNote that this warning will become an error in 1.5 as mentionned in the message.<issue_closed>\n<issue_comment>username_4: i have the same problem. did you solve it?\r\n\r\nLegacy autograd function with non-static forward method is deprecated. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n<issue_comment>username_3: Hi,\r\n\r\nAs the error mentions you should define the Function as in the doc. You can find more information [here](https://pytorch.org/docs/stable/notes/extending.html) as well."}
{"repo": "pytorch/pytorch", "issue_id": 577683373, "issue_number": 34473, "timestamp": "2020-03-09 06:48:37+00:00", "text": "", "context": "<issue_start><issue_comment>Title: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3 and UserWarning: Legacy autograd function object was called twice.  You will probably get incorrect gradients from this computation, as the saved tensors from the second invocation will clobber the saved tensors from the first invocation.  Please consider rewriting your autograd function in the modern style; for information on the new format, please see: https://pytorch.org/docs/stable/notes/extending.html#extending-torch-autograd 626\nusername_0: I have been running this code for object detection but while running the code ,I am getting a warning \r\n\r\n\r\n..\\torch\\csrc\\autograd\\python_function.cpp:638: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\r\n..\\torch\\csrc\\autograd\\python_function.cpp:664: UserWarning: Legacy autograd function object was called twice.  You will probably get incorrect gradients from this computation, as the saved tensors from the second invocation will clobber the saved tensors from the first invocation.  Please consider rewriting your autograd function in the modern style; for information on the new format, please see: https://pytorch.org/docs/stable/notes/extending.html#extending-torch-autograd\r\n626\r\n\r\nafter every step.\r\n\r\n## Code\r\n\r\n#Importing of Libraries\r\nimport torch\r\nfrom torch.autograd import Variable\r\nimport cv2\r\nfrom data import BaseTransform, VOC_CLASSES as labelmap\r\nfrom ssd import build_ssd\r\nimport imageio\r\n\r\n# Defining a function that will do the detection\r\ndef detect(frame, net, transform):\r\n    height, width = frame.shape[:2]\r\n    frame_t = transform(frame)[0]\r\n    x = torch.from_numpy(frame_t).permute(2,0,1)\r\n    x = Variable(x.unsqueeze(0))\r\n    y = net(x)\r\n    detections = y.data # We create the detections tensor contained in the output y.\r\n    scale = torch.Tensor([width, height, width, height]) # We create a tensor object of dimensions [width, height, width, height].\r\n    for i in range(detections.size(1)): # For every class:\r\n        j = 0 # We initialize the loop variable j that will correspond to the occurrences of the class.\r\n        while detections[0, i, j, 0] >= 0.6: # We take into account all the occurrences j of the class i that have a matching score larger than 0.6.\r\n            pt = (detections[0, i, j, 1:] * scale).numpy() # We get the coordinates of the points at the upper left and the lower right of the detector rectangle.\r\n            cv2.rectangle(frame, (int(pt[0]), int(pt[1])), (int(pt[2]), int(pt[3])), (255, 0, 0), 2) # We draw a rectangle around the detected object.\r\n            cv2.putText(frame, labelmap[i - 1], (int(pt[0]), int(pt[1])), cv2.FONT_HERSHEY_SIMPLEX, 2, (255, 255, 255), 2, cv2.LINE_AA) # We put the label of the class right above the rectangle.\r\n            j += 1 # We increment j to get to the next occurrence.\r\n    return frame\r\n\r\n# Creating the SSD neural network\r\nnet = build_ssd('test')\r\nnet.load_state_dict(torch.load('ssd300_mAP_77.43_v2.pth', map_location = lambda storage, loc: storage ))\r\n\r\n# Creating the transformation\r\ntransform = BaseTransform(net.size, (104/256.0, 117/256.0, 123/256.0))\r\n \r\n\r\n# Doing some object detection on a video\r\nreader = imageio.get_reader('ghystreet.mp4')\r\nfps = reader.get_meta_data()['fps']\r\nwriter = imageio.get_writer('ghystreetout.mp4', fps = fps)\r\nfor i, frame in enumerate(reader): # We iterate on the frames of the output video:\r\n    frame = detect(frame, net.eval(), transform) # We call our detect function (defined above) to detect the object on the frame.\r\n    writer.append_data(frame) # We add the next frame in the output video.\r\n    print(i) # We print the number of the processed frame.\r\nwriter.close() # We close the process that handles the creation of the output video.<issue_closed>\n<issue_comment>username_1: Please prepend the following code to ignore warnings.\r\n```\r\nimport warnings\r\nwarnings.filterwarnings(\"ignore\")\r\n```\n<issue_comment>username_0: okay...but for the  2nd warning it is also affecting the gradients,its saying to rewrite the autograd function in new style. So where to change ?\n<issue_comment>username_1: Please use `Tensor` instead of `Variable`. More details could be retrieved here. https://pytorch.org/blog/pytorch-0_4_0-migration-guide/#merging-tensor-and-variable-and-classes\n<issue_comment>username_0: I did but it could not import Tensor, i watched the blog, but couldn't identify what to change.\n<issue_comment>username_1: I have been running this code for object detection but while running the code ,I am getting a warning \r\n\r\n\r\n..\\torch\\csrc\\autograd\\python_function.cpp:638: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\r\n..\\torch\\csrc\\autograd\\python_function.cpp:664: UserWarning: Legacy autograd function object was called twice.  You will probably get incorrect gradients from this computation, as the saved tensors from the second invocation will clobber the saved tensors from the first invocation.  Please consider rewriting your autograd function in the modern style; for information on the new format, please see: https://pytorch.org/docs/stable/notes/extending.html#extending-torch-autograd\r\n626\r\n\r\nafter every step.\r\n\r\n## Code\r\n\r\n#Importing of Libraries\r\nimport torch\r\nfrom torch.autograd import Variable\r\nimport cv2\r\nfrom data import BaseTransform, VOC_CLASSES as labelmap\r\nfrom ssd import build_ssd\r\nimport imageio\r\n\r\n# Defining a function that will do the detection\r\ndef detect(frame, net, transform):\r\n    height, width = frame.shape[:2]\r\n    frame_t = transform(frame)[0]\r\n    x = torch.from_numpy(frame_t).permute(2,0,1)\r\n    x = Variable(x.unsqueeze(0))\r\n    y = net(x)\r\n    detections = y.data # We create the detections tensor contained in the output y.\r\n    scale = torch.Tensor([width, height, width, height]) # We create a tensor object of dimensions [width, height, width, height].\r\n    for i in range(detections.size(1)): # For every class:\r\n        j = 0 # We initialize the loop variable j that will correspond to the occurrences of the class.\r\n        while detections[0, i, j, 0] >= 0.6: # We take into account all the occurrences j of the class i that have a matching score larger than 0.6.\r\n            pt = (detections[0, i, j, 1:] * scale).numpy() # We get the coordinates of the points at the upper left and the lower right of the detector rectangle.\r\n            cv2.rectangle(frame, (int(pt[0]), int(pt[1])), (int(pt[2]), int(pt[3])), (255, 0, 0), 2) # We draw a rectangle around the detected object.\r\n            cv2.putText(frame, labelmap[i - 1], (int(pt[0]), int(pt[1])), cv2.FONT_HERSHEY_SIMPLEX, 2, (255, 255, 255), 2, cv2.LINE_AA) # We put the label of the class right above the rectangle.\r\n            j += 1 # We increment j to get to the next occurrence.\r\n    return frame\r\n\r\n# Creating the SSD neural network\r\nnet = build_ssd('test')\r\nnet.load_state_dict(torch.load('ssd300_mAP_77.43_v2.pth', map_location = lambda storage, loc: storage ))\r\n\r\n# Creating the transformation\r\ntransform = BaseTransform(net.size, (104/256.0, 117/256.0, 123/256.0))\r\n \r\n\r\n# Doing some object detection on a video\r\nreader = imageio.get_reader('ghystreet.mp4')\r\nfps = reader.get_meta_data()['fps']\r\nwriter = imageio.get_writer('ghystreetout.mp4', fps = fps)\r\nfor i, frame in enumerate(reader): # We iterate on the frames of the output video:\r\n    frame = detect(frame, net.eval(), transform) # We call our detect function (defined above) to detect the object on the frame.\r\n    writer.append_data(frame) # We add the next frame in the output video.\r\n    print(i) # We print the number of the processed frame.\r\nwriter.close() # We close the process that handles the creation of the output video.\n<issue_comment>username_2: You have to rewrite your custom autograd functions with @staticmethod forward and backward methods, as the warning suggests.\n<issue_comment>username_0: Can you specify on which file the changes have to be made.\n<issue_comment>username_3: In your net, you use a custom autograd.Function that is an old style one.  As mentionned in the warning, you should follow the instructions in this doc on how to write a proper custom Function: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function\r\n\r\nYou have to make the change wherever you define the new `autograd.Function`.\r\n\r\nNote that this warning will become an error in 1.5 as mentionned in the message.<issue_closed>\n<issue_comment>username_4: i have the same problem. did you solve it?\r\n\r\nLegacy autograd function with non-static forward method is deprecated. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n<issue_comment>username_3: Hi,\r\n\r\nAs the error mentions you should define the Function as in the doc. You can find more information [here](https://pytorch.org/docs/stable/notes/extending.html) as well."}
{"repo": "pytorch/pytorch", "issue_id": 577683373, "issue_number": 34473, "timestamp": "2020-03-09 06:48:37+00:00", "text": "Please prepend the following code to ignore warnings.\r\n```\r\nimport warnings\r\nwarnings.filterwarnings(\"ignore\")\r\n```", "context": "<issue_start><issue_comment>Title: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3 and UserWarning: Legacy autograd function object was called twice.  You will probably get incorrect gradients from this computation, as the saved tensors from the second invocation will clobber the saved tensors from the first invocation.  Please consider rewriting your autograd function in the modern style; for information on the new format, please see: https://pytorch.org/docs/stable/notes/extending.html#extending-torch-autograd 626\nusername_0: I have been running this code for object detection but while running the code ,I am getting a warning \r\n\r\n\r\n..\\torch\\csrc\\autograd\\python_function.cpp:638: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\r\n..\\torch\\csrc\\autograd\\python_function.cpp:664: UserWarning: Legacy autograd function object was called twice.  You will probably get incorrect gradients from this computation, as the saved tensors from the second invocation will clobber the saved tensors from the first invocation.  Please consider rewriting your autograd function in the modern style; for information on the new format, please see: https://pytorch.org/docs/stable/notes/extending.html#extending-torch-autograd\r\n626\r\n\r\nafter every step.\r\n\r\n## Code\r\n\r\n#Importing of Libraries\r\nimport torch\r\nfrom torch.autograd import Variable\r\nimport cv2\r\nfrom data import BaseTransform, VOC_CLASSES as labelmap\r\nfrom ssd import build_ssd\r\nimport imageio\r\n\r\n# Defining a function that will do the detection\r\ndef detect(frame, net, transform):\r\n    height, width = frame.shape[:2]\r\n    frame_t = transform(frame)[0]\r\n    x = torch.from_numpy(frame_t).permute(2,0,1)\r\n    x = Variable(x.unsqueeze(0))\r\n    y = net(x)\r\n    detections = y.data # We create the detections tensor contained in the output y.\r\n    scale = torch.Tensor([width, height, width, height]) # We create a tensor object of dimensions [width, height, width, height].\r\n    for i in range(detections.size(1)): # For every class:\r\n        j = 0 # We initialize the loop variable j that will correspond to the occurrences of the class.\r\n        while detections[0, i, j, 0] >= 0.6: # We take into account all the occurrences j of the class i that have a matching score larger than 0.6.\r\n            pt = (detections[0, i, j, 1:] * scale).numpy() # We get the coordinates of the points at the upper left and the lower right of the detector rectangle.\r\n            cv2.rectangle(frame, (int(pt[0]), int(pt[1])), (int(pt[2]), int(pt[3])), (255, 0, 0), 2) # We draw a rectangle around the detected object.\r\n            cv2.putText(frame, labelmap[i - 1], (int(pt[0]), int(pt[1])), cv2.FONT_HERSHEY_SIMPLEX, 2, (255, 255, 255), 2, cv2.LINE_AA) # We put the label of the class right above the rectangle.\r\n            j += 1 # We increment j to get to the next occurrence.\r\n    return frame\r\n\r\n# Creating the SSD neural network\r\nnet = build_ssd('test')\r\nnet.load_state_dict(torch.load('ssd300_mAP_77.43_v2.pth', map_location = lambda storage, loc: storage ))\r\n\r\n# Creating the transformation\r\ntransform = BaseTransform(net.size, (104/256.0, 117/256.0, 123/256.0))\r\n \r\n\r\n# Doing some object detection on a video\r\nreader = imageio.get_reader('ghystreet.mp4')\r\nfps = reader.get_meta_data()['fps']\r\nwriter = imageio.get_writer('ghystreetout.mp4', fps = fps)\r\nfor i, frame in enumerate(reader): # We iterate on the frames of the output video:\r\n    frame = detect(frame, net.eval(), transform) # We call our detect function (defined above) to detect the object on the frame.\r\n    writer.append_data(frame) # We add the next frame in the output video.\r\n    print(i) # We print the number of the processed frame.\r\nwriter.close() # We close the process that handles the creation of the output video.<issue_closed>\n<issue_comment>username_1: Please prepend the following code to ignore warnings.\r\n```\r\nimport warnings\r\nwarnings.filterwarnings(\"ignore\")\r\n```\n<issue_comment>username_0: okay...but for the  2nd warning it is also affecting the gradients,its saying to rewrite the autograd function in new style. So where to change ?\n<issue_comment>username_1: Please use `Tensor` instead of `Variable`. More details could be retrieved here. https://pytorch.org/blog/pytorch-0_4_0-migration-guide/#merging-tensor-and-variable-and-classes\n<issue_comment>username_0: I did but it could not import Tensor, i watched the blog, but couldn't identify what to change.\n<issue_comment>username_1: I have been running this code for object detection but while running the code ,I am getting a warning \r\n\r\n\r\n..\\torch\\csrc\\autograd\\python_function.cpp:638: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\r\n..\\torch\\csrc\\autograd\\python_function.cpp:664: UserWarning: Legacy autograd function object was called twice.  You will probably get incorrect gradients from this computation, as the saved tensors from the second invocation will clobber the saved tensors from the first invocation.  Please consider rewriting your autograd function in the modern style; for information on the new format, please see: https://pytorch.org/docs/stable/notes/extending.html#extending-torch-autograd\r\n626\r\n\r\nafter every step.\r\n\r\n## Code\r\n\r\n#Importing of Libraries\r\nimport torch\r\nfrom torch.autograd import Variable\r\nimport cv2\r\nfrom data import BaseTransform, VOC_CLASSES as labelmap\r\nfrom ssd import build_ssd\r\nimport imageio\r\n\r\n# Defining a function that will do the detection\r\ndef detect(frame, net, transform):\r\n    height, width = frame.shape[:2]\r\n    frame_t = transform(frame)[0]\r\n    x = torch.from_numpy(frame_t).permute(2,0,1)\r\n    x = Variable(x.unsqueeze(0))\r\n    y = net(x)\r\n    detections = y.data # We create the detections tensor contained in the output y.\r\n    scale = torch.Tensor([width, height, width, height]) # We create a tensor object of dimensions [width, height, width, height].\r\n    for i in range(detections.size(1)): # For every class:\r\n        j = 0 # We initialize the loop variable j that will correspond to the occurrences of the class.\r\n        while detections[0, i, j, 0] >= 0.6: # We take into account all the occurrences j of the class i that have a matching score larger than 0.6.\r\n            pt = (detections[0, i, j, 1:] * scale).numpy() # We get the coordinates of the points at the upper left and the lower right of the detector rectangle.\r\n            cv2.rectangle(frame, (int(pt[0]), int(pt[1])), (int(pt[2]), int(pt[3])), (255, 0, 0), 2) # We draw a rectangle around the detected object.\r\n            cv2.putText(frame, labelmap[i - 1], (int(pt[0]), int(pt[1])), cv2.FONT_HERSHEY_SIMPLEX, 2, (255, 255, 255), 2, cv2.LINE_AA) # We put the label of the class right above the rectangle.\r\n            j += 1 # We increment j to get to the next occurrence.\r\n    return frame\r\n\r\n# Creating the SSD neural network\r\nnet = build_ssd('test')\r\nnet.load_state_dict(torch.load('ssd300_mAP_77.43_v2.pth', map_location = lambda storage, loc: storage ))\r\n\r\n# Creating the transformation\r\ntransform = BaseTransform(net.size, (104/256.0, 117/256.0, 123/256.0))\r\n \r\n\r\n# Doing some object detection on a video\r\nreader = imageio.get_reader('ghystreet.mp4')\r\nfps = reader.get_meta_data()['fps']\r\nwriter = imageio.get_writer('ghystreetout.mp4', fps = fps)\r\nfor i, frame in enumerate(reader): # We iterate on the frames of the output video:\r\n    frame = detect(frame, net.eval(), transform) # We call our detect function (defined above) to detect the object on the frame.\r\n    writer.append_data(frame) # We add the next frame in the output video.\r\n    print(i) # We print the number of the processed frame.\r\nwriter.close() # We close the process that handles the creation of the output video.\n<issue_comment>username_2: You have to rewrite your custom autograd functions with @staticmethod forward and backward methods, as the warning suggests.\n<issue_comment>username_0: Can you specify on which file the changes have to be made.\n<issue_comment>username_3: In your net, you use a custom autograd.Function that is an old style one.  As mentionned in the warning, you should follow the instructions in this doc on how to write a proper custom Function: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function\r\n\r\nYou have to make the change wherever you define the new `autograd.Function`.\r\n\r\nNote that this warning will become an error in 1.5 as mentionned in the message.<issue_closed>\n<issue_comment>username_4: i have the same problem. did you solve it?\r\n\r\nLegacy autograd function with non-static forward method is deprecated. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n<issue_comment>username_3: Hi,\r\n\r\nAs the error mentions you should define the Function as in the doc. You can find more information [here](https://pytorch.org/docs/stable/notes/extending.html) as well."}
{"repo": "pytorch/pytorch", "issue_id": 577683373, "issue_number": 34473, "timestamp": "2020-03-09 06:53:53+00:00", "text": "okay...but for the  2nd warning it is also affecting the gradients,its saying to rewrite the autograd function in new style. So where to change ?", "context": "<issue_start><issue_comment>Title: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3 and UserWarning: Legacy autograd function object was called twice.  You will probably get incorrect gradients from this computation, as the saved tensors from the second invocation will clobber the saved tensors from the first invocation.  Please consider rewriting your autograd function in the modern style; for information on the new format, please see: https://pytorch.org/docs/stable/notes/extending.html#extending-torch-autograd 626\nusername_0: I have been running this code for object detection but while running the code ,I am getting a warning \r\n\r\n\r\n..\\torch\\csrc\\autograd\\python_function.cpp:638: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\r\n..\\torch\\csrc\\autograd\\python_function.cpp:664: UserWarning: Legacy autograd function object was called twice.  You will probably get incorrect gradients from this computation, as the saved tensors from the second invocation will clobber the saved tensors from the first invocation.  Please consider rewriting your autograd function in the modern style; for information on the new format, please see: https://pytorch.org/docs/stable/notes/extending.html#extending-torch-autograd\r\n626\r\n\r\nafter every step.\r\n\r\n## Code\r\n\r\n#Importing of Libraries\r\nimport torch\r\nfrom torch.autograd import Variable\r\nimport cv2\r\nfrom data import BaseTransform, VOC_CLASSES as labelmap\r\nfrom ssd import build_ssd\r\nimport imageio\r\n\r\n# Defining a function that will do the detection\r\ndef detect(frame, net, transform):\r\n    height, width = frame.shape[:2]\r\n    frame_t = transform(frame)[0]\r\n    x = torch.from_numpy(frame_t).permute(2,0,1)\r\n    x = Variable(x.unsqueeze(0))\r\n    y = net(x)\r\n    detections = y.data # We create the detections tensor contained in the output y.\r\n    scale = torch.Tensor([width, height, width, height]) # We create a tensor object of dimensions [width, height, width, height].\r\n    for i in range(detections.size(1)): # For every class:\r\n        j = 0 # We initialize the loop variable j that will correspond to the occurrences of the class.\r\n        while detections[0, i, j, 0] >= 0.6: # We take into account all the occurrences j of the class i that have a matching score larger than 0.6.\r\n            pt = (detections[0, i, j, 1:] * scale).numpy() # We get the coordinates of the points at the upper left and the lower right of the detector rectangle.\r\n            cv2.rectangle(frame, (int(pt[0]), int(pt[1])), (int(pt[2]), int(pt[3])), (255, 0, 0), 2) # We draw a rectangle around the detected object.\r\n            cv2.putText(frame, labelmap[i - 1], (int(pt[0]), int(pt[1])), cv2.FONT_HERSHEY_SIMPLEX, 2, (255, 255, 255), 2, cv2.LINE_AA) # We put the label of the class right above the rectangle.\r\n            j += 1 # We increment j to get to the next occurrence.\r\n    return frame\r\n\r\n# Creating the SSD neural network\r\nnet = build_ssd('test')\r\nnet.load_state_dict(torch.load('ssd300_mAP_77.43_v2.pth', map_location = lambda storage, loc: storage ))\r\n\r\n# Creating the transformation\r\ntransform = BaseTransform(net.size, (104/256.0, 117/256.0, 123/256.0))\r\n \r\n\r\n# Doing some object detection on a video\r\nreader = imageio.get_reader('ghystreet.mp4')\r\nfps = reader.get_meta_data()['fps']\r\nwriter = imageio.get_writer('ghystreetout.mp4', fps = fps)\r\nfor i, frame in enumerate(reader): # We iterate on the frames of the output video:\r\n    frame = detect(frame, net.eval(), transform) # We call our detect function (defined above) to detect the object on the frame.\r\n    writer.append_data(frame) # We add the next frame in the output video.\r\n    print(i) # We print the number of the processed frame.\r\nwriter.close() # We close the process that handles the creation of the output video.<issue_closed>\n<issue_comment>username_1: Please prepend the following code to ignore warnings.\r\n```\r\nimport warnings\r\nwarnings.filterwarnings(\"ignore\")\r\n```\n<issue_comment>username_0: okay...but for the  2nd warning it is also affecting the gradients,its saying to rewrite the autograd function in new style. So where to change ?\n<issue_comment>username_1: Please use `Tensor` instead of `Variable`. More details could be retrieved here. https://pytorch.org/blog/pytorch-0_4_0-migration-guide/#merging-tensor-and-variable-and-classes\n<issue_comment>username_0: I did but it could not import Tensor, i watched the blog, but couldn't identify what to change.\n<issue_comment>username_1: I have been running this code for object detection but while running the code ,I am getting a warning \r\n\r\n\r\n..\\torch\\csrc\\autograd\\python_function.cpp:638: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\r\n..\\torch\\csrc\\autograd\\python_function.cpp:664: UserWarning: Legacy autograd function object was called twice.  You will probably get incorrect gradients from this computation, as the saved tensors from the second invocation will clobber the saved tensors from the first invocation.  Please consider rewriting your autograd function in the modern style; for information on the new format, please see: https://pytorch.org/docs/stable/notes/extending.html#extending-torch-autograd\r\n626\r\n\r\nafter every step.\r\n\r\n## Code\r\n\r\n#Importing of Libraries\r\nimport torch\r\nfrom torch.autograd import Variable\r\nimport cv2\r\nfrom data import BaseTransform, VOC_CLASSES as labelmap\r\nfrom ssd import build_ssd\r\nimport imageio\r\n\r\n# Defining a function that will do the detection\r\ndef detect(frame, net, transform):\r\n    height, width = frame.shape[:2]\r\n    frame_t = transform(frame)[0]\r\n    x = torch.from_numpy(frame_t).permute(2,0,1)\r\n    x = Variable(x.unsqueeze(0))\r\n    y = net(x)\r\n    detections = y.data # We create the detections tensor contained in the output y.\r\n    scale = torch.Tensor([width, height, width, height]) # We create a tensor object of dimensions [width, height, width, height].\r\n    for i in range(detections.size(1)): # For every class:\r\n        j = 0 # We initialize the loop variable j that will correspond to the occurrences of the class.\r\n        while detections[0, i, j, 0] >= 0.6: # We take into account all the occurrences j of the class i that have a matching score larger than 0.6.\r\n            pt = (detections[0, i, j, 1:] * scale).numpy() # We get the coordinates of the points at the upper left and the lower right of the detector rectangle.\r\n            cv2.rectangle(frame, (int(pt[0]), int(pt[1])), (int(pt[2]), int(pt[3])), (255, 0, 0), 2) # We draw a rectangle around the detected object.\r\n            cv2.putText(frame, labelmap[i - 1], (int(pt[0]), int(pt[1])), cv2.FONT_HERSHEY_SIMPLEX, 2, (255, 255, 255), 2, cv2.LINE_AA) # We put the label of the class right above the rectangle.\r\n            j += 1 # We increment j to get to the next occurrence.\r\n    return frame\r\n\r\n# Creating the SSD neural network\r\nnet = build_ssd('test')\r\nnet.load_state_dict(torch.load('ssd300_mAP_77.43_v2.pth', map_location = lambda storage, loc: storage ))\r\n\r\n# Creating the transformation\r\ntransform = BaseTransform(net.size, (104/256.0, 117/256.0, 123/256.0))\r\n \r\n\r\n# Doing some object detection on a video\r\nreader = imageio.get_reader('ghystreet.mp4')\r\nfps = reader.get_meta_data()['fps']\r\nwriter = imageio.get_writer('ghystreetout.mp4', fps = fps)\r\nfor i, frame in enumerate(reader): # We iterate on the frames of the output video:\r\n    frame = detect(frame, net.eval(), transform) # We call our detect function (defined above) to detect the object on the frame.\r\n    writer.append_data(frame) # We add the next frame in the output video.\r\n    print(i) # We print the number of the processed frame.\r\nwriter.close() # We close the process that handles the creation of the output video.\n<issue_comment>username_2: You have to rewrite your custom autograd functions with @staticmethod forward and backward methods, as the warning suggests.\n<issue_comment>username_0: Can you specify on which file the changes have to be made.\n<issue_comment>username_3: In your net, you use a custom autograd.Function that is an old style one.  As mentionned in the warning, you should follow the instructions in this doc on how to write a proper custom Function: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function\r\n\r\nYou have to make the change wherever you define the new `autograd.Function`.\r\n\r\nNote that this warning will become an error in 1.5 as mentionned in the message.<issue_closed>\n<issue_comment>username_4: i have the same problem. did you solve it?\r\n\r\nLegacy autograd function with non-static forward method is deprecated. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n<issue_comment>username_3: Hi,\r\n\r\nAs the error mentions you should define the Function as in the doc. You can find more information [here](https://pytorch.org/docs/stable/notes/extending.html) as well."}
{"repo": "pytorch/pytorch", "issue_id": 577683373, "issue_number": 34473, "timestamp": "2020-03-09 06:59:13+00:00", "text": "Please use `Tensor` instead of `Variable`. More details could be retrieved here. https://pytorch.org/blog/pytorch-0_4_0-migration-guide/#merging-tensor-and-variable-and-classes", "context": "<issue_start><issue_comment>Title: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3 and UserWarning: Legacy autograd function object was called twice.  You will probably get incorrect gradients from this computation, as the saved tensors from the second invocation will clobber the saved tensors from the first invocation.  Please consider rewriting your autograd function in the modern style; for information on the new format, please see: https://pytorch.org/docs/stable/notes/extending.html#extending-torch-autograd 626\nusername_0: I have been running this code for object detection but while running the code ,I am getting a warning \r\n\r\n\r\n..\\torch\\csrc\\autograd\\python_function.cpp:638: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\r\n..\\torch\\csrc\\autograd\\python_function.cpp:664: UserWarning: Legacy autograd function object was called twice.  You will probably get incorrect gradients from this computation, as the saved tensors from the second invocation will clobber the saved tensors from the first invocation.  Please consider rewriting your autograd function in the modern style; for information on the new format, please see: https://pytorch.org/docs/stable/notes/extending.html#extending-torch-autograd\r\n626\r\n\r\nafter every step.\r\n\r\n## Code\r\n\r\n#Importing of Libraries\r\nimport torch\r\nfrom torch.autograd import Variable\r\nimport cv2\r\nfrom data import BaseTransform, VOC_CLASSES as labelmap\r\nfrom ssd import build_ssd\r\nimport imageio\r\n\r\n# Defining a function that will do the detection\r\ndef detect(frame, net, transform):\r\n    height, width = frame.shape[:2]\r\n    frame_t = transform(frame)[0]\r\n    x = torch.from_numpy(frame_t).permute(2,0,1)\r\n    x = Variable(x.unsqueeze(0))\r\n    y = net(x)\r\n    detections = y.data # We create the detections tensor contained in the output y.\r\n    scale = torch.Tensor([width, height, width, height]) # We create a tensor object of dimensions [width, height, width, height].\r\n    for i in range(detections.size(1)): # For every class:\r\n        j = 0 # We initialize the loop variable j that will correspond to the occurrences of the class.\r\n        while detections[0, i, j, 0] >= 0.6: # We take into account all the occurrences j of the class i that have a matching score larger than 0.6.\r\n            pt = (detections[0, i, j, 1:] * scale).numpy() # We get the coordinates of the points at the upper left and the lower right of the detector rectangle.\r\n            cv2.rectangle(frame, (int(pt[0]), int(pt[1])), (int(pt[2]), int(pt[3])), (255, 0, 0), 2) # We draw a rectangle around the detected object.\r\n            cv2.putText(frame, labelmap[i - 1], (int(pt[0]), int(pt[1])), cv2.FONT_HERSHEY_SIMPLEX, 2, (255, 255, 255), 2, cv2.LINE_AA) # We put the label of the class right above the rectangle.\r\n            j += 1 # We increment j to get to the next occurrence.\r\n    return frame\r\n\r\n# Creating the SSD neural network\r\nnet = build_ssd('test')\r\nnet.load_state_dict(torch.load('ssd300_mAP_77.43_v2.pth', map_location = lambda storage, loc: storage ))\r\n\r\n# Creating the transformation\r\ntransform = BaseTransform(net.size, (104/256.0, 117/256.0, 123/256.0))\r\n \r\n\r\n# Doing some object detection on a video\r\nreader = imageio.get_reader('ghystreet.mp4')\r\nfps = reader.get_meta_data()['fps']\r\nwriter = imageio.get_writer('ghystreetout.mp4', fps = fps)\r\nfor i, frame in enumerate(reader): # We iterate on the frames of the output video:\r\n    frame = detect(frame, net.eval(), transform) # We call our detect function (defined above) to detect the object on the frame.\r\n    writer.append_data(frame) # We add the next frame in the output video.\r\n    print(i) # We print the number of the processed frame.\r\nwriter.close() # We close the process that handles the creation of the output video.<issue_closed>\n<issue_comment>username_1: Please prepend the following code to ignore warnings.\r\n```\r\nimport warnings\r\nwarnings.filterwarnings(\"ignore\")\r\n```\n<issue_comment>username_0: okay...but for the  2nd warning it is also affecting the gradients,its saying to rewrite the autograd function in new style. So where to change ?\n<issue_comment>username_1: Please use `Tensor` instead of `Variable`. More details could be retrieved here. https://pytorch.org/blog/pytorch-0_4_0-migration-guide/#merging-tensor-and-variable-and-classes\n<issue_comment>username_0: I did but it could not import Tensor, i watched the blog, but couldn't identify what to change.\n<issue_comment>username_1: I have been running this code for object detection but while running the code ,I am getting a warning \r\n\r\n\r\n..\\torch\\csrc\\autograd\\python_function.cpp:638: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\r\n..\\torch\\csrc\\autograd\\python_function.cpp:664: UserWarning: Legacy autograd function object was called twice.  You will probably get incorrect gradients from this computation, as the saved tensors from the second invocation will clobber the saved tensors from the first invocation.  Please consider rewriting your autograd function in the modern style; for information on the new format, please see: https://pytorch.org/docs/stable/notes/extending.html#extending-torch-autograd\r\n626\r\n\r\nafter every step.\r\n\r\n## Code\r\n\r\n#Importing of Libraries\r\nimport torch\r\nfrom torch.autograd import Variable\r\nimport cv2\r\nfrom data import BaseTransform, VOC_CLASSES as labelmap\r\nfrom ssd import build_ssd\r\nimport imageio\r\n\r\n# Defining a function that will do the detection\r\ndef detect(frame, net, transform):\r\n    height, width = frame.shape[:2]\r\n    frame_t = transform(frame)[0]\r\n    x = torch.from_numpy(frame_t).permute(2,0,1)\r\n    x = Variable(x.unsqueeze(0))\r\n    y = net(x)\r\n    detections = y.data # We create the detections tensor contained in the output y.\r\n    scale = torch.Tensor([width, height, width, height]) # We create a tensor object of dimensions [width, height, width, height].\r\n    for i in range(detections.size(1)): # For every class:\r\n        j = 0 # We initialize the loop variable j that will correspond to the occurrences of the class.\r\n        while detections[0, i, j, 0] >= 0.6: # We take into account all the occurrences j of the class i that have a matching score larger than 0.6.\r\n            pt = (detections[0, i, j, 1:] * scale).numpy() # We get the coordinates of the points at the upper left and the lower right of the detector rectangle.\r\n            cv2.rectangle(frame, (int(pt[0]), int(pt[1])), (int(pt[2]), int(pt[3])), (255, 0, 0), 2) # We draw a rectangle around the detected object.\r\n            cv2.putText(frame, labelmap[i - 1], (int(pt[0]), int(pt[1])), cv2.FONT_HERSHEY_SIMPLEX, 2, (255, 255, 255), 2, cv2.LINE_AA) # We put the label of the class right above the rectangle.\r\n            j += 1 # We increment j to get to the next occurrence.\r\n    return frame\r\n\r\n# Creating the SSD neural network\r\nnet = build_ssd('test')\r\nnet.load_state_dict(torch.load('ssd300_mAP_77.43_v2.pth', map_location = lambda storage, loc: storage ))\r\n\r\n# Creating the transformation\r\ntransform = BaseTransform(net.size, (104/256.0, 117/256.0, 123/256.0))\r\n \r\n\r\n# Doing some object detection on a video\r\nreader = imageio.get_reader('ghystreet.mp4')\r\nfps = reader.get_meta_data()['fps']\r\nwriter = imageio.get_writer('ghystreetout.mp4', fps = fps)\r\nfor i, frame in enumerate(reader): # We iterate on the frames of the output video:\r\n    frame = detect(frame, net.eval(), transform) # We call our detect function (defined above) to detect the object on the frame.\r\n    writer.append_data(frame) # We add the next frame in the output video.\r\n    print(i) # We print the number of the processed frame.\r\nwriter.close() # We close the process that handles the creation of the output video.\n<issue_comment>username_2: You have to rewrite your custom autograd functions with @staticmethod forward and backward methods, as the warning suggests.\n<issue_comment>username_0: Can you specify on which file the changes have to be made.\n<issue_comment>username_3: In your net, you use a custom autograd.Function that is an old style one.  As mentionned in the warning, you should follow the instructions in this doc on how to write a proper custom Function: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function\r\n\r\nYou have to make the change wherever you define the new `autograd.Function`.\r\n\r\nNote that this warning will become an error in 1.5 as mentionned in the message.<issue_closed>\n<issue_comment>username_4: i have the same problem. did you solve it?\r\n\r\nLegacy autograd function with non-static forward method is deprecated. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n<issue_comment>username_3: Hi,\r\n\r\nAs the error mentions you should define the Function as in the doc. You can find more information [here](https://pytorch.org/docs/stable/notes/extending.html) as well."}
{"repo": "pytorch/pytorch", "issue_id": 577683373, "issue_number": 34473, "timestamp": "2020-03-09 07:16:53+00:00", "text": "I did but it could not import Tensor, i watched the blog, but couldn't identify what to change.", "context": "<issue_start><issue_comment>Title: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3 and UserWarning: Legacy autograd function object was called twice.  You will probably get incorrect gradients from this computation, as the saved tensors from the second invocation will clobber the saved tensors from the first invocation.  Please consider rewriting your autograd function in the modern style; for information on the new format, please see: https://pytorch.org/docs/stable/notes/extending.html#extending-torch-autograd 626\nusername_0: I have been running this code for object detection but while running the code ,I am getting a warning \r\n\r\n\r\n..\\torch\\csrc\\autograd\\python_function.cpp:638: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\r\n..\\torch\\csrc\\autograd\\python_function.cpp:664: UserWarning: Legacy autograd function object was called twice.  You will probably get incorrect gradients from this computation, as the saved tensors from the second invocation will clobber the saved tensors from the first invocation.  Please consider rewriting your autograd function in the modern style; for information on the new format, please see: https://pytorch.org/docs/stable/notes/extending.html#extending-torch-autograd\r\n626\r\n\r\nafter every step.\r\n\r\n## Code\r\n\r\n#Importing of Libraries\r\nimport torch\r\nfrom torch.autograd import Variable\r\nimport cv2\r\nfrom data import BaseTransform, VOC_CLASSES as labelmap\r\nfrom ssd import build_ssd\r\nimport imageio\r\n\r\n# Defining a function that will do the detection\r\ndef detect(frame, net, transform):\r\n    height, width = frame.shape[:2]\r\n    frame_t = transform(frame)[0]\r\n    x = torch.from_numpy(frame_t).permute(2,0,1)\r\n    x = Variable(x.unsqueeze(0))\r\n    y = net(x)\r\n    detections = y.data # We create the detections tensor contained in the output y.\r\n    scale = torch.Tensor([width, height, width, height]) # We create a tensor object of dimensions [width, height, width, height].\r\n    for i in range(detections.size(1)): # For every class:\r\n        j = 0 # We initialize the loop variable j that will correspond to the occurrences of the class.\r\n        while detections[0, i, j, 0] >= 0.6: # We take into account all the occurrences j of the class i that have a matching score larger than 0.6.\r\n            pt = (detections[0, i, j, 1:] * scale).numpy() # We get the coordinates of the points at the upper left and the lower right of the detector rectangle.\r\n            cv2.rectangle(frame, (int(pt[0]), int(pt[1])), (int(pt[2]), int(pt[3])), (255, 0, 0), 2) # We draw a rectangle around the detected object.\r\n            cv2.putText(frame, labelmap[i - 1], (int(pt[0]), int(pt[1])), cv2.FONT_HERSHEY_SIMPLEX, 2, (255, 255, 255), 2, cv2.LINE_AA) # We put the label of the class right above the rectangle.\r\n            j += 1 # We increment j to get to the next occurrence.\r\n    return frame\r\n\r\n# Creating the SSD neural network\r\nnet = build_ssd('test')\r\nnet.load_state_dict(torch.load('ssd300_mAP_77.43_v2.pth', map_location = lambda storage, loc: storage ))\r\n\r\n# Creating the transformation\r\ntransform = BaseTransform(net.size, (104/256.0, 117/256.0, 123/256.0))\r\n \r\n\r\n# Doing some object detection on a video\r\nreader = imageio.get_reader('ghystreet.mp4')\r\nfps = reader.get_meta_data()['fps']\r\nwriter = imageio.get_writer('ghystreetout.mp4', fps = fps)\r\nfor i, frame in enumerate(reader): # We iterate on the frames of the output video:\r\n    frame = detect(frame, net.eval(), transform) # We call our detect function (defined above) to detect the object on the frame.\r\n    writer.append_data(frame) # We add the next frame in the output video.\r\n    print(i) # We print the number of the processed frame.\r\nwriter.close() # We close the process that handles the creation of the output video.<issue_closed>\n<issue_comment>username_1: Please prepend the following code to ignore warnings.\r\n```\r\nimport warnings\r\nwarnings.filterwarnings(\"ignore\")\r\n```\n<issue_comment>username_0: okay...but for the  2nd warning it is also affecting the gradients,its saying to rewrite the autograd function in new style. So where to change ?\n<issue_comment>username_1: Please use `Tensor` instead of `Variable`. More details could be retrieved here. https://pytorch.org/blog/pytorch-0_4_0-migration-guide/#merging-tensor-and-variable-and-classes\n<issue_comment>username_0: I did but it could not import Tensor, i watched the blog, but couldn't identify what to change.\n<issue_comment>username_1: I have been running this code for object detection but while running the code ,I am getting a warning \r\n\r\n\r\n..\\torch\\csrc\\autograd\\python_function.cpp:638: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\r\n..\\torch\\csrc\\autograd\\python_function.cpp:664: UserWarning: Legacy autograd function object was called twice.  You will probably get incorrect gradients from this computation, as the saved tensors from the second invocation will clobber the saved tensors from the first invocation.  Please consider rewriting your autograd function in the modern style; for information on the new format, please see: https://pytorch.org/docs/stable/notes/extending.html#extending-torch-autograd\r\n626\r\n\r\nafter every step.\r\n\r\n## Code\r\n\r\n#Importing of Libraries\r\nimport torch\r\nfrom torch.autograd import Variable\r\nimport cv2\r\nfrom data import BaseTransform, VOC_CLASSES as labelmap\r\nfrom ssd import build_ssd\r\nimport imageio\r\n\r\n# Defining a function that will do the detection\r\ndef detect(frame, net, transform):\r\n    height, width = frame.shape[:2]\r\n    frame_t = transform(frame)[0]\r\n    x = torch.from_numpy(frame_t).permute(2,0,1)\r\n    x = Variable(x.unsqueeze(0))\r\n    y = net(x)\r\n    detections = y.data # We create the detections tensor contained in the output y.\r\n    scale = torch.Tensor([width, height, width, height]) # We create a tensor object of dimensions [width, height, width, height].\r\n    for i in range(detections.size(1)): # For every class:\r\n        j = 0 # We initialize the loop variable j that will correspond to the occurrences of the class.\r\n        while detections[0, i, j, 0] >= 0.6: # We take into account all the occurrences j of the class i that have a matching score larger than 0.6.\r\n            pt = (detections[0, i, j, 1:] * scale).numpy() # We get the coordinates of the points at the upper left and the lower right of the detector rectangle.\r\n            cv2.rectangle(frame, (int(pt[0]), int(pt[1])), (int(pt[2]), int(pt[3])), (255, 0, 0), 2) # We draw a rectangle around the detected object.\r\n            cv2.putText(frame, labelmap[i - 1], (int(pt[0]), int(pt[1])), cv2.FONT_HERSHEY_SIMPLEX, 2, (255, 255, 255), 2, cv2.LINE_AA) # We put the label of the class right above the rectangle.\r\n            j += 1 # We increment j to get to the next occurrence.\r\n    return frame\r\n\r\n# Creating the SSD neural network\r\nnet = build_ssd('test')\r\nnet.load_state_dict(torch.load('ssd300_mAP_77.43_v2.pth', map_location = lambda storage, loc: storage ))\r\n\r\n# Creating the transformation\r\ntransform = BaseTransform(net.size, (104/256.0, 117/256.0, 123/256.0))\r\n \r\n\r\n# Doing some object detection on a video\r\nreader = imageio.get_reader('ghystreet.mp4')\r\nfps = reader.get_meta_data()['fps']\r\nwriter = imageio.get_writer('ghystreetout.mp4', fps = fps)\r\nfor i, frame in enumerate(reader): # We iterate on the frames of the output video:\r\n    frame = detect(frame, net.eval(), transform) # We call our detect function (defined above) to detect the object on the frame.\r\n    writer.append_data(frame) # We add the next frame in the output video.\r\n    print(i) # We print the number of the processed frame.\r\nwriter.close() # We close the process that handles the creation of the output video.\n<issue_comment>username_2: You have to rewrite your custom autograd functions with @staticmethod forward and backward methods, as the warning suggests.\n<issue_comment>username_0: Can you specify on which file the changes have to be made.\n<issue_comment>username_3: In your net, you use a custom autograd.Function that is an old style one.  As mentionned in the warning, you should follow the instructions in this doc on how to write a proper custom Function: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function\r\n\r\nYou have to make the change wherever you define the new `autograd.Function`.\r\n\r\nNote that this warning will become an error in 1.5 as mentionned in the message.<issue_closed>\n<issue_comment>username_4: i have the same problem. did you solve it?\r\n\r\nLegacy autograd function with non-static forward method is deprecated. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n<issue_comment>username_3: Hi,\r\n\r\nAs the error mentions you should define the Function as in the doc. You can find more information [here](https://pytorch.org/docs/stable/notes/extending.html) as well."}
{"repo": "pytorch/pytorch", "issue_id": 577683373, "issue_number": 34473, "timestamp": "2020-03-09 11:44:06+00:00", "text": "I have been running this code for object detection but while running the code ,I am getting a warning \r\n\r\n\r\n..\\torch\\csrc\\autograd\\python_function.cpp:638: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\r\n..\\torch\\csrc\\autograd\\python_function.cpp:664: UserWarning: Legacy autograd function object was called twice.  You will probably get incorrect gradients from this computation, as the saved tensors from the second invocation will clobber the saved tensors from the first invocation.  Please consider rewriting your autograd function in the modern style; for information on the new format, please see: https://pytorch.org/docs/stable/notes/extending.html#extending-torch-autograd\r\n626\r\n\r\nafter every step.\r\n\r\n## Code\r\n\r\n#Importing of Libraries\r\nimport torch\r\nfrom torch.autograd import Variable\r\nimport cv2\r\nfrom data import BaseTransform, VOC_CLASSES as labelmap\r\nfrom ssd import build_ssd\r\nimport imageio\r\n\r\n# Defining a function that will do the detection\r\ndef detect(frame, net, transform):\r\n    height, width = frame.shape[:2]\r\n    frame_t = transform(frame)[0]\r\n    x = torch.from_numpy(frame_t).permute(2,0,1)\r\n    x = Variable(x.unsqueeze(0))\r\n    y = net(x)\r\n    detections = y.data # We create the detections tensor contained in the output y.\r\n    scale = torch.Tensor([width, height, width, height]) # We create a tensor object of dimensions [width, height, width, height].\r\n    for i in range(detections.size(1)): # For every class:\r\n        j = 0 # We initialize the loop variable j that will correspond to the occurrences of the class.\r\n        while detections[0, i, j, 0] >= 0.6: # We take into account all the occurrences j of the class i that have a matching score larger than 0.6.\r\n            pt = (detections[0, i, j, 1:] * scale).numpy() # We get the coordinates of the points at the upper left and the lower right of the detector rectangle.\r\n            cv2.rectangle(frame, (int(pt[0]), int(pt[1])), (int(pt[2]), int(pt[3])), (255, 0, 0), 2) # We draw a rectangle around the detected object.\r\n            cv2.putText(frame, labelmap[i - 1], (int(pt[0]), int(pt[1])), cv2.FONT_HERSHEY_SIMPLEX, 2, (255, 255, 255), 2, cv2.LINE_AA) # We put the label of the class right above the rectangle.\r\n            j += 1 # We increment j to get to the next occurrence.\r\n    return frame\r\n\r\n# Creating the SSD neural network\r\nnet = build_ssd('test')\r\nnet.load_state_dict(torch.load('ssd300_mAP_77.43_v2.pth', map_location = lambda storage, loc: storage ))\r\n\r\n# Creating the transformation\r\ntransform = BaseTransform(net.size, (104/256.0, 117/256.0, 123/256.0))\r\n \r\n\r\n# Doing some object detection on a video\r\nreader = imageio.get_reader('ghystreet.mp4')\r\nfps = reader.get_meta_data()['fps']\r\nwriter = imageio.get_writer('ghystreetout.mp4', fps = fps)\r\nfor i, frame in enumerate(reader): # We iterate on the frames of the output video:\r\n    frame = detect(frame, net.eval(), transform) # We call our detect function (defined above) to detect the object on the frame.\r\n    writer.append_data(frame) # We add the next frame in the output video.\r\n    print(i) # We print the number of the processed frame.\r\nwriter.close() # We close the process that handles the creation of the output video.", "context": "<issue_start><issue_comment>Title: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3 and UserWarning: Legacy autograd function object was called twice.  You will probably get incorrect gradients from this computation, as the saved tensors from the second invocation will clobber the saved tensors from the first invocation.  Please consider rewriting your autograd function in the modern style; for information on the new format, please see: https://pytorch.org/docs/stable/notes/extending.html#extending-torch-autograd 626\nusername_0: I have been running this code for object detection but while running the code ,I am getting a warning \r\n\r\n\r\n..\\torch\\csrc\\autograd\\python_function.cpp:638: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\r\n..\\torch\\csrc\\autograd\\python_function.cpp:664: UserWarning: Legacy autograd function object was called twice.  You will probably get incorrect gradients from this computation, as the saved tensors from the second invocation will clobber the saved tensors from the first invocation.  Please consider rewriting your autograd function in the modern style; for information on the new format, please see: https://pytorch.org/docs/stable/notes/extending.html#extending-torch-autograd\r\n626\r\n\r\nafter every step.\r\n\r\n## Code\r\n\r\n#Importing of Libraries\r\nimport torch\r\nfrom torch.autograd import Variable\r\nimport cv2\r\nfrom data import BaseTransform, VOC_CLASSES as labelmap\r\nfrom ssd import build_ssd\r\nimport imageio\r\n\r\n# Defining a function that will do the detection\r\ndef detect(frame, net, transform):\r\n    height, width = frame.shape[:2]\r\n    frame_t = transform(frame)[0]\r\n    x = torch.from_numpy(frame_t).permute(2,0,1)\r\n    x = Variable(x.unsqueeze(0))\r\n    y = net(x)\r\n    detections = y.data # We create the detections tensor contained in the output y.\r\n    scale = torch.Tensor([width, height, width, height]) # We create a tensor object of dimensions [width, height, width, height].\r\n    for i in range(detections.size(1)): # For every class:\r\n        j = 0 # We initialize the loop variable j that will correspond to the occurrences of the class.\r\n        while detections[0, i, j, 0] >= 0.6: # We take into account all the occurrences j of the class i that have a matching score larger than 0.6.\r\n            pt = (detections[0, i, j, 1:] * scale).numpy() # We get the coordinates of the points at the upper left and the lower right of the detector rectangle.\r\n            cv2.rectangle(frame, (int(pt[0]), int(pt[1])), (int(pt[2]), int(pt[3])), (255, 0, 0), 2) # We draw a rectangle around the detected object.\r\n            cv2.putText(frame, labelmap[i - 1], (int(pt[0]), int(pt[1])), cv2.FONT_HERSHEY_SIMPLEX, 2, (255, 255, 255), 2, cv2.LINE_AA) # We put the label of the class right above the rectangle.\r\n            j += 1 # We increment j to get to the next occurrence.\r\n    return frame\r\n\r\n# Creating the SSD neural network\r\nnet = build_ssd('test')\r\nnet.load_state_dict(torch.load('ssd300_mAP_77.43_v2.pth', map_location = lambda storage, loc: storage ))\r\n\r\n# Creating the transformation\r\ntransform = BaseTransform(net.size, (104/256.0, 117/256.0, 123/256.0))\r\n \r\n\r\n# Doing some object detection on a video\r\nreader = imageio.get_reader('ghystreet.mp4')\r\nfps = reader.get_meta_data()['fps']\r\nwriter = imageio.get_writer('ghystreetout.mp4', fps = fps)\r\nfor i, frame in enumerate(reader): # We iterate on the frames of the output video:\r\n    frame = detect(frame, net.eval(), transform) # We call our detect function (defined above) to detect the object on the frame.\r\n    writer.append_data(frame) # We add the next frame in the output video.\r\n    print(i) # We print the number of the processed frame.\r\nwriter.close() # We close the process that handles the creation of the output video.<issue_closed>\n<issue_comment>username_1: Please prepend the following code to ignore warnings.\r\n```\r\nimport warnings\r\nwarnings.filterwarnings(\"ignore\")\r\n```\n<issue_comment>username_0: okay...but for the  2nd warning it is also affecting the gradients,its saying to rewrite the autograd function in new style. So where to change ?\n<issue_comment>username_1: Please use `Tensor` instead of `Variable`. More details could be retrieved here. https://pytorch.org/blog/pytorch-0_4_0-migration-guide/#merging-tensor-and-variable-and-classes\n<issue_comment>username_0: I did but it could not import Tensor, i watched the blog, but couldn't identify what to change.\n<issue_comment>username_1: I have been running this code for object detection but while running the code ,I am getting a warning \r\n\r\n\r\n..\\torch\\csrc\\autograd\\python_function.cpp:638: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\r\n..\\torch\\csrc\\autograd\\python_function.cpp:664: UserWarning: Legacy autograd function object was called twice.  You will probably get incorrect gradients from this computation, as the saved tensors from the second invocation will clobber the saved tensors from the first invocation.  Please consider rewriting your autograd function in the modern style; for information on the new format, please see: https://pytorch.org/docs/stable/notes/extending.html#extending-torch-autograd\r\n626\r\n\r\nafter every step.\r\n\r\n## Code\r\n\r\n#Importing of Libraries\r\nimport torch\r\nfrom torch.autograd import Variable\r\nimport cv2\r\nfrom data import BaseTransform, VOC_CLASSES as labelmap\r\nfrom ssd import build_ssd\r\nimport imageio\r\n\r\n# Defining a function that will do the detection\r\ndef detect(frame, net, transform):\r\n    height, width = frame.shape[:2]\r\n    frame_t = transform(frame)[0]\r\n    x = torch.from_numpy(frame_t).permute(2,0,1)\r\n    x = Variable(x.unsqueeze(0))\r\n    y = net(x)\r\n    detections = y.data # We create the detections tensor contained in the output y.\r\n    scale = torch.Tensor([width, height, width, height]) # We create a tensor object of dimensions [width, height, width, height].\r\n    for i in range(detections.size(1)): # For every class:\r\n        j = 0 # We initialize the loop variable j that will correspond to the occurrences of the class.\r\n        while detections[0, i, j, 0] >= 0.6: # We take into account all the occurrences j of the class i that have a matching score larger than 0.6.\r\n            pt = (detections[0, i, j, 1:] * scale).numpy() # We get the coordinates of the points at the upper left and the lower right of the detector rectangle.\r\n            cv2.rectangle(frame, (int(pt[0]), int(pt[1])), (int(pt[2]), int(pt[3])), (255, 0, 0), 2) # We draw a rectangle around the detected object.\r\n            cv2.putText(frame, labelmap[i - 1], (int(pt[0]), int(pt[1])), cv2.FONT_HERSHEY_SIMPLEX, 2, (255, 255, 255), 2, cv2.LINE_AA) # We put the label of the class right above the rectangle.\r\n            j += 1 # We increment j to get to the next occurrence.\r\n    return frame\r\n\r\n# Creating the SSD neural network\r\nnet = build_ssd('test')\r\nnet.load_state_dict(torch.load('ssd300_mAP_77.43_v2.pth', map_location = lambda storage, loc: storage ))\r\n\r\n# Creating the transformation\r\ntransform = BaseTransform(net.size, (104/256.0, 117/256.0, 123/256.0))\r\n \r\n\r\n# Doing some object detection on a video\r\nreader = imageio.get_reader('ghystreet.mp4')\r\nfps = reader.get_meta_data()['fps']\r\nwriter = imageio.get_writer('ghystreetout.mp4', fps = fps)\r\nfor i, frame in enumerate(reader): # We iterate on the frames of the output video:\r\n    frame = detect(frame, net.eval(), transform) # We call our detect function (defined above) to detect the object on the frame.\r\n    writer.append_data(frame) # We add the next frame in the output video.\r\n    print(i) # We print the number of the processed frame.\r\nwriter.close() # We close the process that handles the creation of the output video.\n<issue_comment>username_2: You have to rewrite your custom autograd functions with @staticmethod forward and backward methods, as the warning suggests.\n<issue_comment>username_0: Can you specify on which file the changes have to be made.\n<issue_comment>username_3: In your net, you use a custom autograd.Function that is an old style one.  As mentionned in the warning, you should follow the instructions in this doc on how to write a proper custom Function: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function\r\n\r\nYou have to make the change wherever you define the new `autograd.Function`.\r\n\r\nNote that this warning will become an error in 1.5 as mentionned in the message.<issue_closed>\n<issue_comment>username_4: i have the same problem. did you solve it?\r\n\r\nLegacy autograd function with non-static forward method is deprecated. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n<issue_comment>username_3: Hi,\r\n\r\nAs the error mentions you should define the Function as in the doc. You can find more information [here](https://pytorch.org/docs/stable/notes/extending.html) as well."}
{"repo": "pytorch/pytorch", "issue_id": 577683373, "issue_number": 34473, "timestamp": "2020-03-09 20:30:33+00:00", "text": "You have to rewrite your custom autograd functions with @staticmethod forward and backward methods, as the warning suggests.", "context": "<issue_start><issue_comment>Title: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3 and UserWarning: Legacy autograd function object was called twice.  You will probably get incorrect gradients from this computation, as the saved tensors from the second invocation will clobber the saved tensors from the first invocation.  Please consider rewriting your autograd function in the modern style; for information on the new format, please see: https://pytorch.org/docs/stable/notes/extending.html#extending-torch-autograd 626\nusername_0: I have been running this code for object detection but while running the code ,I am getting a warning \r\n\r\n\r\n..\\torch\\csrc\\autograd\\python_function.cpp:638: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\r\n..\\torch\\csrc\\autograd\\python_function.cpp:664: UserWarning: Legacy autograd function object was called twice.  You will probably get incorrect gradients from this computation, as the saved tensors from the second invocation will clobber the saved tensors from the first invocation.  Please consider rewriting your autograd function in the modern style; for information on the new format, please see: https://pytorch.org/docs/stable/notes/extending.html#extending-torch-autograd\r\n626\r\n\r\nafter every step.\r\n\r\n## Code\r\n\r\n#Importing of Libraries\r\nimport torch\r\nfrom torch.autograd import Variable\r\nimport cv2\r\nfrom data import BaseTransform, VOC_CLASSES as labelmap\r\nfrom ssd import build_ssd\r\nimport imageio\r\n\r\n# Defining a function that will do the detection\r\ndef detect(frame, net, transform):\r\n    height, width = frame.shape[:2]\r\n    frame_t = transform(frame)[0]\r\n    x = torch.from_numpy(frame_t).permute(2,0,1)\r\n    x = Variable(x.unsqueeze(0))\r\n    y = net(x)\r\n    detections = y.data # We create the detections tensor contained in the output y.\r\n    scale = torch.Tensor([width, height, width, height]) # We create a tensor object of dimensions [width, height, width, height].\r\n    for i in range(detections.size(1)): # For every class:\r\n        j = 0 # We initialize the loop variable j that will correspond to the occurrences of the class.\r\n        while detections[0, i, j, 0] >= 0.6: # We take into account all the occurrences j of the class i that have a matching score larger than 0.6.\r\n            pt = (detections[0, i, j, 1:] * scale).numpy() # We get the coordinates of the points at the upper left and the lower right of the detector rectangle.\r\n            cv2.rectangle(frame, (int(pt[0]), int(pt[1])), (int(pt[2]), int(pt[3])), (255, 0, 0), 2) # We draw a rectangle around the detected object.\r\n            cv2.putText(frame, labelmap[i - 1], (int(pt[0]), int(pt[1])), cv2.FONT_HERSHEY_SIMPLEX, 2, (255, 255, 255), 2, cv2.LINE_AA) # We put the label of the class right above the rectangle.\r\n            j += 1 # We increment j to get to the next occurrence.\r\n    return frame\r\n\r\n# Creating the SSD neural network\r\nnet = build_ssd('test')\r\nnet.load_state_dict(torch.load('ssd300_mAP_77.43_v2.pth', map_location = lambda storage, loc: storage ))\r\n\r\n# Creating the transformation\r\ntransform = BaseTransform(net.size, (104/256.0, 117/256.0, 123/256.0))\r\n \r\n\r\n# Doing some object detection on a video\r\nreader = imageio.get_reader('ghystreet.mp4')\r\nfps = reader.get_meta_data()['fps']\r\nwriter = imageio.get_writer('ghystreetout.mp4', fps = fps)\r\nfor i, frame in enumerate(reader): # We iterate on the frames of the output video:\r\n    frame = detect(frame, net.eval(), transform) # We call our detect function (defined above) to detect the object on the frame.\r\n    writer.append_data(frame) # We add the next frame in the output video.\r\n    print(i) # We print the number of the processed frame.\r\nwriter.close() # We close the process that handles the creation of the output video.<issue_closed>\n<issue_comment>username_1: Please prepend the following code to ignore warnings.\r\n```\r\nimport warnings\r\nwarnings.filterwarnings(\"ignore\")\r\n```\n<issue_comment>username_0: okay...but for the  2nd warning it is also affecting the gradients,its saying to rewrite the autograd function in new style. So where to change ?\n<issue_comment>username_1: Please use `Tensor` instead of `Variable`. More details could be retrieved here. https://pytorch.org/blog/pytorch-0_4_0-migration-guide/#merging-tensor-and-variable-and-classes\n<issue_comment>username_0: I did but it could not import Tensor, i watched the blog, but couldn't identify what to change.\n<issue_comment>username_1: I have been running this code for object detection but while running the code ,I am getting a warning \r\n\r\n\r\n..\\torch\\csrc\\autograd\\python_function.cpp:638: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\r\n..\\torch\\csrc\\autograd\\python_function.cpp:664: UserWarning: Legacy autograd function object was called twice.  You will probably get incorrect gradients from this computation, as the saved tensors from the second invocation will clobber the saved tensors from the first invocation.  Please consider rewriting your autograd function in the modern style; for information on the new format, please see: https://pytorch.org/docs/stable/notes/extending.html#extending-torch-autograd\r\n626\r\n\r\nafter every step.\r\n\r\n## Code\r\n\r\n#Importing of Libraries\r\nimport torch\r\nfrom torch.autograd import Variable\r\nimport cv2\r\nfrom data import BaseTransform, VOC_CLASSES as labelmap\r\nfrom ssd import build_ssd\r\nimport imageio\r\n\r\n# Defining a function that will do the detection\r\ndef detect(frame, net, transform):\r\n    height, width = frame.shape[:2]\r\n    frame_t = transform(frame)[0]\r\n    x = torch.from_numpy(frame_t).permute(2,0,1)\r\n    x = Variable(x.unsqueeze(0))\r\n    y = net(x)\r\n    detections = y.data # We create the detections tensor contained in the output y.\r\n    scale = torch.Tensor([width, height, width, height]) # We create a tensor object of dimensions [width, height, width, height].\r\n    for i in range(detections.size(1)): # For every class:\r\n        j = 0 # We initialize the loop variable j that will correspond to the occurrences of the class.\r\n        while detections[0, i, j, 0] >= 0.6: # We take into account all the occurrences j of the class i that have a matching score larger than 0.6.\r\n            pt = (detections[0, i, j, 1:] * scale).numpy() # We get the coordinates of the points at the upper left and the lower right of the detector rectangle.\r\n            cv2.rectangle(frame, (int(pt[0]), int(pt[1])), (int(pt[2]), int(pt[3])), (255, 0, 0), 2) # We draw a rectangle around the detected object.\r\n            cv2.putText(frame, labelmap[i - 1], (int(pt[0]), int(pt[1])), cv2.FONT_HERSHEY_SIMPLEX, 2, (255, 255, 255), 2, cv2.LINE_AA) # We put the label of the class right above the rectangle.\r\n            j += 1 # We increment j to get to the next occurrence.\r\n    return frame\r\n\r\n# Creating the SSD neural network\r\nnet = build_ssd('test')\r\nnet.load_state_dict(torch.load('ssd300_mAP_77.43_v2.pth', map_location = lambda storage, loc: storage ))\r\n\r\n# Creating the transformation\r\ntransform = BaseTransform(net.size, (104/256.0, 117/256.0, 123/256.0))\r\n \r\n\r\n# Doing some object detection on a video\r\nreader = imageio.get_reader('ghystreet.mp4')\r\nfps = reader.get_meta_data()['fps']\r\nwriter = imageio.get_writer('ghystreetout.mp4', fps = fps)\r\nfor i, frame in enumerate(reader): # We iterate on the frames of the output video:\r\n    frame = detect(frame, net.eval(), transform) # We call our detect function (defined above) to detect the object on the frame.\r\n    writer.append_data(frame) # We add the next frame in the output video.\r\n    print(i) # We print the number of the processed frame.\r\nwriter.close() # We close the process that handles the creation of the output video.\n<issue_comment>username_2: You have to rewrite your custom autograd functions with @staticmethod forward and backward methods, as the warning suggests.\n<issue_comment>username_0: Can you specify on which file the changes have to be made.\n<issue_comment>username_3: In your net, you use a custom autograd.Function that is an old style one.  As mentionned in the warning, you should follow the instructions in this doc on how to write a proper custom Function: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function\r\n\r\nYou have to make the change wherever you define the new `autograd.Function`.\r\n\r\nNote that this warning will become an error in 1.5 as mentionned in the message.<issue_closed>\n<issue_comment>username_4: i have the same problem. did you solve it?\r\n\r\nLegacy autograd function with non-static forward method is deprecated. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n<issue_comment>username_3: Hi,\r\n\r\nAs the error mentions you should define the Function as in the doc. You can find more information [here](https://pytorch.org/docs/stable/notes/extending.html) as well."}
{"repo": "pytorch/pytorch", "issue_id": 577683373, "issue_number": 34473, "timestamp": "2020-03-10 05:48:56+00:00", "text": "Can you specify on which file the changes have to be made.", "context": "<issue_start><issue_comment>Title: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3 and UserWarning: Legacy autograd function object was called twice.  You will probably get incorrect gradients from this computation, as the saved tensors from the second invocation will clobber the saved tensors from the first invocation.  Please consider rewriting your autograd function in the modern style; for information on the new format, please see: https://pytorch.org/docs/stable/notes/extending.html#extending-torch-autograd 626\nusername_0: I have been running this code for object detection but while running the code ,I am getting a warning \r\n\r\n\r\n..\\torch\\csrc\\autograd\\python_function.cpp:638: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\r\n..\\torch\\csrc\\autograd\\python_function.cpp:664: UserWarning: Legacy autograd function object was called twice.  You will probably get incorrect gradients from this computation, as the saved tensors from the second invocation will clobber the saved tensors from the first invocation.  Please consider rewriting your autograd function in the modern style; for information on the new format, please see: https://pytorch.org/docs/stable/notes/extending.html#extending-torch-autograd\r\n626\r\n\r\nafter every step.\r\n\r\n## Code\r\n\r\n#Importing of Libraries\r\nimport torch\r\nfrom torch.autograd import Variable\r\nimport cv2\r\nfrom data import BaseTransform, VOC_CLASSES as labelmap\r\nfrom ssd import build_ssd\r\nimport imageio\r\n\r\n# Defining a function that will do the detection\r\ndef detect(frame, net, transform):\r\n    height, width = frame.shape[:2]\r\n    frame_t = transform(frame)[0]\r\n    x = torch.from_numpy(frame_t).permute(2,0,1)\r\n    x = Variable(x.unsqueeze(0))\r\n    y = net(x)\r\n    detections = y.data # We create the detections tensor contained in the output y.\r\n    scale = torch.Tensor([width, height, width, height]) # We create a tensor object of dimensions [width, height, width, height].\r\n    for i in range(detections.size(1)): # For every class:\r\n        j = 0 # We initialize the loop variable j that will correspond to the occurrences of the class.\r\n        while detections[0, i, j, 0] >= 0.6: # We take into account all the occurrences j of the class i that have a matching score larger than 0.6.\r\n            pt = (detections[0, i, j, 1:] * scale).numpy() # We get the coordinates of the points at the upper left and the lower right of the detector rectangle.\r\n            cv2.rectangle(frame, (int(pt[0]), int(pt[1])), (int(pt[2]), int(pt[3])), (255, 0, 0), 2) # We draw a rectangle around the detected object.\r\n            cv2.putText(frame, labelmap[i - 1], (int(pt[0]), int(pt[1])), cv2.FONT_HERSHEY_SIMPLEX, 2, (255, 255, 255), 2, cv2.LINE_AA) # We put the label of the class right above the rectangle.\r\n            j += 1 # We increment j to get to the next occurrence.\r\n    return frame\r\n\r\n# Creating the SSD neural network\r\nnet = build_ssd('test')\r\nnet.load_state_dict(torch.load('ssd300_mAP_77.43_v2.pth', map_location = lambda storage, loc: storage ))\r\n\r\n# Creating the transformation\r\ntransform = BaseTransform(net.size, (104/256.0, 117/256.0, 123/256.0))\r\n \r\n\r\n# Doing some object detection on a video\r\nreader = imageio.get_reader('ghystreet.mp4')\r\nfps = reader.get_meta_data()['fps']\r\nwriter = imageio.get_writer('ghystreetout.mp4', fps = fps)\r\nfor i, frame in enumerate(reader): # We iterate on the frames of the output video:\r\n    frame = detect(frame, net.eval(), transform) # We call our detect function (defined above) to detect the object on the frame.\r\n    writer.append_data(frame) # We add the next frame in the output video.\r\n    print(i) # We print the number of the processed frame.\r\nwriter.close() # We close the process that handles the creation of the output video.<issue_closed>\n<issue_comment>username_1: Please prepend the following code to ignore warnings.\r\n```\r\nimport warnings\r\nwarnings.filterwarnings(\"ignore\")\r\n```\n<issue_comment>username_0: okay...but for the  2nd warning it is also affecting the gradients,its saying to rewrite the autograd function in new style. So where to change ?\n<issue_comment>username_1: Please use `Tensor` instead of `Variable`. More details could be retrieved here. https://pytorch.org/blog/pytorch-0_4_0-migration-guide/#merging-tensor-and-variable-and-classes\n<issue_comment>username_0: I did but it could not import Tensor, i watched the blog, but couldn't identify what to change.\n<issue_comment>username_1: I have been running this code for object detection but while running the code ,I am getting a warning \r\n\r\n\r\n..\\torch\\csrc\\autograd\\python_function.cpp:638: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\r\n..\\torch\\csrc\\autograd\\python_function.cpp:664: UserWarning: Legacy autograd function object was called twice.  You will probably get incorrect gradients from this computation, as the saved tensors from the second invocation will clobber the saved tensors from the first invocation.  Please consider rewriting your autograd function in the modern style; for information on the new format, please see: https://pytorch.org/docs/stable/notes/extending.html#extending-torch-autograd\r\n626\r\n\r\nafter every step.\r\n\r\n## Code\r\n\r\n#Importing of Libraries\r\nimport torch\r\nfrom torch.autograd import Variable\r\nimport cv2\r\nfrom data import BaseTransform, VOC_CLASSES as labelmap\r\nfrom ssd import build_ssd\r\nimport imageio\r\n\r\n# Defining a function that will do the detection\r\ndef detect(frame, net, transform):\r\n    height, width = frame.shape[:2]\r\n    frame_t = transform(frame)[0]\r\n    x = torch.from_numpy(frame_t).permute(2,0,1)\r\n    x = Variable(x.unsqueeze(0))\r\n    y = net(x)\r\n    detections = y.data # We create the detections tensor contained in the output y.\r\n    scale = torch.Tensor([width, height, width, height]) # We create a tensor object of dimensions [width, height, width, height].\r\n    for i in range(detections.size(1)): # For every class:\r\n        j = 0 # We initialize the loop variable j that will correspond to the occurrences of the class.\r\n        while detections[0, i, j, 0] >= 0.6: # We take into account all the occurrences j of the class i that have a matching score larger than 0.6.\r\n            pt = (detections[0, i, j, 1:] * scale).numpy() # We get the coordinates of the points at the upper left and the lower right of the detector rectangle.\r\n            cv2.rectangle(frame, (int(pt[0]), int(pt[1])), (int(pt[2]), int(pt[3])), (255, 0, 0), 2) # We draw a rectangle around the detected object.\r\n            cv2.putText(frame, labelmap[i - 1], (int(pt[0]), int(pt[1])), cv2.FONT_HERSHEY_SIMPLEX, 2, (255, 255, 255), 2, cv2.LINE_AA) # We put the label of the class right above the rectangle.\r\n            j += 1 # We increment j to get to the next occurrence.\r\n    return frame\r\n\r\n# Creating the SSD neural network\r\nnet = build_ssd('test')\r\nnet.load_state_dict(torch.load('ssd300_mAP_77.43_v2.pth', map_location = lambda storage, loc: storage ))\r\n\r\n# Creating the transformation\r\ntransform = BaseTransform(net.size, (104/256.0, 117/256.0, 123/256.0))\r\n \r\n\r\n# Doing some object detection on a video\r\nreader = imageio.get_reader('ghystreet.mp4')\r\nfps = reader.get_meta_data()['fps']\r\nwriter = imageio.get_writer('ghystreetout.mp4', fps = fps)\r\nfor i, frame in enumerate(reader): # We iterate on the frames of the output video:\r\n    frame = detect(frame, net.eval(), transform) # We call our detect function (defined above) to detect the object on the frame.\r\n    writer.append_data(frame) # We add the next frame in the output video.\r\n    print(i) # We print the number of the processed frame.\r\nwriter.close() # We close the process that handles the creation of the output video.\n<issue_comment>username_2: You have to rewrite your custom autograd functions with @staticmethod forward and backward methods, as the warning suggests.\n<issue_comment>username_0: Can you specify on which file the changes have to be made.\n<issue_comment>username_3: In your net, you use a custom autograd.Function that is an old style one.  As mentionned in the warning, you should follow the instructions in this doc on how to write a proper custom Function: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function\r\n\r\nYou have to make the change wherever you define the new `autograd.Function`.\r\n\r\nNote that this warning will become an error in 1.5 as mentionned in the message.<issue_closed>\n<issue_comment>username_4: i have the same problem. did you solve it?\r\n\r\nLegacy autograd function with non-static forward method is deprecated. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n<issue_comment>username_3: Hi,\r\n\r\nAs the error mentions you should define the Function as in the doc. You can find more information [here](https://pytorch.org/docs/stable/notes/extending.html) as well."}
{"repo": "pytorch/pytorch", "issue_id": 577683373, "issue_number": 34473, "timestamp": "2020-03-10 15:37:32+00:00", "text": "In your net, you use a custom autograd.Function that is an old style one.  As mentionned in the warning, you should follow the instructions in this doc on how to write a proper custom Function: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function\r\n\r\nYou have to make the change wherever you define the new `autograd.Function`.\r\n\r\nNote that this warning will become an error in 1.5 as mentionned in the message.", "context": "<issue_start><issue_comment>Title: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3 and UserWarning: Legacy autograd function object was called twice.  You will probably get incorrect gradients from this computation, as the saved tensors from the second invocation will clobber the saved tensors from the first invocation.  Please consider rewriting your autograd function in the modern style; for information on the new format, please see: https://pytorch.org/docs/stable/notes/extending.html#extending-torch-autograd 626\nusername_0: I have been running this code for object detection but while running the code ,I am getting a warning \r\n\r\n\r\n..\\torch\\csrc\\autograd\\python_function.cpp:638: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\r\n..\\torch\\csrc\\autograd\\python_function.cpp:664: UserWarning: Legacy autograd function object was called twice.  You will probably get incorrect gradients from this computation, as the saved tensors from the second invocation will clobber the saved tensors from the first invocation.  Please consider rewriting your autograd function in the modern style; for information on the new format, please see: https://pytorch.org/docs/stable/notes/extending.html#extending-torch-autograd\r\n626\r\n\r\nafter every step.\r\n\r\n## Code\r\n\r\n#Importing of Libraries\r\nimport torch\r\nfrom torch.autograd import Variable\r\nimport cv2\r\nfrom data import BaseTransform, VOC_CLASSES as labelmap\r\nfrom ssd import build_ssd\r\nimport imageio\r\n\r\n# Defining a function that will do the detection\r\ndef detect(frame, net, transform):\r\n    height, width = frame.shape[:2]\r\n    frame_t = transform(frame)[0]\r\n    x = torch.from_numpy(frame_t).permute(2,0,1)\r\n    x = Variable(x.unsqueeze(0))\r\n    y = net(x)\r\n    detections = y.data # We create the detections tensor contained in the output y.\r\n    scale = torch.Tensor([width, height, width, height]) # We create a tensor object of dimensions [width, height, width, height].\r\n    for i in range(detections.size(1)): # For every class:\r\n        j = 0 # We initialize the loop variable j that will correspond to the occurrences of the class.\r\n        while detections[0, i, j, 0] >= 0.6: # We take into account all the occurrences j of the class i that have a matching score larger than 0.6.\r\n            pt = (detections[0, i, j, 1:] * scale).numpy() # We get the coordinates of the points at the upper left and the lower right of the detector rectangle.\r\n            cv2.rectangle(frame, (int(pt[0]), int(pt[1])), (int(pt[2]), int(pt[3])), (255, 0, 0), 2) # We draw a rectangle around the detected object.\r\n            cv2.putText(frame, labelmap[i - 1], (int(pt[0]), int(pt[1])), cv2.FONT_HERSHEY_SIMPLEX, 2, (255, 255, 255), 2, cv2.LINE_AA) # We put the label of the class right above the rectangle.\r\n            j += 1 # We increment j to get to the next occurrence.\r\n    return frame\r\n\r\n# Creating the SSD neural network\r\nnet = build_ssd('test')\r\nnet.load_state_dict(torch.load('ssd300_mAP_77.43_v2.pth', map_location = lambda storage, loc: storage ))\r\n\r\n# Creating the transformation\r\ntransform = BaseTransform(net.size, (104/256.0, 117/256.0, 123/256.0))\r\n \r\n\r\n# Doing some object detection on a video\r\nreader = imageio.get_reader('ghystreet.mp4')\r\nfps = reader.get_meta_data()['fps']\r\nwriter = imageio.get_writer('ghystreetout.mp4', fps = fps)\r\nfor i, frame in enumerate(reader): # We iterate on the frames of the output video:\r\n    frame = detect(frame, net.eval(), transform) # We call our detect function (defined above) to detect the object on the frame.\r\n    writer.append_data(frame) # We add the next frame in the output video.\r\n    print(i) # We print the number of the processed frame.\r\nwriter.close() # We close the process that handles the creation of the output video.<issue_closed>\n<issue_comment>username_1: Please prepend the following code to ignore warnings.\r\n```\r\nimport warnings\r\nwarnings.filterwarnings(\"ignore\")\r\n```\n<issue_comment>username_0: okay...but for the  2nd warning it is also affecting the gradients,its saying to rewrite the autograd function in new style. So where to change ?\n<issue_comment>username_1: Please use `Tensor` instead of `Variable`. More details could be retrieved here. https://pytorch.org/blog/pytorch-0_4_0-migration-guide/#merging-tensor-and-variable-and-classes\n<issue_comment>username_0: I did but it could not import Tensor, i watched the blog, but couldn't identify what to change.\n<issue_comment>username_1: I have been running this code for object detection but while running the code ,I am getting a warning \r\n\r\n\r\n..\\torch\\csrc\\autograd\\python_function.cpp:638: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\r\n..\\torch\\csrc\\autograd\\python_function.cpp:664: UserWarning: Legacy autograd function object was called twice.  You will probably get incorrect gradients from this computation, as the saved tensors from the second invocation will clobber the saved tensors from the first invocation.  Please consider rewriting your autograd function in the modern style; for information on the new format, please see: https://pytorch.org/docs/stable/notes/extending.html#extending-torch-autograd\r\n626\r\n\r\nafter every step.\r\n\r\n## Code\r\n\r\n#Importing of Libraries\r\nimport torch\r\nfrom torch.autograd import Variable\r\nimport cv2\r\nfrom data import BaseTransform, VOC_CLASSES as labelmap\r\nfrom ssd import build_ssd\r\nimport imageio\r\n\r\n# Defining a function that will do the detection\r\ndef detect(frame, net, transform):\r\n    height, width = frame.shape[:2]\r\n    frame_t = transform(frame)[0]\r\n    x = torch.from_numpy(frame_t).permute(2,0,1)\r\n    x = Variable(x.unsqueeze(0))\r\n    y = net(x)\r\n    detections = y.data # We create the detections tensor contained in the output y.\r\n    scale = torch.Tensor([width, height, width, height]) # We create a tensor object of dimensions [width, height, width, height].\r\n    for i in range(detections.size(1)): # For every class:\r\n        j = 0 # We initialize the loop variable j that will correspond to the occurrences of the class.\r\n        while detections[0, i, j, 0] >= 0.6: # We take into account all the occurrences j of the class i that have a matching score larger than 0.6.\r\n            pt = (detections[0, i, j, 1:] * scale).numpy() # We get the coordinates of the points at the upper left and the lower right of the detector rectangle.\r\n            cv2.rectangle(frame, (int(pt[0]), int(pt[1])), (int(pt[2]), int(pt[3])), (255, 0, 0), 2) # We draw a rectangle around the detected object.\r\n            cv2.putText(frame, labelmap[i - 1], (int(pt[0]), int(pt[1])), cv2.FONT_HERSHEY_SIMPLEX, 2, (255, 255, 255), 2, cv2.LINE_AA) # We put the label of the class right above the rectangle.\r\n            j += 1 # We increment j to get to the next occurrence.\r\n    return frame\r\n\r\n# Creating the SSD neural network\r\nnet = build_ssd('test')\r\nnet.load_state_dict(torch.load('ssd300_mAP_77.43_v2.pth', map_location = lambda storage, loc: storage ))\r\n\r\n# Creating the transformation\r\ntransform = BaseTransform(net.size, (104/256.0, 117/256.0, 123/256.0))\r\n \r\n\r\n# Doing some object detection on a video\r\nreader = imageio.get_reader('ghystreet.mp4')\r\nfps = reader.get_meta_data()['fps']\r\nwriter = imageio.get_writer('ghystreetout.mp4', fps = fps)\r\nfor i, frame in enumerate(reader): # We iterate on the frames of the output video:\r\n    frame = detect(frame, net.eval(), transform) # We call our detect function (defined above) to detect the object on the frame.\r\n    writer.append_data(frame) # We add the next frame in the output video.\r\n    print(i) # We print the number of the processed frame.\r\nwriter.close() # We close the process that handles the creation of the output video.\n<issue_comment>username_2: You have to rewrite your custom autograd functions with @staticmethod forward and backward methods, as the warning suggests.\n<issue_comment>username_0: Can you specify on which file the changes have to be made.\n<issue_comment>username_3: In your net, you use a custom autograd.Function that is an old style one.  As mentionned in the warning, you should follow the instructions in this doc on how to write a proper custom Function: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function\r\n\r\nYou have to make the change wherever you define the new `autograd.Function`.\r\n\r\nNote that this warning will become an error in 1.5 as mentionned in the message.<issue_closed>\n<issue_comment>username_4: i have the same problem. did you solve it?\r\n\r\nLegacy autograd function with non-static forward method is deprecated. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n<issue_comment>username_3: Hi,\r\n\r\nAs the error mentions you should define the Function as in the doc. You can find more information [here](https://pytorch.org/docs/stable/notes/extending.html) as well."}
{"repo": "pytorch/pytorch", "issue_id": 577683373, "issue_number": 34473, "timestamp": "2020-03-10 15:37:33+00:00", "text": "", "context": "<issue_start><issue_comment>Title: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3 and UserWarning: Legacy autograd function object was called twice.  You will probably get incorrect gradients from this computation, as the saved tensors from the second invocation will clobber the saved tensors from the first invocation.  Please consider rewriting your autograd function in the modern style; for information on the new format, please see: https://pytorch.org/docs/stable/notes/extending.html#extending-torch-autograd 626\nusername_0: I have been running this code for object detection but while running the code ,I am getting a warning \r\n\r\n\r\n..\\torch\\csrc\\autograd\\python_function.cpp:638: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\r\n..\\torch\\csrc\\autograd\\python_function.cpp:664: UserWarning: Legacy autograd function object was called twice.  You will probably get incorrect gradients from this computation, as the saved tensors from the second invocation will clobber the saved tensors from the first invocation.  Please consider rewriting your autograd function in the modern style; for information on the new format, please see: https://pytorch.org/docs/stable/notes/extending.html#extending-torch-autograd\r\n626\r\n\r\nafter every step.\r\n\r\n## Code\r\n\r\n#Importing of Libraries\r\nimport torch\r\nfrom torch.autograd import Variable\r\nimport cv2\r\nfrom data import BaseTransform, VOC_CLASSES as labelmap\r\nfrom ssd import build_ssd\r\nimport imageio\r\n\r\n# Defining a function that will do the detection\r\ndef detect(frame, net, transform):\r\n    height, width = frame.shape[:2]\r\n    frame_t = transform(frame)[0]\r\n    x = torch.from_numpy(frame_t).permute(2,0,1)\r\n    x = Variable(x.unsqueeze(0))\r\n    y = net(x)\r\n    detections = y.data # We create the detections tensor contained in the output y.\r\n    scale = torch.Tensor([width, height, width, height]) # We create a tensor object of dimensions [width, height, width, height].\r\n    for i in range(detections.size(1)): # For every class:\r\n        j = 0 # We initialize the loop variable j that will correspond to the occurrences of the class.\r\n        while detections[0, i, j, 0] >= 0.6: # We take into account all the occurrences j of the class i that have a matching score larger than 0.6.\r\n            pt = (detections[0, i, j, 1:] * scale).numpy() # We get the coordinates of the points at the upper left and the lower right of the detector rectangle.\r\n            cv2.rectangle(frame, (int(pt[0]), int(pt[1])), (int(pt[2]), int(pt[3])), (255, 0, 0), 2) # We draw a rectangle around the detected object.\r\n            cv2.putText(frame, labelmap[i - 1], (int(pt[0]), int(pt[1])), cv2.FONT_HERSHEY_SIMPLEX, 2, (255, 255, 255), 2, cv2.LINE_AA) # We put the label of the class right above the rectangle.\r\n            j += 1 # We increment j to get to the next occurrence.\r\n    return frame\r\n\r\n# Creating the SSD neural network\r\nnet = build_ssd('test')\r\nnet.load_state_dict(torch.load('ssd300_mAP_77.43_v2.pth', map_location = lambda storage, loc: storage ))\r\n\r\n# Creating the transformation\r\ntransform = BaseTransform(net.size, (104/256.0, 117/256.0, 123/256.0))\r\n \r\n\r\n# Doing some object detection on a video\r\nreader = imageio.get_reader('ghystreet.mp4')\r\nfps = reader.get_meta_data()['fps']\r\nwriter = imageio.get_writer('ghystreetout.mp4', fps = fps)\r\nfor i, frame in enumerate(reader): # We iterate on the frames of the output video:\r\n    frame = detect(frame, net.eval(), transform) # We call our detect function (defined above) to detect the object on the frame.\r\n    writer.append_data(frame) # We add the next frame in the output video.\r\n    print(i) # We print the number of the processed frame.\r\nwriter.close() # We close the process that handles the creation of the output video.<issue_closed>\n<issue_comment>username_1: Please prepend the following code to ignore warnings.\r\n```\r\nimport warnings\r\nwarnings.filterwarnings(\"ignore\")\r\n```\n<issue_comment>username_0: okay...but for the  2nd warning it is also affecting the gradients,its saying to rewrite the autograd function in new style. So where to change ?\n<issue_comment>username_1: Please use `Tensor` instead of `Variable`. More details could be retrieved here. https://pytorch.org/blog/pytorch-0_4_0-migration-guide/#merging-tensor-and-variable-and-classes\n<issue_comment>username_0: I did but it could not import Tensor, i watched the blog, but couldn't identify what to change.\n<issue_comment>username_1: I have been running this code for object detection but while running the code ,I am getting a warning \r\n\r\n\r\n..\\torch\\csrc\\autograd\\python_function.cpp:638: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\r\n..\\torch\\csrc\\autograd\\python_function.cpp:664: UserWarning: Legacy autograd function object was called twice.  You will probably get incorrect gradients from this computation, as the saved tensors from the second invocation will clobber the saved tensors from the first invocation.  Please consider rewriting your autograd function in the modern style; for information on the new format, please see: https://pytorch.org/docs/stable/notes/extending.html#extending-torch-autograd\r\n626\r\n\r\nafter every step.\r\n\r\n## Code\r\n\r\n#Importing of Libraries\r\nimport torch\r\nfrom torch.autograd import Variable\r\nimport cv2\r\nfrom data import BaseTransform, VOC_CLASSES as labelmap\r\nfrom ssd import build_ssd\r\nimport imageio\r\n\r\n# Defining a function that will do the detection\r\ndef detect(frame, net, transform):\r\n    height, width = frame.shape[:2]\r\n    frame_t = transform(frame)[0]\r\n    x = torch.from_numpy(frame_t).permute(2,0,1)\r\n    x = Variable(x.unsqueeze(0))\r\n    y = net(x)\r\n    detections = y.data # We create the detections tensor contained in the output y.\r\n    scale = torch.Tensor([width, height, width, height]) # We create a tensor object of dimensions [width, height, width, height].\r\n    for i in range(detections.size(1)): # For every class:\r\n        j = 0 # We initialize the loop variable j that will correspond to the occurrences of the class.\r\n        while detections[0, i, j, 0] >= 0.6: # We take into account all the occurrences j of the class i that have a matching score larger than 0.6.\r\n            pt = (detections[0, i, j, 1:] * scale).numpy() # We get the coordinates of the points at the upper left and the lower right of the detector rectangle.\r\n            cv2.rectangle(frame, (int(pt[0]), int(pt[1])), (int(pt[2]), int(pt[3])), (255, 0, 0), 2) # We draw a rectangle around the detected object.\r\n            cv2.putText(frame, labelmap[i - 1], (int(pt[0]), int(pt[1])), cv2.FONT_HERSHEY_SIMPLEX, 2, (255, 255, 255), 2, cv2.LINE_AA) # We put the label of the class right above the rectangle.\r\n            j += 1 # We increment j to get to the next occurrence.\r\n    return frame\r\n\r\n# Creating the SSD neural network\r\nnet = build_ssd('test')\r\nnet.load_state_dict(torch.load('ssd300_mAP_77.43_v2.pth', map_location = lambda storage, loc: storage ))\r\n\r\n# Creating the transformation\r\ntransform = BaseTransform(net.size, (104/256.0, 117/256.0, 123/256.0))\r\n \r\n\r\n# Doing some object detection on a video\r\nreader = imageio.get_reader('ghystreet.mp4')\r\nfps = reader.get_meta_data()['fps']\r\nwriter = imageio.get_writer('ghystreetout.mp4', fps = fps)\r\nfor i, frame in enumerate(reader): # We iterate on the frames of the output video:\r\n    frame = detect(frame, net.eval(), transform) # We call our detect function (defined above) to detect the object on the frame.\r\n    writer.append_data(frame) # We add the next frame in the output video.\r\n    print(i) # We print the number of the processed frame.\r\nwriter.close() # We close the process that handles the creation of the output video.\n<issue_comment>username_2: You have to rewrite your custom autograd functions with @staticmethod forward and backward methods, as the warning suggests.\n<issue_comment>username_0: Can you specify on which file the changes have to be made.\n<issue_comment>username_3: In your net, you use a custom autograd.Function that is an old style one.  As mentionned in the warning, you should follow the instructions in this doc on how to write a proper custom Function: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function\r\n\r\nYou have to make the change wherever you define the new `autograd.Function`.\r\n\r\nNote that this warning will become an error in 1.5 as mentionned in the message.<issue_closed>\n<issue_comment>username_4: i have the same problem. did you solve it?\r\n\r\nLegacy autograd function with non-static forward method is deprecated. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n<issue_comment>username_3: Hi,\r\n\r\nAs the error mentions you should define the Function as in the doc. You can find more information [here](https://pytorch.org/docs/stable/notes/extending.html) as well."}
{"repo": "pytorch/pytorch", "issue_id": 577683373, "issue_number": 34473, "timestamp": "2020-05-01 19:20:57+00:00", "text": "i have the same problem. did you solve it?\r\n\r\nLegacy autograd function with non-static forward method is deprecated. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)", "context": "<issue_start><issue_comment>Title: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3 and UserWarning: Legacy autograd function object was called twice.  You will probably get incorrect gradients from this computation, as the saved tensors from the second invocation will clobber the saved tensors from the first invocation.  Please consider rewriting your autograd function in the modern style; for information on the new format, please see: https://pytorch.org/docs/stable/notes/extending.html#extending-torch-autograd 626\nusername_0: I have been running this code for object detection but while running the code ,I am getting a warning \r\n\r\n\r\n..\\torch\\csrc\\autograd\\python_function.cpp:638: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\r\n..\\torch\\csrc\\autograd\\python_function.cpp:664: UserWarning: Legacy autograd function object was called twice.  You will probably get incorrect gradients from this computation, as the saved tensors from the second invocation will clobber the saved tensors from the first invocation.  Please consider rewriting your autograd function in the modern style; for information on the new format, please see: https://pytorch.org/docs/stable/notes/extending.html#extending-torch-autograd\r\n626\r\n\r\nafter every step.\r\n\r\n## Code\r\n\r\n#Importing of Libraries\r\nimport torch\r\nfrom torch.autograd import Variable\r\nimport cv2\r\nfrom data import BaseTransform, VOC_CLASSES as labelmap\r\nfrom ssd import build_ssd\r\nimport imageio\r\n\r\n# Defining a function that will do the detection\r\ndef detect(frame, net, transform):\r\n    height, width = frame.shape[:2]\r\n    frame_t = transform(frame)[0]\r\n    x = torch.from_numpy(frame_t).permute(2,0,1)\r\n    x = Variable(x.unsqueeze(0))\r\n    y = net(x)\r\n    detections = y.data # We create the detections tensor contained in the output y.\r\n    scale = torch.Tensor([width, height, width, height]) # We create a tensor object of dimensions [width, height, width, height].\r\n    for i in range(detections.size(1)): # For every class:\r\n        j = 0 # We initialize the loop variable j that will correspond to the occurrences of the class.\r\n        while detections[0, i, j, 0] >= 0.6: # We take into account all the occurrences j of the class i that have a matching score larger than 0.6.\r\n            pt = (detections[0, i, j, 1:] * scale).numpy() # We get the coordinates of the points at the upper left and the lower right of the detector rectangle.\r\n            cv2.rectangle(frame, (int(pt[0]), int(pt[1])), (int(pt[2]), int(pt[3])), (255, 0, 0), 2) # We draw a rectangle around the detected object.\r\n            cv2.putText(frame, labelmap[i - 1], (int(pt[0]), int(pt[1])), cv2.FONT_HERSHEY_SIMPLEX, 2, (255, 255, 255), 2, cv2.LINE_AA) # We put the label of the class right above the rectangle.\r\n            j += 1 # We increment j to get to the next occurrence.\r\n    return frame\r\n\r\n# Creating the SSD neural network\r\nnet = build_ssd('test')\r\nnet.load_state_dict(torch.load('ssd300_mAP_77.43_v2.pth', map_location = lambda storage, loc: storage ))\r\n\r\n# Creating the transformation\r\ntransform = BaseTransform(net.size, (104/256.0, 117/256.0, 123/256.0))\r\n \r\n\r\n# Doing some object detection on a video\r\nreader = imageio.get_reader('ghystreet.mp4')\r\nfps = reader.get_meta_data()['fps']\r\nwriter = imageio.get_writer('ghystreetout.mp4', fps = fps)\r\nfor i, frame in enumerate(reader): # We iterate on the frames of the output video:\r\n    frame = detect(frame, net.eval(), transform) # We call our detect function (defined above) to detect the object on the frame.\r\n    writer.append_data(frame) # We add the next frame in the output video.\r\n    print(i) # We print the number of the processed frame.\r\nwriter.close() # We close the process that handles the creation of the output video.<issue_closed>\n<issue_comment>username_1: Please prepend the following code to ignore warnings.\r\n```\r\nimport warnings\r\nwarnings.filterwarnings(\"ignore\")\r\n```\n<issue_comment>username_0: okay...but for the  2nd warning it is also affecting the gradients,its saying to rewrite the autograd function in new style. So where to change ?\n<issue_comment>username_1: Please use `Tensor` instead of `Variable`. More details could be retrieved here. https://pytorch.org/blog/pytorch-0_4_0-migration-guide/#merging-tensor-and-variable-and-classes\n<issue_comment>username_0: I did but it could not import Tensor, i watched the blog, but couldn't identify what to change.\n<issue_comment>username_1: I have been running this code for object detection but while running the code ,I am getting a warning \r\n\r\n\r\n..\\torch\\csrc\\autograd\\python_function.cpp:638: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\r\n..\\torch\\csrc\\autograd\\python_function.cpp:664: UserWarning: Legacy autograd function object was called twice.  You will probably get incorrect gradients from this computation, as the saved tensors from the second invocation will clobber the saved tensors from the first invocation.  Please consider rewriting your autograd function in the modern style; for information on the new format, please see: https://pytorch.org/docs/stable/notes/extending.html#extending-torch-autograd\r\n626\r\n\r\nafter every step.\r\n\r\n## Code\r\n\r\n#Importing of Libraries\r\nimport torch\r\nfrom torch.autograd import Variable\r\nimport cv2\r\nfrom data import BaseTransform, VOC_CLASSES as labelmap\r\nfrom ssd import build_ssd\r\nimport imageio\r\n\r\n# Defining a function that will do the detection\r\ndef detect(frame, net, transform):\r\n    height, width = frame.shape[:2]\r\n    frame_t = transform(frame)[0]\r\n    x = torch.from_numpy(frame_t).permute(2,0,1)\r\n    x = Variable(x.unsqueeze(0))\r\n    y = net(x)\r\n    detections = y.data # We create the detections tensor contained in the output y.\r\n    scale = torch.Tensor([width, height, width, height]) # We create a tensor object of dimensions [width, height, width, height].\r\n    for i in range(detections.size(1)): # For every class:\r\n        j = 0 # We initialize the loop variable j that will correspond to the occurrences of the class.\r\n        while detections[0, i, j, 0] >= 0.6: # We take into account all the occurrences j of the class i that have a matching score larger than 0.6.\r\n            pt = (detections[0, i, j, 1:] * scale).numpy() # We get the coordinates of the points at the upper left and the lower right of the detector rectangle.\r\n            cv2.rectangle(frame, (int(pt[0]), int(pt[1])), (int(pt[2]), int(pt[3])), (255, 0, 0), 2) # We draw a rectangle around the detected object.\r\n            cv2.putText(frame, labelmap[i - 1], (int(pt[0]), int(pt[1])), cv2.FONT_HERSHEY_SIMPLEX, 2, (255, 255, 255), 2, cv2.LINE_AA) # We put the label of the class right above the rectangle.\r\n            j += 1 # We increment j to get to the next occurrence.\r\n    return frame\r\n\r\n# Creating the SSD neural network\r\nnet = build_ssd('test')\r\nnet.load_state_dict(torch.load('ssd300_mAP_77.43_v2.pth', map_location = lambda storage, loc: storage ))\r\n\r\n# Creating the transformation\r\ntransform = BaseTransform(net.size, (104/256.0, 117/256.0, 123/256.0))\r\n \r\n\r\n# Doing some object detection on a video\r\nreader = imageio.get_reader('ghystreet.mp4')\r\nfps = reader.get_meta_data()['fps']\r\nwriter = imageio.get_writer('ghystreetout.mp4', fps = fps)\r\nfor i, frame in enumerate(reader): # We iterate on the frames of the output video:\r\n    frame = detect(frame, net.eval(), transform) # We call our detect function (defined above) to detect the object on the frame.\r\n    writer.append_data(frame) # We add the next frame in the output video.\r\n    print(i) # We print the number of the processed frame.\r\nwriter.close() # We close the process that handles the creation of the output video.\n<issue_comment>username_2: You have to rewrite your custom autograd functions with @staticmethod forward and backward methods, as the warning suggests.\n<issue_comment>username_0: Can you specify on which file the changes have to be made.\n<issue_comment>username_3: In your net, you use a custom autograd.Function that is an old style one.  As mentionned in the warning, you should follow the instructions in this doc on how to write a proper custom Function: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function\r\n\r\nYou have to make the change wherever you define the new `autograd.Function`.\r\n\r\nNote that this warning will become an error in 1.5 as mentionned in the message.<issue_closed>\n<issue_comment>username_4: i have the same problem. did you solve it?\r\n\r\nLegacy autograd function with non-static forward method is deprecated. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n<issue_comment>username_3: Hi,\r\n\r\nAs the error mentions you should define the Function as in the doc. You can find more information [here](https://pytorch.org/docs/stable/notes/extending.html) as well."}
{"repo": "pytorch/pytorch", "issue_id": 577683373, "issue_number": 34473, "timestamp": "2020-05-01 19:39:52+00:00", "text": "Hi,\r\n\r\nAs the error mentions you should define the Function as in the doc. You can find more information [here](https://pytorch.org/docs/stable/notes/extending.html) as well.", "context": "<issue_start><issue_comment>Title: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3 and UserWarning: Legacy autograd function object was called twice.  You will probably get incorrect gradients from this computation, as the saved tensors from the second invocation will clobber the saved tensors from the first invocation.  Please consider rewriting your autograd function in the modern style; for information on the new format, please see: https://pytorch.org/docs/stable/notes/extending.html#extending-torch-autograd 626\nusername_0: I have been running this code for object detection but while running the code ,I am getting a warning \r\n\r\n\r\n..\\torch\\csrc\\autograd\\python_function.cpp:638: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\r\n..\\torch\\csrc\\autograd\\python_function.cpp:664: UserWarning: Legacy autograd function object was called twice.  You will probably get incorrect gradients from this computation, as the saved tensors from the second invocation will clobber the saved tensors from the first invocation.  Please consider rewriting your autograd function in the modern style; for information on the new format, please see: https://pytorch.org/docs/stable/notes/extending.html#extending-torch-autograd\r\n626\r\n\r\nafter every step.\r\n\r\n## Code\r\n\r\n#Importing of Libraries\r\nimport torch\r\nfrom torch.autograd import Variable\r\nimport cv2\r\nfrom data import BaseTransform, VOC_CLASSES as labelmap\r\nfrom ssd import build_ssd\r\nimport imageio\r\n\r\n# Defining a function that will do the detection\r\ndef detect(frame, net, transform):\r\n    height, width = frame.shape[:2]\r\n    frame_t = transform(frame)[0]\r\n    x = torch.from_numpy(frame_t).permute(2,0,1)\r\n    x = Variable(x.unsqueeze(0))\r\n    y = net(x)\r\n    detections = y.data # We create the detections tensor contained in the output y.\r\n    scale = torch.Tensor([width, height, width, height]) # We create a tensor object of dimensions [width, height, width, height].\r\n    for i in range(detections.size(1)): # For every class:\r\n        j = 0 # We initialize the loop variable j that will correspond to the occurrences of the class.\r\n        while detections[0, i, j, 0] >= 0.6: # We take into account all the occurrences j of the class i that have a matching score larger than 0.6.\r\n            pt = (detections[0, i, j, 1:] * scale).numpy() # We get the coordinates of the points at the upper left and the lower right of the detector rectangle.\r\n            cv2.rectangle(frame, (int(pt[0]), int(pt[1])), (int(pt[2]), int(pt[3])), (255, 0, 0), 2) # We draw a rectangle around the detected object.\r\n            cv2.putText(frame, labelmap[i - 1], (int(pt[0]), int(pt[1])), cv2.FONT_HERSHEY_SIMPLEX, 2, (255, 255, 255), 2, cv2.LINE_AA) # We put the label of the class right above the rectangle.\r\n            j += 1 # We increment j to get to the next occurrence.\r\n    return frame\r\n\r\n# Creating the SSD neural network\r\nnet = build_ssd('test')\r\nnet.load_state_dict(torch.load('ssd300_mAP_77.43_v2.pth', map_location = lambda storage, loc: storage ))\r\n\r\n# Creating the transformation\r\ntransform = BaseTransform(net.size, (104/256.0, 117/256.0, 123/256.0))\r\n \r\n\r\n# Doing some object detection on a video\r\nreader = imageio.get_reader('ghystreet.mp4')\r\nfps = reader.get_meta_data()['fps']\r\nwriter = imageio.get_writer('ghystreetout.mp4', fps = fps)\r\nfor i, frame in enumerate(reader): # We iterate on the frames of the output video:\r\n    frame = detect(frame, net.eval(), transform) # We call our detect function (defined above) to detect the object on the frame.\r\n    writer.append_data(frame) # We add the next frame in the output video.\r\n    print(i) # We print the number of the processed frame.\r\nwriter.close() # We close the process that handles the creation of the output video.<issue_closed>\n<issue_comment>username_1: Please prepend the following code to ignore warnings.\r\n```\r\nimport warnings\r\nwarnings.filterwarnings(\"ignore\")\r\n```\n<issue_comment>username_0: okay...but for the  2nd warning it is also affecting the gradients,its saying to rewrite the autograd function in new style. So where to change ?\n<issue_comment>username_1: Please use `Tensor` instead of `Variable`. More details could be retrieved here. https://pytorch.org/blog/pytorch-0_4_0-migration-guide/#merging-tensor-and-variable-and-classes\n<issue_comment>username_0: I did but it could not import Tensor, i watched the blog, but couldn't identify what to change.\n<issue_comment>username_1: I have been running this code for object detection but while running the code ,I am getting a warning \r\n\r\n\r\n..\\torch\\csrc\\autograd\\python_function.cpp:638: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\r\n..\\torch\\csrc\\autograd\\python_function.cpp:664: UserWarning: Legacy autograd function object was called twice.  You will probably get incorrect gradients from this computation, as the saved tensors from the second invocation will clobber the saved tensors from the first invocation.  Please consider rewriting your autograd function in the modern style; for information on the new format, please see: https://pytorch.org/docs/stable/notes/extending.html#extending-torch-autograd\r\n626\r\n\r\nafter every step.\r\n\r\n## Code\r\n\r\n#Importing of Libraries\r\nimport torch\r\nfrom torch.autograd import Variable\r\nimport cv2\r\nfrom data import BaseTransform, VOC_CLASSES as labelmap\r\nfrom ssd import build_ssd\r\nimport imageio\r\n\r\n# Defining a function that will do the detection\r\ndef detect(frame, net, transform):\r\n    height, width = frame.shape[:2]\r\n    frame_t = transform(frame)[0]\r\n    x = torch.from_numpy(frame_t).permute(2,0,1)\r\n    x = Variable(x.unsqueeze(0))\r\n    y = net(x)\r\n    detections = y.data # We create the detections tensor contained in the output y.\r\n    scale = torch.Tensor([width, height, width, height]) # We create a tensor object of dimensions [width, height, width, height].\r\n    for i in range(detections.size(1)): # For every class:\r\n        j = 0 # We initialize the loop variable j that will correspond to the occurrences of the class.\r\n        while detections[0, i, j, 0] >= 0.6: # We take into account all the occurrences j of the class i that have a matching score larger than 0.6.\r\n            pt = (detections[0, i, j, 1:] * scale).numpy() # We get the coordinates of the points at the upper left and the lower right of the detector rectangle.\r\n            cv2.rectangle(frame, (int(pt[0]), int(pt[1])), (int(pt[2]), int(pt[3])), (255, 0, 0), 2) # We draw a rectangle around the detected object.\r\n            cv2.putText(frame, labelmap[i - 1], (int(pt[0]), int(pt[1])), cv2.FONT_HERSHEY_SIMPLEX, 2, (255, 255, 255), 2, cv2.LINE_AA) # We put the label of the class right above the rectangle.\r\n            j += 1 # We increment j to get to the next occurrence.\r\n    return frame\r\n\r\n# Creating the SSD neural network\r\nnet = build_ssd('test')\r\nnet.load_state_dict(torch.load('ssd300_mAP_77.43_v2.pth', map_location = lambda storage, loc: storage ))\r\n\r\n# Creating the transformation\r\ntransform = BaseTransform(net.size, (104/256.0, 117/256.0, 123/256.0))\r\n \r\n\r\n# Doing some object detection on a video\r\nreader = imageio.get_reader('ghystreet.mp4')\r\nfps = reader.get_meta_data()['fps']\r\nwriter = imageio.get_writer('ghystreetout.mp4', fps = fps)\r\nfor i, frame in enumerate(reader): # We iterate on the frames of the output video:\r\n    frame = detect(frame, net.eval(), transform) # We call our detect function (defined above) to detect the object on the frame.\r\n    writer.append_data(frame) # We add the next frame in the output video.\r\n    print(i) # We print the number of the processed frame.\r\nwriter.close() # We close the process that handles the creation of the output video.\n<issue_comment>username_2: You have to rewrite your custom autograd functions with @staticmethod forward and backward methods, as the warning suggests.\n<issue_comment>username_0: Can you specify on which file the changes have to be made.\n<issue_comment>username_3: In your net, you use a custom autograd.Function that is an old style one.  As mentionned in the warning, you should follow the instructions in this doc on how to write a proper custom Function: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function\r\n\r\nYou have to make the change wherever you define the new `autograd.Function`.\r\n\r\nNote that this warning will become an error in 1.5 as mentionned in the message.<issue_closed>\n<issue_comment>username_4: i have the same problem. did you solve it?\r\n\r\nLegacy autograd function with non-static forward method is deprecated. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n<issue_comment>username_3: Hi,\r\n\r\nAs the error mentions you should define the Function as in the doc. You can find more information [here](https://pytorch.org/docs/stable/notes/extending.html) as well."}
{"repo": "pytorch/pytorch", "issue_id": 601203476, "issue_number": 36724, "timestamp": "2020-04-16T16:02:15Z", "text": "Benchmark with same build settings on same system.\r\ngcc : version 7.5.0 (Ubuntu 7.5.0-3ubuntu1~18.04)\r\nCUDA : 10.1\r\nGPU : 1050ti\r\n\r\n```python\r\nimport timeit\r\n\r\nfor n, t in [(10_000, 20000),\r\n             (100_000, 20000)]:\r\n    for dtype in ('torch.half', 'torch.float', 'torch.double'):\r\n        print(f'torch.erf(a) a.numel() == {n} for {t} times {dtype}')\r\n        print(timeit.timeit(f'torch.erf(a); torch.cuda.synchronize()',\r\n                              setup=f'import torch; a=torch.arange({n}, dtype={dtype}, device=\"cuda\")',\r\n                              number=t))\r\n```\r\n\r\nBefore:\r\n\r\n```\r\ntorch.erf(a) a.numel() == 10000 for 20000 times torch.half\r\n0.29057903600187274\r\ntorch.erf(a) a.numel() == 10000 for 20000 times torch.float\r\n0.2836507789979805\r\ntorch.erf(a) a.numel() == 10000 for 20000 times torch.double\r\n0.44974555500084534\r\ntorch.erf(a) a.numel() == 100000 for 20000 times torch.half\r\n0.31807255600142526\r\ntorch.erf(a) a.numel() == 100000 for 20000 times torch.float\r\n0.3216503109979385\r\ntorch.erf(a) a.numel() == 100000 for 20000 times torch.double\r\n2.0413486910001666\r\n```\r\n\r\nAfter:\r\n\r\n```\r\ntorch.erf(a) a.numel() == 10000 for 20000 times torch.half\r\n0.2867302739996376\r\ntorch.erf(a) a.numel() == 10000 for 20000 times torch.float\r\n0.28851128199858067\r\ntorch.erf(a) a.numel() == 10000 for 20000 times torch.double\r\n0.4592030350013374\r\ntorch.erf(a) a.numel() == 100000 for 20000 times torch.half\r\n0.28704102400115517\r\ntorch.erf(a) a.numel() == 100000 for 20000 times torch.float\r\n0.29036039400125446\r\ntorch.erf(a) a.numel() == 100000 for 20000 times torch.double\r\n2.04035638699861\r\n```", "context": "<issue_start><issue_comment>Title: Migrate `erf` and `erf_` from the TH to Aten (CUDA) : Closes #24558\nusername_0: Benchmark with same build settings on same system.\r\ngcc : version 7.5.0 (Ubuntu 7.5.0-3ubuntu1~18.04)\r\nCUDA : 10.1\r\nGPU : 1050ti\r\n\r\n```python\r\nimport timeit\r\n\r\nfor n, t in [(10_000, 20000),\r\n             (100_000, 20000)]:\r\n    for dtype in ('torch.half', 'torch.float', 'torch.double'):\r\n        print(f'torch.erf(a) a.numel() == {n} for {t} times {dtype}')\r\n        print(timeit.timeit(f'torch.erf(a); torch.cuda.synchronize()',\r\n                              setup=f'import torch; a=torch.arange({n}, dtype={dtype}, device=\"cuda\")',\r\n                              number=t))\r\n```\r\n\r\nBefore:\r\n\r\n```\r\ntorch.erf(a) a.numel() == 10000 for 20000 times torch.half\r\n0.29057903600187274\r\ntorch.erf(a) a.numel() == 10000 for 20000 times torch.float\r\n0.2836507789979805\r\ntorch.erf(a) a.numel() == 10000 for 20000 times torch.double\r\n0.44974555500084534\r\ntorch.erf(a) a.numel() == 100000 for 20000 times torch.half\r\n0.31807255600142526\r\ntorch.erf(a) a.numel() == 100000 for 20000 times torch.float\r\n0.3216503109979385\r\ntorch.erf(a) a.numel() == 100000 for 20000 times torch.double\r\n2.0413486910001666\r\n```\r\n\r\nAfter:\r\n\r\n```\r\ntorch.erf(a) a.numel() == 10000 for 20000 times torch.half\r\n0.2867302739996376\r\ntorch.erf(a) a.numel() == 10000 for 20000 times torch.float\r\n0.28851128199858067\r\ntorch.erf(a) a.numel() == 10000 for 20000 times torch.double\r\n0.4592030350013374\r\ntorch.erf(a) a.numel() == 100000 for 20000 times torch.half\r\n0.28704102400115517\r\ntorch.erf(a) a.numel() == 100000 for 20000 times torch.float\r\n0.29036039400125446\r\ntorch.erf(a) a.numel() == 100000 for 20000 times torch.double\r\n2.04035638699861\r\n```\n<issue_comment>username_1: @username_0 Would you mind rebasing on top of pytorch master? \r\nI'm concerned this PR might indeed break xla (although the current failure is not related.) Thanks!\n<issue_comment>username_0: @username_1 Sure I will.\n<issue_comment>username_0: @pytorchbot rebase this please\n<issue_comment>username_2: Please rebase\n<issue_comment>username_0: @username_2 Have rebased this. The failing test looks irrelevant to the PR. Should I rebase again for the failing test?\n<issue_comment>username_0: @username_2 Have rebased and all the tests have ran. Can you please take a look again. Thanks :)\n<issue_comment>username_0: @username_2 Gentle ping for reminder.\n<issue_comment>username_2: Sorry, that is one of the cases when we modify high contention file and it is getting changed faster than we merge issues. Please rebase again and I will land it first in line.\n<issue_comment>username_0: @username_2 Sure will do that once, #36906 is in, as there would probably be more conflicts post that.\n<issue_comment>username_0: @username_2 PTAL:)\n<issue_comment>username_0: @username_2 Gentle ping for reminder."}
{"repo": "pytorch/pytorch", "issue_id": 601203476, "issue_number": 36724, "timestamp": "2020-04-16 19:01:35+00:00", "text": "@username_0 Would you mind rebasing on top of pytorch master? \r\nI'm concerned this PR might indeed break xla (although the current failure is not related.) Thanks!", "context": "<issue_start><issue_comment>Title: Migrate `erf` and `erf_` from the TH to Aten (CUDA) : Closes #24558\nusername_0: Benchmark with same build settings on same system.\r\ngcc : version 7.5.0 (Ubuntu 7.5.0-3ubuntu1~18.04)\r\nCUDA : 10.1\r\nGPU : 1050ti\r\n\r\n```python\r\nimport timeit\r\n\r\nfor n, t in [(10_000, 20000),\r\n             (100_000, 20000)]:\r\n    for dtype in ('torch.half', 'torch.float', 'torch.double'):\r\n        print(f'torch.erf(a) a.numel() == {n} for {t} times {dtype}')\r\n        print(timeit.timeit(f'torch.erf(a); torch.cuda.synchronize()',\r\n                              setup=f'import torch; a=torch.arange({n}, dtype={dtype}, device=\"cuda\")',\r\n                              number=t))\r\n```\r\n\r\nBefore:\r\n\r\n```\r\ntorch.erf(a) a.numel() == 10000 for 20000 times torch.half\r\n0.29057903600187274\r\ntorch.erf(a) a.numel() == 10000 for 20000 times torch.float\r\n0.2836507789979805\r\ntorch.erf(a) a.numel() == 10000 for 20000 times torch.double\r\n0.44974555500084534\r\ntorch.erf(a) a.numel() == 100000 for 20000 times torch.half\r\n0.31807255600142526\r\ntorch.erf(a) a.numel() == 100000 for 20000 times torch.float\r\n0.3216503109979385\r\ntorch.erf(a) a.numel() == 100000 for 20000 times torch.double\r\n2.0413486910001666\r\n```\r\n\r\nAfter:\r\n\r\n```\r\ntorch.erf(a) a.numel() == 10000 for 20000 times torch.half\r\n0.2867302739996376\r\ntorch.erf(a) a.numel() == 10000 for 20000 times torch.float\r\n0.28851128199858067\r\ntorch.erf(a) a.numel() == 10000 for 20000 times torch.double\r\n0.4592030350013374\r\ntorch.erf(a) a.numel() == 100000 for 20000 times torch.half\r\n0.28704102400115517\r\ntorch.erf(a) a.numel() == 100000 for 20000 times torch.float\r\n0.29036039400125446\r\ntorch.erf(a) a.numel() == 100000 for 20000 times torch.double\r\n2.04035638699861\r\n```\n<issue_comment>username_1: @username_0 Would you mind rebasing on top of pytorch master? \r\nI'm concerned this PR might indeed break xla (although the current failure is not related.) Thanks!\n<issue_comment>username_0: @username_1 Sure I will.\n<issue_comment>username_0: @pytorchbot rebase this please\n<issue_comment>username_2: Please rebase\n<issue_comment>username_0: @username_2 Have rebased this. The failing test looks irrelevant to the PR. Should I rebase again for the failing test?\n<issue_comment>username_0: @username_2 Have rebased and all the tests have ran. Can you please take a look again. Thanks :)\n<issue_comment>username_0: @username_2 Gentle ping for reminder.\n<issue_comment>username_2: Sorry, that is one of the cases when we modify high contention file and it is getting changed faster than we merge issues. Please rebase again and I will land it first in line.\n<issue_comment>username_0: @username_2 Sure will do that once, #36906 is in, as there would probably be more conflicts post that.\n<issue_comment>username_0: @username_2 PTAL:)\n<issue_comment>username_0: @username_2 Gentle ping for reminder."}
{"repo": "pytorch/pytorch", "issue_id": 601203476, "issue_number": 36724, "timestamp": "2020-04-16 20:09:52+00:00", "text": "@username_1 Sure I will.", "context": "<issue_start><issue_comment>Title: Migrate `erf` and `erf_` from the TH to Aten (CUDA) : Closes #24558\nusername_0: Benchmark with same build settings on same system.\r\ngcc : version 7.5.0 (Ubuntu 7.5.0-3ubuntu1~18.04)\r\nCUDA : 10.1\r\nGPU : 1050ti\r\n\r\n```python\r\nimport timeit\r\n\r\nfor n, t in [(10_000, 20000),\r\n             (100_000, 20000)]:\r\n    for dtype in ('torch.half', 'torch.float', 'torch.double'):\r\n        print(f'torch.erf(a) a.numel() == {n} for {t} times {dtype}')\r\n        print(timeit.timeit(f'torch.erf(a); torch.cuda.synchronize()',\r\n                              setup=f'import torch; a=torch.arange({n}, dtype={dtype}, device=\"cuda\")',\r\n                              number=t))\r\n```\r\n\r\nBefore:\r\n\r\n```\r\ntorch.erf(a) a.numel() == 10000 for 20000 times torch.half\r\n0.29057903600187274\r\ntorch.erf(a) a.numel() == 10000 for 20000 times torch.float\r\n0.2836507789979805\r\ntorch.erf(a) a.numel() == 10000 for 20000 times torch.double\r\n0.44974555500084534\r\ntorch.erf(a) a.numel() == 100000 for 20000 times torch.half\r\n0.31807255600142526\r\ntorch.erf(a) a.numel() == 100000 for 20000 times torch.float\r\n0.3216503109979385\r\ntorch.erf(a) a.numel() == 100000 for 20000 times torch.double\r\n2.0413486910001666\r\n```\r\n\r\nAfter:\r\n\r\n```\r\ntorch.erf(a) a.numel() == 10000 for 20000 times torch.half\r\n0.2867302739996376\r\ntorch.erf(a) a.numel() == 10000 for 20000 times torch.float\r\n0.28851128199858067\r\ntorch.erf(a) a.numel() == 10000 for 20000 times torch.double\r\n0.4592030350013374\r\ntorch.erf(a) a.numel() == 100000 for 20000 times torch.half\r\n0.28704102400115517\r\ntorch.erf(a) a.numel() == 100000 for 20000 times torch.float\r\n0.29036039400125446\r\ntorch.erf(a) a.numel() == 100000 for 20000 times torch.double\r\n2.04035638699861\r\n```\n<issue_comment>username_1: @username_0 Would you mind rebasing on top of pytorch master? \r\nI'm concerned this PR might indeed break xla (although the current failure is not related.) Thanks!\n<issue_comment>username_0: @username_1 Sure I will.\n<issue_comment>username_0: @pytorchbot rebase this please\n<issue_comment>username_2: Please rebase\n<issue_comment>username_0: @username_2 Have rebased this. The failing test looks irrelevant to the PR. Should I rebase again for the failing test?\n<issue_comment>username_0: @username_2 Have rebased and all the tests have ran. Can you please take a look again. Thanks :)\n<issue_comment>username_0: @username_2 Gentle ping for reminder.\n<issue_comment>username_2: Sorry, that is one of the cases when we modify high contention file and it is getting changed faster than we merge issues. Please rebase again and I will land it first in line.\n<issue_comment>username_0: @username_2 Sure will do that once, #36906 is in, as there would probably be more conflicts post that.\n<issue_comment>username_0: @username_2 PTAL:)\n<issue_comment>username_0: @username_2 Gentle ping for reminder."}
{"repo": "pytorch/pytorch", "issue_id": 601203476, "issue_number": 36724, "timestamp": "2020-04-16 20:10:22+00:00", "text": "@pytorchbot rebase this please", "context": "<issue_start><issue_comment>Title: Migrate `erf` and `erf_` from the TH to Aten (CUDA) : Closes #24558\nusername_0: Benchmark with same build settings on same system.\r\ngcc : version 7.5.0 (Ubuntu 7.5.0-3ubuntu1~18.04)\r\nCUDA : 10.1\r\nGPU : 1050ti\r\n\r\n```python\r\nimport timeit\r\n\r\nfor n, t in [(10_000, 20000),\r\n             (100_000, 20000)]:\r\n    for dtype in ('torch.half', 'torch.float', 'torch.double'):\r\n        print(f'torch.erf(a) a.numel() == {n} for {t} times {dtype}')\r\n        print(timeit.timeit(f'torch.erf(a); torch.cuda.synchronize()',\r\n                              setup=f'import torch; a=torch.arange({n}, dtype={dtype}, device=\"cuda\")',\r\n                              number=t))\r\n```\r\n\r\nBefore:\r\n\r\n```\r\ntorch.erf(a) a.numel() == 10000 for 20000 times torch.half\r\n0.29057903600187274\r\ntorch.erf(a) a.numel() == 10000 for 20000 times torch.float\r\n0.2836507789979805\r\ntorch.erf(a) a.numel() == 10000 for 20000 times torch.double\r\n0.44974555500084534\r\ntorch.erf(a) a.numel() == 100000 for 20000 times torch.half\r\n0.31807255600142526\r\ntorch.erf(a) a.numel() == 100000 for 20000 times torch.float\r\n0.3216503109979385\r\ntorch.erf(a) a.numel() == 100000 for 20000 times torch.double\r\n2.0413486910001666\r\n```\r\n\r\nAfter:\r\n\r\n```\r\ntorch.erf(a) a.numel() == 10000 for 20000 times torch.half\r\n0.2867302739996376\r\ntorch.erf(a) a.numel() == 10000 for 20000 times torch.float\r\n0.28851128199858067\r\ntorch.erf(a) a.numel() == 10000 for 20000 times torch.double\r\n0.4592030350013374\r\ntorch.erf(a) a.numel() == 100000 for 20000 times torch.half\r\n0.28704102400115517\r\ntorch.erf(a) a.numel() == 100000 for 20000 times torch.float\r\n0.29036039400125446\r\ntorch.erf(a) a.numel() == 100000 for 20000 times torch.double\r\n2.04035638699861\r\n```\n<issue_comment>username_1: @username_0 Would you mind rebasing on top of pytorch master? \r\nI'm concerned this PR might indeed break xla (although the current failure is not related.) Thanks!\n<issue_comment>username_0: @username_1 Sure I will.\n<issue_comment>username_0: @pytorchbot rebase this please\n<issue_comment>username_2: Please rebase\n<issue_comment>username_0: @username_2 Have rebased this. The failing test looks irrelevant to the PR. Should I rebase again for the failing test?\n<issue_comment>username_0: @username_2 Have rebased and all the tests have ran. Can you please take a look again. Thanks :)\n<issue_comment>username_0: @username_2 Gentle ping for reminder.\n<issue_comment>username_2: Sorry, that is one of the cases when we modify high contention file and it is getting changed faster than we merge issues. Please rebase again and I will land it first in line.\n<issue_comment>username_0: @username_2 Sure will do that once, #36906 is in, as there would probably be more conflicts post that.\n<issue_comment>username_0: @username_2 PTAL:)\n<issue_comment>username_0: @username_2 Gentle ping for reminder."}
{"repo": "pytorch/pytorch", "issue_id": 601203476, "issue_number": 36724, "timestamp": "2020-04-22 17:36:09+00:00", "text": "Please rebase", "context": "<issue_start><issue_comment>Title: Migrate `erf` and `erf_` from the TH to Aten (CUDA) : Closes #24558\nusername_0: Benchmark with same build settings on same system.\r\ngcc : version 7.5.0 (Ubuntu 7.5.0-3ubuntu1~18.04)\r\nCUDA : 10.1\r\nGPU : 1050ti\r\n\r\n```python\r\nimport timeit\r\n\r\nfor n, t in [(10_000, 20000),\r\n             (100_000, 20000)]:\r\n    for dtype in ('torch.half', 'torch.float', 'torch.double'):\r\n        print(f'torch.erf(a) a.numel() == {n} for {t} times {dtype}')\r\n        print(timeit.timeit(f'torch.erf(a); torch.cuda.synchronize()',\r\n                              setup=f'import torch; a=torch.arange({n}, dtype={dtype}, device=\"cuda\")',\r\n                              number=t))\r\n```\r\n\r\nBefore:\r\n\r\n```\r\ntorch.erf(a) a.numel() == 10000 for 20000 times torch.half\r\n0.29057903600187274\r\ntorch.erf(a) a.numel() == 10000 for 20000 times torch.float\r\n0.2836507789979805\r\ntorch.erf(a) a.numel() == 10000 for 20000 times torch.double\r\n0.44974555500084534\r\ntorch.erf(a) a.numel() == 100000 for 20000 times torch.half\r\n0.31807255600142526\r\ntorch.erf(a) a.numel() == 100000 for 20000 times torch.float\r\n0.3216503109979385\r\ntorch.erf(a) a.numel() == 100000 for 20000 times torch.double\r\n2.0413486910001666\r\n```\r\n\r\nAfter:\r\n\r\n```\r\ntorch.erf(a) a.numel() == 10000 for 20000 times torch.half\r\n0.2867302739996376\r\ntorch.erf(a) a.numel() == 10000 for 20000 times torch.float\r\n0.28851128199858067\r\ntorch.erf(a) a.numel() == 10000 for 20000 times torch.double\r\n0.4592030350013374\r\ntorch.erf(a) a.numel() == 100000 for 20000 times torch.half\r\n0.28704102400115517\r\ntorch.erf(a) a.numel() == 100000 for 20000 times torch.float\r\n0.29036039400125446\r\ntorch.erf(a) a.numel() == 100000 for 20000 times torch.double\r\n2.04035638699861\r\n```\n<issue_comment>username_1: @username_0 Would you mind rebasing on top of pytorch master? \r\nI'm concerned this PR might indeed break xla (although the current failure is not related.) Thanks!\n<issue_comment>username_0: @username_1 Sure I will.\n<issue_comment>username_0: @pytorchbot rebase this please\n<issue_comment>username_2: Please rebase\n<issue_comment>username_0: @username_2 Have rebased this. The failing test looks irrelevant to the PR. Should I rebase again for the failing test?\n<issue_comment>username_0: @username_2 Have rebased and all the tests have ran. Can you please take a look again. Thanks :)\n<issue_comment>username_0: @username_2 Gentle ping for reminder.\n<issue_comment>username_2: Sorry, that is one of the cases when we modify high contention file and it is getting changed faster than we merge issues. Please rebase again and I will land it first in line.\n<issue_comment>username_0: @username_2 Sure will do that once, #36906 is in, as there would probably be more conflicts post that.\n<issue_comment>username_0: @username_2 PTAL:)\n<issue_comment>username_0: @username_2 Gentle ping for reminder."}
{"repo": "pytorch/pytorch", "issue_id": 601203476, "issue_number": 36724, "timestamp": "2020-04-22 22:05:01+00:00", "text": "@username_2 Have rebased this. The failing test looks irrelevant to the PR. Should I rebase again for the failing test?", "context": "<issue_start><issue_comment>Title: Migrate `erf` and `erf_` from the TH to Aten (CUDA) : Closes #24558\nusername_0: Benchmark with same build settings on same system.\r\ngcc : version 7.5.0 (Ubuntu 7.5.0-3ubuntu1~18.04)\r\nCUDA : 10.1\r\nGPU : 1050ti\r\n\r\n```python\r\nimport timeit\r\n\r\nfor n, t in [(10_000, 20000),\r\n             (100_000, 20000)]:\r\n    for dtype in ('torch.half', 'torch.float', 'torch.double'):\r\n        print(f'torch.erf(a) a.numel() == {n} for {t} times {dtype}')\r\n        print(timeit.timeit(f'torch.erf(a); torch.cuda.synchronize()',\r\n                              setup=f'import torch; a=torch.arange({n}, dtype={dtype}, device=\"cuda\")',\r\n                              number=t))\r\n```\r\n\r\nBefore:\r\n\r\n```\r\ntorch.erf(a) a.numel() == 10000 for 20000 times torch.half\r\n0.29057903600187274\r\ntorch.erf(a) a.numel() == 10000 for 20000 times torch.float\r\n0.2836507789979805\r\ntorch.erf(a) a.numel() == 10000 for 20000 times torch.double\r\n0.44974555500084534\r\ntorch.erf(a) a.numel() == 100000 for 20000 times torch.half\r\n0.31807255600142526\r\ntorch.erf(a) a.numel() == 100000 for 20000 times torch.float\r\n0.3216503109979385\r\ntorch.erf(a) a.numel() == 100000 for 20000 times torch.double\r\n2.0413486910001666\r\n```\r\n\r\nAfter:\r\n\r\n```\r\ntorch.erf(a) a.numel() == 10000 for 20000 times torch.half\r\n0.2867302739996376\r\ntorch.erf(a) a.numel() == 10000 for 20000 times torch.float\r\n0.28851128199858067\r\ntorch.erf(a) a.numel() == 10000 for 20000 times torch.double\r\n0.4592030350013374\r\ntorch.erf(a) a.numel() == 100000 for 20000 times torch.half\r\n0.28704102400115517\r\ntorch.erf(a) a.numel() == 100000 for 20000 times torch.float\r\n0.29036039400125446\r\ntorch.erf(a) a.numel() == 100000 for 20000 times torch.double\r\n2.04035638699861\r\n```\n<issue_comment>username_1: @username_0 Would you mind rebasing on top of pytorch master? \r\nI'm concerned this PR might indeed break xla (although the current failure is not related.) Thanks!\n<issue_comment>username_0: @username_1 Sure I will.\n<issue_comment>username_0: @pytorchbot rebase this please\n<issue_comment>username_2: Please rebase\n<issue_comment>username_0: @username_2 Have rebased this. The failing test looks irrelevant to the PR. Should I rebase again for the failing test?\n<issue_comment>username_0: @username_2 Have rebased and all the tests have ran. Can you please take a look again. Thanks :)\n<issue_comment>username_0: @username_2 Gentle ping for reminder.\n<issue_comment>username_2: Sorry, that is one of the cases when we modify high contention file and it is getting changed faster than we merge issues. Please rebase again and I will land it first in line.\n<issue_comment>username_0: @username_2 Sure will do that once, #36906 is in, as there would probably be more conflicts post that.\n<issue_comment>username_0: @username_2 PTAL:)\n<issue_comment>username_0: @username_2 Gentle ping for reminder."}
{"repo": "pytorch/pytorch", "issue_id": 601203476, "issue_number": 36724, "timestamp": "2020-04-24 05:30:16+00:00", "text": "@username_2 Have rebased and all the tests have ran. Can you please take a look again. Thanks :)", "context": "<issue_start><issue_comment>Title: Migrate `erf` and `erf_` from the TH to Aten (CUDA) : Closes #24558\nusername_0: Benchmark with same build settings on same system.\r\ngcc : version 7.5.0 (Ubuntu 7.5.0-3ubuntu1~18.04)\r\nCUDA : 10.1\r\nGPU : 1050ti\r\n\r\n```python\r\nimport timeit\r\n\r\nfor n, t in [(10_000, 20000),\r\n             (100_000, 20000)]:\r\n    for dtype in ('torch.half', 'torch.float', 'torch.double'):\r\n        print(f'torch.erf(a) a.numel() == {n} for {t} times {dtype}')\r\n        print(timeit.timeit(f'torch.erf(a); torch.cuda.synchronize()',\r\n                              setup=f'import torch; a=torch.arange({n}, dtype={dtype}, device=\"cuda\")',\r\n                              number=t))\r\n```\r\n\r\nBefore:\r\n\r\n```\r\ntorch.erf(a) a.numel() == 10000 for 20000 times torch.half\r\n0.29057903600187274\r\ntorch.erf(a) a.numel() == 10000 for 20000 times torch.float\r\n0.2836507789979805\r\ntorch.erf(a) a.numel() == 10000 for 20000 times torch.double\r\n0.44974555500084534\r\ntorch.erf(a) a.numel() == 100000 for 20000 times torch.half\r\n0.31807255600142526\r\ntorch.erf(a) a.numel() == 100000 for 20000 times torch.float\r\n0.3216503109979385\r\ntorch.erf(a) a.numel() == 100000 for 20000 times torch.double\r\n2.0413486910001666\r\n```\r\n\r\nAfter:\r\n\r\n```\r\ntorch.erf(a) a.numel() == 10000 for 20000 times torch.half\r\n0.2867302739996376\r\ntorch.erf(a) a.numel() == 10000 for 20000 times torch.float\r\n0.28851128199858067\r\ntorch.erf(a) a.numel() == 10000 for 20000 times torch.double\r\n0.4592030350013374\r\ntorch.erf(a) a.numel() == 100000 for 20000 times torch.half\r\n0.28704102400115517\r\ntorch.erf(a) a.numel() == 100000 for 20000 times torch.float\r\n0.29036039400125446\r\ntorch.erf(a) a.numel() == 100000 for 20000 times torch.double\r\n2.04035638699861\r\n```\n<issue_comment>username_1: @username_0 Would you mind rebasing on top of pytorch master? \r\nI'm concerned this PR might indeed break xla (although the current failure is not related.) Thanks!\n<issue_comment>username_0: @username_1 Sure I will.\n<issue_comment>username_0: @pytorchbot rebase this please\n<issue_comment>username_2: Please rebase\n<issue_comment>username_0: @username_2 Have rebased this. The failing test looks irrelevant to the PR. Should I rebase again for the failing test?\n<issue_comment>username_0: @username_2 Have rebased and all the tests have ran. Can you please take a look again. Thanks :)\n<issue_comment>username_0: @username_2 Gentle ping for reminder.\n<issue_comment>username_2: Sorry, that is one of the cases when we modify high contention file and it is getting changed faster than we merge issues. Please rebase again and I will land it first in line.\n<issue_comment>username_0: @username_2 Sure will do that once, #36906 is in, as there would probably be more conflicts post that.\n<issue_comment>username_0: @username_2 PTAL:)\n<issue_comment>username_0: @username_2 Gentle ping for reminder."}
{"repo": "pytorch/pytorch", "issue_id": 601203476, "issue_number": 36724, "timestamp": "2020-04-24 20:37:31+00:00", "text": "@username_2 Gentle ping for reminder.", "context": "<issue_start><issue_comment>Title: Migrate `erf` and `erf_` from the TH to Aten (CUDA) : Closes #24558\nusername_0: Benchmark with same build settings on same system.\r\ngcc : version 7.5.0 (Ubuntu 7.5.0-3ubuntu1~18.04)\r\nCUDA : 10.1\r\nGPU : 1050ti\r\n\r\n```python\r\nimport timeit\r\n\r\nfor n, t in [(10_000, 20000),\r\n             (100_000, 20000)]:\r\n    for dtype in ('torch.half', 'torch.float', 'torch.double'):\r\n        print(f'torch.erf(a) a.numel() == {n} for {t} times {dtype}')\r\n        print(timeit.timeit(f'torch.erf(a); torch.cuda.synchronize()',\r\n                              setup=f'import torch; a=torch.arange({n}, dtype={dtype}, device=\"cuda\")',\r\n                              number=t))\r\n```\r\n\r\nBefore:\r\n\r\n```\r\ntorch.erf(a) a.numel() == 10000 for 20000 times torch.half\r\n0.29057903600187274\r\ntorch.erf(a) a.numel() == 10000 for 20000 times torch.float\r\n0.2836507789979805\r\ntorch.erf(a) a.numel() == 10000 for 20000 times torch.double\r\n0.44974555500084534\r\ntorch.erf(a) a.numel() == 100000 for 20000 times torch.half\r\n0.31807255600142526\r\ntorch.erf(a) a.numel() == 100000 for 20000 times torch.float\r\n0.3216503109979385\r\ntorch.erf(a) a.numel() == 100000 for 20000 times torch.double\r\n2.0413486910001666\r\n```\r\n\r\nAfter:\r\n\r\n```\r\ntorch.erf(a) a.numel() == 10000 for 20000 times torch.half\r\n0.2867302739996376\r\ntorch.erf(a) a.numel() == 10000 for 20000 times torch.float\r\n0.28851128199858067\r\ntorch.erf(a) a.numel() == 10000 for 20000 times torch.double\r\n0.4592030350013374\r\ntorch.erf(a) a.numel() == 100000 for 20000 times torch.half\r\n0.28704102400115517\r\ntorch.erf(a) a.numel() == 100000 for 20000 times torch.float\r\n0.29036039400125446\r\ntorch.erf(a) a.numel() == 100000 for 20000 times torch.double\r\n2.04035638699861\r\n```\n<issue_comment>username_1: @username_0 Would you mind rebasing on top of pytorch master? \r\nI'm concerned this PR might indeed break xla (although the current failure is not related.) Thanks!\n<issue_comment>username_0: @username_1 Sure I will.\n<issue_comment>username_0: @pytorchbot rebase this please\n<issue_comment>username_2: Please rebase\n<issue_comment>username_0: @username_2 Have rebased this. The failing test looks irrelevant to the PR. Should I rebase again for the failing test?\n<issue_comment>username_0: @username_2 Have rebased and all the tests have ran. Can you please take a look again. Thanks :)\n<issue_comment>username_0: @username_2 Gentle ping for reminder.\n<issue_comment>username_2: Sorry, that is one of the cases when we modify high contention file and it is getting changed faster than we merge issues. Please rebase again and I will land it first in line.\n<issue_comment>username_0: @username_2 Sure will do that once, #36906 is in, as there would probably be more conflicts post that.\n<issue_comment>username_0: @username_2 PTAL:)\n<issue_comment>username_0: @username_2 Gentle ping for reminder."}
{"repo": "pytorch/pytorch", "issue_id": 601203476, "issue_number": 36724, "timestamp": "2020-04-30 22:30:42+00:00", "text": "Sorry, that is one of the cases when we modify high contention file and it is getting changed faster than we merge issues. Please rebase again and I will land it first in line.", "context": "<issue_start><issue_comment>Title: Migrate `erf` and `erf_` from the TH to Aten (CUDA) : Closes #24558\nusername_0: Benchmark with same build settings on same system.\r\ngcc : version 7.5.0 (Ubuntu 7.5.0-3ubuntu1~18.04)\r\nCUDA : 10.1\r\nGPU : 1050ti\r\n\r\n```python\r\nimport timeit\r\n\r\nfor n, t in [(10_000, 20000),\r\n             (100_000, 20000)]:\r\n    for dtype in ('torch.half', 'torch.float', 'torch.double'):\r\n        print(f'torch.erf(a) a.numel() == {n} for {t} times {dtype}')\r\n        print(timeit.timeit(f'torch.erf(a); torch.cuda.synchronize()',\r\n                              setup=f'import torch; a=torch.arange({n}, dtype={dtype}, device=\"cuda\")',\r\n                              number=t))\r\n```\r\n\r\nBefore:\r\n\r\n```\r\ntorch.erf(a) a.numel() == 10000 for 20000 times torch.half\r\n0.29057903600187274\r\ntorch.erf(a) a.numel() == 10000 for 20000 times torch.float\r\n0.2836507789979805\r\ntorch.erf(a) a.numel() == 10000 for 20000 times torch.double\r\n0.44974555500084534\r\ntorch.erf(a) a.numel() == 100000 for 20000 times torch.half\r\n0.31807255600142526\r\ntorch.erf(a) a.numel() == 100000 for 20000 times torch.float\r\n0.3216503109979385\r\ntorch.erf(a) a.numel() == 100000 for 20000 times torch.double\r\n2.0413486910001666\r\n```\r\n\r\nAfter:\r\n\r\n```\r\ntorch.erf(a) a.numel() == 10000 for 20000 times torch.half\r\n0.2867302739996376\r\ntorch.erf(a) a.numel() == 10000 for 20000 times torch.float\r\n0.28851128199858067\r\ntorch.erf(a) a.numel() == 10000 for 20000 times torch.double\r\n0.4592030350013374\r\ntorch.erf(a) a.numel() == 100000 for 20000 times torch.half\r\n0.28704102400115517\r\ntorch.erf(a) a.numel() == 100000 for 20000 times torch.float\r\n0.29036039400125446\r\ntorch.erf(a) a.numel() == 100000 for 20000 times torch.double\r\n2.04035638699861\r\n```\n<issue_comment>username_1: @username_0 Would you mind rebasing on top of pytorch master? \r\nI'm concerned this PR might indeed break xla (although the current failure is not related.) Thanks!\n<issue_comment>username_0: @username_1 Sure I will.\n<issue_comment>username_0: @pytorchbot rebase this please\n<issue_comment>username_2: Please rebase\n<issue_comment>username_0: @username_2 Have rebased this. The failing test looks irrelevant to the PR. Should I rebase again for the failing test?\n<issue_comment>username_0: @username_2 Have rebased and all the tests have ran. Can you please take a look again. Thanks :)\n<issue_comment>username_0: @username_2 Gentle ping for reminder.\n<issue_comment>username_2: Sorry, that is one of the cases when we modify high contention file and it is getting changed faster than we merge issues. Please rebase again and I will land it first in line.\n<issue_comment>username_0: @username_2 Sure will do that once, #36906 is in, as there would probably be more conflicts post that.\n<issue_comment>username_0: @username_2 PTAL:)\n<issue_comment>username_0: @username_2 Gentle ping for reminder."}
{"repo": "pytorch/pytorch", "issue_id": 601203476, "issue_number": 36724, "timestamp": "2020-05-01 12:41:23+00:00", "text": "@username_2 Sure will do that once, #36906 is in, as there would probably be more conflicts post that.", "context": "<issue_start><issue_comment>Title: Migrate `erf` and `erf_` from the TH to Aten (CUDA) : Closes #24558\nusername_0: Benchmark with same build settings on same system.\r\ngcc : version 7.5.0 (Ubuntu 7.5.0-3ubuntu1~18.04)\r\nCUDA : 10.1\r\nGPU : 1050ti\r\n\r\n```python\r\nimport timeit\r\n\r\nfor n, t in [(10_000, 20000),\r\n             (100_000, 20000)]:\r\n    for dtype in ('torch.half', 'torch.float', 'torch.double'):\r\n        print(f'torch.erf(a) a.numel() == {n} for {t} times {dtype}')\r\n        print(timeit.timeit(f'torch.erf(a); torch.cuda.synchronize()',\r\n                              setup=f'import torch; a=torch.arange({n}, dtype={dtype}, device=\"cuda\")',\r\n                              number=t))\r\n```\r\n\r\nBefore:\r\n\r\n```\r\ntorch.erf(a) a.numel() == 10000 for 20000 times torch.half\r\n0.29057903600187274\r\ntorch.erf(a) a.numel() == 10000 for 20000 times torch.float\r\n0.2836507789979805\r\ntorch.erf(a) a.numel() == 10000 for 20000 times torch.double\r\n0.44974555500084534\r\ntorch.erf(a) a.numel() == 100000 for 20000 times torch.half\r\n0.31807255600142526\r\ntorch.erf(a) a.numel() == 100000 for 20000 times torch.float\r\n0.3216503109979385\r\ntorch.erf(a) a.numel() == 100000 for 20000 times torch.double\r\n2.0413486910001666\r\n```\r\n\r\nAfter:\r\n\r\n```\r\ntorch.erf(a) a.numel() == 10000 for 20000 times torch.half\r\n0.2867302739996376\r\ntorch.erf(a) a.numel() == 10000 for 20000 times torch.float\r\n0.28851128199858067\r\ntorch.erf(a) a.numel() == 10000 for 20000 times torch.double\r\n0.4592030350013374\r\ntorch.erf(a) a.numel() == 100000 for 20000 times torch.half\r\n0.28704102400115517\r\ntorch.erf(a) a.numel() == 100000 for 20000 times torch.float\r\n0.29036039400125446\r\ntorch.erf(a) a.numel() == 100000 for 20000 times torch.double\r\n2.04035638699861\r\n```\n<issue_comment>username_1: @username_0 Would you mind rebasing on top of pytorch master? \r\nI'm concerned this PR might indeed break xla (although the current failure is not related.) Thanks!\n<issue_comment>username_0: @username_1 Sure I will.\n<issue_comment>username_0: @pytorchbot rebase this please\n<issue_comment>username_2: Please rebase\n<issue_comment>username_0: @username_2 Have rebased this. The failing test looks irrelevant to the PR. Should I rebase again for the failing test?\n<issue_comment>username_0: @username_2 Have rebased and all the tests have ran. Can you please take a look again. Thanks :)\n<issue_comment>username_0: @username_2 Gentle ping for reminder.\n<issue_comment>username_2: Sorry, that is one of the cases when we modify high contention file and it is getting changed faster than we merge issues. Please rebase again and I will land it first in line.\n<issue_comment>username_0: @username_2 Sure will do that once, #36906 is in, as there would probably be more conflicts post that.\n<issue_comment>username_0: @username_2 PTAL:)\n<issue_comment>username_0: @username_2 Gentle ping for reminder."}
{"repo": "pytorch/pytorch", "issue_id": 601203476, "issue_number": 36724, "timestamp": "2020-05-01 23:19:54+00:00", "text": "@username_2 PTAL:)", "context": "<issue_start><issue_comment>Title: Migrate `erf` and `erf_` from the TH to Aten (CUDA) : Closes #24558\nusername_0: Benchmark with same build settings on same system.\r\ngcc : version 7.5.0 (Ubuntu 7.5.0-3ubuntu1~18.04)\r\nCUDA : 10.1\r\nGPU : 1050ti\r\n\r\n```python\r\nimport timeit\r\n\r\nfor n, t in [(10_000, 20000),\r\n             (100_000, 20000)]:\r\n    for dtype in ('torch.half', 'torch.float', 'torch.double'):\r\n        print(f'torch.erf(a) a.numel() == {n} for {t} times {dtype}')\r\n        print(timeit.timeit(f'torch.erf(a); torch.cuda.synchronize()',\r\n                              setup=f'import torch; a=torch.arange({n}, dtype={dtype}, device=\"cuda\")',\r\n                              number=t))\r\n```\r\n\r\nBefore:\r\n\r\n```\r\ntorch.erf(a) a.numel() == 10000 for 20000 times torch.half\r\n0.29057903600187274\r\ntorch.erf(a) a.numel() == 10000 for 20000 times torch.float\r\n0.2836507789979805\r\ntorch.erf(a) a.numel() == 10000 for 20000 times torch.double\r\n0.44974555500084534\r\ntorch.erf(a) a.numel() == 100000 for 20000 times torch.half\r\n0.31807255600142526\r\ntorch.erf(a) a.numel() == 100000 for 20000 times torch.float\r\n0.3216503109979385\r\ntorch.erf(a) a.numel() == 100000 for 20000 times torch.double\r\n2.0413486910001666\r\n```\r\n\r\nAfter:\r\n\r\n```\r\ntorch.erf(a) a.numel() == 10000 for 20000 times torch.half\r\n0.2867302739996376\r\ntorch.erf(a) a.numel() == 10000 for 20000 times torch.float\r\n0.28851128199858067\r\ntorch.erf(a) a.numel() == 10000 for 20000 times torch.double\r\n0.4592030350013374\r\ntorch.erf(a) a.numel() == 100000 for 20000 times torch.half\r\n0.28704102400115517\r\ntorch.erf(a) a.numel() == 100000 for 20000 times torch.float\r\n0.29036039400125446\r\ntorch.erf(a) a.numel() == 100000 for 20000 times torch.double\r\n2.04035638699861\r\n```\n<issue_comment>username_1: @username_0 Would you mind rebasing on top of pytorch master? \r\nI'm concerned this PR might indeed break xla (although the current failure is not related.) Thanks!\n<issue_comment>username_0: @username_1 Sure I will.\n<issue_comment>username_0: @pytorchbot rebase this please\n<issue_comment>username_2: Please rebase\n<issue_comment>username_0: @username_2 Have rebased this. The failing test looks irrelevant to the PR. Should I rebase again for the failing test?\n<issue_comment>username_0: @username_2 Have rebased and all the tests have ran. Can you please take a look again. Thanks :)\n<issue_comment>username_0: @username_2 Gentle ping for reminder.\n<issue_comment>username_2: Sorry, that is one of the cases when we modify high contention file and it is getting changed faster than we merge issues. Please rebase again and I will land it first in line.\n<issue_comment>username_0: @username_2 Sure will do that once, #36906 is in, as there would probably be more conflicts post that.\n<issue_comment>username_0: @username_2 PTAL:)\n<issue_comment>username_0: @username_2 Gentle ping for reminder."}
{"repo": "pytorch/pytorch", "issue_id": 601203476, "issue_number": 36724, "timestamp": "2020-05-04 14:42:21+00:00", "text": "@username_2 Gentle ping for reminder.", "context": "<issue_start><issue_comment>Title: Migrate `erf` and `erf_` from the TH to Aten (CUDA) : Closes #24558\nusername_0: Benchmark with same build settings on same system.\r\ngcc : version 7.5.0 (Ubuntu 7.5.0-3ubuntu1~18.04)\r\nCUDA : 10.1\r\nGPU : 1050ti\r\n\r\n```python\r\nimport timeit\r\n\r\nfor n, t in [(10_000, 20000),\r\n             (100_000, 20000)]:\r\n    for dtype in ('torch.half', 'torch.float', 'torch.double'):\r\n        print(f'torch.erf(a) a.numel() == {n} for {t} times {dtype}')\r\n        print(timeit.timeit(f'torch.erf(a); torch.cuda.synchronize()',\r\n                              setup=f'import torch; a=torch.arange({n}, dtype={dtype}, device=\"cuda\")',\r\n                              number=t))\r\n```\r\n\r\nBefore:\r\n\r\n```\r\ntorch.erf(a) a.numel() == 10000 for 20000 times torch.half\r\n0.29057903600187274\r\ntorch.erf(a) a.numel() == 10000 for 20000 times torch.float\r\n0.2836507789979805\r\ntorch.erf(a) a.numel() == 10000 for 20000 times torch.double\r\n0.44974555500084534\r\ntorch.erf(a) a.numel() == 100000 for 20000 times torch.half\r\n0.31807255600142526\r\ntorch.erf(a) a.numel() == 100000 for 20000 times torch.float\r\n0.3216503109979385\r\ntorch.erf(a) a.numel() == 100000 for 20000 times torch.double\r\n2.0413486910001666\r\n```\r\n\r\nAfter:\r\n\r\n```\r\ntorch.erf(a) a.numel() == 10000 for 20000 times torch.half\r\n0.2867302739996376\r\ntorch.erf(a) a.numel() == 10000 for 20000 times torch.float\r\n0.28851128199858067\r\ntorch.erf(a) a.numel() == 10000 for 20000 times torch.double\r\n0.4592030350013374\r\ntorch.erf(a) a.numel() == 100000 for 20000 times torch.half\r\n0.28704102400115517\r\ntorch.erf(a) a.numel() == 100000 for 20000 times torch.float\r\n0.29036039400125446\r\ntorch.erf(a) a.numel() == 100000 for 20000 times torch.double\r\n2.04035638699861\r\n```\n<issue_comment>username_1: @username_0 Would you mind rebasing on top of pytorch master? \r\nI'm concerned this PR might indeed break xla (although the current failure is not related.) Thanks!\n<issue_comment>username_0: @username_1 Sure I will.\n<issue_comment>username_0: @pytorchbot rebase this please\n<issue_comment>username_2: Please rebase\n<issue_comment>username_0: @username_2 Have rebased this. The failing test looks irrelevant to the PR. Should I rebase again for the failing test?\n<issue_comment>username_0: @username_2 Have rebased and all the tests have ran. Can you please take a look again. Thanks :)\n<issue_comment>username_0: @username_2 Gentle ping for reminder.\n<issue_comment>username_2: Sorry, that is one of the cases when we modify high contention file and it is getting changed faster than we merge issues. Please rebase again and I will land it first in line.\n<issue_comment>username_0: @username_2 Sure will do that once, #36906 is in, as there would probably be more conflicts post that.\n<issue_comment>username_0: @username_2 PTAL:)\n<issue_comment>username_0: @username_2 Gentle ping for reminder."}
{"repo": "pytorch/pytorch", "issue_id": 720860121, "issue_number": 46287, "timestamp": "2020-10-13T21:26:04Z", "text": "Stack from [ghstack](https://github.com/username_1/ghstack):\n* #46289 Allow vmap to accept nested python data structures as inputs\n* #46288 Implement `_broadcast_to_and_flatten(pytree, spec)`\n* **#46287 Add utilities to support handling of nested python data structures**\n\nThis adds a lightweight `pytree` implementation that is similar to and\ninspired by JAX pytrees, tensorflow.nest, deepmind/tree,\nTorchBeast's TensorNest, etc.\n\nA *pytree* is Python nested data structure. It is a tree in the sense\nthat nodes are Python collections (e.g., list, tuple, dict) and the leaves\nare Python values. Furthermore, a pytree should not contain reference\ncycles.\n\nThis PR:\n- adds support for flattening and unflattening nested Python list/dict/tuples\n\nContext: nested Tensor inputs for vmap\n--------------------------------------\nRight now, vmap is restricted to taking in flat lists of tensors. This\nis because vmap needs to be able to convert every tensor in the input\nthat is being vmapped over into a BatchedTensor.\n\nWith a pytree library, we can simply flatten the input data structure\n(returning the leaves), map all of the Tensors in the flat input to\nBatchedTensors, and unflatten the flat list of BatchedTensors into a new\ninput. Or equivalently, with a `tree_map` function, we can map a nested\npython data structure containing Tensors into one containing\nBatchedTensors.\n\nFuture work\n-----------\nIn some future PRs, we'll add nested input support for vmap. The\nprerequisites for that are:\n- a `broadcast_to(small, big)` that broadcasts `small` up to `big`.\n  This is for handling the in_dims to vmap: the in_dims structure must\n  be compatible with the structure of the inputs.\n\nTest Plan\n---------\n- New tests in test/test_pytree.py", "context": "<issue_start><issue_comment>Title: Add utilities to support handling of nested python data structures\nusername_0: Stack from [ghstack](https://github.com/username_1/ghstack):\n* #46289 Allow vmap to accept nested python data structures as inputs\n* #46288 Implement `_broadcast_to_and_flatten(pytree, spec)`\n* **#46287 Add utilities to support handling of nested python data structures**\n\nThis adds a lightweight `pytree` implementation that is similar to and\ninspired by JAX pytrees, tensorflow.nest, deepmind/tree,\nTorchBeast's TensorNest, etc.\n\nA *pytree* is Python nested data structure. It is a tree in the sense\nthat nodes are Python collections (e.g., list, tuple, dict) and the leaves\nare Python values. Furthermore, a pytree should not contain reference\ncycles.\n\nThis PR:\n- adds support for flattening and unflattening nested Python list/dict/tuples\n\nContext: nested Tensor inputs for vmap\n--------------------------------------\nRight now, vmap is restricted to taking in flat lists of tensors. This\nis because vmap needs to be able to convert every tensor in the input\nthat is being vmapped over into a BatchedTensor.\n\nWith a pytree library, we can simply flatten the input data structure\n(returning the leaves), map all of the Tensors in the flat input to\nBatchedTensors, and unflatten the flat list of BatchedTensors into a new\ninput. Or equivalently, with a `tree_map` function, we can map a nested\npython data structure containing Tensors into one containing\nBatchedTensors.\n\nFuture work\n-----------\nIn some future PRs, we'll add nested input support for vmap. The\nprerequisites for that are:\n- a `broadcast_to(small, big)` that broadcasts `small` up to `big`.\n  This is for handling the in_dims to vmap: the in_dims structure must\n  be compatible with the structure of the inputs.\n\nTest Plan\n---------\n- New tests in test/test_pytree.py\n<issue_comment>username_1: Can this be used to subsume the flatten/unflatten in torch/autograd/function.py for IOFunction?\n<issue_comment>username_1: @zdevito I cc'ed you because ISTR you showing me that looked pretty similar\n<issue_comment>username_1: Yeah, Richard did perf measurements on the top diff of this stack and this is noticeably slower (unsurprisingly). But I think as long as this doesn't slow down \"core\" codepaths it is not a big deal (and I definitely agree that moving it to C++ will speed it up).\n<issue_comment>username_2: In what kind of use cases? In particular maybe we can have a fast path for the \"basic\" use case where it is already a flat collection of Tensor (the only acceptable thing right now).\n<issue_comment>username_0: This adds 10us for a function that takes in 4 arguments. There are ways to speed it up (add a fast path like you suggested, or move parts of the implementation to C++)"}
{"repo": "pytorch/pytorch", "issue_id": 720860121, "issue_number": 46287, "timestamp": "2020-10-16 16:23:05+00:00", "text": "Can this be used to subsume the flatten/unflatten in torch/autograd/function.py for IOFunction?", "context": "<issue_start><issue_comment>Title: Add utilities to support handling of nested python data structures\nusername_0: Stack from [ghstack](https://github.com/username_1/ghstack):\n* #46289 Allow vmap to accept nested python data structures as inputs\n* #46288 Implement `_broadcast_to_and_flatten(pytree, spec)`\n* **#46287 Add utilities to support handling of nested python data structures**\n\nThis adds a lightweight `pytree` implementation that is similar to and\ninspired by JAX pytrees, tensorflow.nest, deepmind/tree,\nTorchBeast's TensorNest, etc.\n\nA *pytree* is Python nested data structure. It is a tree in the sense\nthat nodes are Python collections (e.g., list, tuple, dict) and the leaves\nare Python values. Furthermore, a pytree should not contain reference\ncycles.\n\nThis PR:\n- adds support for flattening and unflattening nested Python list/dict/tuples\n\nContext: nested Tensor inputs for vmap\n--------------------------------------\nRight now, vmap is restricted to taking in flat lists of tensors. This\nis because vmap needs to be able to convert every tensor in the input\nthat is being vmapped over into a BatchedTensor.\n\nWith a pytree library, we can simply flatten the input data structure\n(returning the leaves), map all of the Tensors in the flat input to\nBatchedTensors, and unflatten the flat list of BatchedTensors into a new\ninput. Or equivalently, with a `tree_map` function, we can map a nested\npython data structure containing Tensors into one containing\nBatchedTensors.\n\nFuture work\n-----------\nIn some future PRs, we'll add nested input support for vmap. The\nprerequisites for that are:\n- a `broadcast_to(small, big)` that broadcasts `small` up to `big`.\n  This is for handling the in_dims to vmap: the in_dims structure must\n  be compatible with the structure of the inputs.\n\nTest Plan\n---------\n- New tests in test/test_pytree.py\n<issue_comment>username_1: Can this be used to subsume the flatten/unflatten in torch/autograd/function.py for IOFunction?\n<issue_comment>username_1: @zdevito I cc'ed you because ISTR you showing me that looked pretty similar\n<issue_comment>username_1: Yeah, Richard did perf measurements on the top diff of this stack and this is noticeably slower (unsurprisingly). But I think as long as this doesn't slow down \"core\" codepaths it is not a big deal (and I definitely agree that moving it to C++ will speed it up).\n<issue_comment>username_2: In what kind of use cases? In particular maybe we can have a fast path for the \"basic\" use case where it is already a flat collection of Tensor (the only acceptable thing right now).\n<issue_comment>username_0: This adds 10us for a function that takes in 4 arguments. There are ways to speed it up (add a fast path like you suggested, or move parts of the implementation to C++)"}
{"repo": "pytorch/pytorch", "issue_id": 720860121, "issue_number": 46287, "timestamp": "2020-10-16 17:19:27+00:00", "text": "@zdevito I cc'ed you because ISTR you showing me that looked pretty similar", "context": "<issue_start><issue_comment>Title: Add utilities to support handling of nested python data structures\nusername_0: Stack from [ghstack](https://github.com/username_1/ghstack):\n* #46289 Allow vmap to accept nested python data structures as inputs\n* #46288 Implement `_broadcast_to_and_flatten(pytree, spec)`\n* **#46287 Add utilities to support handling of nested python data structures**\n\nThis adds a lightweight `pytree` implementation that is similar to and\ninspired by JAX pytrees, tensorflow.nest, deepmind/tree,\nTorchBeast's TensorNest, etc.\n\nA *pytree* is Python nested data structure. It is a tree in the sense\nthat nodes are Python collections (e.g., list, tuple, dict) and the leaves\nare Python values. Furthermore, a pytree should not contain reference\ncycles.\n\nThis PR:\n- adds support for flattening and unflattening nested Python list/dict/tuples\n\nContext: nested Tensor inputs for vmap\n--------------------------------------\nRight now, vmap is restricted to taking in flat lists of tensors. This\nis because vmap needs to be able to convert every tensor in the input\nthat is being vmapped over into a BatchedTensor.\n\nWith a pytree library, we can simply flatten the input data structure\n(returning the leaves), map all of the Tensors in the flat input to\nBatchedTensors, and unflatten the flat list of BatchedTensors into a new\ninput. Or equivalently, with a `tree_map` function, we can map a nested\npython data structure containing Tensors into one containing\nBatchedTensors.\n\nFuture work\n-----------\nIn some future PRs, we'll add nested input support for vmap. The\nprerequisites for that are:\n- a `broadcast_to(small, big)` that broadcasts `small` up to `big`.\n  This is for handling the in_dims to vmap: the in_dims structure must\n  be compatible with the structure of the inputs.\n\nTest Plan\n---------\n- New tests in test/test_pytree.py\n<issue_comment>username_1: Can this be used to subsume the flatten/unflatten in torch/autograd/function.py for IOFunction?\n<issue_comment>username_1: @zdevito I cc'ed you because ISTR you showing me that looked pretty similar\n<issue_comment>username_1: Yeah, Richard did perf measurements on the top diff of this stack and this is noticeably slower (unsurprisingly). But I think as long as this doesn't slow down \"core\" codepaths it is not a big deal (and I definitely agree that moving it to C++ will speed it up).\n<issue_comment>username_2: In what kind of use cases? In particular maybe we can have a fast path for the \"basic\" use case where it is already a flat collection of Tensor (the only acceptable thing right now).\n<issue_comment>username_0: This adds 10us for a function that takes in 4 arguments. There are ways to speed it up (add a fast path like you suggested, or move parts of the implementation to C++)"}
{"repo": "pytorch/pytorch", "issue_id": 720860121, "issue_number": 46287, "timestamp": "2020-10-16 18:39:12+00:00", "text": "Yeah, Richard did perf measurements on the top diff of this stack and this is noticeably slower (unsurprisingly). But I think as long as this doesn't slow down \"core\" codepaths it is not a big deal (and I definitely agree that moving it to C++ will speed it up).", "context": "<issue_start><issue_comment>Title: Add utilities to support handling of nested python data structures\nusername_0: Stack from [ghstack](https://github.com/username_1/ghstack):\n* #46289 Allow vmap to accept nested python data structures as inputs\n* #46288 Implement `_broadcast_to_and_flatten(pytree, spec)`\n* **#46287 Add utilities to support handling of nested python data structures**\n\nThis adds a lightweight `pytree` implementation that is similar to and\ninspired by JAX pytrees, tensorflow.nest, deepmind/tree,\nTorchBeast's TensorNest, etc.\n\nA *pytree* is Python nested data structure. It is a tree in the sense\nthat nodes are Python collections (e.g., list, tuple, dict) and the leaves\nare Python values. Furthermore, a pytree should not contain reference\ncycles.\n\nThis PR:\n- adds support for flattening and unflattening nested Python list/dict/tuples\n\nContext: nested Tensor inputs for vmap\n--------------------------------------\nRight now, vmap is restricted to taking in flat lists of tensors. This\nis because vmap needs to be able to convert every tensor in the input\nthat is being vmapped over into a BatchedTensor.\n\nWith a pytree library, we can simply flatten the input data structure\n(returning the leaves), map all of the Tensors in the flat input to\nBatchedTensors, and unflatten the flat list of BatchedTensors into a new\ninput. Or equivalently, with a `tree_map` function, we can map a nested\npython data structure containing Tensors into one containing\nBatchedTensors.\n\nFuture work\n-----------\nIn some future PRs, we'll add nested input support for vmap. The\nprerequisites for that are:\n- a `broadcast_to(small, big)` that broadcasts `small` up to `big`.\n  This is for handling the in_dims to vmap: the in_dims structure must\n  be compatible with the structure of the inputs.\n\nTest Plan\n---------\n- New tests in test/test_pytree.py\n<issue_comment>username_1: Can this be used to subsume the flatten/unflatten in torch/autograd/function.py for IOFunction?\n<issue_comment>username_1: @zdevito I cc'ed you because ISTR you showing me that looked pretty similar\n<issue_comment>username_1: Yeah, Richard did perf measurements on the top diff of this stack and this is noticeably slower (unsurprisingly). But I think as long as this doesn't slow down \"core\" codepaths it is not a big deal (and I definitely agree that moving it to C++ will speed it up).\n<issue_comment>username_2: In what kind of use cases? In particular maybe we can have a fast path for the \"basic\" use case where it is already a flat collection of Tensor (the only acceptable thing right now).\n<issue_comment>username_0: This adds 10us for a function that takes in 4 arguments. There are ways to speed it up (add a fast path like you suggested, or move parts of the implementation to C++)"}
{"repo": "pytorch/pytorch", "issue_id": 720860121, "issue_number": 46287, "timestamp": "2020-10-17 14:30:24+00:00", "text": "In what kind of use cases? In particular maybe we can have a fast path for the \"basic\" use case where it is already a flat collection of Tensor (the only acceptable thing right now).", "context": "<issue_start><issue_comment>Title: Add utilities to support handling of nested python data structures\nusername_0: Stack from [ghstack](https://github.com/username_1/ghstack):\n* #46289 Allow vmap to accept nested python data structures as inputs\n* #46288 Implement `_broadcast_to_and_flatten(pytree, spec)`\n* **#46287 Add utilities to support handling of nested python data structures**\n\nThis adds a lightweight `pytree` implementation that is similar to and\ninspired by JAX pytrees, tensorflow.nest, deepmind/tree,\nTorchBeast's TensorNest, etc.\n\nA *pytree* is Python nested data structure. It is a tree in the sense\nthat nodes are Python collections (e.g., list, tuple, dict) and the leaves\nare Python values. Furthermore, a pytree should not contain reference\ncycles.\n\nThis PR:\n- adds support for flattening and unflattening nested Python list/dict/tuples\n\nContext: nested Tensor inputs for vmap\n--------------------------------------\nRight now, vmap is restricted to taking in flat lists of tensors. This\nis because vmap needs to be able to convert every tensor in the input\nthat is being vmapped over into a BatchedTensor.\n\nWith a pytree library, we can simply flatten the input data structure\n(returning the leaves), map all of the Tensors in the flat input to\nBatchedTensors, and unflatten the flat list of BatchedTensors into a new\ninput. Or equivalently, with a `tree_map` function, we can map a nested\npython data structure containing Tensors into one containing\nBatchedTensors.\n\nFuture work\n-----------\nIn some future PRs, we'll add nested input support for vmap. The\nprerequisites for that are:\n- a `broadcast_to(small, big)` that broadcasts `small` up to `big`.\n  This is for handling the in_dims to vmap: the in_dims structure must\n  be compatible with the structure of the inputs.\n\nTest Plan\n---------\n- New tests in test/test_pytree.py\n<issue_comment>username_1: Can this be used to subsume the flatten/unflatten in torch/autograd/function.py for IOFunction?\n<issue_comment>username_1: @zdevito I cc'ed you because ISTR you showing me that looked pretty similar\n<issue_comment>username_1: Yeah, Richard did perf measurements on the top diff of this stack and this is noticeably slower (unsurprisingly). But I think as long as this doesn't slow down \"core\" codepaths it is not a big deal (and I definitely agree that moving it to C++ will speed it up).\n<issue_comment>username_2: In what kind of use cases? In particular maybe we can have a fast path for the \"basic\" use case where it is already a flat collection of Tensor (the only acceptable thing right now).\n<issue_comment>username_0: This adds 10us for a function that takes in 4 arguments. There are ways to speed it up (add a fast path like you suggested, or move parts of the implementation to C++)"}
{"repo": "pytorch/pytorch", "issue_id": 720860121, "issue_number": 46287, "timestamp": "2020-10-19 14:08:53+00:00", "text": "This adds 10us for a function that takes in 4 arguments. There are ways to speed it up (add a fast path like you suggested, or move parts of the implementation to C++)", "context": "<issue_start><issue_comment>Title: Add utilities to support handling of nested python data structures\nusername_0: Stack from [ghstack](https://github.com/username_1/ghstack):\n* #46289 Allow vmap to accept nested python data structures as inputs\n* #46288 Implement `_broadcast_to_and_flatten(pytree, spec)`\n* **#46287 Add utilities to support handling of nested python data structures**\n\nThis adds a lightweight `pytree` implementation that is similar to and\ninspired by JAX pytrees, tensorflow.nest, deepmind/tree,\nTorchBeast's TensorNest, etc.\n\nA *pytree* is Python nested data structure. It is a tree in the sense\nthat nodes are Python collections (e.g., list, tuple, dict) and the leaves\nare Python values. Furthermore, a pytree should not contain reference\ncycles.\n\nThis PR:\n- adds support for flattening and unflattening nested Python list/dict/tuples\n\nContext: nested Tensor inputs for vmap\n--------------------------------------\nRight now, vmap is restricted to taking in flat lists of tensors. This\nis because vmap needs to be able to convert every tensor in the input\nthat is being vmapped over into a BatchedTensor.\n\nWith a pytree library, we can simply flatten the input data structure\n(returning the leaves), map all of the Tensors in the flat input to\nBatchedTensors, and unflatten the flat list of BatchedTensors into a new\ninput. Or equivalently, with a `tree_map` function, we can map a nested\npython data structure containing Tensors into one containing\nBatchedTensors.\n\nFuture work\n-----------\nIn some future PRs, we'll add nested input support for vmap. The\nprerequisites for that are:\n- a `broadcast_to(small, big)` that broadcasts `small` up to `big`.\n  This is for handling the in_dims to vmap: the in_dims structure must\n  be compatible with the structure of the inputs.\n\nTest Plan\n---------\n- New tests in test/test_pytree.py\n<issue_comment>username_1: Can this be used to subsume the flatten/unflatten in torch/autograd/function.py for IOFunction?\n<issue_comment>username_1: @zdevito I cc'ed you because ISTR you showing me that looked pretty similar\n<issue_comment>username_1: Yeah, Richard did perf measurements on the top diff of this stack and this is noticeably slower (unsurprisingly). But I think as long as this doesn't slow down \"core\" codepaths it is not a big deal (and I definitely agree that moving it to C++ will speed it up).\n<issue_comment>username_2: In what kind of use cases? In particular maybe we can have a fast path for the \"basic\" use case where it is already a flat collection of Tensor (the only acceptable thing right now).\n<issue_comment>username_0: This adds 10us for a function that takes in 4 arguments. There are ways to speed it up (add a fast path like you suggested, or move parts of the implementation to C++)"}
{"repo": "pytorch/pytorch", "issue_id": 781124850, "issue_number": 50194, "timestamp": "2021-01-07T08:19:26Z", "text": "Stack from [ghstack](https://github.com/ezyang/ghstack):\r\n* **#50194 [JIT] Print out CU address in `ClassType::repr_str()`**\r\n\r\n**Summary**\r\n`ClassType::repr_str()` prints out only the name of a `ClassType`, which\r\nis not always enough to disambiguate it. In some situations, two\r\n`ClassTypes` are compared and do not match despite having identical\r\nnames because they are in separate compilation units. In such cases, the\r\nerror message can seem nonsensical (e.g. `expected type T but found type\r\nT`). This commit modifies `ClassType::repr_str()` so that it prints out\r\nthe address of the type's compilation unit to make these messages less\r\npuzzling (e.g. `expected type T (0x239023) but found type T (0x230223)`).\r\n\r\n**Test Plan**\r\nThis commit adds a unit test, `ClassTypeTest.IdenticalTypesDifferentCus`\r\nthat reproduces this situation.\r\n\r\n**Fixes**\r\nThis commit fixes #46212.", "context": "<issue_start><issue_comment>Title: [JIT] Print out CU address in `ClassType::repr_str()`\nusername_0: Stack from [ghstack](https://github.com/ezyang/ghstack):\r\n* **#50194 [JIT] Print out CU address in `ClassType::repr_str()`**\r\n\r\n**Summary**\r\n`ClassType::repr_str()` prints out only the name of a `ClassType`, which\r\nis not always enough to disambiguate it. In some situations, two\r\n`ClassTypes` are compared and do not match despite having identical\r\nnames because they are in separate compilation units. In such cases, the\r\nerror message can seem nonsensical (e.g. `expected type T but found type\r\nT`). This commit modifies `ClassType::repr_str()` so that it prints out\r\nthe address of the type's compilation unit to make these messages less\r\npuzzling (e.g. `expected type T (0x239023) but found type T (0x230223)`).\r\n\r\n**Test Plan**\r\nThis commit adds a unit test, `ClassTypeTest.IdenticalTypesDifferentCus`\r\nthat reproduces this situation.\r\n\r\n**Fixes**\r\nThis commit fixes #46212.\n<issue_comment>username_0: Hm, it's probably a better idea to print out the type as `ClassType (compilation unit: 0x2398579)` instead of `ClassType (0x2398579)` which is probably just as confusing as `ClassType`.\n<issue_comment>username_0: For clarity, the original issue was opened by someone who used the same TorchScript class in two different modules, saved them into two different archives, and then tried to pass an instance of that class generated by one module to the other. This didn't work because the one TorchScript class corresponds to two different JIT type objects (since the modules were serialized separately), and those objects are not equal (`ClassType::operator==`) to each other because their compilation units don't match. I think the fix here is to save both into the same archive, but I think the error message (e.g. `expected type T but got type T`) can be improved, and that's what this PR is for.\n<issue_comment>username_0: Ping, I have made the requested changes."}
{"repo": "pytorch/pytorch", "issue_id": 781124850, "issue_number": 50194, "timestamp": "2021-01-07 19:32:03+00:00", "text": "Hm, it's probably a better idea to print out the type as `ClassType (compilation unit: 0x2398579)` instead of `ClassType (0x2398579)` which is probably just as confusing as `ClassType`.", "context": "<issue_start><issue_comment>Title: [JIT] Print out CU address in `ClassType::repr_str()`\nusername_0: Stack from [ghstack](https://github.com/ezyang/ghstack):\r\n* **#50194 [JIT] Print out CU address in `ClassType::repr_str()`**\r\n\r\n**Summary**\r\n`ClassType::repr_str()` prints out only the name of a `ClassType`, which\r\nis not always enough to disambiguate it. In some situations, two\r\n`ClassTypes` are compared and do not match despite having identical\r\nnames because they are in separate compilation units. In such cases, the\r\nerror message can seem nonsensical (e.g. `expected type T but found type\r\nT`). This commit modifies `ClassType::repr_str()` so that it prints out\r\nthe address of the type's compilation unit to make these messages less\r\npuzzling (e.g. `expected type T (0x239023) but found type T (0x230223)`).\r\n\r\n**Test Plan**\r\nThis commit adds a unit test, `ClassTypeTest.IdenticalTypesDifferentCus`\r\nthat reproduces this situation.\r\n\r\n**Fixes**\r\nThis commit fixes #46212.\n<issue_comment>username_0: Hm, it's probably a better idea to print out the type as `ClassType (compilation unit: 0x2398579)` instead of `ClassType (0x2398579)` which is probably just as confusing as `ClassType`.\n<issue_comment>username_0: For clarity, the original issue was opened by someone who used the same TorchScript class in two different modules, saved them into two different archives, and then tried to pass an instance of that class generated by one module to the other. This didn't work because the one TorchScript class corresponds to two different JIT type objects (since the modules were serialized separately), and those objects are not equal (`ClassType::operator==`) to each other because their compilation units don't match. I think the fix here is to save both into the same archive, but I think the error message (e.g. `expected type T but got type T`) can be improved, and that's what this PR is for.\n<issue_comment>username_0: Ping, I have made the requested changes."}
{"repo": "pytorch/pytorch", "issue_id": 781124850, "issue_number": 50194, "timestamp": "2021-01-07 21:10:15+00:00", "text": "For clarity, the original issue was opened by someone who used the same TorchScript class in two different modules, saved them into two different archives, and then tried to pass an instance of that class generated by one module to the other. This didn't work because the one TorchScript class corresponds to two different JIT type objects (since the modules were serialized separately), and those objects are not equal (`ClassType::operator==`) to each other because their compilation units don't match. I think the fix here is to save both into the same archive, but I think the error message (e.g. `expected type T but got type T`) can be improved, and that's what this PR is for.", "context": "<issue_start><issue_comment>Title: [JIT] Print out CU address in `ClassType::repr_str()`\nusername_0: Stack from [ghstack](https://github.com/ezyang/ghstack):\r\n* **#50194 [JIT] Print out CU address in `ClassType::repr_str()`**\r\n\r\n**Summary**\r\n`ClassType::repr_str()` prints out only the name of a `ClassType`, which\r\nis not always enough to disambiguate it. In some situations, two\r\n`ClassTypes` are compared and do not match despite having identical\r\nnames because they are in separate compilation units. In such cases, the\r\nerror message can seem nonsensical (e.g. `expected type T but found type\r\nT`). This commit modifies `ClassType::repr_str()` so that it prints out\r\nthe address of the type's compilation unit to make these messages less\r\npuzzling (e.g. `expected type T (0x239023) but found type T (0x230223)`).\r\n\r\n**Test Plan**\r\nThis commit adds a unit test, `ClassTypeTest.IdenticalTypesDifferentCus`\r\nthat reproduces this situation.\r\n\r\n**Fixes**\r\nThis commit fixes #46212.\n<issue_comment>username_0: Hm, it's probably a better idea to print out the type as `ClassType (compilation unit: 0x2398579)` instead of `ClassType (0x2398579)` which is probably just as confusing as `ClassType`.\n<issue_comment>username_0: For clarity, the original issue was opened by someone who used the same TorchScript class in two different modules, saved them into two different archives, and then tried to pass an instance of that class generated by one module to the other. This didn't work because the one TorchScript class corresponds to two different JIT type objects (since the modules were serialized separately), and those objects are not equal (`ClassType::operator==`) to each other because their compilation units don't match. I think the fix here is to save both into the same archive, but I think the error message (e.g. `expected type T but got type T`) can be improved, and that's what this PR is for.\n<issue_comment>username_0: Ping, I have made the requested changes."}
{"repo": "pytorch/pytorch", "issue_id": 781124850, "issue_number": 50194, "timestamp": "2021-01-12 00:59:58+00:00", "text": "Ping, I have made the requested changes.", "context": "<issue_start><issue_comment>Title: [JIT] Print out CU address in `ClassType::repr_str()`\nusername_0: Stack from [ghstack](https://github.com/ezyang/ghstack):\r\n* **#50194 [JIT] Print out CU address in `ClassType::repr_str()`**\r\n\r\n**Summary**\r\n`ClassType::repr_str()` prints out only the name of a `ClassType`, which\r\nis not always enough to disambiguate it. In some situations, two\r\n`ClassTypes` are compared and do not match despite having identical\r\nnames because they are in separate compilation units. In such cases, the\r\nerror message can seem nonsensical (e.g. `expected type T but found type\r\nT`). This commit modifies `ClassType::repr_str()` so that it prints out\r\nthe address of the type's compilation unit to make these messages less\r\npuzzling (e.g. `expected type T (0x239023) but found type T (0x230223)`).\r\n\r\n**Test Plan**\r\nThis commit adds a unit test, `ClassTypeTest.IdenticalTypesDifferentCus`\r\nthat reproduces this situation.\r\n\r\n**Fixes**\r\nThis commit fixes #46212.\n<issue_comment>username_0: Hm, it's probably a better idea to print out the type as `ClassType (compilation unit: 0x2398579)` instead of `ClassType (0x2398579)` which is probably just as confusing as `ClassType`.\n<issue_comment>username_0: For clarity, the original issue was opened by someone who used the same TorchScript class in two different modules, saved them into two different archives, and then tried to pass an instance of that class generated by one module to the other. This didn't work because the one TorchScript class corresponds to two different JIT type objects (since the modules were serialized separately), and those objects are not equal (`ClassType::operator==`) to each other because their compilation units don't match. I think the fix here is to save both into the same archive, but I think the error message (e.g. `expected type T but got type T`) can be improved, and that's what this PR is for.\n<issue_comment>username_0: Ping, I have made the requested changes."}
{"repo": "pytorch/pytorch", "issue_id": 911821614, "issue_number": 59479, "timestamp": "2021-06-04T20:40:39Z", "text": "Fixes https://github.com/pytorch/pytorch/issues/59477.\r\n\r\n```python\r\nIn [1]: import torch\r\n\r\nIn [2]: x = torch.rand(3, 3, dtype=torch.double, requires_grad=True)\r\n\r\nIn [3]: y = torch.rand(3, 3, dtype=torch.double)\r\n\r\nIn [4]: torch.autograd.gradgradcheck(lambda x, y: torch.nn.functional.binary_cross_entropy(x, y, reduction='sum'), [x, y])\r\nOut[4]: True\r\n\r\nIn [5]: torch.autograd.gradgradcheck(lambda x, y: torch.nn.functional.binary_cross_entropy(x, y, reduction='mean'), [x, y])\r\nOut[5]: True\r\n\r\nIn [6]: torch.autograd.gradcheck(lambda x, y: torch.nn.functional.binary_cross_entropy(x, y, reduction='sum'), [x, y])\r\nOut[6]: True\r\n\r\n```\r\n\r\nMore comprehensive testing could be added in https://github.com/pytorch/pytorch/pull/59447.", "context": "<issue_start><issue_comment>Title: fix double backward for `binary_cross_entropy` loss function when `reduction=sum`.\nusername_0: Fixes https://github.com/pytorch/pytorch/issues/59477.\r\n\r\n```python\r\nIn [1]: import torch\r\n\r\nIn [2]: x = torch.rand(3, 3, dtype=torch.double, requires_grad=True)\r\n\r\nIn [3]: y = torch.rand(3, 3, dtype=torch.double)\r\n\r\nIn [4]: torch.autograd.gradgradcheck(lambda x, y: torch.nn.functional.binary_cross_entropy(x, y, reduction='sum'), [x, y])\r\nOut[4]: True\r\n\r\nIn [5]: torch.autograd.gradgradcheck(lambda x, y: torch.nn.functional.binary_cross_entropy(x, y, reduction='mean'), [x, y])\r\nOut[5]: True\r\n\r\nIn [6]: torch.autograd.gradcheck(lambda x, y: torch.nn.functional.binary_cross_entropy(x, y, reduction='sum'), [x, y])\r\nOut[6]: True\r\n\r\n```\r\n\r\nMore comprehensive testing could be added in https://github.com/pytorch/pytorch/pull/59447."}
