{"question":"What is the configuration set in the package that gets published to npm?","answer":"The configuration set in the package that gets published to npm can be found at https://github.com/facebook/react/blob/master/packages/react/package.json#L8.","id":2,"text":"That's a private package that will never be published so I don't think there's any value in having this. The real package that gets published to npm does have this set (https://github.com/facebook/react/blob/master/packages/react/package.json#L8). Thanks though!","context":"<issue_start><issue_comment>Title: Update package.json\nusername_0: Homepage URL for package.json\n<issue_comment>username_1: That's a private package that will never be published so I don't think there's any value in having this. The real package that gets published to npm does have this set (https://github.com/facebook/react/blob/master/packages/react/package.json#L8). Thanks though!","repo":"facebook/react","issue_id":145259386,"issue_number":6394}
{"question":"What value is assigned to REACT_ELEMENT_TYPE in the source code provided?","answer":"Symbol.for('react.element')","id":4,"text":"Thanks for the PR but I believe the original is clearer. REACT_ELEMENT_TYPE [is](https://github.com/username_0/react/blob/bedbc5f066b7a45520ab78ecb806ab557485535a/src/shared/utils/ReactElementSymbol.js#L18) Symbol.for('react.element'). Cheers!","context":"<issue_start><issue_comment>Title: Fix docs on ReactElement\nusername_0: We should test $$typeof field against REACT_ELEMENT_TYPE to check if something is a React Element.\n<issue_comment>username_1: Thanks for the PR but I believe the original is clearer. REACT_ELEMENT_TYPE [is](https://github.com/username_0/react/blob/bedbc5f066b7a45520ab78ecb806ab557485535a/src/shared/utils/ReactElementSymbol.js#L18) Symbol.for('react.element'). Cheers!\n<issue_comment>username_0: REACT_ELEMENT_TYPE is a plain number if there is no native Symbol nor polyfill.\n<issue_comment>username_1: Right, but this is a fallback case and doesn't seem as important to document here to me.\r\nMaybe could add \"or a magic number as a fallback\".\n<issue_comment>username_0: Got it. Then I'll just ignore it. Thanks.","repo":"facebook/react","issue_id":186222382,"issue_number":8160}
{"question":"What could be the reason for the failure of test cases related to the HalfTensor, even though 'ne' and 'fill' methods are generated in the source code?","answer":"The failure of test cases related to the HalfTensor could be due to a discrepancy between the expected implementation of the 'ne' and 'fill' methods and the actual implementation in the source code.","id":10,"text":"@username_1 Hmm now I am bit curious about the reasons for the failure of the test cases. It says that ne and fill are not implemented for the HalfTensor. From from the source code I think we are actually generating those methods since there isn't any restriction in the cwrap definitions?","context":"<issue_start><issue_comment>Title: fix torch.is_tensor not recognizing HalfTensor\nusername_0: Fix #1922.\n<issue_comment>username_1: This needs some additional changes in the tests\n<issue_comment>username_0: @username_1 Hmm now I am bit curious about the reasons for the failure of the test cases. It says that ne and fill are not implemented for the HalfTensor. From from the source code I think we are actually generating those methods since there isn't any restriction in the cwrap definitions?\n<issue_comment>username_2: @username_0 CPU HalfTensor has no methods implemented except copy and serialization stuff. So almost all tests have to be filtered out for CPU HalfTensor\n<issue_comment>username_3: what we do in TH is basically define an \"is_tensor\" and a \"has_tensor_math\"; then you write the tests based on which of those you are actually testing.\n<issue_comment>username_0: @username_2 so I took a look at `test_torch.py`, it looks like the only reference to tensor_classes is in test_print. For other tests usually they are self-contained about which types they are testing against.\n<issue_comment>username_4: @pytorchbot add to whitelist\n<issue_comment>username_0: After fixing this issue. I wonder why it would be a good idea to have a CPU HalfTensor at all. Since HalfTensor is a subtype of Tensor, we would expect it to support the same interface as Tensor. However, the current implementation of HalfTensor does not allow such behaviour.\n<issue_comment>username_2: on the CPU, HalfTensor is just there for serialization purposes. For example saving a GPU HalfTensor and being able to load the weights back on a CPU-only machine.\r\n\r\nOther than that it has no purpose, as Half is not a native type on CPU and all operations will be very very very slow.","repo":"pytorch/pytorch","issue_id":239293180,"issue_number":1934}
{"question":"What methods are implemented for CPU HalfTensor according to the text?","answer":"CPU HalfTensor has no methods implemented except copy and serialization stuff.","id":11,"text":"@username_0 CPU HalfTensor has no methods implemented except copy and serialization stuff. So almost all tests have to be filtered out for CPU HalfTensor","context":"<issue_start><issue_comment>Title: fix torch.is_tensor not recognizing HalfTensor\nusername_0: Fix #1922.\n<issue_comment>username_1: This needs some additional changes in the tests\n<issue_comment>username_0: @username_1 Hmm now I am bit curious about the reasons for the failure of the test cases. It says that ne and fill are not implemented for the HalfTensor. From from the source code I think we are actually generating those methods since there isn't any restriction in the cwrap definitions?\n<issue_comment>username_2: @username_0 CPU HalfTensor has no methods implemented except copy and serialization stuff. So almost all tests have to be filtered out for CPU HalfTensor\n<issue_comment>username_3: what we do in TH is basically define an \"is_tensor\" and a \"has_tensor_math\"; then you write the tests based on which of those you are actually testing.\n<issue_comment>username_0: @username_2 so I took a look at `test_torch.py`, it looks like the only reference to tensor_classes is in test_print. For other tests usually they are self-contained about which types they are testing against.\n<issue_comment>username_4: @pytorchbot add to whitelist\n<issue_comment>username_0: After fixing this issue. I wonder why it would be a good idea to have a CPU HalfTensor at all. Since HalfTensor is a subtype of Tensor, we would expect it to support the same interface as Tensor. However, the current implementation of HalfTensor does not allow such behaviour.\n<issue_comment>username_2: on the CPU, HalfTensor is just there for serialization purposes. For example saving a GPU HalfTensor and being able to load the weights back on a CPU-only machine.\r\n\r\nOther than that it has no purpose, as Half is not a native type on CPU and all operations will be very very very slow.","repo":"pytorch/pytorch","issue_id":239293180,"issue_number":1934}
{"question":"What are the key definitions in TH for writing tests?","answer":"The key definitions in TH for writing tests are \"is_tensor\" and \"has_tensor_math\".","id":12,"text":"what we do in TH is basically define an \"is_tensor\" and a \"has_tensor_math\"; then you write the tests based on which of those you are actually testing.","context":"<issue_start><issue_comment>Title: fix torch.is_tensor not recognizing HalfTensor\nusername_0: Fix #1922.\n<issue_comment>username_1: This needs some additional changes in the tests\n<issue_comment>username_0: @username_1 Hmm now I am bit curious about the reasons for the failure of the test cases. It says that ne and fill are not implemented for the HalfTensor. From from the source code I think we are actually generating those methods since there isn't any restriction in the cwrap definitions?\n<issue_comment>username_2: @username_0 CPU HalfTensor has no methods implemented except copy and serialization stuff. So almost all tests have to be filtered out for CPU HalfTensor\n<issue_comment>username_3: what we do in TH is basically define an \"is_tensor\" and a \"has_tensor_math\"; then you write the tests based on which of those you are actually testing.\n<issue_comment>username_0: @username_2 so I took a look at `test_torch.py`, it looks like the only reference to tensor_classes is in test_print. For other tests usually they are self-contained about which types they are testing against.\n<issue_comment>username_4: @pytorchbot add to whitelist\n<issue_comment>username_0: After fixing this issue. I wonder why it would be a good idea to have a CPU HalfTensor at all. Since HalfTensor is a subtype of Tensor, we would expect it to support the same interface as Tensor. However, the current implementation of HalfTensor does not allow such behaviour.\n<issue_comment>username_2: on the CPU, HalfTensor is just there for serialization purposes. For example saving a GPU HalfTensor and being able to load the weights back on a CPU-only machine.\r\n\r\nOther than that it has no purpose, as Half is not a native type on CPU and all operations will be very very very slow.","repo":"pytorch/pytorch","issue_id":239293180,"issue_number":1934}
{"question":"What error message is the user encountering when trying to run their code?","answer":"The user is encountering a 'RuntimeError: DataLoader worker is killed by signal: Illegal instruction' error message.","id":131,"text":"Still can't get this to work.\r\n\r\nhttps://stackoverflow.com/questions/53449927/runtimeerror-dataloader-worker-is-killed-by-signal-illegal-instruction\r\n\r\nPlease help.","context":"<issue_start><issue_comment>Title: Problem in DataLoader (same problem with #4643, but not solved by it)\nusername_0: - OS: Ubuntu 16.04\r\n- PyTorch version: torch-0.3.1-cp36-cp36-linux_x86_64.whl\r\n- How you installed PyTorch (conda, pip, source): pip3 from the binary\r\n- Python version: 3.6\r\n- CUDA/cuDNN version: 9.1\r\n- GPU models and configuration: GTX 1070 Ti\r\n\r\nAfter install using the wheel file, I have changed the torch/utils/data/Dataloader.py according to the changes shown at https://github.com/pytorch/pytorch/pull/4643\r\n\r\nBut the same error still happen when I try the tutorial example to load image database:\r\n\r\nException ignored in: <bound method DataLoaderIter.__del__ of <torch.utils.data.dataloader.DataLoaderIter object at 0x56090c8c7348>>\r\nTraceback (most recent call last):\r\n  File \"/home/username_0/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 375, in __del__\r\n  File \"/home/username_0/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 361, in _shutdown_workers\r\n  File \"/home/username_0/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 39, in _get_from_queue\r\n  File \"/usr/pkg/lib/python3.6/multiprocessing/queues.py\", line 337, in get\r\nImportError: sys.meta_path is None, Python is likely shutting down\r\n\r\nSo could anyone help me on that? Or is there any other file I should modify to get it run?\n<issue_comment>username_0: And, I am not sure how can I use the changes in torch/csrc/DataLoader.cpp, should I clone all the source code on GitHub and build the newest binary to fully fix this my problem?\n<issue_comment>username_1: when does this error happen? is it happening when you exit python?\n<issue_comment>username_2: @username_1 and @username_3,\r\nI am running this:\r\n\r\npython train.py --dataroot ./datasets/horse2zebra --name horse2zebra_cyclegan --model cycle_gan --no_dropout --loadSize 64 --nThreads 1\r\n\r\nfrom \r\n\r\nhttps://github.com/junyanz/pytorch-CycleGAN-and-pix2pix\r\n\r\nThe model is succefully created:\r\nmodel [CycleGANModel] was created\r\n\r\nI am on ubuntu 16.04, pytorch 3.1, cuda 9.0 and cuDnn 7.1\r\n\r\nThe error I get is traced below:\r\ncreate web directory ./checkpoints/horse2zebra_cyclegan/web...\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 22, in <module>\r\n    for i, data in enumerate(dataset):\r\n  File \"/home/sam/Documents/ComputerVision/pytorch-CycleGAN-and-pix2pix/data/custom_dataset_data_loader.py\", line 44, in __iter__\r\n    for i, data in enumerate(self.dataloader):\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 275, in __next__\r\n    idx, batch = self._get_batch()\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 254, in _get_batch\r\n    return self.data_queue.get()\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/queues.py\", line 335, in get\r\n    res = self._reader.recv_bytes()\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\r\n    buf = self._recv_bytes(maxlength)\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\r\n    buf = self._recv(4)\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\r\n    chunk = read(handle, remaining)\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 175, in handler\r\n    _error_if_any_worker_fails()\r\nRuntimeError: DataLoader worker (pid 3792) is killed by signal: Illegal instruction.\n<issue_comment>username_3: @username_2 that error is because you are running it on a computer that is too old and does not have SSE4 instruction on CPU.\n<issue_comment>username_1: @username_2 Also, it is more likely due to that your CPU doesn't support the operations done in the dataset.__getitem__.\n<issue_comment>username_2: @username_1 & @username_3,\r\n\r\nSome months ago I made it work in windows10 (I think)\r\nThe my windows machine crashed. So here I am. Sorry\r\n\r\nHere is what I see in cat /proc/cpuinfo:\r\n\r\nprocessor\t: 0\r\nvendor_id\t: AuthenticAMD\r\ncpu family\t: 21\r\nmodel\t\t: 2\r\nmodel name\t: AMD FX(tm)-4300 Quad-Core Processor\r\nstepping\t: 0\r\nmicrocode\t: 0x600081c\r\ncpu MHz\t\t: 1400.000\r\ncache size\t: 2048 KB\r\nphysical id\t: 0\r\nsiblings\t: 4\r\ncore id\t\t: 0\r\ncpu cores\t: 2\r\napicid\t\t: 16\r\ninitial apicid\t: 0\r\nfpu\t\t: yes\r\nfpu_exception\t: yes\r\ncpuid level\t: 13\r\nwp\t\t: yes\r\nflags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf pni pclmulqdq monitor ssse3 fma cx16 **sse4_1 sse4_2** popcnt aes xsave avx f16c lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs xop skinit wdt lwp fma4 tce nodeid_msr tbm topoext perfctr_core perfctr_nb cpb hw_pstate retpoline retpoline_amd rsb_ctxsw vmmcall bmi1 arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold\r\nbugs\t\t: fxsave_leak sysret_ss_attrs null_seg spectre_v1 spectre_v2\r\nbogomips\t: 7634.15\r\nTLB size\t: 1536 4K pages\r\nclflush size\t: 64\r\ncache_alignment\t: 64\r\naddress sizes\t: 48 bits physical, 48 bits virtual\r\npower management: ts ttp tm 100mhzsteps hwpstate cpb eff_freq_ro\r\n\r\n@username_1 how do I check if CPU support dataset.getitem?\n<issue_comment>username_1: @username_2 run with `nThreads=0`\n<issue_comment>username_2: tried it with nThreads=0\r\ndifferent error...\r\nmodel [CycleGANModel] was created\r\ncreate web directory ./checkpoints/horse2zebra_cyclegan/web...\r\nIllegal instruction (core dumped)\n<issue_comment>username_2: I see something I am uneasy about:\r\nsee bold below\r\n\r\nCustomDatasetDataLoader\r\ndataset [UnalignedDataset] was created\r\n/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torchvision-0.2.0-py3.6.egg/torchvision/transforms/transforms.py:156: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\r\n#training images = 1334\r\ncycle_gan\r\n**initialization method [normal]\r\ninitialization method [normal]\r\ninitialization method [normal]\r\ninitialization method [normal]**\n<issue_comment>username_1: @username_2 No it's same error. We just wrap it in a nice way so you can see it even if it's in the workers. \r\n\r\nIsn't that expected? CycleGAN has four networks: D_X D_Y G F .\n<issue_comment>username_1: @username_2 anyways, it confirms my hypothesis that your CPU doesn't support the operation in `dataset.__getitem__`.\n<issue_comment>username_2: Oh... Can you suggest a workaround?\n<issue_comment>username_1: @username_2 You can try to put debug statements in CycleGAN code and see which one isn't supported by your CPU (probably some PIL operations), and search for replacements. Sorry I don't have much else to offer.\n<issue_comment>username_2: @username_1 \r\nThank you sir !<issue_closed>\n<issue_comment>username_1: closed due to inactivity. feel free to reopen if you can reproduce with latest stable (0.4 at the moment of writing).\n<issue_comment>username_4: Still can't get this to work.\r\n\r\nhttps://stackoverflow.com/questions/53449927/runtimeerror-dataloader-worker-is-killed-by-signal-illegal-instruction\r\n\r\nPlease help.","repo":"pytorch/pytorch","issue_id":304991946,"issue_number":5761}
{"question":"Where is the reference to tensor_classes found in the test_torch.py file?","answer":"The reference to tensor_classes is found in the test_print function in the test_torch.py file.","id":13,"text":"@username_2 so I took a look at `test_torch.py`, it looks like the only reference to tensor_classes is in test_print. For other tests usually they are self-contained about which types they are testing against.","context":"<issue_start><issue_comment>Title: fix torch.is_tensor not recognizing HalfTensor\nusername_0: Fix #1922.\n<issue_comment>username_1: This needs some additional changes in the tests\n<issue_comment>username_0: @username_1 Hmm now I am bit curious about the reasons for the failure of the test cases. It says that ne and fill are not implemented for the HalfTensor. From from the source code I think we are actually generating those methods since there isn't any restriction in the cwrap definitions?\n<issue_comment>username_2: @username_0 CPU HalfTensor has no methods implemented except copy and serialization stuff. So almost all tests have to be filtered out for CPU HalfTensor\n<issue_comment>username_3: what we do in TH is basically define an \"is_tensor\" and a \"has_tensor_math\"; then you write the tests based on which of those you are actually testing.\n<issue_comment>username_0: @username_2 so I took a look at `test_torch.py`, it looks like the only reference to tensor_classes is in test_print. For other tests usually they are self-contained about which types they are testing against.\n<issue_comment>username_4: @pytorchbot add to whitelist\n<issue_comment>username_0: After fixing this issue. I wonder why it would be a good idea to have a CPU HalfTensor at all. Since HalfTensor is a subtype of Tensor, we would expect it to support the same interface as Tensor. However, the current implementation of HalfTensor does not allow such behaviour.\n<issue_comment>username_2: on the CPU, HalfTensor is just there for serialization purposes. For example saving a GPU HalfTensor and being able to load the weights back on a CPU-only machine.\r\n\r\nOther than that it has no purpose, as Half is not a native type on CPU and all operations will be very very very slow.","repo":"pytorch/pytorch","issue_id":239293180,"issue_number":1934}
{"question":"Why is it important for a CPU HalfTensor to support the same interface as a Tensor?","answer":"It is important for a CPU HalfTensor to support the same interface as a Tensor because HalfTensor is a subtype of Tensor, and consistency in interface allows for interoperability and ease of use in working with tensors.","id":15,"text":"After fixing this issue. I wonder why it would be a good idea to have a CPU HalfTensor at all. Since HalfTensor is a subtype of Tensor, we would expect it to support the same interface as Tensor. However, the current implementation of HalfTensor does not allow such behaviour.","context":"<issue_start><issue_comment>Title: fix torch.is_tensor not recognizing HalfTensor\nusername_0: Fix #1922.\n<issue_comment>username_1: This needs some additional changes in the tests\n<issue_comment>username_0: @username_1 Hmm now I am bit curious about the reasons for the failure of the test cases. It says that ne and fill are not implemented for the HalfTensor. From from the source code I think we are actually generating those methods since there isn't any restriction in the cwrap definitions?\n<issue_comment>username_2: @username_0 CPU HalfTensor has no methods implemented except copy and serialization stuff. So almost all tests have to be filtered out for CPU HalfTensor\n<issue_comment>username_3: what we do in TH is basically define an \"is_tensor\" and a \"has_tensor_math\"; then you write the tests based on which of those you are actually testing.\n<issue_comment>username_0: @username_2 so I took a look at `test_torch.py`, it looks like the only reference to tensor_classes is in test_print. For other tests usually they are self-contained about which types they are testing against.\n<issue_comment>username_4: @pytorchbot add to whitelist\n<issue_comment>username_0: After fixing this issue. I wonder why it would be a good idea to have a CPU HalfTensor at all. Since HalfTensor is a subtype of Tensor, we would expect it to support the same interface as Tensor. However, the current implementation of HalfTensor does not allow such behaviour.\n<issue_comment>username_2: on the CPU, HalfTensor is just there for serialization purposes. For example saving a GPU HalfTensor and being able to load the weights back on a CPU-only machine.\r\n\r\nOther than that it has no purpose, as Half is not a native type on CPU and all operations will be very very very slow.","repo":"pytorch/pytorch","issue_id":239293180,"issue_number":1934}
{"question":"What is the purpose of HalfTensor on the CPU according to the text?","answer":"HalfTensor on the CPU is used only for serialization purposes, such as saving a GPU HalfTensor and loading weights on a CPU-only machine. It has no other purpose on the CPU due to Half not being a native type, resulting in very slow operations.","id":16,"text":"on the CPU, HalfTensor is just there for serialization purposes. For example saving a GPU HalfTensor and being able to load the weights back on a CPU-only machine.\r\n\r\nOther than that it has no purpose, as Half is not a native type on CPU and all operations will be very very very slow.","context":"<issue_start><issue_comment>Title: fix torch.is_tensor not recognizing HalfTensor\nusername_0: Fix #1922.\n<issue_comment>username_1: This needs some additional changes in the tests\n<issue_comment>username_0: @username_1 Hmm now I am bit curious about the reasons for the failure of the test cases. It says that ne and fill are not implemented for the HalfTensor. From from the source code I think we are actually generating those methods since there isn't any restriction in the cwrap definitions?\n<issue_comment>username_2: @username_0 CPU HalfTensor has no methods implemented except copy and serialization stuff. So almost all tests have to be filtered out for CPU HalfTensor\n<issue_comment>username_3: what we do in TH is basically define an \"is_tensor\" and a \"has_tensor_math\"; then you write the tests based on which of those you are actually testing.\n<issue_comment>username_0: @username_2 so I took a look at `test_torch.py`, it looks like the only reference to tensor_classes is in test_print. For other tests usually they are self-contained about which types they are testing against.\n<issue_comment>username_4: @pytorchbot add to whitelist\n<issue_comment>username_0: After fixing this issue. I wonder why it would be a good idea to have a CPU HalfTensor at all. Since HalfTensor is a subtype of Tensor, we would expect it to support the same interface as Tensor. However, the current implementation of HalfTensor does not allow such behaviour.\n<issue_comment>username_2: on the CPU, HalfTensor is just there for serialization purposes. For example saving a GPU HalfTensor and being able to load the weights back on a CPU-only machine.\r\n\r\nOther than that it has no purpose, as Half is not a native type on CPU and all operations will be very very very slow.","repo":"pytorch/pytorch","issue_id":239293180,"issue_number":1934}
{"question":"What resources can help with integrating Facebook authentication in a React app and handling authentication and authorization in a Facebook app according to the text?","answer":"The user is seeking guidance on handling authentication and authorization in a Facebook app. They have looked through books but found no relevant content. They are asking for recommendations on books, tutorials, or links that can point them in the right direction.","id":17,"text":"In fact I would like to integrate Facebook authentication in my React app, and save user credentials in the database, along with additional data that will be used to authenticate with an API.\r\n\r\nI have no idea about how to handle authentication and authorization in a Facebook app. I went through some few books, but none of them touched that aspect. \r\n\r\nCan you point me to a book, a tutorial, any link ? whatever you think can point me to the right direction.\r\n\r\nThanks in advance.","context":"<issue_start><issue_comment>Title: (Non-issue) Need advise for adding authentication in a react app\nusername_0: In fact I would like to integrate Facebook authentication in my React app, and save user credentials in the database, along with additional data that will be used to authenticate with an API.\r\n\r\nI have no idea about how to handle authentication and authorization in a Facebook app. I went through some few books, but none of them touched that aspect. \r\n\r\nCan you point me to a book, a tutorial, any link ? whatever you think can point me to the right direction.\r\n\r\nThanks in advance.\n<issue_comment>username_1: @username_0  The best technique out there right now is to use Jason Web Token(JWT).\r\nI am sure you can find decent amount of information on that online\n<issue_comment>username_0: @username_1, thanks for your reply. I am fine with the API authentication, I opted for HAWK in HapiJS. So I can include Hawk credentials in my request header, and fetch data from my API.\r\n\r\nThis is what I want to achieve:\r\n\r\n1. User try to access a route in React Router that required Authentication (No idea how to set it)\r\n2. User is redirected to a Login component that allow Facebook login\r\n3. Upon successful login authentication, Facebook credentials are saved in a database, along with hawk credentials, and stored in a cookie\r\n4. User can now authenticate with the API using credentials in his cookie and fetch data.\r\n\r\nFrom what I know about JWT and HAWK, they are only concerned with step 4. My problem is steps 1 to 3.\r\n\r\nIs that possible ? Am I getting something wrong ?\r\n\r\nThanks<issue_closed>","repo":"facebook/react","issue_id":248247023,"issue_number":10395}
{"question":"What is the best technique mentioned in the text for authentication and security?","answer":"Jason Web Token (JWT)","id":18,"text":"@username_0  The best technique out there right now is to use Jason Web Token(JWT).\r\nI am sure you can find decent amount of information on that online","context":"<issue_start><issue_comment>Title: (Non-issue) Need advise for adding authentication in a react app\nusername_0: In fact I would like to integrate Facebook authentication in my React app, and save user credentials in the database, along with additional data that will be used to authenticate with an API.\r\n\r\nI have no idea about how to handle authentication and authorization in a Facebook app. I went through some few books, but none of them touched that aspect. \r\n\r\nCan you point me to a book, a tutorial, any link ? whatever you think can point me to the right direction.\r\n\r\nThanks in advance.\n<issue_comment>username_1: @username_0  The best technique out there right now is to use Jason Web Token(JWT).\r\nI am sure you can find decent amount of information on that online\n<issue_comment>username_0: @username_1, thanks for your reply. I am fine with the API authentication, I opted for HAWK in HapiJS. So I can include Hawk credentials in my request header, and fetch data from my API.\r\n\r\nThis is what I want to achieve:\r\n\r\n1. User try to access a route in React Router that required Authentication (No idea how to set it)\r\n2. User is redirected to a Login component that allow Facebook login\r\n3. Upon successful login authentication, Facebook credentials are saved in a database, along with hawk credentials, and stored in a cookie\r\n4. User can now authenticate with the API using credentials in his cookie and fetch data.\r\n\r\nFrom what I know about JWT and HAWK, they are only concerned with step 4. My problem is steps 1 to 3.\r\n\r\nIs that possible ? Am I getting something wrong ?\r\n\r\nThanks<issue_closed>","repo":"facebook/react","issue_id":248247023,"issue_number":10395}
{"question":"What steps are involved in implementing authentication flow for a React Router route that requires authentication and includes Facebook login, according to the user's requirements?","answer":"The steps involved in implementing the authentication flow for a React Router route according to the user's requirements include: 1. User tries to access a route in React Router that requires Authentication 2. User is redirected to a Login component that allows Facebook login 3. Upon successful Facebook login authentication, Facebook credentials are saved in a database along with HAWK credentials and stored in a cookie for future API authentication","id":19,"text":"@username_1, thanks for your reply. I am fine with the API authentication, I opted for HAWK in HapiJS. So I can include Hawk credentials in my request header, and fetch data from my API.\r\n\r\nThis is what I want to achieve:\r\n\r\n1. User try to access a route in React Router that required Authentication (No idea how to set it)\r\n2. User is redirected to a Login component that allow Facebook login\r\n3. Upon successful login authentication, Facebook credentials are saved in a database, along with hawk credentials, and stored in a cookie\r\n4. User can now authenticate with the API using credentials in his cookie and fetch data.\r\n\r\nFrom what I know about JWT and HAWK, they are only concerned with step 4. My problem is steps 1 to 3.\r\n\r\nIs that possible ? Am I getting something wrong ?\r\n\r\nThanks","context":"<issue_start><issue_comment>Title: (Non-issue) Need advise for adding authentication in a react app\nusername_0: In fact I would like to integrate Facebook authentication in my React app, and save user credentials in the database, along with additional data that will be used to authenticate with an API.\r\n\r\nI have no idea about how to handle authentication and authorization in a Facebook app. I went through some few books, but none of them touched that aspect. \r\n\r\nCan you point me to a book, a tutorial, any link ? whatever you think can point me to the right direction.\r\n\r\nThanks in advance.\n<issue_comment>username_1: @username_0  The best technique out there right now is to use Jason Web Token(JWT).\r\nI am sure you can find decent amount of information on that online\n<issue_comment>username_0: @username_1, thanks for your reply. I am fine with the API authentication, I opted for HAWK in HapiJS. So I can include Hawk credentials in my request header, and fetch data from my API.\r\n\r\nThis is what I want to achieve:\r\n\r\n1. User try to access a route in React Router that required Authentication (No idea how to set it)\r\n2. User is redirected to a Login component that allow Facebook login\r\n3. Upon successful login authentication, Facebook credentials are saved in a database, along with hawk credentials, and stored in a cookie\r\n4. User can now authenticate with the API using credentials in his cookie and fetch data.\r\n\r\nFrom what I know about JWT and HAWK, they are only concerned with step 4. My problem is steps 1 to 3.\r\n\r\nIs that possible ? Am I getting something wrong ?\r\n\r\nThanks<issue_closed>","repo":"facebook/react","issue_id":248247023,"issue_number":10395}
{"question":"What is the purpose of the wrapper added around CUDA tests in the code snippet?","answer":"The purpose of the wrapper is to check if the CUDA memory usage before and after the tests stays constant, in order to test if CUDA methods have memory leaks.","id":20,"text":"With `torch.cuda.memory_allocated` available, we can test if CUDA methods has memory leaks. This PR adds a wrapper around each CUDA tests that checks if the CUDA memory usage before and after stays constant.","context":"<issue_start><issue_comment>Title: Add memory leak check in CUDA tests\nusername_0: With `torch.cuda.memory_allocated` available, we can test if CUDA methods has memory leaks. This PR adds a wrapper around each CUDA tests that checks if the CUDA memory usage before and after stays constant.\n<issue_comment>username_1: this is kinda cool!\n<issue_comment>username_2: Excellent. Have you purposely introduced a memory leak and verified that this test catches it?\n<issue_comment>username_0: @username_2 I added a test for the new method in new commit.\r\n\r\ncc @username_3 for changes to `run_test.py`.\n<issue_comment>username_3: Cool, just make sure the tests actually run in the CI, even when green\n<issue_comment>username_4: Does it have any effect on the perf of the tests?\n<issue_comment>username_0: In local tests and CI test, this new mechanism detected `MaxUnpool3d` (`test_nn.TestNN.test_MaxUnpool3d_net_cuda`) and `MultiLabelMarginCriterion` leaking memory (`test_legacy_nn.TestNN.test_MultiLabelMarginCriterion_1d_cuda`).\n<issue_comment>username_0: @username_4 I don't have concrete numbers on full test suite. But for `test_cuda` on devgpu, the total time went from `20s` to `58s`. The overall test time increase on CUDA tests is likely less than 3 fold because `test_cuda` tests are small and thus the wrapper overhead area more obvious.\r\n\r\nStill, I think there are good reasons to get this in. The above detected leaks are good examples.\r\n\r\nOnce I fix those leaks, I will run the full suite and report the total increased time here.\n<issue_comment>username_2: Not for this PR, but we can do something similar for CPU leaks too; all we need to do is distinguish between tensor allocations (unlikely to stay live across tests) and metadata allocations (we won't track leaks for those.)\r\n\r\nAlthough, when @gchanan has his way all the manual refcounting in TH will go away and it should become a lot harder to make this kind of mistake.\n<issue_comment>username_0: @pytorchbot retest this please\n<issue_comment>username_2: Maybe, to avoid having to fix all of the leaking kernels before we merge this, we should just blacklist all of the currently failing tests and then subsequently fix them.\n<issue_comment>username_0: This PR also fixed the `test_cuda.py` running `TestTorch` problem, so the total test time increase on CUDA 9, CUDNN7, and py3 CI is 27s.\n<issue_comment>username_0: @username_4 @username_1 Require a review! \r\n\r\npr/caffe2-py2-gcc5-ubuntu16.04-test fail is unrelated and is being fixed.\n<issue_comment>username_2: @pytorchbot retest this please\n<issue_comment>username_0: @pytorchbot retest this please\n<issue_comment>username_0: @pytorchbot retest this please\n<issue_comment>username_0: @pytorchbot retest this please\n<issue_comment>username_2: @pytorchbot retest this please\n<issue_comment>username_0: @pytorchbot retest this please","repo":"pytorch/pytorch","issue_id":320115344,"issue_number":7270}
{"question":"What memory leaks were detected in local tests and CI tests related to `MaxUnpool3d` and `MultiLabelMarginCriterion` in PyTorch?","answer":"In local tests and CI tests, memory leaks were detected in `MaxUnpool3d` (`test_nn.TestNN.test_MaxUnpool3d_net_cuda`) and `MultiLabelMarginCriterion` (`test_legacy_nn.TestNN.test_MultiLabelMarginCriterion_1d_cuda`) in PyTorch.","id":26,"text":"In local tests and CI test, this new mechanism detected `MaxUnpool3d` (`test_nn.TestNN.test_MaxUnpool3d_net_cuda`) and `MultiLabelMarginCriterion` leaking memory (`test_legacy_nn.TestNN.test_MultiLabelMarginCriterion_1d_cuda`).","context":"<issue_start><issue_comment>Title: Add memory leak check in CUDA tests\nusername_0: With `torch.cuda.memory_allocated` available, we can test if CUDA methods has memory leaks. This PR adds a wrapper around each CUDA tests that checks if the CUDA memory usage before and after stays constant.\n<issue_comment>username_1: this is kinda cool!\n<issue_comment>username_2: Excellent. Have you purposely introduced a memory leak and verified that this test catches it?\n<issue_comment>username_0: @username_2 I added a test for the new method in new commit.\r\n\r\ncc @username_3 for changes to `run_test.py`.\n<issue_comment>username_3: Cool, just make sure the tests actually run in the CI, even when green\n<issue_comment>username_4: Does it have any effect on the perf of the tests?\n<issue_comment>username_0: In local tests and CI test, this new mechanism detected `MaxUnpool3d` (`test_nn.TestNN.test_MaxUnpool3d_net_cuda`) and `MultiLabelMarginCriterion` leaking memory (`test_legacy_nn.TestNN.test_MultiLabelMarginCriterion_1d_cuda`).\n<issue_comment>username_0: @username_4 I don't have concrete numbers on full test suite. But for `test_cuda` on devgpu, the total time went from `20s` to `58s`. The overall test time increase on CUDA tests is likely less than 3 fold because `test_cuda` tests are small and thus the wrapper overhead area more obvious.\r\n\r\nStill, I think there are good reasons to get this in. The above detected leaks are good examples.\r\n\r\nOnce I fix those leaks, I will run the full suite and report the total increased time here.\n<issue_comment>username_2: Not for this PR, but we can do something similar for CPU leaks too; all we need to do is distinguish between tensor allocations (unlikely to stay live across tests) and metadata allocations (we won't track leaks for those.)\r\n\r\nAlthough, when @gchanan has his way all the manual refcounting in TH will go away and it should become a lot harder to make this kind of mistake.\n<issue_comment>username_0: @pytorchbot retest this please\n<issue_comment>username_2: Maybe, to avoid having to fix all of the leaking kernels before we merge this, we should just blacklist all of the currently failing tests and then subsequently fix them.\n<issue_comment>username_0: This PR also fixed the `test_cuda.py` running `TestTorch` problem, so the total test time increase on CUDA 9, CUDNN7, and py3 CI is 27s.\n<issue_comment>username_0: @username_4 @username_1 Require a review! \r\n\r\npr/caffe2-py2-gcc5-ubuntu16.04-test fail is unrelated and is being fixed.\n<issue_comment>username_2: @pytorchbot retest this please\n<issue_comment>username_0: @pytorchbot retest this please\n<issue_comment>username_0: @pytorchbot retest this please\n<issue_comment>username_0: @pytorchbot retest this please\n<issue_comment>username_2: @pytorchbot retest this please\n<issue_comment>username_0: @pytorchbot retest this please","repo":"pytorch/pytorch","issue_id":320115344,"issue_number":7270}
{"question":"What was the total time increase for `test_cuda` on devgpu according to the text?","answer":"The total time for `test_cuda` on devgpu increased from 20s to 58s.","id":27,"text":"@username_4 I don't have concrete numbers on full test suite. But for `test_cuda` on devgpu, the total time went from `20s` to `58s`. The overall test time increase on CUDA tests is likely less than 3 fold because `test_cuda` tests are small and thus the wrapper overhead area more obvious.\r\n\r\nStill, I think there are good reasons to get this in. The above detected leaks are good examples.\r\n\r\nOnce I fix those leaks, I will run the full suite and report the total increased time here.","context":"<issue_start><issue_comment>Title: Add memory leak check in CUDA tests\nusername_0: With `torch.cuda.memory_allocated` available, we can test if CUDA methods has memory leaks. This PR adds a wrapper around each CUDA tests that checks if the CUDA memory usage before and after stays constant.\n<issue_comment>username_1: this is kinda cool!\n<issue_comment>username_2: Excellent. Have you purposely introduced a memory leak and verified that this test catches it?\n<issue_comment>username_0: @username_2 I added a test for the new method in new commit.\r\n\r\ncc @username_3 for changes to `run_test.py`.\n<issue_comment>username_3: Cool, just make sure the tests actually run in the CI, even when green\n<issue_comment>username_4: Does it have any effect on the perf of the tests?\n<issue_comment>username_0: In local tests and CI test, this new mechanism detected `MaxUnpool3d` (`test_nn.TestNN.test_MaxUnpool3d_net_cuda`) and `MultiLabelMarginCriterion` leaking memory (`test_legacy_nn.TestNN.test_MultiLabelMarginCriterion_1d_cuda`).\n<issue_comment>username_0: @username_4 I don't have concrete numbers on full test suite. But for `test_cuda` on devgpu, the total time went from `20s` to `58s`. The overall test time increase on CUDA tests is likely less than 3 fold because `test_cuda` tests are small and thus the wrapper overhead area more obvious.\r\n\r\nStill, I think there are good reasons to get this in. The above detected leaks are good examples.\r\n\r\nOnce I fix those leaks, I will run the full suite and report the total increased time here.\n<issue_comment>username_2: Not for this PR, but we can do something similar for CPU leaks too; all we need to do is distinguish between tensor allocations (unlikely to stay live across tests) and metadata allocations (we won't track leaks for those.)\r\n\r\nAlthough, when @gchanan has his way all the manual refcounting in TH will go away and it should become a lot harder to make this kind of mistake.\n<issue_comment>username_0: @pytorchbot retest this please\n<issue_comment>username_2: Maybe, to avoid having to fix all of the leaking kernels before we merge this, we should just blacklist all of the currently failing tests and then subsequently fix them.\n<issue_comment>username_0: This PR also fixed the `test_cuda.py` running `TestTorch` problem, so the total test time increase on CUDA 9, CUDNN7, and py3 CI is 27s.\n<issue_comment>username_0: @username_4 @username_1 Require a review! \r\n\r\npr/caffe2-py2-gcc5-ubuntu16.04-test fail is unrelated and is being fixed.\n<issue_comment>username_2: @pytorchbot retest this please\n<issue_comment>username_0: @pytorchbot retest this please\n<issue_comment>username_0: @pytorchbot retest this please\n<issue_comment>username_0: @pytorchbot retest this please\n<issue_comment>username_2: @pytorchbot retest this please\n<issue_comment>username_0: @pytorchbot retest this please","repo":"pytorch/pytorch","issue_id":320115344,"issue_number":7270}
{"question":"What improvement is expected when @gchanan eliminates manual refcounting in TH?","answer":"It should become a lot harder to make memory leak mistakes.","id":28,"text":"Not for this PR, but we can do something similar for CPU leaks too; all we need to do is distinguish between tensor allocations (unlikely to stay live across tests) and metadata allocations (we won't track leaks for those.)\r\n\r\nAlthough, when @gchanan has his way all the manual refcounting in TH will go away and it should become a lot harder to make this kind of mistake.","context":"<issue_start><issue_comment>Title: Add memory leak check in CUDA tests\nusername_0: With `torch.cuda.memory_allocated` available, we can test if CUDA methods has memory leaks. This PR adds a wrapper around each CUDA tests that checks if the CUDA memory usage before and after stays constant.\n<issue_comment>username_1: this is kinda cool!\n<issue_comment>username_2: Excellent. Have you purposely introduced a memory leak and verified that this test catches it?\n<issue_comment>username_0: @username_2 I added a test for the new method in new commit.\r\n\r\ncc @username_3 for changes to `run_test.py`.\n<issue_comment>username_3: Cool, just make sure the tests actually run in the CI, even when green\n<issue_comment>username_4: Does it have any effect on the perf of the tests?\n<issue_comment>username_0: In local tests and CI test, this new mechanism detected `MaxUnpool3d` (`test_nn.TestNN.test_MaxUnpool3d_net_cuda`) and `MultiLabelMarginCriterion` leaking memory (`test_legacy_nn.TestNN.test_MultiLabelMarginCriterion_1d_cuda`).\n<issue_comment>username_0: @username_4 I don't have concrete numbers on full test suite. But for `test_cuda` on devgpu, the total time went from `20s` to `58s`. The overall test time increase on CUDA tests is likely less than 3 fold because `test_cuda` tests are small and thus the wrapper overhead area more obvious.\r\n\r\nStill, I think there are good reasons to get this in. The above detected leaks are good examples.\r\n\r\nOnce I fix those leaks, I will run the full suite and report the total increased time here.\n<issue_comment>username_2: Not for this PR, but we can do something similar for CPU leaks too; all we need to do is distinguish between tensor allocations (unlikely to stay live across tests) and metadata allocations (we won't track leaks for those.)\r\n\r\nAlthough, when @gchanan has his way all the manual refcounting in TH will go away and it should become a lot harder to make this kind of mistake.\n<issue_comment>username_0: @pytorchbot retest this please\n<issue_comment>username_2: Maybe, to avoid having to fix all of the leaking kernels before we merge this, we should just blacklist all of the currently failing tests and then subsequently fix them.\n<issue_comment>username_0: This PR also fixed the `test_cuda.py` running `TestTorch` problem, so the total test time increase on CUDA 9, CUDNN7, and py3 CI is 27s.\n<issue_comment>username_0: @username_4 @username_1 Require a review! \r\n\r\npr/caffe2-py2-gcc5-ubuntu16.04-test fail is unrelated and is being fixed.\n<issue_comment>username_2: @pytorchbot retest this please\n<issue_comment>username_0: @pytorchbot retest this please\n<issue_comment>username_0: @pytorchbot retest this please\n<issue_comment>username_0: @pytorchbot retest this please\n<issue_comment>username_2: @pytorchbot retest this please\n<issue_comment>username_0: @pytorchbot retest this please","repo":"pytorch/pytorch","issue_id":320115344,"issue_number":7270}
{"question":"What strategy is proposed to address the issue of leaking kernels in tests before merging?","answer":"The proposed strategy is to blacklist all of the currently failing tests and then subsequently fix them.","id":30,"text":"Maybe, to avoid having to fix all of the leaking kernels before we merge this, we should just blacklist all of the currently failing tests and then subsequently fix them.","context":"<issue_start><issue_comment>Title: Add memory leak check in CUDA tests\nusername_0: With `torch.cuda.memory_allocated` available, we can test if CUDA methods has memory leaks. This PR adds a wrapper around each CUDA tests that checks if the CUDA memory usage before and after stays constant.\n<issue_comment>username_1: this is kinda cool!\n<issue_comment>username_2: Excellent. Have you purposely introduced a memory leak and verified that this test catches it?\n<issue_comment>username_0: @username_2 I added a test for the new method in new commit.\r\n\r\ncc @username_3 for changes to `run_test.py`.\n<issue_comment>username_3: Cool, just make sure the tests actually run in the CI, even when green\n<issue_comment>username_4: Does it have any effect on the perf of the tests?\n<issue_comment>username_0: In local tests and CI test, this new mechanism detected `MaxUnpool3d` (`test_nn.TestNN.test_MaxUnpool3d_net_cuda`) and `MultiLabelMarginCriterion` leaking memory (`test_legacy_nn.TestNN.test_MultiLabelMarginCriterion_1d_cuda`).\n<issue_comment>username_0: @username_4 I don't have concrete numbers on full test suite. But for `test_cuda` on devgpu, the total time went from `20s` to `58s`. The overall test time increase on CUDA tests is likely less than 3 fold because `test_cuda` tests are small and thus the wrapper overhead area more obvious.\r\n\r\nStill, I think there are good reasons to get this in. The above detected leaks are good examples.\r\n\r\nOnce I fix those leaks, I will run the full suite and report the total increased time here.\n<issue_comment>username_2: Not for this PR, but we can do something similar for CPU leaks too; all we need to do is distinguish between tensor allocations (unlikely to stay live across tests) and metadata allocations (we won't track leaks for those.)\r\n\r\nAlthough, when @gchanan has his way all the manual refcounting in TH will go away and it should become a lot harder to make this kind of mistake.\n<issue_comment>username_0: @pytorchbot retest this please\n<issue_comment>username_2: Maybe, to avoid having to fix all of the leaking kernels before we merge this, we should just blacklist all of the currently failing tests and then subsequently fix them.\n<issue_comment>username_0: This PR also fixed the `test_cuda.py` running `TestTorch` problem, so the total test time increase on CUDA 9, CUDNN7, and py3 CI is 27s.\n<issue_comment>username_0: @username_4 @username_1 Require a review! \r\n\r\npr/caffe2-py2-gcc5-ubuntu16.04-test fail is unrelated and is being fixed.\n<issue_comment>username_2: @pytorchbot retest this please\n<issue_comment>username_0: @pytorchbot retest this please\n<issue_comment>username_0: @pytorchbot retest this please\n<issue_comment>username_0: @pytorchbot retest this please\n<issue_comment>username_2: @pytorchbot retest this please\n<issue_comment>username_0: @pytorchbot retest this please","repo":"pytorch/pytorch","issue_id":320115344,"issue_number":7270}
{"question":"Why is the duplication of code intentional and why is there a desire to avoid dependencies on the `react` package in the context provided?","answer":"The duplication of code is intentional to prevent dependencies on the `react` package and avoid accidentally inlining code from it during bundling, maintaining independence and control over the codebase.","id":55,"text":"Thanks, but we don't want a dependency on this package from `react`, and we also don't want to accidentally inline some code from it during bundling. The duplication is intentional.","context":"<issue_start><issue_comment>Title: removed duplicate code\nusername_0: use `isElement` from `react-is` inside `isValidElement`\n<issue_comment>username_1: Thanks, but we don't want a dependency on this package from `react`, and we also don't want to accidentally inline some code from it during bundling. The duplication is intentional.","repo":"facebook/react","issue_id":650765838,"issue_number":19253}
{"question":"How do Chrome and Safari differ in terms of the triggering order of console logs in a React application?","answer":"Chrome and Safari trigger the console.logs in a different order, where Chrome shows 'useEffect in child' before 'timeout in render', while Safari shows 'timeout in render' before 'useEffect in child'.","id":56,"text":"In chrome and safari, the ```console.log``` was triggered in different order. [Example](https://codesandbox.io/s/amazing-payne-4lmxo) will show different result if opened in different browsers.\r\n\r\nReact version: 16.13.1\r\n\r\n## Steps To Reproduce\r\n\r\n1. Run following code in chrome and safari.\r\n2. Check console log\r\n\r\n```jsx\r\nimport React, { useState, useEffect } from \"react\";\r\n\r\nconst Test = props => {\r\n  useEffect(() => {\r\n    console.log(\"useEffect in child\");\r\n  }, []);\r\n\r\n  return \"Test useEffect\";\r\n};\r\n\r\nconst App = () => {\r\n  setTimeout(() => {\r\n    console.log(\"timeout in render\");\r\n  }, 0);\r\n\r\n  return <Test />;\r\n};\r\n\r\nexport default App\r\n```\r\n\r\nLink to code example:\r\n\r\n[Example](https://codesandbox.io/s/amazing-payne-4lmxo)\r\n\r\n## The current behavior\r\n\r\nThe message is not the same order;\r\n\r\n```jsx\r\n/**\r\n * chrome result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n\r\n/**\r\n * safari result\r\n *\r\n * timeout in render\r\n * useEffect in child\r\n */\r\n```\r\n\r\n## The expected behavior\r\n\r\nThe message should be same;\r\n\r\n```jsx\r\n/**\r\n * chrome result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n\r\n/**\r\n * safari result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n```","context":"<issue_start><issue_comment>Title: Bug: The trigger timing of \"useEffect\" is different in chrome and safari.\nusername_0: In chrome and safari, the ```console.log``` was triggered in different order. [Example](https://codesandbox.io/s/amazing-payne-4lmxo) will show different result if opened in different browsers.\r\n\r\nReact version: 16.13.1\r\n\r\n## Steps To Reproduce\r\n\r\n1. Run following code in chrome and safari.\r\n2. Check console log\r\n\r\n```jsx\r\nimport React, { useState, useEffect } from \"react\";\r\n\r\nconst Test = props => {\r\n  useEffect(() => {\r\n    console.log(\"useEffect in child\");\r\n  }, []);\r\n\r\n  return \"Test useEffect\";\r\n};\r\n\r\nconst App = () => {\r\n  setTimeout(() => {\r\n    console.log(\"timeout in render\");\r\n  }, 0);\r\n\r\n  return <Test />;\r\n};\r\n\r\nexport default App\r\n```\r\n\r\nLink to code example:\r\n\r\n[Example](https://codesandbox.io/s/amazing-payne-4lmxo)\r\n\r\n## The current behavior\r\n\r\nThe message is not the same order;\r\n\r\n```jsx\r\n/**\r\n * chrome result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n\r\n/**\r\n * safari result\r\n *\r\n * timeout in render\r\n * useEffect in child\r\n */\r\n```\r\n\r\n## The expected behavior\r\n\r\nThe message should be same;\r\n\r\n```jsx\r\n/**\r\n * chrome result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n\r\n/**\r\n * safari result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n```<issue_closed>\n<issue_comment>username_0: In chrome and safari, the ```console.log``` was triggered in different order. [Example](https://codesandbox.io/s/amazing-payne-4lmxo) will show different result if opened in different browsers.\r\n\r\nReact version: 16.13.1\r\n\r\n## Steps To Reproduce\r\n\r\n1. Run following code in chrome and safari.\r\n2. Check console log\r\n\r\n```jsx\r\nimport React, { useState, useEffect } from \"react\";\r\n\r\nconst Test = props => {\r\n  useEffect(() => {\r\n    console.log(\"useEffect in child\");\r\n  }, []);\r\n\r\n  return \"Test useEffect\";\r\n};\r\n\r\nconst App = () => {\r\n  setTimeout(() => {\r\n    console.log(\"timeout in render\");\r\n  }, 0);\r\n\r\n  return <Test />;\r\n};\r\n\r\nexport default App\r\n```\r\n\r\nLink to code example:\r\n\r\n[Example](https://codesandbox.io/s/amazing-payne-4lmxo)\r\n\r\n## The current behavior\r\n\r\nThe message is not the same order;\r\n\r\n```jsx\r\n/**\r\n * chrome result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n\r\n/**\r\n * safari result\r\n *\r\n * timeout in render\r\n * useEffect in child\r\n */\r\n```\r\n\r\n## The expected behavior\r\n\r\nThe message should be same;\r\n\r\n```jsx\r\n/**\r\n * chrome result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n\r\n/**\r\n * safari result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n```\n<issue_comment>username_1: You do the side effect in the render method:\r\n```typescript\r\nconst App = () => {\r\n  setTimeout(() => {\r\n    console.log(\"timeout in render\");\r\n  }, 0);\r\n\r\n  return <Test />;\r\n};\r\n```\r\nwhich is not allowed. React doesn't guarantee this kind of orders. you should put `setTimeout` inside an useEffect.\n<issue_comment>username_0: Yes, i agree this is not allowed. But different browsers should have same result.I think react should achieve lifecycle with more stable browser interfaces.\n<issue_comment>username_2: Many reasons why this is not a bug:\r\n\r\n - you are not allowed to make side effects in render. If you do that, the outcome is essentially \"undefined behaviour\", that is, nothing is guaranteed\r\n\r\n- React provides no guarantees about timing of `useEffect` with respect to render anyway.\r\n\r\nSo while this is an inconsistency and it could be fixed, there is not good reason to do so.\n<issue_comment>username_3: It is not supported to call `setTimeout` during rendering. React provides absolutely no guarantees about the order or the number of component render calls  so you can't rely on subtle timing aspects of them.\r\n\r\nIf I wrap it in `useEffect` I don't observe the difference.<issue_closed>","repo":"facebook/react","issue_id":664173077,"issue_number":19438}
{"question":"What does it mean when a class is declared with greater visibility than the type of its field in C++ programming?","answer":"In C++ programming, when a class is declared with greater visibility than the type of its field, it can lead to certain issues and constraints, such as access violations and potential compiler warnings or errors. This situation can arise when a class has stricter visibility settings than the types of its fields, which may conflict with the visibility rules of the programming language.","id":39,"text":"See https://pybind11.readthedocs.io/en/stable/faq.html#someclass-declared-with-greater-visibility-than-the-type-of-its-field-someclass-member-wattributes\r\n\r\nLet's see whether it passes CI","context":"<issue_start><issue_comment>Title: Add hidden visibility to _C.so\nusername_0: See https://pybind11.readthedocs.io/en/stable/faq.html#someclass-declared-with-greater-visibility-than-the-type-of-its-field-someclass-member-wattributes\r\n\r\nLet's see whether it passes CI\n<issue_comment>username_1: wouldn't C++ extensions fail because of this? they require the symbols coming out of libtorch_python and stuff to be global visibility in the current process, or else they fail to load at runtime (but the C++ extensions will compile fine)\n<issue_comment>username_2: UBSAN failure looks real:\r\n\r\n```\r\nJun 21 16:45:39 /var/lib/jenkins/workspace/build/aten/src/ATen/Functions.h:2779:12: runtime error: call to function at::TypeDefault::randn(c10::ArrayRef<long>, c10::TensorOptions const&) through pointer to incorrect function type 'at::Tensor (*)(c10::ArrayRef<long>, const c10::TensorOptions &)'\r\nJun 21 16:45:39 (/opt/conda/lib/python3.6/site-packages/torch/lib/libtorch.so+0xa6b7860): note: at::TypeDefault::randn(c10::ArrayRef<long>, c10::TensorOptions const&) defined here\r\nJun 21 16:45:40     #0 0x7f25f2696882 in torch::randn(c10::ArrayRef<long>, c10::TensorOptions const&) (/opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_python.so+0x11aa882)\r\nJun 21 16:45:40     #1 0x7f25f2694ccc in torch::autograd::dispatch_randn(c10::ArrayRef<long>, c10::TensorOptions const&) (/opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_python.so+0x11a8ccc)\r\nJun 21 16:45:40     #2 0x7f25f258d08d in torch::autograd::THPVariable_randn(_object*, _object*, _object*) (/opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_python.so+0x10a108d)\r\nJun 21 16:45:40     #3 0x558025dc7743 in _PyCFunction_FastCallDict /tmp/build/80754af9/python_1546130271559/work/Objects/methodobject.c:231\r\nJun 21 16:45:40     #4 0x558025e4e42b in call_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4851\r\nJun 21 16:45:40     #5 0x558025e73389 in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3335\r\nJun 21 16:45:40     #6 0x558025e47bfd in _PyEval_EvalCodeWithName /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4166\r\nJun 21 16:45:40     #7 0x558025e48770 in fast_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4992\r\nJun 21 16:45:40     #8 0x558025e4e504 in call_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4872\r\nJun 21 16:45:40     #9 0x558025e73389 in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3335\r\nJun 21 16:45:40     #10 0x558025e49288 in _PyEval_EvalCodeWithName /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4166\r\nJun 21 16:45:40     #11 0x558025e49288 in PyEval_EvalCodeEx /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4187\r\nJun 21 16:45:40     #12 0x558025e4a01b in PyEval_EvalCode /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:731\r\nJun 21 16:45:40     #13 0x558025e70c8a in builtin_exec_impl.isra.11 /tmp/build/80754af9/python_1546130271559/work/Python/bltinmodule.c:983\r\nJun 21 16:45:40     #14 0x558025e70c8a in builtin_exec /tmp/build/80754af9/python_1546130271559/work/Python/clinic/bltinmodule.c.h:283\r\nJun 21 16:45:40     #15 0x558025dca710 in PyCFunction_Call /tmp/build/80754af9/python_1546130271559/work/Objects/methodobject.c:114\r\nJun 21 16:45:40     #16 0x558025e784ac in do_call_core /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:5116\r\nJun 21 16:45:40     #17 0x558025e784ac in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3404\r\nJun 21 16:45:40     #18 0x558025e478e3 in _PyEval_EvalCodeWithName /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4166\r\nJun 21 16:45:40     #19 0x558025e48770 in fast_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4992\r\nJun 21 16:45:40     #20 0x558025e4e504 in call_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4872\r\nJun 21 16:45:40     #21 0x558025e73389 in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3335\r\nJun 21 16:45:40     #22 0x558025e4853a in _PyFunction_FastCall /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4933\r\nJun 21 16:45:40     #23 0x558025e4853a in fast_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4968\r\nJun 21 16:45:40     #24 0x558025e4e504 in call_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4872\r\nJun 21 16:45:40     #25 0x558025e73389 in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3335\r\nJun 21 16:45:40     #26 0x558025e4853a in _PyFunction_FastCall /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4933\r\nJun 21 16:45:40     #27 0x558025e4853a in fast_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4968\r\nJun 21 16:45:40     #28 0x558025e4e504 in call_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4872\r\nJun 21 16:45:40     #29 0x558025e73389 in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3335\r\nJun 21 16:45:40     #30 0x558025e4853a in _PyFunction_FastCall /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4933\r\nJun 21 16:45:40     #31 0x558025e4853a in fast_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4968\r\nJun 21 16:45:40     #32 0x558025e4e504 in call_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4872\r\nJun 21 16:45:40     #33 0x558025e73389 in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3335\r\nJun 21 16:45:40     #34 0x558025e48baa in _PyFunction_FastCall /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4933\r\nJun 21 16:45:40     #35 0x558025e48baa in _PyFunction_FastCallDict /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:5035\r\nJun 21 16:45:40     #36 0x558025dc7b0e in _PyObject_FastCallDict /tmp/build/80754af9/python_1546130271559/work/Objects/abstract.c:2310\r\nJun 21 16:45:40     #37 0x558025e0980f in _PyObject_CallMethodIdObjArgs /tmp/build/80754af9/python_1546130271559/work/Objects/abstract.c:2796\r\nJun 21 16:45:40     #38 0x558025dbeb0f in PyImport_ImportModuleLevelObject /tmp/build/80754af9/python_1546130271559/work/Python/import.c:1578\r\nJun 21 16:45:40     #39 0x558025e75a8a in import_name /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:5245\r\nJun 21 16:45:40     #40 0x558025e75a8a in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:2899\r\nJun 21 16:45:40     #41 0x558025e49288 in _PyEval_EvalCodeWithName /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4166\r\nJun 21 16:45:40     #42 0x558025e49288 in PyEval_EvalCodeEx /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4187\r\nJun 21 16:45:40     #43 0x558025e4a01b in PyEval_EvalCode /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:731\r\nJun 21 16:45:40     #44 0x558025ecc3c3 in run_mod /tmp/build/80754af9/python_1546130271559/work/Python/pythonrun.c:1025\r\nJun 21 16:45:40     #45 0x558025ecc7c0 in PyRun_FileExFlags /tmp/build/80754af9/python_1546130271559/work/Python/pythonrun.c:978\r\nJun 21 16:45:40     #46 0x558025ecc9c2 in PyRun_SimpleFileExFlags /tmp/build/80754af9/python_1546130271559/work/Python/pythonrun.c:419\r\nJun 21 16:45:40     #47 0x558025ed04b2 in run_file /tmp/build/80754af9/python_1546130271559/work/Modules/main.c:340\r\nJun 21 16:45:40     #48 0x558025ed04b2 in Py_Main /tmp/build/80754af9/python_1546130271559/work/Modules/main.c:811\r\nJun 21 16:45:40     #49 0x558025d9902d in main /tmp/build/80754af9/python_1546130271559/work/Programs/python.c:69\r\nJun 21 16:45:40     #50 0x7f26053c082f in __libc_start_main /build/glibc-LK5gWL/glibc-2.23/csu/../csu/libc-start.c:291\r\nJun 21 16:45:40     #51 0x558025e79e0d in _start /home/rdonnelly/mc/conda-bld/compilers_linux-64_1534865402226/work/.build/src/glibc-2.12.2/csu/../sysdeps/x86_64/elf/start.S:103\r\nJun 21 16:45:40 \r\nJun 21 16:45:40 SUMMARY: UndefinedBehaviorSanitizer: undefined-behavior /var/lib/jenkins/workspace/build/aten/src/ATen/Functions.h:2779:12 in \r\nJun 21 16:45:40 Traceback (most recent call last):\r\n```\n<issue_comment>username_2: I guess, ideally, we'd expose symbols that our ours, but not expose anything from pybind11. I don't know how easy that is to do. (Maybe this is a reason not to use pybind11, but that ship has kind of sailed...)\n<issue_comment>username_3: If everything works on MSVC with its default option -- that is, no exposure unless explicitly asked to -- could replacing all occurrences of `__declspec(dllexport)` with something also compatible with `gcc` work? Like something similar to [this](https://stackoverflow.com/a/8258786/1150462)?\n<issue_comment>username_0: Yes, I realized that it needs adjusting annotations.\r\n\r\nAs @username_3 pointed out, we should have already annotated all the right things to make Windows work. But we use windows-only macro in THP_API/THP_CLASS (https://github.com/pytorch/pytorch/blob/master/torch/csrc/THP_export.h). We should do the same thing as for other places - https://github.com/pytorch/pytorch/blob/master/c10/macros/Export.h\r\n\r\nI'll try to get back to it at some point.","repo":"pytorch/pytorch","issue_id":459270796,"issue_number":22076}
{"question":"Why might C++ extensions fail to load at runtime when using libtorch_python symbols?","answer":"C++ extensions may fail to load at runtime when using libtorch_python symbols because they require the symbols to be globally visible in the current process. Without global visibility of these symbols, the extensions will fail to load, even though they may compile without errors.","id":40,"text":"wouldn't C++ extensions fail because of this? they require the symbols coming out of libtorch_python and stuff to be global visibility in the current process, or else they fail to load at runtime (but the C++ extensions will compile fine)","context":"<issue_start><issue_comment>Title: Add hidden visibility to _C.so\nusername_0: See https://pybind11.readthedocs.io/en/stable/faq.html#someclass-declared-with-greater-visibility-than-the-type-of-its-field-someclass-member-wattributes\r\n\r\nLet's see whether it passes CI\n<issue_comment>username_1: wouldn't C++ extensions fail because of this? they require the symbols coming out of libtorch_python and stuff to be global visibility in the current process, or else they fail to load at runtime (but the C++ extensions will compile fine)\n<issue_comment>username_2: UBSAN failure looks real:\r\n\r\n```\r\nJun 21 16:45:39 /var/lib/jenkins/workspace/build/aten/src/ATen/Functions.h:2779:12: runtime error: call to function at::TypeDefault::randn(c10::ArrayRef<long>, c10::TensorOptions const&) through pointer to incorrect function type 'at::Tensor (*)(c10::ArrayRef<long>, const c10::TensorOptions &)'\r\nJun 21 16:45:39 (/opt/conda/lib/python3.6/site-packages/torch/lib/libtorch.so+0xa6b7860): note: at::TypeDefault::randn(c10::ArrayRef<long>, c10::TensorOptions const&) defined here\r\nJun 21 16:45:40     #0 0x7f25f2696882 in torch::randn(c10::ArrayRef<long>, c10::TensorOptions const&) (/opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_python.so+0x11aa882)\r\nJun 21 16:45:40     #1 0x7f25f2694ccc in torch::autograd::dispatch_randn(c10::ArrayRef<long>, c10::TensorOptions const&) (/opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_python.so+0x11a8ccc)\r\nJun 21 16:45:40     #2 0x7f25f258d08d in torch::autograd::THPVariable_randn(_object*, _object*, _object*) (/opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_python.so+0x10a108d)\r\nJun 21 16:45:40     #3 0x558025dc7743 in _PyCFunction_FastCallDict /tmp/build/80754af9/python_1546130271559/work/Objects/methodobject.c:231\r\nJun 21 16:45:40     #4 0x558025e4e42b in call_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4851\r\nJun 21 16:45:40     #5 0x558025e73389 in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3335\r\nJun 21 16:45:40     #6 0x558025e47bfd in _PyEval_EvalCodeWithName /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4166\r\nJun 21 16:45:40     #7 0x558025e48770 in fast_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4992\r\nJun 21 16:45:40     #8 0x558025e4e504 in call_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4872\r\nJun 21 16:45:40     #9 0x558025e73389 in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3335\r\nJun 21 16:45:40     #10 0x558025e49288 in _PyEval_EvalCodeWithName /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4166\r\nJun 21 16:45:40     #11 0x558025e49288 in PyEval_EvalCodeEx /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4187\r\nJun 21 16:45:40     #12 0x558025e4a01b in PyEval_EvalCode /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:731\r\nJun 21 16:45:40     #13 0x558025e70c8a in builtin_exec_impl.isra.11 /tmp/build/80754af9/python_1546130271559/work/Python/bltinmodule.c:983\r\nJun 21 16:45:40     #14 0x558025e70c8a in builtin_exec /tmp/build/80754af9/python_1546130271559/work/Python/clinic/bltinmodule.c.h:283\r\nJun 21 16:45:40     #15 0x558025dca710 in PyCFunction_Call /tmp/build/80754af9/python_1546130271559/work/Objects/methodobject.c:114\r\nJun 21 16:45:40     #16 0x558025e784ac in do_call_core /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:5116\r\nJun 21 16:45:40     #17 0x558025e784ac in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3404\r\nJun 21 16:45:40     #18 0x558025e478e3 in _PyEval_EvalCodeWithName /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4166\r\nJun 21 16:45:40     #19 0x558025e48770 in fast_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4992\r\nJun 21 16:45:40     #20 0x558025e4e504 in call_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4872\r\nJun 21 16:45:40     #21 0x558025e73389 in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3335\r\nJun 21 16:45:40     #22 0x558025e4853a in _PyFunction_FastCall /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4933\r\nJun 21 16:45:40     #23 0x558025e4853a in fast_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4968\r\nJun 21 16:45:40     #24 0x558025e4e504 in call_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4872\r\nJun 21 16:45:40     #25 0x558025e73389 in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3335\r\nJun 21 16:45:40     #26 0x558025e4853a in _PyFunction_FastCall /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4933\r\nJun 21 16:45:40     #27 0x558025e4853a in fast_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4968\r\nJun 21 16:45:40     #28 0x558025e4e504 in call_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4872\r\nJun 21 16:45:40     #29 0x558025e73389 in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3335\r\nJun 21 16:45:40     #30 0x558025e4853a in _PyFunction_FastCall /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4933\r\nJun 21 16:45:40     #31 0x558025e4853a in fast_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4968\r\nJun 21 16:45:40     #32 0x558025e4e504 in call_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4872\r\nJun 21 16:45:40     #33 0x558025e73389 in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3335\r\nJun 21 16:45:40     #34 0x558025e48baa in _PyFunction_FastCall /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4933\r\nJun 21 16:45:40     #35 0x558025e48baa in _PyFunction_FastCallDict /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:5035\r\nJun 21 16:45:40     #36 0x558025dc7b0e in _PyObject_FastCallDict /tmp/build/80754af9/python_1546130271559/work/Objects/abstract.c:2310\r\nJun 21 16:45:40     #37 0x558025e0980f in _PyObject_CallMethodIdObjArgs /tmp/build/80754af9/python_1546130271559/work/Objects/abstract.c:2796\r\nJun 21 16:45:40     #38 0x558025dbeb0f in PyImport_ImportModuleLevelObject /tmp/build/80754af9/python_1546130271559/work/Python/import.c:1578\r\nJun 21 16:45:40     #39 0x558025e75a8a in import_name /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:5245\r\nJun 21 16:45:40     #40 0x558025e75a8a in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:2899\r\nJun 21 16:45:40     #41 0x558025e49288 in _PyEval_EvalCodeWithName /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4166\r\nJun 21 16:45:40     #42 0x558025e49288 in PyEval_EvalCodeEx /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4187\r\nJun 21 16:45:40     #43 0x558025e4a01b in PyEval_EvalCode /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:731\r\nJun 21 16:45:40     #44 0x558025ecc3c3 in run_mod /tmp/build/80754af9/python_1546130271559/work/Python/pythonrun.c:1025\r\nJun 21 16:45:40     #45 0x558025ecc7c0 in PyRun_FileExFlags /tmp/build/80754af9/python_1546130271559/work/Python/pythonrun.c:978\r\nJun 21 16:45:40     #46 0x558025ecc9c2 in PyRun_SimpleFileExFlags /tmp/build/80754af9/python_1546130271559/work/Python/pythonrun.c:419\r\nJun 21 16:45:40     #47 0x558025ed04b2 in run_file /tmp/build/80754af9/python_1546130271559/work/Modules/main.c:340\r\nJun 21 16:45:40     #48 0x558025ed04b2 in Py_Main /tmp/build/80754af9/python_1546130271559/work/Modules/main.c:811\r\nJun 21 16:45:40     #49 0x558025d9902d in main /tmp/build/80754af9/python_1546130271559/work/Programs/python.c:69\r\nJun 21 16:45:40     #50 0x7f26053c082f in __libc_start_main /build/glibc-LK5gWL/glibc-2.23/csu/../csu/libc-start.c:291\r\nJun 21 16:45:40     #51 0x558025e79e0d in _start /home/rdonnelly/mc/conda-bld/compilers_linux-64_1534865402226/work/.build/src/glibc-2.12.2/csu/../sysdeps/x86_64/elf/start.S:103\r\nJun 21 16:45:40 \r\nJun 21 16:45:40 SUMMARY: UndefinedBehaviorSanitizer: undefined-behavior /var/lib/jenkins/workspace/build/aten/src/ATen/Functions.h:2779:12 in \r\nJun 21 16:45:40 Traceback (most recent call last):\r\n```\n<issue_comment>username_2: I guess, ideally, we'd expose symbols that our ours, but not expose anything from pybind11. I don't know how easy that is to do. (Maybe this is a reason not to use pybind11, but that ship has kind of sailed...)\n<issue_comment>username_3: If everything works on MSVC with its default option -- that is, no exposure unless explicitly asked to -- could replacing all occurrences of `__declspec(dllexport)` with something also compatible with `gcc` work? Like something similar to [this](https://stackoverflow.com/a/8258786/1150462)?\n<issue_comment>username_0: Yes, I realized that it needs adjusting annotations.\r\n\r\nAs @username_3 pointed out, we should have already annotated all the right things to make Windows work. But we use windows-only macro in THP_API/THP_CLASS (https://github.com/pytorch/pytorch/blob/master/torch/csrc/THP_export.h). We should do the same thing as for other places - https://github.com/pytorch/pytorch/blob/master/c10/macros/Export.h\r\n\r\nI'll try to get back to it at some point.","repo":"pytorch/pytorch","issue_id":459270796,"issue_number":22076}
{"question":"What should be considered when comparing performance in TorchScript according to the text?","answer":"When comparing performance in TorchScript, it is crucial to allow for a compilation time and runtime optimization warmup for the input, as the first time JIT/TorchScript sees an input, it will specialize and optimize against that input.","id":178,"text":"@username_0 Also, it seems like you are only timing it in 3 iterations, and the perf numbers seems including the compilation time. The first time JIT/TorchScript sees an input it will specialize and do optimizations against that input, so when comparing performance please allow a compilation time and runtime optimization warmup for the input.","context":"<issue_start><issue_comment>Title: TorchScript Poor CUDA Performance \nusername_0: The performance of a TorchScript that handles the computation in the [Adam Optimizer](https://github.com/pytorch/pytorch/blob/master/torch/optim/adam.py) is worse than using the native PyTorch operations on CUDA. \r\n\r\n### Background\r\nI am trying to improve the GPU performance of the [Adam Optimizer](https://github.com/pytorch/pytorch/blob/master/torch/optim/adam.py) since it is the bottleneck in one of our workflows. \r\n\r\n### Results\r\nI have written a standalone script that compares PyTorch native tensor operations, TorchScript, and Numba (three runs each):\r\n```\r\nnative :  0.0506\r\nnative :  0.0322\r\nnative :  0.0322\r\nTScript:  0.1917\r\nTScript:  0.0442\r\nTScript:  0.0442\r\nnumba  :  0.1638\r\nnumba  :  0.0145\r\nnumba  :  0.0143\r\n```\r\nI suspect the problem is that TorchScript does not fuse all operations into a single CUDA. It generates two CUDA kernels (see the `PYTORCH_FUSION_DEBUG` output below)\r\n\r\n### Standalone Script\r\n```python\r\nimport math\r\nimport torch\r\nimport time\r\nimport random\r\nimport sys\r\n\r\ndef kernel_native(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n    exp_avg.mul_(beta1).add_(1 - beta1, grad)\r\n    exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\r\n    denom = exp_avg_sq.sqrt().add_(eps)\r\n    param.addcdiv_(-step_size, exp_avg, denom)   \r\n\r\n@torch.jit.script\r\ndef kernel_jit_script(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n    exp_avg.mul_(beta1)\r\n    exp_avg.add_((1 - beta1) + grad)\r\n    exp_avg_sq.mul_(beta2)\r\n    exp_avg_sq.add_((1 - beta2)*(grad*grad))\r\n    denom = exp_avg_sq.sqrt().add_(eps)\r\n    param += -step_size * (exp_avg / denom)\r\n\r\nfrom numba import cuda\r\n@cuda.jit\r\ndef kernel_numba_cuda(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    i = cuda.grid(1)\r\n    if i > grad.size:\r\n        return\r\n\r\n    exp_avg[i] = exp_avg[i] * beta1 + (1 - beta1) + grad[i]\r\n    exp_avg_sq[i] = exp_avg_sq[i] * beta2 + (1 - beta2) *grad[i]*grad[i]\r\n    denom = math.sqrt(exp_avg_sq[i]) + eps\r\n    param[i] += -step_size * (exp_avg[i] / denom)\r\n\r\ndef kernel_numba(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n\r\n    import ctypes\r\n    import numpy as np\r\n    def get_devicendarray(t):\r\n        assert t.type() == 'torch.cuda.FloatTensor'\r\n        numpy_dtype = np.float32\r\n        itemsize = 4\r\n\r\n        ctx = cuda.cudadrv.driver.driver.get_context()\r\n        mp = cuda.cudadrv.driver.MemoryPointer(ctx, ctypes.c_ulong(t.data_ptr()), t.numel()*itemsize)\r\n        return cuda.cudadrv.devicearray.DeviceNDArray(t.numel(), itemsize, numpy_dtype(), gpu_data=mp, stream=torch.cuda.current_stream().cuda_stream)\r\n\r\n    numba_param = get_devicendarray(param)\r\n    numba_grad = get_devicendarray(grad)\r\n    numba_exp_avg = get_devicendarray(exp_avg)\r\n    numba_exp_avg_sq = get_devicendarray(exp_avg_sq)\r\n    threadsperblock = 32\r\n    blockspergrid = (param.numel() + (threadsperblock - 1)) // threadsperblock\r\n    kernel_numba_cuda[blockspergrid, threadsperblock](numba_param, numba_grad, numba_exp_avg, numba_exp_avg_sq, beta1, beta2, step_size, eps)\r\n\r\nkernels = [(\"native \", kernel_native), (\"TScript\", kernel_jit_script), (\"numba  \", kernel_numba)]\r\n[Truncated]\n      size_t t1_dimIndex0 = t1_linearIndex ;\r\n      t1_offset += t1_dimIndex0 ;\r\n      IndexType t3_offset = 0;\r\n      IndexType t3_linearIndex = linearIndex;\r\n      \r\n      //printf(\"tensor t3 sizes[0] = %d, strides[0] = %d\\n\", t3.sizes[0],t3.strides[0]);\r\n      size_t t3_dimIndex0 = t3_linearIndex ;\r\n      t3_offset += t3_dimIndex0 ;\r\n      \r\n      // calculate the results\r\n      float n0 = __ldg(&t0.data[t0_offset]);\r\n      float n1 = __ldg(&t1.data[t1_offset]);\r\n      double n2 = s2;\r\n      float n3 = n0 / n1;\r\n      float n4 = n3 * ((float) n2);\r\n      t3.data[t3_offset] = n4;\r\n      \r\n    }\r\n}\r\n```\n<issue_comment>username_1: @username_0 can you try removing all the in-place operations in your scripted version?\r\nIIRC in-place ops block fusion.\r\n\r\nI was btw able to fuse most of sparse adam by removing those in-place ops, see https://github.com/pytorch/pytorch/pull/23522#issuecomment-516952462\n<issue_comment>username_2: cc: @username_3 and also relevant WIP PR: [JIT version of Adagrad](https://github.com/pytorch/pytorch/pull/23640)\n<issue_comment>username_3: Yeah the CUDA fusion does not support fusing in-place operations yet, Can you try the suggestion by @username_1 to see if you get a better perf?\n<issue_comment>username_3: @username_0 Also, it seems like you are only timing it in 3 iterations, and the perf numbers seems including the compilation time. The first time JIT/TorchScript sees an input it will specialize and do optimizations against that input, so when comparing performance please allow a compilation time and runtime optimization warmup for the input.\n<issue_comment>username_4: @username_3 That made a big difference to my traced module too. Any other tricks like this? It slows down the non-JIT version, however.\n<issue_comment>username_3: @username_4 That's pretty much all, let me summarize:\r\n\r\n1. JIT fusion does not support in-place operation fusion and reduction operation yet.\r\n2. we did the fusion code generation at the first time we see an input that the shape we haven't seen before (like if the number of dimensions are different), this will help us generate faster cuda kernels. \r\n\r\nSo the TorchScript compilation time will be gone after compile and the first run :)\n<issue_comment>username_0: Thanks for the feedback!\r\n\r\nI have removed all in-place operations and added three kernel implementations:\r\n```python\r\n\r\nimport math\r\nimport torch\r\nimport time\r\nimport random\r\nimport sys\r\n\r\ndef kernel_native(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n    exp_avg.mul_(beta1).add_(1 - beta1, grad)\r\n    exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\r\n    denom = exp_avg_sq.sqrt().add_(eps)\r\n    param.addcdiv_(-step_size, exp_avg, denom)   \r\n\r\n@torch.jit.script\r\ndef kernel_jit_script1(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n    exp_avg *= beta1\r\n    exp_avg += (1 - beta1) + grad\r\n    exp_avg_sq *= beta2\r\n    exp_avg_sq += (1 - beta2)*(grad*grad)\r\n    param += -step_size * (exp_avg / exp_avg_sq.sqrt() + eps)\r\n\r\n@torch.jit.script\r\ndef kernel_jit_script2(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n    exp_avg[:] = exp_avg * beta1 + (1 - beta1) + grad\r\n    exp_avg_sq[:] = exp_avg_sq * beta2 + (1 - beta2)*(grad*grad)\r\n    param[:] = param -step_size * (exp_avg / exp_avg_sq.sqrt() + eps)\r\n\r\n@torch.jit.script\r\ndef kernel_jit_script3(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> Tuple[Tensor, Tensor, Tensor, Tensor]\r\n    exp_avg = exp_avg * beta1 + (1 - beta1) + grad\r\n    exp_avg_sq = exp_avg_sq * beta2 + (1 - beta2)*(grad*grad)\r\n    param = param -step_size * (exp_avg / exp_avg_sq.sqrt() + eps)\r\n    return (param, grad, exp_avg, exp_avg_sq)\r\n\r\n\r\nfrom numba import cuda\r\n@cuda.jit\r\ndef kernel_numba_cuda(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    i = cuda.grid(1)\r\n    if i > grad.size:\r\n        return\r\n\r\n    exp_avg[i] = exp_avg[i] * beta1 + (1 - beta1) + grad[i]\r\n    exp_avg_sq[i] = exp_avg_sq[i] * beta2 + (1 - beta2) *grad[i]*grad[i]\r\n    denom = math.sqrt(exp_avg_sq[i]) + eps\r\n    param[i] += -step_size * (exp_avg[i] / denom)\r\n\r\ndef kernel_numba(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n\r\n    import ctypes\r\n    import numpy as np\r\n    def get_devicendarray(t):\r\n        assert t.type() == 'torch.cuda.FloatTensor'\r\n        numpy_dtype = np.float32\r\n        itemsize = 4\r\n\r\n        ctx = cuda.cudadrv.driver.driver.get_context()\r\n        mp = cuda.cudadrv.driver.MemoryPointer(ctx, ctypes.c_ulong(t.data_ptr()), t.numel()*itemsize)\r\n        return cuda.cudadrv.devicearray.DeviceNDArray(t.numel(), itemsize, numpy_dtype(), gpu_data=mp, stream=torch.cuda.current_stream().cuda_stream)\r\n\r\n    numba_param = get_devicendarray(param)\r\n    numba_grad = get_devicendarray(grad)\r\n    numba_exp_avg = get_devicendarray(exp_avg)\r\n    numba_exp_avg_sq = get_devicendarray(exp_avg_sq)\r\n    threadsperblock = 32\r\n    blockspergrid = (param.numel() + (threadsperblock - 1)) // threadsperblock\r\n    kernel_numba_cuda[blockspergrid, threadsperblock](numba_param, numba_grad, numba_exp_avg, numba_exp_avg_sq, beta1, beta2, step_size, eps)\r\n\r\nkernels = [(\"native \", kernel_native), (\"numba  \", kernel_numba)]\r\nshape = (4194304*64,)\r\nparam, grad, exp_avg, exp_avg_sq = [torch.rand(shape).to(\"cuda\") for x in range(4)]\r\n[Truncated]\n        res = func(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps)\r\n        if res is not None:\r\n            param[:], _, exp_avg[:], exp_avg_sq[:] = res\r\n        torch.cuda.synchronize()\r\n        t2 = time.time()\r\n        print(\"%s: \" % name, t2-t1)\r\n```\r\nThe results are:\r\n```\r\nnative :  0.0323\r\nnumba  :  0.0147\r\nTScript1:  0.0369\r\nTScript2:  0.0352\r\nTScript3:  0.0280\r\n```\r\nTScript3 is now better than native but still far from numba. TScript3 is now generating a single CUDA kernel but the back-copying of the result now dominates the timing. If I remove the line `param[:], _, exp_avg[:], exp_avg_sq[:] = res` the performance is almost on pair with numba:\r\n```\r\nTScript3 (no back-copy):  0.01664\r\n```\r\nThe problem is I really do need to update the values :/\n<issue_comment>username_5: cc @mruberry","repo":"pytorch/pytorch","issue_id":475728990,"issue_number":23655}
{"question":"What type of failure is indicated in the given text chunk related to the 'at::TypeDefault::randn' function call?","answer":"UBSAN failure","id":41,"text":"UBSAN failure looks real:\r\n\r\n```\r\nJun 21 16:45:39 /var/lib/jenkins/workspace/build/aten/src/ATen/Functions.h:2779:12: runtime error: call to function at::TypeDefault::randn(c10::ArrayRef<long>, c10::TensorOptions const&) through pointer to incorrect function type 'at::Tensor (*)(c10::ArrayRef<long>, const c10::TensorOptions &)'\r\nJun 21 16:45:39 (/opt/conda/lib/python3.6/site-packages/torch/lib/libtorch.so+0xa6b7860): note: at::TypeDefault::randn(c10::ArrayRef<long>, c10::TensorOptions const&) defined here\r\nJun 21 16:45:40     #0 0x7f25f2696882 in torch::randn(c10::ArrayRef<long>, c10::TensorOptions const&) (/opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_python.so+0x11aa882)\r\nJun 21 16:45:40     #1 0x7f25f2694ccc in torch::autograd::dispatch_randn(c10::ArrayRef<long>, c10::TensorOptions const&) (/opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_python.so+0x11a8ccc)\r\nJun 21 16:45:40     #2 0x7f25f258d08d in torch::autograd::THPVariable_randn(_object*, _object*, _object*) (/opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_python.so+0x10a108d)\r\nJun 21 16:45:40     #3 0x558025dc7743 in _PyCFunction_FastCallDict /tmp/build/80754af9/python_1546130271559/work/Objects/methodobject.c:231\r\nJun 21 16:45:40     #4 0x558025e4e42b in call_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4851\r\nJun 21 16:45:40     #5 0x558025e73389 in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3335\r\nJun 21 16:45:40     #6 0x558025e47bfd in _PyEval_EvalCodeWithName /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4166\r\nJun 21 16:45:40     #7 0x558025e48770 in fast_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4992\r\nJun 21 16:45:40     #8 0x558025e4e504 in call_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4872\r\nJun 21 16:45:40     #9 0x558025e73389 in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3335\r\nJun 21 16:45:40     #10 0x558025e49288 in _PyEval_EvalCodeWithName /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4166\r\nJun 21 16:45:40     #11 0x558025e49288 in PyEval_EvalCodeEx /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4187\r\nJun 21 16:45:40     #12 0x558025e4a01b in PyEval_EvalCode /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:731\r\nJun 21 16:45:40     #13 0x558025e70c8a in builtin_exec_impl.isra.11 /tmp/build/80754af9/python_1546130271559/work/Python/bltinmodule.c:983\r\nJun 21 16:45:40     #14 0x558025e70c8a in builtin_exec /tmp/build/80754af9/python_1546130271559/work/Python/clinic/bltinmodule.c.h:283\r\nJun 21 16:45:40     #15 0x558025dca710 in PyCFunction_Call /tmp/build/80754af9/python_1546130271559/work/Objects/methodobject.c:114\r\nJun 21 16:45:40     #16 0x558025e784ac in do_call_core /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:5116\r\nJun 21 16:45:40     #17 0x558025e784ac in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3404\r\nJun 21 16:45:40     #18 0x558025e478e3 in _PyEval_EvalCodeWithName /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4166\r\nJun 21 16:45:40     #19 0x558025e48770 in fast_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4992\r\nJun 21 16:45:40     #20 0x558025e4e504 in call_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4872\r\nJun 21 16:45:40     #21 0x558025e73389 in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3335\r\nJun 21 16:45:40     #22 0x558025e4853a in _PyFunction_FastCall /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4933\r\nJun 21 16:45:40     #23 0x558025e4853a in fast_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4968\r\nJun 21 16:45:40     #24 0x558025e4e504 in call_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4872\r\nJun 21 16:45:40     #25 0x558025e73389 in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3335\r\nJun 21 16:45:40     #26 0x558025e4853a in _PyFunction_FastCall /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4933\r\nJun 21 16:45:40     #27 0x558025e4853a in fast_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4968\r\nJun 21 16:45:40     #28 0x558025e4e504 in call_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4872\r\nJun 21 16:45:40     #29 0x558025e73389 in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3335\r\nJun 21 16:45:40     #30 0x558025e4853a in _PyFunction_FastCall /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4933\r\nJun 21 16:45:40     #31 0x558025e4853a in fast_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4968\r\nJun 21 16:45:40     #32 0x558025e4e504 in call_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4872\r\nJun 21 16:45:40     #33 0x558025e73389 in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3335\r\nJun 21 16:45:40     #34 0x558025e48baa in _PyFunction_FastCall /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4933\r\nJun 21 16:45:40     #35 0x558025e48baa in _PyFunction_FastCallDict /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:5035\r\nJun 21 16:45:40     #36 0x558025dc7b0e in _PyObject_FastCallDict /tmp/build/80754af9/python_1546130271559/work/Objects/abstract.c:2310\r\nJun 21 16:45:40     #37 0x558025e0980f in _PyObject_CallMethodIdObjArgs /tmp/build/80754af9/python_1546130271559/work/Objects/abstract.c:2796\r\nJun 21 16:45:40     #38 0x558025dbeb0f in PyImport_ImportModuleLevelObject /tmp/build/80754af9/python_1546130271559/work/Python/import.c:1578\r\nJun 21 16:45:40     #39 0x558025e75a8a in import_name /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:5245\r\nJun 21 16:45:40     #40 0x558025e75a8a in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:2899\r\nJun 21 16:45:40     #41 0x558025e49288 in _PyEval_EvalCodeWithName /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4166\r\nJun 21 16:45:40     #42 0x558025e49288 in PyEval_EvalCodeEx /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4187\r\nJun 21 16:45:40     #43 0x558025e4a01b in PyEval_EvalCode /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:731\r\nJun 21 16:45:40     #44 0x558025ecc3c3 in run_mod /tmp/build/80754af9/python_1546130271559/work/Python/pythonrun.c:1025\r\nJun 21 16:45:40     #45 0x558025ecc7c0 in PyRun_FileExFlags /tmp/build/80754af9/python_1546130271559/work/Python/pythonrun.c:978\r\nJun 21 16:45:40     #46 0x558025ecc9c2 in PyRun_SimpleFileExFlags /tmp/build/80754af9/python_1546130271559/work/Python/pythonrun.c:419\r\nJun 21 16:45:40     #47 0x558025ed04b2 in run_file /tmp/build/80754af9/python_1546130271559/work/Modules/main.c:340\r\nJun 21 16:45:40     #48 0x558025ed04b2 in Py_Main /tmp/build/80754af9/python_1546130271559/work/Modules/main.c:811\r\nJun 21 16:45:40     #49 0x558025d9902d in main /tmp/build/80754af9/python_1546130271559/work/Programs/python.c:69\r\nJun 21 16:45:40     #50 0x7f26053c082f in __libc_start_main /build/glibc-LK5gWL/glibc-2.23/csu/../csu/libc-start.c:291\r\nJun 21 16:45:40     #51 0x558025e79e0d in _start /home/rdonnelly/mc/conda-bld/compilers_linux-64_1534865402226/work/.build/src/glibc-2.12.2/csu/../sysdeps/x86_64/elf/start.S:103\r\nJun 21 16:45:40 \r\nJun 21 16:45:40 SUMMARY: UndefinedBehaviorSanitizer: undefined-behavior /var/lib/jenkins/workspace/build/aten/src/ATen/Functions.h:2779:12 in \r\nJun 21 16:45:40 Traceback (most recent call last):\r\n```","context":"<issue_start><issue_comment>Title: Add hidden visibility to _C.so\nusername_0: See https://pybind11.readthedocs.io/en/stable/faq.html#someclass-declared-with-greater-visibility-than-the-type-of-its-field-someclass-member-wattributes\r\n\r\nLet's see whether it passes CI\n<issue_comment>username_1: wouldn't C++ extensions fail because of this? they require the symbols coming out of libtorch_python and stuff to be global visibility in the current process, or else they fail to load at runtime (but the C++ extensions will compile fine)\n<issue_comment>username_2: UBSAN failure looks real:\r\n\r\n```\r\nJun 21 16:45:39 /var/lib/jenkins/workspace/build/aten/src/ATen/Functions.h:2779:12: runtime error: call to function at::TypeDefault::randn(c10::ArrayRef<long>, c10::TensorOptions const&) through pointer to incorrect function type 'at::Tensor (*)(c10::ArrayRef<long>, const c10::TensorOptions &)'\r\nJun 21 16:45:39 (/opt/conda/lib/python3.6/site-packages/torch/lib/libtorch.so+0xa6b7860): note: at::TypeDefault::randn(c10::ArrayRef<long>, c10::TensorOptions const&) defined here\r\nJun 21 16:45:40     #0 0x7f25f2696882 in torch::randn(c10::ArrayRef<long>, c10::TensorOptions const&) (/opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_python.so+0x11aa882)\r\nJun 21 16:45:40     #1 0x7f25f2694ccc in torch::autograd::dispatch_randn(c10::ArrayRef<long>, c10::TensorOptions const&) (/opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_python.so+0x11a8ccc)\r\nJun 21 16:45:40     #2 0x7f25f258d08d in torch::autograd::THPVariable_randn(_object*, _object*, _object*) (/opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_python.so+0x10a108d)\r\nJun 21 16:45:40     #3 0x558025dc7743 in _PyCFunction_FastCallDict /tmp/build/80754af9/python_1546130271559/work/Objects/methodobject.c:231\r\nJun 21 16:45:40     #4 0x558025e4e42b in call_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4851\r\nJun 21 16:45:40     #5 0x558025e73389 in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3335\r\nJun 21 16:45:40     #6 0x558025e47bfd in _PyEval_EvalCodeWithName /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4166\r\nJun 21 16:45:40     #7 0x558025e48770 in fast_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4992\r\nJun 21 16:45:40     #8 0x558025e4e504 in call_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4872\r\nJun 21 16:45:40     #9 0x558025e73389 in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3335\r\nJun 21 16:45:40     #10 0x558025e49288 in _PyEval_EvalCodeWithName /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4166\r\nJun 21 16:45:40     #11 0x558025e49288 in PyEval_EvalCodeEx /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4187\r\nJun 21 16:45:40     #12 0x558025e4a01b in PyEval_EvalCode /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:731\r\nJun 21 16:45:40     #13 0x558025e70c8a in builtin_exec_impl.isra.11 /tmp/build/80754af9/python_1546130271559/work/Python/bltinmodule.c:983\r\nJun 21 16:45:40     #14 0x558025e70c8a in builtin_exec /tmp/build/80754af9/python_1546130271559/work/Python/clinic/bltinmodule.c.h:283\r\nJun 21 16:45:40     #15 0x558025dca710 in PyCFunction_Call /tmp/build/80754af9/python_1546130271559/work/Objects/methodobject.c:114\r\nJun 21 16:45:40     #16 0x558025e784ac in do_call_core /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:5116\r\nJun 21 16:45:40     #17 0x558025e784ac in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3404\r\nJun 21 16:45:40     #18 0x558025e478e3 in _PyEval_EvalCodeWithName /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4166\r\nJun 21 16:45:40     #19 0x558025e48770 in fast_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4992\r\nJun 21 16:45:40     #20 0x558025e4e504 in call_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4872\r\nJun 21 16:45:40     #21 0x558025e73389 in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3335\r\nJun 21 16:45:40     #22 0x558025e4853a in _PyFunction_FastCall /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4933\r\nJun 21 16:45:40     #23 0x558025e4853a in fast_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4968\r\nJun 21 16:45:40     #24 0x558025e4e504 in call_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4872\r\nJun 21 16:45:40     #25 0x558025e73389 in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3335\r\nJun 21 16:45:40     #26 0x558025e4853a in _PyFunction_FastCall /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4933\r\nJun 21 16:45:40     #27 0x558025e4853a in fast_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4968\r\nJun 21 16:45:40     #28 0x558025e4e504 in call_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4872\r\nJun 21 16:45:40     #29 0x558025e73389 in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3335\r\nJun 21 16:45:40     #30 0x558025e4853a in _PyFunction_FastCall /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4933\r\nJun 21 16:45:40     #31 0x558025e4853a in fast_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4968\r\nJun 21 16:45:40     #32 0x558025e4e504 in call_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4872\r\nJun 21 16:45:40     #33 0x558025e73389 in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3335\r\nJun 21 16:45:40     #34 0x558025e48baa in _PyFunction_FastCall /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4933\r\nJun 21 16:45:40     #35 0x558025e48baa in _PyFunction_FastCallDict /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:5035\r\nJun 21 16:45:40     #36 0x558025dc7b0e in _PyObject_FastCallDict /tmp/build/80754af9/python_1546130271559/work/Objects/abstract.c:2310\r\nJun 21 16:45:40     #37 0x558025e0980f in _PyObject_CallMethodIdObjArgs /tmp/build/80754af9/python_1546130271559/work/Objects/abstract.c:2796\r\nJun 21 16:45:40     #38 0x558025dbeb0f in PyImport_ImportModuleLevelObject /tmp/build/80754af9/python_1546130271559/work/Python/import.c:1578\r\nJun 21 16:45:40     #39 0x558025e75a8a in import_name /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:5245\r\nJun 21 16:45:40     #40 0x558025e75a8a in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:2899\r\nJun 21 16:45:40     #41 0x558025e49288 in _PyEval_EvalCodeWithName /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4166\r\nJun 21 16:45:40     #42 0x558025e49288 in PyEval_EvalCodeEx /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4187\r\nJun 21 16:45:40     #43 0x558025e4a01b in PyEval_EvalCode /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:731\r\nJun 21 16:45:40     #44 0x558025ecc3c3 in run_mod /tmp/build/80754af9/python_1546130271559/work/Python/pythonrun.c:1025\r\nJun 21 16:45:40     #45 0x558025ecc7c0 in PyRun_FileExFlags /tmp/build/80754af9/python_1546130271559/work/Python/pythonrun.c:978\r\nJun 21 16:45:40     #46 0x558025ecc9c2 in PyRun_SimpleFileExFlags /tmp/build/80754af9/python_1546130271559/work/Python/pythonrun.c:419\r\nJun 21 16:45:40     #47 0x558025ed04b2 in run_file /tmp/build/80754af9/python_1546130271559/work/Modules/main.c:340\r\nJun 21 16:45:40     #48 0x558025ed04b2 in Py_Main /tmp/build/80754af9/python_1546130271559/work/Modules/main.c:811\r\nJun 21 16:45:40     #49 0x558025d9902d in main /tmp/build/80754af9/python_1546130271559/work/Programs/python.c:69\r\nJun 21 16:45:40     #50 0x7f26053c082f in __libc_start_main /build/glibc-LK5gWL/glibc-2.23/csu/../csu/libc-start.c:291\r\nJun 21 16:45:40     #51 0x558025e79e0d in _start /home/rdonnelly/mc/conda-bld/compilers_linux-64_1534865402226/work/.build/src/glibc-2.12.2/csu/../sysdeps/x86_64/elf/start.S:103\r\nJun 21 16:45:40 \r\nJun 21 16:45:40 SUMMARY: UndefinedBehaviorSanitizer: undefined-behavior /var/lib/jenkins/workspace/build/aten/src/ATen/Functions.h:2779:12 in \r\nJun 21 16:45:40 Traceback (most recent call last):\r\n```\n<issue_comment>username_2: I guess, ideally, we'd expose symbols that our ours, but not expose anything from pybind11. I don't know how easy that is to do. (Maybe this is a reason not to use pybind11, but that ship has kind of sailed...)\n<issue_comment>username_3: If everything works on MSVC with its default option -- that is, no exposure unless explicitly asked to -- could replacing all occurrences of `__declspec(dllexport)` with something also compatible with `gcc` work? Like something similar to [this](https://stackoverflow.com/a/8258786/1150462)?\n<issue_comment>username_0: Yes, I realized that it needs adjusting annotations.\r\n\r\nAs @username_3 pointed out, we should have already annotated all the right things to make Windows work. But we use windows-only macro in THP_API/THP_CLASS (https://github.com/pytorch/pytorch/blob/master/torch/csrc/THP_export.h). We should do the same thing as for other places - https://github.com/pytorch/pytorch/blob/master/c10/macros/Export.h\r\n\r\nI'll try to get back to it at some point.","repo":"pytorch/pytorch","issue_id":459270796,"issue_number":22076}
{"question":"What is the suggested ideal approach regarding the exposure of symbols, particularly in relation to pybind11?","answer":"The ideal approach is to expose symbols specific to the project without exposing anything from pybind11, although it may be challenging to implement this. The text implies a hindsight regret about using pybind11.","id":42,"text":"I guess, ideally, we'd expose symbols that our ours, but not expose anything from pybind11. I don't know how easy that is to do. (Maybe this is a reason not to use pybind11, but that ship has kind of sailed...)","context":"<issue_start><issue_comment>Title: Add hidden visibility to _C.so\nusername_0: See https://pybind11.readthedocs.io/en/stable/faq.html#someclass-declared-with-greater-visibility-than-the-type-of-its-field-someclass-member-wattributes\r\n\r\nLet's see whether it passes CI\n<issue_comment>username_1: wouldn't C++ extensions fail because of this? they require the symbols coming out of libtorch_python and stuff to be global visibility in the current process, or else they fail to load at runtime (but the C++ extensions will compile fine)\n<issue_comment>username_2: UBSAN failure looks real:\r\n\r\n```\r\nJun 21 16:45:39 /var/lib/jenkins/workspace/build/aten/src/ATen/Functions.h:2779:12: runtime error: call to function at::TypeDefault::randn(c10::ArrayRef<long>, c10::TensorOptions const&) through pointer to incorrect function type 'at::Tensor (*)(c10::ArrayRef<long>, const c10::TensorOptions &)'\r\nJun 21 16:45:39 (/opt/conda/lib/python3.6/site-packages/torch/lib/libtorch.so+0xa6b7860): note: at::TypeDefault::randn(c10::ArrayRef<long>, c10::TensorOptions const&) defined here\r\nJun 21 16:45:40     #0 0x7f25f2696882 in torch::randn(c10::ArrayRef<long>, c10::TensorOptions const&) (/opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_python.so+0x11aa882)\r\nJun 21 16:45:40     #1 0x7f25f2694ccc in torch::autograd::dispatch_randn(c10::ArrayRef<long>, c10::TensorOptions const&) (/opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_python.so+0x11a8ccc)\r\nJun 21 16:45:40     #2 0x7f25f258d08d in torch::autograd::THPVariable_randn(_object*, _object*, _object*) (/opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_python.so+0x10a108d)\r\nJun 21 16:45:40     #3 0x558025dc7743 in _PyCFunction_FastCallDict /tmp/build/80754af9/python_1546130271559/work/Objects/methodobject.c:231\r\nJun 21 16:45:40     #4 0x558025e4e42b in call_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4851\r\nJun 21 16:45:40     #5 0x558025e73389 in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3335\r\nJun 21 16:45:40     #6 0x558025e47bfd in _PyEval_EvalCodeWithName /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4166\r\nJun 21 16:45:40     #7 0x558025e48770 in fast_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4992\r\nJun 21 16:45:40     #8 0x558025e4e504 in call_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4872\r\nJun 21 16:45:40     #9 0x558025e73389 in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3335\r\nJun 21 16:45:40     #10 0x558025e49288 in _PyEval_EvalCodeWithName /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4166\r\nJun 21 16:45:40     #11 0x558025e49288 in PyEval_EvalCodeEx /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4187\r\nJun 21 16:45:40     #12 0x558025e4a01b in PyEval_EvalCode /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:731\r\nJun 21 16:45:40     #13 0x558025e70c8a in builtin_exec_impl.isra.11 /tmp/build/80754af9/python_1546130271559/work/Python/bltinmodule.c:983\r\nJun 21 16:45:40     #14 0x558025e70c8a in builtin_exec /tmp/build/80754af9/python_1546130271559/work/Python/clinic/bltinmodule.c.h:283\r\nJun 21 16:45:40     #15 0x558025dca710 in PyCFunction_Call /tmp/build/80754af9/python_1546130271559/work/Objects/methodobject.c:114\r\nJun 21 16:45:40     #16 0x558025e784ac in do_call_core /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:5116\r\nJun 21 16:45:40     #17 0x558025e784ac in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3404\r\nJun 21 16:45:40     #18 0x558025e478e3 in _PyEval_EvalCodeWithName /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4166\r\nJun 21 16:45:40     #19 0x558025e48770 in fast_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4992\r\nJun 21 16:45:40     #20 0x558025e4e504 in call_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4872\r\nJun 21 16:45:40     #21 0x558025e73389 in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3335\r\nJun 21 16:45:40     #22 0x558025e4853a in _PyFunction_FastCall /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4933\r\nJun 21 16:45:40     #23 0x558025e4853a in fast_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4968\r\nJun 21 16:45:40     #24 0x558025e4e504 in call_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4872\r\nJun 21 16:45:40     #25 0x558025e73389 in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3335\r\nJun 21 16:45:40     #26 0x558025e4853a in _PyFunction_FastCall /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4933\r\nJun 21 16:45:40     #27 0x558025e4853a in fast_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4968\r\nJun 21 16:45:40     #28 0x558025e4e504 in call_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4872\r\nJun 21 16:45:40     #29 0x558025e73389 in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3335\r\nJun 21 16:45:40     #30 0x558025e4853a in _PyFunction_FastCall /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4933\r\nJun 21 16:45:40     #31 0x558025e4853a in fast_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4968\r\nJun 21 16:45:40     #32 0x558025e4e504 in call_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4872\r\nJun 21 16:45:40     #33 0x558025e73389 in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3335\r\nJun 21 16:45:40     #34 0x558025e48baa in _PyFunction_FastCall /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4933\r\nJun 21 16:45:40     #35 0x558025e48baa in _PyFunction_FastCallDict /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:5035\r\nJun 21 16:45:40     #36 0x558025dc7b0e in _PyObject_FastCallDict /tmp/build/80754af9/python_1546130271559/work/Objects/abstract.c:2310\r\nJun 21 16:45:40     #37 0x558025e0980f in _PyObject_CallMethodIdObjArgs /tmp/build/80754af9/python_1546130271559/work/Objects/abstract.c:2796\r\nJun 21 16:45:40     #38 0x558025dbeb0f in PyImport_ImportModuleLevelObject /tmp/build/80754af9/python_1546130271559/work/Python/import.c:1578\r\nJun 21 16:45:40     #39 0x558025e75a8a in import_name /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:5245\r\nJun 21 16:45:40     #40 0x558025e75a8a in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:2899\r\nJun 21 16:45:40     #41 0x558025e49288 in _PyEval_EvalCodeWithName /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4166\r\nJun 21 16:45:40     #42 0x558025e49288 in PyEval_EvalCodeEx /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4187\r\nJun 21 16:45:40     #43 0x558025e4a01b in PyEval_EvalCode /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:731\r\nJun 21 16:45:40     #44 0x558025ecc3c3 in run_mod /tmp/build/80754af9/python_1546130271559/work/Python/pythonrun.c:1025\r\nJun 21 16:45:40     #45 0x558025ecc7c0 in PyRun_FileExFlags /tmp/build/80754af9/python_1546130271559/work/Python/pythonrun.c:978\r\nJun 21 16:45:40     #46 0x558025ecc9c2 in PyRun_SimpleFileExFlags /tmp/build/80754af9/python_1546130271559/work/Python/pythonrun.c:419\r\nJun 21 16:45:40     #47 0x558025ed04b2 in run_file /tmp/build/80754af9/python_1546130271559/work/Modules/main.c:340\r\nJun 21 16:45:40     #48 0x558025ed04b2 in Py_Main /tmp/build/80754af9/python_1546130271559/work/Modules/main.c:811\r\nJun 21 16:45:40     #49 0x558025d9902d in main /tmp/build/80754af9/python_1546130271559/work/Programs/python.c:69\r\nJun 21 16:45:40     #50 0x7f26053c082f in __libc_start_main /build/glibc-LK5gWL/glibc-2.23/csu/../csu/libc-start.c:291\r\nJun 21 16:45:40     #51 0x558025e79e0d in _start /home/rdonnelly/mc/conda-bld/compilers_linux-64_1534865402226/work/.build/src/glibc-2.12.2/csu/../sysdeps/x86_64/elf/start.S:103\r\nJun 21 16:45:40 \r\nJun 21 16:45:40 SUMMARY: UndefinedBehaviorSanitizer: undefined-behavior /var/lib/jenkins/workspace/build/aten/src/ATen/Functions.h:2779:12 in \r\nJun 21 16:45:40 Traceback (most recent call last):\r\n```\n<issue_comment>username_2: I guess, ideally, we'd expose symbols that our ours, but not expose anything from pybind11. I don't know how easy that is to do. (Maybe this is a reason not to use pybind11, but that ship has kind of sailed...)\n<issue_comment>username_3: If everything works on MSVC with its default option -- that is, no exposure unless explicitly asked to -- could replacing all occurrences of `__declspec(dllexport)` with something also compatible with `gcc` work? Like something similar to [this](https://stackoverflow.com/a/8258786/1150462)?\n<issue_comment>username_0: Yes, I realized that it needs adjusting annotations.\r\n\r\nAs @username_3 pointed out, we should have already annotated all the right things to make Windows work. But we use windows-only macro in THP_API/THP_CLASS (https://github.com/pytorch/pytorch/blob/master/torch/csrc/THP_export.h). We should do the same thing as for other places - https://github.com/pytorch/pytorch/blob/master/c10/macros/Export.h\r\n\r\nI'll try to get back to it at some point.","repo":"pytorch/pytorch","issue_id":459270796,"issue_number":22076}
{"question":"Is it possible to replace all occurrences of `__declspec(dllexport)` with something compatible with `gcc` like the approach described in the Stack Overflow answer?","answer":"Yes, it is possible to replace all occurrences of `__declspec(dllexport)` with something compatible with `gcc` like the approach mentioned in the Stack Overflow answer.","id":43,"text":"If everything works on MSVC with its default option -- that is, no exposure unless explicitly asked to -- could replacing all occurrences of `__declspec(dllexport)` with something also compatible with `gcc` work? Like something similar to [this](https://stackoverflow.com/a/8258786/1150462)?","context":"<issue_start><issue_comment>Title: Add hidden visibility to _C.so\nusername_0: See https://pybind11.readthedocs.io/en/stable/faq.html#someclass-declared-with-greater-visibility-than-the-type-of-its-field-someclass-member-wattributes\r\n\r\nLet's see whether it passes CI\n<issue_comment>username_1: wouldn't C++ extensions fail because of this? they require the symbols coming out of libtorch_python and stuff to be global visibility in the current process, or else they fail to load at runtime (but the C++ extensions will compile fine)\n<issue_comment>username_2: UBSAN failure looks real:\r\n\r\n```\r\nJun 21 16:45:39 /var/lib/jenkins/workspace/build/aten/src/ATen/Functions.h:2779:12: runtime error: call to function at::TypeDefault::randn(c10::ArrayRef<long>, c10::TensorOptions const&) through pointer to incorrect function type 'at::Tensor (*)(c10::ArrayRef<long>, const c10::TensorOptions &)'\r\nJun 21 16:45:39 (/opt/conda/lib/python3.6/site-packages/torch/lib/libtorch.so+0xa6b7860): note: at::TypeDefault::randn(c10::ArrayRef<long>, c10::TensorOptions const&) defined here\r\nJun 21 16:45:40     #0 0x7f25f2696882 in torch::randn(c10::ArrayRef<long>, c10::TensorOptions const&) (/opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_python.so+0x11aa882)\r\nJun 21 16:45:40     #1 0x7f25f2694ccc in torch::autograd::dispatch_randn(c10::ArrayRef<long>, c10::TensorOptions const&) (/opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_python.so+0x11a8ccc)\r\nJun 21 16:45:40     #2 0x7f25f258d08d in torch::autograd::THPVariable_randn(_object*, _object*, _object*) (/opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_python.so+0x10a108d)\r\nJun 21 16:45:40     #3 0x558025dc7743 in _PyCFunction_FastCallDict /tmp/build/80754af9/python_1546130271559/work/Objects/methodobject.c:231\r\nJun 21 16:45:40     #4 0x558025e4e42b in call_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4851\r\nJun 21 16:45:40     #5 0x558025e73389 in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3335\r\nJun 21 16:45:40     #6 0x558025e47bfd in _PyEval_EvalCodeWithName /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4166\r\nJun 21 16:45:40     #7 0x558025e48770 in fast_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4992\r\nJun 21 16:45:40     #8 0x558025e4e504 in call_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4872\r\nJun 21 16:45:40     #9 0x558025e73389 in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3335\r\nJun 21 16:45:40     #10 0x558025e49288 in _PyEval_EvalCodeWithName /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4166\r\nJun 21 16:45:40     #11 0x558025e49288 in PyEval_EvalCodeEx /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4187\r\nJun 21 16:45:40     #12 0x558025e4a01b in PyEval_EvalCode /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:731\r\nJun 21 16:45:40     #13 0x558025e70c8a in builtin_exec_impl.isra.11 /tmp/build/80754af9/python_1546130271559/work/Python/bltinmodule.c:983\r\nJun 21 16:45:40     #14 0x558025e70c8a in builtin_exec /tmp/build/80754af9/python_1546130271559/work/Python/clinic/bltinmodule.c.h:283\r\nJun 21 16:45:40     #15 0x558025dca710 in PyCFunction_Call /tmp/build/80754af9/python_1546130271559/work/Objects/methodobject.c:114\r\nJun 21 16:45:40     #16 0x558025e784ac in do_call_core /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:5116\r\nJun 21 16:45:40     #17 0x558025e784ac in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3404\r\nJun 21 16:45:40     #18 0x558025e478e3 in _PyEval_EvalCodeWithName /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4166\r\nJun 21 16:45:40     #19 0x558025e48770 in fast_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4992\r\nJun 21 16:45:40     #20 0x558025e4e504 in call_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4872\r\nJun 21 16:45:40     #21 0x558025e73389 in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3335\r\nJun 21 16:45:40     #22 0x558025e4853a in _PyFunction_FastCall /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4933\r\nJun 21 16:45:40     #23 0x558025e4853a in fast_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4968\r\nJun 21 16:45:40     #24 0x558025e4e504 in call_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4872\r\nJun 21 16:45:40     #25 0x558025e73389 in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3335\r\nJun 21 16:45:40     #26 0x558025e4853a in _PyFunction_FastCall /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4933\r\nJun 21 16:45:40     #27 0x558025e4853a in fast_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4968\r\nJun 21 16:45:40     #28 0x558025e4e504 in call_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4872\r\nJun 21 16:45:40     #29 0x558025e73389 in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3335\r\nJun 21 16:45:40     #30 0x558025e4853a in _PyFunction_FastCall /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4933\r\nJun 21 16:45:40     #31 0x558025e4853a in fast_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4968\r\nJun 21 16:45:40     #32 0x558025e4e504 in call_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4872\r\nJun 21 16:45:40     #33 0x558025e73389 in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3335\r\nJun 21 16:45:40     #34 0x558025e48baa in _PyFunction_FastCall /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4933\r\nJun 21 16:45:40     #35 0x558025e48baa in _PyFunction_FastCallDict /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:5035\r\nJun 21 16:45:40     #36 0x558025dc7b0e in _PyObject_FastCallDict /tmp/build/80754af9/python_1546130271559/work/Objects/abstract.c:2310\r\nJun 21 16:45:40     #37 0x558025e0980f in _PyObject_CallMethodIdObjArgs /tmp/build/80754af9/python_1546130271559/work/Objects/abstract.c:2796\r\nJun 21 16:45:40     #38 0x558025dbeb0f in PyImport_ImportModuleLevelObject /tmp/build/80754af9/python_1546130271559/work/Python/import.c:1578\r\nJun 21 16:45:40     #39 0x558025e75a8a in import_name /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:5245\r\nJun 21 16:45:40     #40 0x558025e75a8a in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:2899\r\nJun 21 16:45:40     #41 0x558025e49288 in _PyEval_EvalCodeWithName /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4166\r\nJun 21 16:45:40     #42 0x558025e49288 in PyEval_EvalCodeEx /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4187\r\nJun 21 16:45:40     #43 0x558025e4a01b in PyEval_EvalCode /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:731\r\nJun 21 16:45:40     #44 0x558025ecc3c3 in run_mod /tmp/build/80754af9/python_1546130271559/work/Python/pythonrun.c:1025\r\nJun 21 16:45:40     #45 0x558025ecc7c0 in PyRun_FileExFlags /tmp/build/80754af9/python_1546130271559/work/Python/pythonrun.c:978\r\nJun 21 16:45:40     #46 0x558025ecc9c2 in PyRun_SimpleFileExFlags /tmp/build/80754af9/python_1546130271559/work/Python/pythonrun.c:419\r\nJun 21 16:45:40     #47 0x558025ed04b2 in run_file /tmp/build/80754af9/python_1546130271559/work/Modules/main.c:340\r\nJun 21 16:45:40     #48 0x558025ed04b2 in Py_Main /tmp/build/80754af9/python_1546130271559/work/Modules/main.c:811\r\nJun 21 16:45:40     #49 0x558025d9902d in main /tmp/build/80754af9/python_1546130271559/work/Programs/python.c:69\r\nJun 21 16:45:40     #50 0x7f26053c082f in __libc_start_main /build/glibc-LK5gWL/glibc-2.23/csu/../csu/libc-start.c:291\r\nJun 21 16:45:40     #51 0x558025e79e0d in _start /home/rdonnelly/mc/conda-bld/compilers_linux-64_1534865402226/work/.build/src/glibc-2.12.2/csu/../sysdeps/x86_64/elf/start.S:103\r\nJun 21 16:45:40 \r\nJun 21 16:45:40 SUMMARY: UndefinedBehaviorSanitizer: undefined-behavior /var/lib/jenkins/workspace/build/aten/src/ATen/Functions.h:2779:12 in \r\nJun 21 16:45:40 Traceback (most recent call last):\r\n```\n<issue_comment>username_2: I guess, ideally, we'd expose symbols that our ours, but not expose anything from pybind11. I don't know how easy that is to do. (Maybe this is a reason not to use pybind11, but that ship has kind of sailed...)\n<issue_comment>username_3: If everything works on MSVC with its default option -- that is, no exposure unless explicitly asked to -- could replacing all occurrences of `__declspec(dllexport)` with something also compatible with `gcc` work? Like something similar to [this](https://stackoverflow.com/a/8258786/1150462)?\n<issue_comment>username_0: Yes, I realized that it needs adjusting annotations.\r\n\r\nAs @username_3 pointed out, we should have already annotated all the right things to make Windows work. But we use windows-only macro in THP_API/THP_CLASS (https://github.com/pytorch/pytorch/blob/master/torch/csrc/THP_export.h). We should do the same thing as for other places - https://github.com/pytorch/pytorch/blob/master/c10/macros/Export.h\r\n\r\nI'll try to get back to it at some point.","repo":"pytorch/pytorch","issue_id":459270796,"issue_number":22076}
{"question":"Why is it important to adjust annotations in code development, and how are annotations used in the provided code snippets?","answer":"Adjusting annotations in code development is crucial for clarity, documentation, and compatibility with specific platforms. In the provided code snippets, there is a reference to using a windows-only macro in the THP_API/THP_CLASS, which highlights the need to annotate code for platform-specific functionality. The comparison to other places in the codebase suggests a consistent annotation practice across different sections for clarity and compatibility.","id":44,"text":"Yes, I realized that it needs adjusting annotations.\r\n\r\nAs @username_3 pointed out, we should have already annotated all the right things to make Windows work. But we use windows-only macro in THP_API/THP_CLASS (https://github.com/pytorch/pytorch/blob/master/torch/csrc/THP_export.h). We should do the same thing as for other places - https://github.com/pytorch/pytorch/blob/master/c10/macros/Export.h\r\n\r\nI'll try to get back to it at some point.","context":"<issue_start><issue_comment>Title: Add hidden visibility to _C.so\nusername_0: See https://pybind11.readthedocs.io/en/stable/faq.html#someclass-declared-with-greater-visibility-than-the-type-of-its-field-someclass-member-wattributes\r\n\r\nLet's see whether it passes CI\n<issue_comment>username_1: wouldn't C++ extensions fail because of this? they require the symbols coming out of libtorch_python and stuff to be global visibility in the current process, or else they fail to load at runtime (but the C++ extensions will compile fine)\n<issue_comment>username_2: UBSAN failure looks real:\r\n\r\n```\r\nJun 21 16:45:39 /var/lib/jenkins/workspace/build/aten/src/ATen/Functions.h:2779:12: runtime error: call to function at::TypeDefault::randn(c10::ArrayRef<long>, c10::TensorOptions const&) through pointer to incorrect function type 'at::Tensor (*)(c10::ArrayRef<long>, const c10::TensorOptions &)'\r\nJun 21 16:45:39 (/opt/conda/lib/python3.6/site-packages/torch/lib/libtorch.so+0xa6b7860): note: at::TypeDefault::randn(c10::ArrayRef<long>, c10::TensorOptions const&) defined here\r\nJun 21 16:45:40     #0 0x7f25f2696882 in torch::randn(c10::ArrayRef<long>, c10::TensorOptions const&) (/opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_python.so+0x11aa882)\r\nJun 21 16:45:40     #1 0x7f25f2694ccc in torch::autograd::dispatch_randn(c10::ArrayRef<long>, c10::TensorOptions const&) (/opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_python.so+0x11a8ccc)\r\nJun 21 16:45:40     #2 0x7f25f258d08d in torch::autograd::THPVariable_randn(_object*, _object*, _object*) (/opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_python.so+0x10a108d)\r\nJun 21 16:45:40     #3 0x558025dc7743 in _PyCFunction_FastCallDict /tmp/build/80754af9/python_1546130271559/work/Objects/methodobject.c:231\r\nJun 21 16:45:40     #4 0x558025e4e42b in call_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4851\r\nJun 21 16:45:40     #5 0x558025e73389 in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3335\r\nJun 21 16:45:40     #6 0x558025e47bfd in _PyEval_EvalCodeWithName /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4166\r\nJun 21 16:45:40     #7 0x558025e48770 in fast_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4992\r\nJun 21 16:45:40     #8 0x558025e4e504 in call_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4872\r\nJun 21 16:45:40     #9 0x558025e73389 in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3335\r\nJun 21 16:45:40     #10 0x558025e49288 in _PyEval_EvalCodeWithName /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4166\r\nJun 21 16:45:40     #11 0x558025e49288 in PyEval_EvalCodeEx /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4187\r\nJun 21 16:45:40     #12 0x558025e4a01b in PyEval_EvalCode /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:731\r\nJun 21 16:45:40     #13 0x558025e70c8a in builtin_exec_impl.isra.11 /tmp/build/80754af9/python_1546130271559/work/Python/bltinmodule.c:983\r\nJun 21 16:45:40     #14 0x558025e70c8a in builtin_exec /tmp/build/80754af9/python_1546130271559/work/Python/clinic/bltinmodule.c.h:283\r\nJun 21 16:45:40     #15 0x558025dca710 in PyCFunction_Call /tmp/build/80754af9/python_1546130271559/work/Objects/methodobject.c:114\r\nJun 21 16:45:40     #16 0x558025e784ac in do_call_core /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:5116\r\nJun 21 16:45:40     #17 0x558025e784ac in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3404\r\nJun 21 16:45:40     #18 0x558025e478e3 in _PyEval_EvalCodeWithName /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4166\r\nJun 21 16:45:40     #19 0x558025e48770 in fast_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4992\r\nJun 21 16:45:40     #20 0x558025e4e504 in call_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4872\r\nJun 21 16:45:40     #21 0x558025e73389 in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3335\r\nJun 21 16:45:40     #22 0x558025e4853a in _PyFunction_FastCall /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4933\r\nJun 21 16:45:40     #23 0x558025e4853a in fast_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4968\r\nJun 21 16:45:40     #24 0x558025e4e504 in call_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4872\r\nJun 21 16:45:40     #25 0x558025e73389 in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3335\r\nJun 21 16:45:40     #26 0x558025e4853a in _PyFunction_FastCall /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4933\r\nJun 21 16:45:40     #27 0x558025e4853a in fast_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4968\r\nJun 21 16:45:40     #28 0x558025e4e504 in call_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4872\r\nJun 21 16:45:40     #29 0x558025e73389 in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3335\r\nJun 21 16:45:40     #30 0x558025e4853a in _PyFunction_FastCall /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4933\r\nJun 21 16:45:40     #31 0x558025e4853a in fast_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4968\r\nJun 21 16:45:40     #32 0x558025e4e504 in call_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4872\r\nJun 21 16:45:40     #33 0x558025e73389 in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3335\r\nJun 21 16:45:40     #34 0x558025e48baa in _PyFunction_FastCall /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4933\r\nJun 21 16:45:40     #35 0x558025e48baa in _PyFunction_FastCallDict /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:5035\r\nJun 21 16:45:40     #36 0x558025dc7b0e in _PyObject_FastCallDict /tmp/build/80754af9/python_1546130271559/work/Objects/abstract.c:2310\r\nJun 21 16:45:40     #37 0x558025e0980f in _PyObject_CallMethodIdObjArgs /tmp/build/80754af9/python_1546130271559/work/Objects/abstract.c:2796\r\nJun 21 16:45:40     #38 0x558025dbeb0f in PyImport_ImportModuleLevelObject /tmp/build/80754af9/python_1546130271559/work/Python/import.c:1578\r\nJun 21 16:45:40     #39 0x558025e75a8a in import_name /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:5245\r\nJun 21 16:45:40     #40 0x558025e75a8a in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:2899\r\nJun 21 16:45:40     #41 0x558025e49288 in _PyEval_EvalCodeWithName /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4166\r\nJun 21 16:45:40     #42 0x558025e49288 in PyEval_EvalCodeEx /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4187\r\nJun 21 16:45:40     #43 0x558025e4a01b in PyEval_EvalCode /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:731\r\nJun 21 16:45:40     #44 0x558025ecc3c3 in run_mod /tmp/build/80754af9/python_1546130271559/work/Python/pythonrun.c:1025\r\nJun 21 16:45:40     #45 0x558025ecc7c0 in PyRun_FileExFlags /tmp/build/80754af9/python_1546130271559/work/Python/pythonrun.c:978\r\nJun 21 16:45:40     #46 0x558025ecc9c2 in PyRun_SimpleFileExFlags /tmp/build/80754af9/python_1546130271559/work/Python/pythonrun.c:419\r\nJun 21 16:45:40     #47 0x558025ed04b2 in run_file /tmp/build/80754af9/python_1546130271559/work/Modules/main.c:340\r\nJun 21 16:45:40     #48 0x558025ed04b2 in Py_Main /tmp/build/80754af9/python_1546130271559/work/Modules/main.c:811\r\nJun 21 16:45:40     #49 0x558025d9902d in main /tmp/build/80754af9/python_1546130271559/work/Programs/python.c:69\r\nJun 21 16:45:40     #50 0x7f26053c082f in __libc_start_main /build/glibc-LK5gWL/glibc-2.23/csu/../csu/libc-start.c:291\r\nJun 21 16:45:40     #51 0x558025e79e0d in _start /home/rdonnelly/mc/conda-bld/compilers_linux-64_1534865402226/work/.build/src/glibc-2.12.2/csu/../sysdeps/x86_64/elf/start.S:103\r\nJun 21 16:45:40 \r\nJun 21 16:45:40 SUMMARY: UndefinedBehaviorSanitizer: undefined-behavior /var/lib/jenkins/workspace/build/aten/src/ATen/Functions.h:2779:12 in \r\nJun 21 16:45:40 Traceback (most recent call last):\r\n```\n<issue_comment>username_2: I guess, ideally, we'd expose symbols that our ours, but not expose anything from pybind11. I don't know how easy that is to do. (Maybe this is a reason not to use pybind11, but that ship has kind of sailed...)\n<issue_comment>username_3: If everything works on MSVC with its default option -- that is, no exposure unless explicitly asked to -- could replacing all occurrences of `__declspec(dllexport)` with something also compatible with `gcc` work? Like something similar to [this](https://stackoverflow.com/a/8258786/1150462)?\n<issue_comment>username_0: Yes, I realized that it needs adjusting annotations.\r\n\r\nAs @username_3 pointed out, we should have already annotated all the right things to make Windows work. But we use windows-only macro in THP_API/THP_CLASS (https://github.com/pytorch/pytorch/blob/master/torch/csrc/THP_export.h). We should do the same thing as for other places - https://github.com/pytorch/pytorch/blob/master/c10/macros/Export.h\r\n\r\nI'll try to get back to it at some point.","repo":"pytorch/pytorch","issue_id":459270796,"issue_number":22076}
{"question":"What error message is encountered when attempting to create a sparse tensor from indices and values on different GPUs in PyTorch?","answer":"device of indices (0) must match device of values (1)","id":45,"text":"##  Bug\r\n\r\nAttempting to create a sparse tensor from a tensor of indices and values placed on another GPU than `cuda:0` (for instance `cuda:1`) fails with error `device of indices (0) must match device of values (1)`.\r\n\r\nPlacing the values on `cuda:0` while leaving the indices on `cuda:1` works and returns a sparse tensor on `cuda:0`.\r\n\r\nThe following demo should show the bug:\r\n```\r\nIn [1]: import torch                                                            \r\n\r\nIn [2]: ind = torch.arange(10).unsqueeze(0).to('cuda:1')                        \r\n\r\nIn [3]: sp_ind = torch.cat((ind, ind), 0)                                       \r\n\r\nIn [4]: sp_val = torch.rand(10).to('cuda:1')                                    \r\n\r\nIn [5]: sp_shape = (10, 10)                                                     \r\n\r\nIn [6]: x = torch.sparse_coo_tensor(sp_ind, sp_val, sp_shape)                   \r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-6-776f23c64083> in <module>\r\n----> 1 x = torch.sparse_coo_tensor(sp_ind, sp_val, sp_shape)\r\n\r\nRuntimeError: device of indices (0) must match device of values (1)\r\n\r\nIn [7]: # Now this                                                              \r\n\r\nIn [8]: x = torch.sparse_coo_tensor(sp_ind, sp_val.cuda(0), sp_shape)           \r\n\r\nIn [9]: x                                                                       \r\nOut[9]: \r\ntensor(indices=tensor([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\r\n                       [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]]),\r\n       values=tensor([0.5324, 0.1738, 0.4186, 0.2776, 0.4665, 0.9525, 0.6555,\r\n                      0.2805, 0.9244, 0.4847]),\r\n       device='cuda:0', size=(10, 10), nnz=10, layout=torch.sparse_coo)\r\n\r\nIn [10]: sp_ind                                                                 \r\nOut[10]: \r\ntensor([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\r\n        [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]], device='cuda:1')\r\n\r\nIn [11]: sp_val                                                                 \r\nOut[11]: \r\ntensor([0.5324, 0.1738, 0.4186, 0.2776, 0.4665, 0.9525, 0.6555, 0.2805, 0.9244,\r\n        0.4847], device='cuda:1')\r\n\r\n```\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n**See code snipped above.**\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n## Expected behavior\r\n\r\nProvided both indices and values are placed on the same device, `torch.sparse_coo_tensor` should return a sparse tensor on that device.\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n## Environment\r\n\r\nPlease copy and paste the output from our\r\n[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)\r\n(or fill out the checklist below manually).\r\n\r\nYou can get the script and run it with:\r\n```\r\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\r\n# For security purposes, please check the contents of collect_env.py before running it.\r\npython collect_env.py\r\n```\r\n\r\n - PyTorch Version (e.g., 1.0): 1.3.0\r\n - OS (e.g., Linux): Ubuntu 18.04 LTS\r\n - How you installed PyTorch (`conda`, `pip`, source): conda\r\n - Build command you used (if compiling from source): N/A\r\n - Python version: 3.7.4\r\n - CUDA/cuDNN version: 10.2/7.5\r\n - GPU models and configuration: 2x2080 Ti with Nvlink\r\n - Any other relevant information:\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context about the problem here. -->","context":"<issue_start><issue_comment>Title: Sparse tensor creation ignores indices placement\nusername_0: ##  Bug\r\n\r\nAttempting to create a sparse tensor from a tensor of indices and values placed on another GPU than `cuda:0` (for instance `cuda:1`) fails with error `device of indices (0) must match device of values (1)`.\r\n\r\nPlacing the values on `cuda:0` while leaving the indices on `cuda:1` works and returns a sparse tensor on `cuda:0`.\r\n\r\nThe following demo should show the bug:\r\n```\r\nIn [1]: import torch                                                            \r\n\r\nIn [2]: ind = torch.arange(10).unsqueeze(0).to('cuda:1')                        \r\n\r\nIn [3]: sp_ind = torch.cat((ind, ind), 0)                                       \r\n\r\nIn [4]: sp_val = torch.rand(10).to('cuda:1')                                    \r\n\r\nIn [5]: sp_shape = (10, 10)                                                     \r\n\r\nIn [6]: x = torch.sparse_coo_tensor(sp_ind, sp_val, sp_shape)                   \r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-6-776f23c64083> in <module>\r\n----> 1 x = torch.sparse_coo_tensor(sp_ind, sp_val, sp_shape)\r\n\r\nRuntimeError: device of indices (0) must match device of values (1)\r\n\r\nIn [7]: # Now this                                                              \r\n\r\nIn [8]: x = torch.sparse_coo_tensor(sp_ind, sp_val.cuda(0), sp_shape)           \r\n\r\nIn [9]: x                                                                       \r\nOut[9]: \r\ntensor(indices=tensor([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\r\n                       [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]]),\r\n       values=tensor([0.5324, 0.1738, 0.4186, 0.2776, 0.4665, 0.9525, 0.6555,\r\n                      0.2805, 0.9244, 0.4847]),\r\n       device='cuda:0', size=(10, 10), nnz=10, layout=torch.sparse_coo)\r\n\r\nIn [10]: sp_ind                                                                 \r\nOut[10]: \r\ntensor([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\r\n        [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]], device='cuda:1')\r\n\r\nIn [11]: sp_val                                                                 \r\nOut[11]: \r\ntensor([0.5324, 0.1738, 0.4186, 0.2776, 0.4665, 0.9525, 0.6555, 0.2805, 0.9244,\r\n        0.4847], device='cuda:1')\r\n\r\n```\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n**See code snipped above.**\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n## Expected behavior\r\n\r\nProvided both indices and values are placed on the same device, `torch.sparse_coo_tensor` should return a sparse tensor on that device.\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n## Environment\r\n\r\nPlease copy and paste the output from our\r\n[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)\r\n(or fill out the checklist below manually).\r\n\r\nYou can get the script and run it with:\r\n```\r\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\r\n# For security purposes, please check the contents of collect_env.py before running it.\r\npython collect_env.py\r\n```\r\n\r\n - PyTorch Version (e.g., 1.0): 1.3.0\r\n - OS (e.g., Linux): Ubuntu 18.04 LTS\r\n - How you installed PyTorch (`conda`, `pip`, source): conda\r\n - Build command you used (if compiling from source): N/A\r\n - Python version: 3.7.4\r\n - CUDA/cuDNN version: 10.2/7.5\r\n - GPU models and configuration: 2x2080 Ti with Nvlink\r\n - Any other relevant information:\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\n<issue_comment>username_1: Oh, I meet the same thing!\n<issue_comment>username_2: I find `torch.sparse.FloatTensor' to be a good walk around.\n<issue_comment>username_3: @username_4 , can we close this issue via https://github.com/pytorch/pytorch/pull/41984 ?\n<issue_comment>username_4: Yes, we can close. It's solved already\n<issue_comment>username_5: Closing issue, since last comment indicates it as resolved.<issue_closed>","repo":"pytorch/pytorch","issue_id":511249220,"issue_number":28500}
{"question":"What action is mentioned in the GitHub pull request regarding the \"_\" prefix registration for operators in PyTorch?","answer":"The \"_\" prefix registration will be removed when the operators are all migrated to mobile.","id":51,"text":"cc @iseeyuan \r\n\r\nThey were added in https://github.com/pytorch/pytorch/pull/33294 and the PR says \"The \"_\" prefix registration will be removed when the operators are all migrated to mobile.\" This should be reflected in the code somewhere.","context":"<issue_start><issue_comment>Title: Meaning of _quantized namespace isn't documented\nusername_0: cc @iseeyuan \r\n\r\nThey were added in https://github.com/pytorch/pytorch/pull/33294 and the PR says \"The \"_\" prefix registration will be removed when the operators are all migrated to mobile.\" This should be reflected in the code somewhere.\n<issue_comment>username_0: I guess I'll fix this when I reorg the operator registrations\n<issue_comment>username_1: Hey @username_0  assigning this to you based on your comment. We can take this up if you can't get to it. Thanks!","repo":"pytorch/pytorch","issue_id":599121342,"issue_number":36510}
{"question":"Why does the `console.log` trigger in a different order in Chrome and Safari when running the provided React code example?","answer":"The difference in the order of `console.log` triggering in Chrome and Safari when running the provided React code example is due to the way each browser handles the asynchronous operations, specifically regarding the timing of the `useEffect` hook's execution and the timeout in the `App` component. Chrome executes the `useEffect` hook first, followed by the timeout in the `App` component, whereas Safari executes them in the opposite order. This discrepancy is a result of the underlying browser engine's implementation of asynchronous tasks, leading to the varying order of console logs.","id":57,"text":"In chrome and safari, the ```console.log``` was triggered in different order. [Example](https://codesandbox.io/s/amazing-payne-4lmxo) will show different result if opened in different browsers.\r\n\r\nReact version: 16.13.1\r\n\r\n## Steps To Reproduce\r\n\r\n1. Run following code in chrome and safari.\r\n2. Check console log\r\n\r\n```jsx\r\nimport React, { useState, useEffect } from \"react\";\r\n\r\nconst Test = props => {\r\n  useEffect(() => {\r\n    console.log(\"useEffect in child\");\r\n  }, []);\r\n\r\n  return \"Test useEffect\";\r\n};\r\n\r\nconst App = () => {\r\n  setTimeout(() => {\r\n    console.log(\"timeout in render\");\r\n  }, 0);\r\n\r\n  return <Test />;\r\n};\r\n\r\nexport default App\r\n```\r\n\r\nLink to code example:\r\n\r\n[Example](https://codesandbox.io/s/amazing-payne-4lmxo)\r\n\r\n## The current behavior\r\n\r\nThe message is not the same order;\r\n\r\n```jsx\r\n/**\r\n * chrome result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n\r\n/**\r\n * safari result\r\n *\r\n * timeout in render\r\n * useEffect in child\r\n */\r\n```\r\n\r\n## The expected behavior\r\n\r\nThe message should be same;\r\n\r\n```jsx\r\n/**\r\n * chrome result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n\r\n/**\r\n * safari result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n```","context":"<issue_start><issue_comment>Title: Bug: The trigger timing of \"useEffect\" is different in chrome and safari.\nusername_0: In chrome and safari, the ```console.log``` was triggered in different order. [Example](https://codesandbox.io/s/amazing-payne-4lmxo) will show different result if opened in different browsers.\r\n\r\nReact version: 16.13.1\r\n\r\n## Steps To Reproduce\r\n\r\n1. Run following code in chrome and safari.\r\n2. Check console log\r\n\r\n```jsx\r\nimport React, { useState, useEffect } from \"react\";\r\n\r\nconst Test = props => {\r\n  useEffect(() => {\r\n    console.log(\"useEffect in child\");\r\n  }, []);\r\n\r\n  return \"Test useEffect\";\r\n};\r\n\r\nconst App = () => {\r\n  setTimeout(() => {\r\n    console.log(\"timeout in render\");\r\n  }, 0);\r\n\r\n  return <Test />;\r\n};\r\n\r\nexport default App\r\n```\r\n\r\nLink to code example:\r\n\r\n[Example](https://codesandbox.io/s/amazing-payne-4lmxo)\r\n\r\n## The current behavior\r\n\r\nThe message is not the same order;\r\n\r\n```jsx\r\n/**\r\n * chrome result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n\r\n/**\r\n * safari result\r\n *\r\n * timeout in render\r\n * useEffect in child\r\n */\r\n```\r\n\r\n## The expected behavior\r\n\r\nThe message should be same;\r\n\r\n```jsx\r\n/**\r\n * chrome result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n\r\n/**\r\n * safari result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n```<issue_closed>\n<issue_comment>username_0: In chrome and safari, the ```console.log``` was triggered in different order. [Example](https://codesandbox.io/s/amazing-payne-4lmxo) will show different result if opened in different browsers.\r\n\r\nReact version: 16.13.1\r\n\r\n## Steps To Reproduce\r\n\r\n1. Run following code in chrome and safari.\r\n2. Check console log\r\n\r\n```jsx\r\nimport React, { useState, useEffect } from \"react\";\r\n\r\nconst Test = props => {\r\n  useEffect(() => {\r\n    console.log(\"useEffect in child\");\r\n  }, []);\r\n\r\n  return \"Test useEffect\";\r\n};\r\n\r\nconst App = () => {\r\n  setTimeout(() => {\r\n    console.log(\"timeout in render\");\r\n  }, 0);\r\n\r\n  return <Test />;\r\n};\r\n\r\nexport default App\r\n```\r\n\r\nLink to code example:\r\n\r\n[Example](https://codesandbox.io/s/amazing-payne-4lmxo)\r\n\r\n## The current behavior\r\n\r\nThe message is not the same order;\r\n\r\n```jsx\r\n/**\r\n * chrome result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n\r\n/**\r\n * safari result\r\n *\r\n * timeout in render\r\n * useEffect in child\r\n */\r\n```\r\n\r\n## The expected behavior\r\n\r\nThe message should be same;\r\n\r\n```jsx\r\n/**\r\n * chrome result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n\r\n/**\r\n * safari result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n```\n<issue_comment>username_1: You do the side effect in the render method:\r\n```typescript\r\nconst App = () => {\r\n  setTimeout(() => {\r\n    console.log(\"timeout in render\");\r\n  }, 0);\r\n\r\n  return <Test />;\r\n};\r\n```\r\nwhich is not allowed. React doesn't guarantee this kind of orders. you should put `setTimeout` inside an useEffect.\n<issue_comment>username_0: Yes, i agree this is not allowed. But different browsers should have same result.I think react should achieve lifecycle with more stable browser interfaces.\n<issue_comment>username_2: Many reasons why this is not a bug:\r\n\r\n - you are not allowed to make side effects in render. If you do that, the outcome is essentially \"undefined behaviour\", that is, nothing is guaranteed\r\n\r\n- React provides no guarantees about timing of `useEffect` with respect to render anyway.\r\n\r\nSo while this is an inconsistency and it could be fixed, there is not good reason to do so.\n<issue_comment>username_3: It is not supported to call `setTimeout` during rendering. React provides absolutely no guarantees about the order or the number of component render calls  so you can't rely on subtle timing aspects of them.\r\n\r\nIf I wrap it in `useEffect` I don't observe the difference.<issue_closed>","repo":"facebook/react","issue_id":664173077,"issue_number":19438}
{"question":"Why is it not recommended to perform side effects in the render method of a React component?","answer":"Performing side effects in the render method is not recommended in React because it can lead to unpredictable behavior due to the asynchronous nature of rendering. Placing side effects like setTimeout directly in the render method can cause the side effect to run multiple times and potentially interfere with the component's lifecycle. Instead, side effects should be placed inside useEffect hooks to ensure they are executed at the appropriate times and avoid issues with component rendering.","id":58,"text":"You do the side effect in the render method:\r\n```typescript\r\nconst App = () => {\r\n  setTimeout(() => {\r\n    console.log(\"timeout in render\");\r\n  }, 0);\r\n\r\n  return <Test />;\r\n};\r\n```\r\nwhich is not allowed. React doesn't guarantee this kind of orders. you should put `setTimeout` inside an useEffect.","context":"<issue_start><issue_comment>Title: Bug: The trigger timing of \"useEffect\" is different in chrome and safari.\nusername_0: In chrome and safari, the ```console.log``` was triggered in different order. [Example](https://codesandbox.io/s/amazing-payne-4lmxo) will show different result if opened in different browsers.\r\n\r\nReact version: 16.13.1\r\n\r\n## Steps To Reproduce\r\n\r\n1. Run following code in chrome and safari.\r\n2. Check console log\r\n\r\n```jsx\r\nimport React, { useState, useEffect } from \"react\";\r\n\r\nconst Test = props => {\r\n  useEffect(() => {\r\n    console.log(\"useEffect in child\");\r\n  }, []);\r\n\r\n  return \"Test useEffect\";\r\n};\r\n\r\nconst App = () => {\r\n  setTimeout(() => {\r\n    console.log(\"timeout in render\");\r\n  }, 0);\r\n\r\n  return <Test />;\r\n};\r\n\r\nexport default App\r\n```\r\n\r\nLink to code example:\r\n\r\n[Example](https://codesandbox.io/s/amazing-payne-4lmxo)\r\n\r\n## The current behavior\r\n\r\nThe message is not the same order;\r\n\r\n```jsx\r\n/**\r\n * chrome result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n\r\n/**\r\n * safari result\r\n *\r\n * timeout in render\r\n * useEffect in child\r\n */\r\n```\r\n\r\n## The expected behavior\r\n\r\nThe message should be same;\r\n\r\n```jsx\r\n/**\r\n * chrome result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n\r\n/**\r\n * safari result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n```<issue_closed>\n<issue_comment>username_0: In chrome and safari, the ```console.log``` was triggered in different order. [Example](https://codesandbox.io/s/amazing-payne-4lmxo) will show different result if opened in different browsers.\r\n\r\nReact version: 16.13.1\r\n\r\n## Steps To Reproduce\r\n\r\n1. Run following code in chrome and safari.\r\n2. Check console log\r\n\r\n```jsx\r\nimport React, { useState, useEffect } from \"react\";\r\n\r\nconst Test = props => {\r\n  useEffect(() => {\r\n    console.log(\"useEffect in child\");\r\n  }, []);\r\n\r\n  return \"Test useEffect\";\r\n};\r\n\r\nconst App = () => {\r\n  setTimeout(() => {\r\n    console.log(\"timeout in render\");\r\n  }, 0);\r\n\r\n  return <Test />;\r\n};\r\n\r\nexport default App\r\n```\r\n\r\nLink to code example:\r\n\r\n[Example](https://codesandbox.io/s/amazing-payne-4lmxo)\r\n\r\n## The current behavior\r\n\r\nThe message is not the same order;\r\n\r\n```jsx\r\n/**\r\n * chrome result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n\r\n/**\r\n * safari result\r\n *\r\n * timeout in render\r\n * useEffect in child\r\n */\r\n```\r\n\r\n## The expected behavior\r\n\r\nThe message should be same;\r\n\r\n```jsx\r\n/**\r\n * chrome result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n\r\n/**\r\n * safari result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n```\n<issue_comment>username_1: You do the side effect in the render method:\r\n```typescript\r\nconst App = () => {\r\n  setTimeout(() => {\r\n    console.log(\"timeout in render\");\r\n  }, 0);\r\n\r\n  return <Test />;\r\n};\r\n```\r\nwhich is not allowed. React doesn't guarantee this kind of orders. you should put `setTimeout` inside an useEffect.\n<issue_comment>username_0: Yes, i agree this is not allowed. But different browsers should have same result.I think react should achieve lifecycle with more stable browser interfaces.\n<issue_comment>username_2: Many reasons why this is not a bug:\r\n\r\n - you are not allowed to make side effects in render. If you do that, the outcome is essentially \"undefined behaviour\", that is, nothing is guaranteed\r\n\r\n- React provides no guarantees about timing of `useEffect` with respect to render anyway.\r\n\r\nSo while this is an inconsistency and it could be fixed, there is not good reason to do so.\n<issue_comment>username_3: It is not supported to call `setTimeout` during rendering. React provides absolutely no guarantees about the order or the number of component render calls  so you can't rely on subtle timing aspects of them.\r\n\r\nIf I wrap it in `useEffect` I don't observe the difference.<issue_closed>","repo":"facebook/react","issue_id":664173077,"issue_number":19438}
{"question":"What suggestion does the speaker have for React regarding browser interfaces and lifecycles?","answer":"The speaker suggests that React should achieve lifecycle with more stable browser interfaces.","id":59,"text":"Yes, i agree this is not allowed. But different browsers should have same result.I think react should achieve lifecycle with more stable browser interfaces.","context":"<issue_start><issue_comment>Title: Bug: The trigger timing of \"useEffect\" is different in chrome and safari.\nusername_0: In chrome and safari, the ```console.log``` was triggered in different order. [Example](https://codesandbox.io/s/amazing-payne-4lmxo) will show different result if opened in different browsers.\r\n\r\nReact version: 16.13.1\r\n\r\n## Steps To Reproduce\r\n\r\n1. Run following code in chrome and safari.\r\n2. Check console log\r\n\r\n```jsx\r\nimport React, { useState, useEffect } from \"react\";\r\n\r\nconst Test = props => {\r\n  useEffect(() => {\r\n    console.log(\"useEffect in child\");\r\n  }, []);\r\n\r\n  return \"Test useEffect\";\r\n};\r\n\r\nconst App = () => {\r\n  setTimeout(() => {\r\n    console.log(\"timeout in render\");\r\n  }, 0);\r\n\r\n  return <Test />;\r\n};\r\n\r\nexport default App\r\n```\r\n\r\nLink to code example:\r\n\r\n[Example](https://codesandbox.io/s/amazing-payne-4lmxo)\r\n\r\n## The current behavior\r\n\r\nThe message is not the same order;\r\n\r\n```jsx\r\n/**\r\n * chrome result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n\r\n/**\r\n * safari result\r\n *\r\n * timeout in render\r\n * useEffect in child\r\n */\r\n```\r\n\r\n## The expected behavior\r\n\r\nThe message should be same;\r\n\r\n```jsx\r\n/**\r\n * chrome result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n\r\n/**\r\n * safari result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n```<issue_closed>\n<issue_comment>username_0: In chrome and safari, the ```console.log``` was triggered in different order. [Example](https://codesandbox.io/s/amazing-payne-4lmxo) will show different result if opened in different browsers.\r\n\r\nReact version: 16.13.1\r\n\r\n## Steps To Reproduce\r\n\r\n1. Run following code in chrome and safari.\r\n2. Check console log\r\n\r\n```jsx\r\nimport React, { useState, useEffect } from \"react\";\r\n\r\nconst Test = props => {\r\n  useEffect(() => {\r\n    console.log(\"useEffect in child\");\r\n  }, []);\r\n\r\n  return \"Test useEffect\";\r\n};\r\n\r\nconst App = () => {\r\n  setTimeout(() => {\r\n    console.log(\"timeout in render\");\r\n  }, 0);\r\n\r\n  return <Test />;\r\n};\r\n\r\nexport default App\r\n```\r\n\r\nLink to code example:\r\n\r\n[Example](https://codesandbox.io/s/amazing-payne-4lmxo)\r\n\r\n## The current behavior\r\n\r\nThe message is not the same order;\r\n\r\n```jsx\r\n/**\r\n * chrome result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n\r\n/**\r\n * safari result\r\n *\r\n * timeout in render\r\n * useEffect in child\r\n */\r\n```\r\n\r\n## The expected behavior\r\n\r\nThe message should be same;\r\n\r\n```jsx\r\n/**\r\n * chrome result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n\r\n/**\r\n * safari result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n```\n<issue_comment>username_1: You do the side effect in the render method:\r\n```typescript\r\nconst App = () => {\r\n  setTimeout(() => {\r\n    console.log(\"timeout in render\");\r\n  }, 0);\r\n\r\n  return <Test />;\r\n};\r\n```\r\nwhich is not allowed. React doesn't guarantee this kind of orders. you should put `setTimeout` inside an useEffect.\n<issue_comment>username_0: Yes, i agree this is not allowed. But different browsers should have same result.I think react should achieve lifecycle with more stable browser interfaces.\n<issue_comment>username_2: Many reasons why this is not a bug:\r\n\r\n - you are not allowed to make side effects in render. If you do that, the outcome is essentially \"undefined behaviour\", that is, nothing is guaranteed\r\n\r\n- React provides no guarantees about timing of `useEffect` with respect to render anyway.\r\n\r\nSo while this is an inconsistency and it could be fixed, there is not good reason to do so.\n<issue_comment>username_3: It is not supported to call `setTimeout` during rendering. React provides absolutely no guarantees about the order or the number of component render calls  so you can't rely on subtle timing aspects of them.\r\n\r\nIf I wrap it in `useEffect` I don't observe the difference.<issue_closed>","repo":"facebook/react","issue_id":664173077,"issue_number":19438}
{"question":"Why is it not a bug to have side effects in render in React?","answer":"It is not a bug to have side effects in render in React because making side effects in render is not allowed and can lead to undefined behavior. React also does not provide guarantees about the timing of useEffect with respect to render.","id":60,"text":"Many reasons why this is not a bug:\r\n\r\n - you are not allowed to make side effects in render. If you do that, the outcome is essentially \"undefined behaviour\", that is, nothing is guaranteed\r\n\r\n- React provides no guarantees about timing of `useEffect` with respect to render anyway.\r\n\r\nSo while this is an inconsistency and it could be fixed, there is not good reason to do so.","context":"<issue_start><issue_comment>Title: Bug: The trigger timing of \"useEffect\" is different in chrome and safari.\nusername_0: In chrome and safari, the ```console.log``` was triggered in different order. [Example](https://codesandbox.io/s/amazing-payne-4lmxo) will show different result if opened in different browsers.\r\n\r\nReact version: 16.13.1\r\n\r\n## Steps To Reproduce\r\n\r\n1. Run following code in chrome and safari.\r\n2. Check console log\r\n\r\n```jsx\r\nimport React, { useState, useEffect } from \"react\";\r\n\r\nconst Test = props => {\r\n  useEffect(() => {\r\n    console.log(\"useEffect in child\");\r\n  }, []);\r\n\r\n  return \"Test useEffect\";\r\n};\r\n\r\nconst App = () => {\r\n  setTimeout(() => {\r\n    console.log(\"timeout in render\");\r\n  }, 0);\r\n\r\n  return <Test />;\r\n};\r\n\r\nexport default App\r\n```\r\n\r\nLink to code example:\r\n\r\n[Example](https://codesandbox.io/s/amazing-payne-4lmxo)\r\n\r\n## The current behavior\r\n\r\nThe message is not the same order;\r\n\r\n```jsx\r\n/**\r\n * chrome result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n\r\n/**\r\n * safari result\r\n *\r\n * timeout in render\r\n * useEffect in child\r\n */\r\n```\r\n\r\n## The expected behavior\r\n\r\nThe message should be same;\r\n\r\n```jsx\r\n/**\r\n * chrome result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n\r\n/**\r\n * safari result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n```<issue_closed>\n<issue_comment>username_0: In chrome and safari, the ```console.log``` was triggered in different order. [Example](https://codesandbox.io/s/amazing-payne-4lmxo) will show different result if opened in different browsers.\r\n\r\nReact version: 16.13.1\r\n\r\n## Steps To Reproduce\r\n\r\n1. Run following code in chrome and safari.\r\n2. Check console log\r\n\r\n```jsx\r\nimport React, { useState, useEffect } from \"react\";\r\n\r\nconst Test = props => {\r\n  useEffect(() => {\r\n    console.log(\"useEffect in child\");\r\n  }, []);\r\n\r\n  return \"Test useEffect\";\r\n};\r\n\r\nconst App = () => {\r\n  setTimeout(() => {\r\n    console.log(\"timeout in render\");\r\n  }, 0);\r\n\r\n  return <Test />;\r\n};\r\n\r\nexport default App\r\n```\r\n\r\nLink to code example:\r\n\r\n[Example](https://codesandbox.io/s/amazing-payne-4lmxo)\r\n\r\n## The current behavior\r\n\r\nThe message is not the same order;\r\n\r\n```jsx\r\n/**\r\n * chrome result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n\r\n/**\r\n * safari result\r\n *\r\n * timeout in render\r\n * useEffect in child\r\n */\r\n```\r\n\r\n## The expected behavior\r\n\r\nThe message should be same;\r\n\r\n```jsx\r\n/**\r\n * chrome result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n\r\n/**\r\n * safari result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n```\n<issue_comment>username_1: You do the side effect in the render method:\r\n```typescript\r\nconst App = () => {\r\n  setTimeout(() => {\r\n    console.log(\"timeout in render\");\r\n  }, 0);\r\n\r\n  return <Test />;\r\n};\r\n```\r\nwhich is not allowed. React doesn't guarantee this kind of orders. you should put `setTimeout` inside an useEffect.\n<issue_comment>username_0: Yes, i agree this is not allowed. But different browsers should have same result.I think react should achieve lifecycle with more stable browser interfaces.\n<issue_comment>username_2: Many reasons why this is not a bug:\r\n\r\n - you are not allowed to make side effects in render. If you do that, the outcome is essentially \"undefined behaviour\", that is, nothing is guaranteed\r\n\r\n- React provides no guarantees about timing of `useEffect` with respect to render anyway.\r\n\r\nSo while this is an inconsistency and it could be fixed, there is not good reason to do so.\n<issue_comment>username_3: It is not supported to call `setTimeout` during rendering. React provides absolutely no guarantees about the order or the number of component render calls  so you can't rely on subtle timing aspects of them.\r\n\r\nIf I wrap it in `useEffect` I don't observe the difference.<issue_closed>","repo":"facebook/react","issue_id":664173077,"issue_number":19438}
{"question":"Why is it not supported to call `setTimeout` during rendering in React?","answer":"Calling `setTimeout` during rendering in React is not supported because React provides no guarantees about the order or the number of component render calls, making it unreliable to rely on timing aspects of rendering. Wrapping `setTimeout` in `useEffect` does not change this behavior.","id":61,"text":"It is not supported to call `setTimeout` during rendering. React provides absolutely no guarantees about the order or the number of component render calls  so you can't rely on subtle timing aspects of them.\r\n\r\nIf I wrap it in `useEffect` I don't observe the difference.","context":"<issue_start><issue_comment>Title: Bug: The trigger timing of \"useEffect\" is different in chrome and safari.\nusername_0: In chrome and safari, the ```console.log``` was triggered in different order. [Example](https://codesandbox.io/s/amazing-payne-4lmxo) will show different result if opened in different browsers.\r\n\r\nReact version: 16.13.1\r\n\r\n## Steps To Reproduce\r\n\r\n1. Run following code in chrome and safari.\r\n2. Check console log\r\n\r\n```jsx\r\nimport React, { useState, useEffect } from \"react\";\r\n\r\nconst Test = props => {\r\n  useEffect(() => {\r\n    console.log(\"useEffect in child\");\r\n  }, []);\r\n\r\n  return \"Test useEffect\";\r\n};\r\n\r\nconst App = () => {\r\n  setTimeout(() => {\r\n    console.log(\"timeout in render\");\r\n  }, 0);\r\n\r\n  return <Test />;\r\n};\r\n\r\nexport default App\r\n```\r\n\r\nLink to code example:\r\n\r\n[Example](https://codesandbox.io/s/amazing-payne-4lmxo)\r\n\r\n## The current behavior\r\n\r\nThe message is not the same order;\r\n\r\n```jsx\r\n/**\r\n * chrome result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n\r\n/**\r\n * safari result\r\n *\r\n * timeout in render\r\n * useEffect in child\r\n */\r\n```\r\n\r\n## The expected behavior\r\n\r\nThe message should be same;\r\n\r\n```jsx\r\n/**\r\n * chrome result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n\r\n/**\r\n * safari result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n```<issue_closed>\n<issue_comment>username_0: In chrome and safari, the ```console.log``` was triggered in different order. [Example](https://codesandbox.io/s/amazing-payne-4lmxo) will show different result if opened in different browsers.\r\n\r\nReact version: 16.13.1\r\n\r\n## Steps To Reproduce\r\n\r\n1. Run following code in chrome and safari.\r\n2. Check console log\r\n\r\n```jsx\r\nimport React, { useState, useEffect } from \"react\";\r\n\r\nconst Test = props => {\r\n  useEffect(() => {\r\n    console.log(\"useEffect in child\");\r\n  }, []);\r\n\r\n  return \"Test useEffect\";\r\n};\r\n\r\nconst App = () => {\r\n  setTimeout(() => {\r\n    console.log(\"timeout in render\");\r\n  }, 0);\r\n\r\n  return <Test />;\r\n};\r\n\r\nexport default App\r\n```\r\n\r\nLink to code example:\r\n\r\n[Example](https://codesandbox.io/s/amazing-payne-4lmxo)\r\n\r\n## The current behavior\r\n\r\nThe message is not the same order;\r\n\r\n```jsx\r\n/**\r\n * chrome result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n\r\n/**\r\n * safari result\r\n *\r\n * timeout in render\r\n * useEffect in child\r\n */\r\n```\r\n\r\n## The expected behavior\r\n\r\nThe message should be same;\r\n\r\n```jsx\r\n/**\r\n * chrome result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n\r\n/**\r\n * safari result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n```\n<issue_comment>username_1: You do the side effect in the render method:\r\n```typescript\r\nconst App = () => {\r\n  setTimeout(() => {\r\n    console.log(\"timeout in render\");\r\n  }, 0);\r\n\r\n  return <Test />;\r\n};\r\n```\r\nwhich is not allowed. React doesn't guarantee this kind of orders. you should put `setTimeout` inside an useEffect.\n<issue_comment>username_0: Yes, i agree this is not allowed. But different browsers should have same result.I think react should achieve lifecycle with more stable browser interfaces.\n<issue_comment>username_2: Many reasons why this is not a bug:\r\n\r\n - you are not allowed to make side effects in render. If you do that, the outcome is essentially \"undefined behaviour\", that is, nothing is guaranteed\r\n\r\n- React provides no guarantees about timing of `useEffect` with respect to render anyway.\r\n\r\nSo while this is an inconsistency and it could be fixed, there is not good reason to do so.\n<issue_comment>username_3: It is not supported to call `setTimeout` during rendering. React provides absolutely no guarantees about the order or the number of component render calls  so you can't rely on subtle timing aspects of them.\r\n\r\nIf I wrap it in `useEffect` I don't observe the difference.<issue_closed>","repo":"facebook/react","issue_id":664173077,"issue_number":19438}
{"question":"What changes were made to support vmap in the accumulate_grad code according to the text?","answer":"Changes made to support vmap in the accumulate_grad code include adding vmap support for Tensor::strides(), replacing instances of empty_strided with new_empty_strided, and changing an in-place operation in accumulate_grad.h.","id":62,"text":"Stack from [ghstack](https://github.com/username_1/ghstack):\n* #49120 Add batched grad testing to gradcheck, turn it on in test_autograd\n* **#49119 Update accumulate_grad to support vmap**\n* #49118 Move inplace_is_vmap_compatible to BatchedTensorImpl.h\n\nI don't know how the accumulate_grad code gets hit, so I went through\nall places in accumulate_grad that are definitely impossible to vmap\nthrough and changed them.\n\nTo support this:\n- I added vmap support for Tensor::strides(). It returns the strides\nthat correspond to the public dimensions of the tensor (not the ones\nbeing vmapped over).\n- Changed an instance of empty_strided to new_empty_strided.\n- Replaced an in-place operation in accumulate_grad.h\n\nTest Plan:\n- added a test for calling strides() inside of vmap\n- added a test that exercise some of the accumulate grad code path. I\ndon't know if it exercises all of the code changes in this PR.\nSuggestions for more test cases are very welcome.","context":"<issue_start><issue_comment>Title: Update accumulate_grad to support vmap\nusername_0: Stack from [ghstack](https://github.com/username_1/ghstack):\n* #49120 Add batched grad testing to gradcheck, turn it on in test_autograd\n* **#49119 Update accumulate_grad to support vmap**\n* #49118 Move inplace_is_vmap_compatible to BatchedTensorImpl.h\n\nI don't know how the accumulate_grad code gets hit, so I went through\nall places in accumulate_grad that are definitely impossible to vmap\nthrough and changed them.\n\nTo support this:\n- I added vmap support for Tensor::strides(). It returns the strides\nthat correspond to the public dimensions of the tensor (not the ones\nbeing vmapped over).\n- Changed an instance of empty_strided to new_empty_strided.\n- Replaced an in-place operation in accumulate_grad.h\n\nTest Plan:\n- added a test for calling strides() inside of vmap\n- added a test that exercise some of the accumulate grad code path. I\ndon't know if it exercises all of the code changes in this PR.\nSuggestions for more test cases are very welcome.\n<issue_comment>username_1: So, now that vmap supports strides, there was something I wasn't too clear about. Suppose that I have a contiguous tensor with strides (8, 4, 2, 1) and let's say I vmap with batch dim = 2 (e.g., not the first dimension). Is the BatchedTensor contiguous? (I don't think it should be!) I couldn't tell from this PR what would happen in this case.\n<issue_comment>username_0: It's not contiguous. The reason behind this is asking \"is a BatchedTensor contiguous\" is equivalent to asking \"are the per-examples of the BatchedTensor contiguous\". The per-examples in your example would have strides (8, 4, 1) which make them not contiguous","repo":"pytorch/pytorch","issue_id":760718102,"issue_number":49119}
{"question":"If a contiguous tensor with strides (8, 4, 2, 1) is vmap'ed with batch dim = 2 (not the first dimension), is the BatchedTensor contiguous?","answer":"The BatchedTensor resulting from vmap in this scenario would not be contiguous. Vmap with a batch dimension that is not the first dimension will introduce non-contiguous strides in the BatchedTensor, making it non-contiguous.","id":63,"text":"So, now that vmap supports strides, there was something I wasn't too clear about. Suppose that I have a contiguous tensor with strides (8, 4, 2, 1) and let's say I vmap with batch dim = 2 (e.g., not the first dimension). Is the BatchedTensor contiguous? (I don't think it should be!) I couldn't tell from this PR what would happen in this case.","context":"<issue_start><issue_comment>Title: Update accumulate_grad to support vmap\nusername_0: Stack from [ghstack](https://github.com/username_1/ghstack):\n* #49120 Add batched grad testing to gradcheck, turn it on in test_autograd\n* **#49119 Update accumulate_grad to support vmap**\n* #49118 Move inplace_is_vmap_compatible to BatchedTensorImpl.h\n\nI don't know how the accumulate_grad code gets hit, so I went through\nall places in accumulate_grad that are definitely impossible to vmap\nthrough and changed them.\n\nTo support this:\n- I added vmap support for Tensor::strides(). It returns the strides\nthat correspond to the public dimensions of the tensor (not the ones\nbeing vmapped over).\n- Changed an instance of empty_strided to new_empty_strided.\n- Replaced an in-place operation in accumulate_grad.h\n\nTest Plan:\n- added a test for calling strides() inside of vmap\n- added a test that exercise some of the accumulate grad code path. I\ndon't know if it exercises all of the code changes in this PR.\nSuggestions for more test cases are very welcome.\n<issue_comment>username_1: So, now that vmap supports strides, there was something I wasn't too clear about. Suppose that I have a contiguous tensor with strides (8, 4, 2, 1) and let's say I vmap with batch dim = 2 (e.g., not the first dimension). Is the BatchedTensor contiguous? (I don't think it should be!) I couldn't tell from this PR what would happen in this case.\n<issue_comment>username_0: It's not contiguous. The reason behind this is asking \"is a BatchedTensor contiguous\" is equivalent to asking \"are the per-examples of the BatchedTensor contiguous\". The per-examples in your example would have strides (8, 4, 1) which make them not contiguous","repo":"pytorch/pytorch","issue_id":760718102,"issue_number":49119}
{"question":"Why is asking if a BatchedTensor is contiguous equivalent to asking if the per-examples of the BatchedTensor are contiguous?","answer":"The reason behind asking if a BatchedTensor is contiguous is equivalent to asking if the per-examples of the BatchedTensor are contiguous because the per-examples in the example provided have strides (8, 4, 1) which make them not contiguous.","id":64,"text":"It's not contiguous. The reason behind this is asking \"is a BatchedTensor contiguous\" is equivalent to asking \"are the per-examples of the BatchedTensor contiguous\". The per-examples in your example would have strides (8, 4, 1) which make them not contiguous","context":"<issue_start><issue_comment>Title: Update accumulate_grad to support vmap\nusername_0: Stack from [ghstack](https://github.com/username_1/ghstack):\n* #49120 Add batched grad testing to gradcheck, turn it on in test_autograd\n* **#49119 Update accumulate_grad to support vmap**\n* #49118 Move inplace_is_vmap_compatible to BatchedTensorImpl.h\n\nI don't know how the accumulate_grad code gets hit, so I went through\nall places in accumulate_grad that are definitely impossible to vmap\nthrough and changed them.\n\nTo support this:\n- I added vmap support for Tensor::strides(). It returns the strides\nthat correspond to the public dimensions of the tensor (not the ones\nbeing vmapped over).\n- Changed an instance of empty_strided to new_empty_strided.\n- Replaced an in-place operation in accumulate_grad.h\n\nTest Plan:\n- added a test for calling strides() inside of vmap\n- added a test that exercise some of the accumulate grad code path. I\ndon't know if it exercises all of the code changes in this PR.\nSuggestions for more test cases are very welcome.\n<issue_comment>username_1: So, now that vmap supports strides, there was something I wasn't too clear about. Suppose that I have a contiguous tensor with strides (8, 4, 2, 1) and let's say I vmap with batch dim = 2 (e.g., not the first dimension). Is the BatchedTensor contiguous? (I don't think it should be!) I couldn't tell from this PR what would happen in this case.\n<issue_comment>username_0: It's not contiguous. The reason behind this is asking \"is a BatchedTensor contiguous\" is equivalent to asking \"are the per-examples of the BatchedTensor contiguous\". The per-examples in your example would have strides (8, 4, 1) which make them not contiguous","repo":"pytorch/pytorch","issue_id":760718102,"issue_number":49119}
{"question":"How can the mentioned addon be added to the React documentation and how can the ReactRAFBatchingStrategy addon or a similar one be used in React applications?","answer":"To add the mentioned addon to the React documentation, it should be included in the appropriate location as specified in the documentation guidelines. For using the ReactRAFBatchingStrategy addon or a similar one in React applications, the addon should be imported into the application code and its functionality should be utilized according to the addon's documentation.","id":65,"text":"I didn't realise this was in the addons bundle, would be good to get it added to the docs.\r\n\r\nhttp://facebook.github.io/react/docs/addons.html\r\nhttps://github.com/facebook/react/blob/master/src/browser/ReactWithAddons.js#L40\r\n\r\nWould be good to also include an example of using https://github.com/facebook/react/blob/master/src/addons/ReactRAFBatchingStrategy.js or something similar.","context":"<issue_start><issue_comment>Title: Document React.addons.batchedUpdates\nusername_0: I didn't realise this was in the addons bundle, would be good to get it added to the docs.\r\n\r\nhttp://facebook.github.io/react/docs/addons.html\r\nhttps://github.com/facebook/react/blob/master/src/browser/ReactWithAddons.js#L40\r\n\r\nWould be good to also include an example of using https://github.com/facebook/react/blob/master/src/addons/ReactRAFBatchingStrategy.js or something similar.\n<issue_comment>username_1: rAF batching isn't supported and won't go in the docs, but batchedUpdates should.\n<issue_comment>username_2: There are also known issues with rAF batching (mostly around controlled components).\n<issue_comment>username_3: Should we deprecate and eventually remove the RAF batching?\n<issue_comment>username_1: It's never been supported in any fashion.\n<issue_comment>username_1: (i.e., it's already deprecated?)\n<issue_comment>username_4: shouldn't this file be deleted in that case?\r\nhttps://github.com/facebook/react/blob/master/src/addons/ReactRAFBatchingStrategy.js\n<issue_comment>username_1: That's not exposed as a public API in any of our builds, but sure, we can delete it  PR incoming.\n<issue_comment>username_5: Killed in https://github.com/facebook/react/pull/6016<issue_closed>\n<issue_comment>username_1: We should document ReactDOM.unstable_batchedUpdates.\n<issue_comment>username_1: I didn't realise this was in the addons bundle, would be good to get it added to the docs.\r\n\r\nhttp://facebook.github.io/react/docs/addons.html\r\nhttps://github.com/facebook/react/blob/master/src/browser/ReactWithAddons.js#L40\r\n\r\nWould be good to also include an example of using https://github.com/facebook/react/blob/master/src/addons/ReactRAFBatchingStrategy.js or something similar.\n<issue_comment>username_5: @username_1 Since when did we start documenting `unstable_` features?  Haven't we always left them intentionally undocumented?  We also have unstable_renderSubtree and unstable_handleError and others, but I thought the decision was to not document them until they become stable.  We document them as part of the release process.\n<issue_comment>username_1: Sure.<issue_closed>","repo":"facebook/react","issue_id":65878895,"issue_number":3570}
{"question":"What functions were identified to potentially be the source of the problem related to CUDA block/threads in the code snippet?","answer":"UpSampleBicubic2d, upsample_get_value_bounded, and upsample_increment_value_bounded were identified as the functions that could potentially be the source of the problem related to CUDA block/threads.","id":156,"text":"it seems just UpSampleBicubic2d is using upsample_get_value_bounded (https://github.com/pytorch/pytorch/pull/19630/files#diff-5092da792c30694ee4adf0d0ae2a37c6R171) and upsample_increment_value_bounded (https://github.com/pytorch/pytorch/pull/19630/files#diff-5092da792c30694ee4adf0d0ae2a37c6R191)\r\n\r\nmaybe the problem could be inside one of these functions ... maybe related to the order of indexes (x, y) ...\r\n\r\nbut the problem seems to be related to cuda block/threads ... so not sure if it is really related to these functions.","context":"<issue_start><issue_comment>Title: [WIP] UpSample GPU Porting \nusername_0: resolves #16158\n<issue_comment>username_1: Fewer errors than before, 4 instead of 8 with the same message:\r\n```\r\nRuntimeError: CUDA error: too many resources requested for launch\r\n```\r\nThis seems to be an existing problem: gh-8103.\r\n\r\n@username_2 could you have a look at this and tell us what you think?\n<issue_comment>username_2: Looking at it very briefly, the `Jetson TX` referenced in the issue only has 256 cores.\r\n\r\nSo while the issue looks the same, I'd probably not expect it on the CI platforms.  FWIW, some recent CI tests in other issues are green.\n<issue_comment>username_2: @pytorchbot retest this please.\n<issue_comment>username_1: same failures.\n<issue_comment>username_2: Is this rebased on the latest master (you can also ask the bot to rebase)?\n<issue_comment>username_0: I rebased that yesterday ... also needed to resolve conflicts because 7 days ago upsample files were changed.\n<issue_comment>username_0: @username_2 do you have any idea about what could be this problem? or a way to debug that?\r\nalso it seems some jobs is taking a lot of time to build, for example, for some job the estimate time is 19h .. not sure if it the regular estimate ..\n<issue_comment>username_2: @username_0 If you can't reproduce it at home it seems hard to debug other than reading at the diffs again.  I can take a look on Monday, it's getting a bit late here.\n<issue_comment>username_2: Also, has anyone found a way to show the actual hardware used on the CI in detail?\n<issue_comment>username_0: @username_2 I am working in parallel on a paperspace environment .. it tooks a lot of time it is running with 8 cpu cores.\n<issue_comment>username_2: But have you ever been able to reproduce this issue on paperspace?\n<issue_comment>username_1: I can reproduce it locally: Arch Linux, CUDA 10.0, RTX2070 GPU. Given the CI failures, I think it should be reproducible for multiple CUDA and GPU versions. I just haven't worked on any CUDA code before, and am short on time, so I'd rather not dig too deep.\n<issue_comment>username_0: not yet .. my last building was with a my previous commit ... I will work again in this task in some minutes :)\n<issue_comment>username_1: @username_0 your previous commit had the same failure though (except for the less clear exception message), and it seems quite reproducible. So I think you'll see it now.\n<issue_comment>username_2: Ah, thanks.  @username_0, then I guess just compiling with \"DEBUG=1\" and stepping through the offending code with gdb may be the fastest way.\n<issue_comment>username_0: @username_2 thanks! I will try that!\n<issue_comment>username_0: it seems just UpSampleBicubic2d is using upsample_get_value_bounded (https://github.com/pytorch/pytorch/pull/19630/files#diff-5092da792c30694ee4adf0d0ae2a37c6R171) and upsample_increment_value_bounded (https://github.com/pytorch/pytorch/pull/19630/files#diff-5092da792c30694ee4adf0d0ae2a37c6R191)\r\n\r\nmaybe the problem could be inside one of these functions ... maybe related to the order of indexes (x, y) ...\r\n\r\nbut the problem seems to be related to cuda block/threads ... so not sure if it is really related to these functions.\n<issue_comment>username_2: The new code uses far more registers than the existing one. I verified that the both versions actually call the offending test case with `1024` in `blockDim`.  So it's very likely a register issue.\r\n\r\n`Existing` uses `64` registers, which seems to be optimal or my card:\r\n\r\n```\r\nptxas info    : 77696 bytes gmem, 72 bytes cmem[3]\r\nptxas info    : Compiling entry function '_Z23bicubic_interp2d_kerneliddb15THCDeviceTensorIdLi4Ei16DefaultPtrTraitsES1_' for 'sm_61'\r\nptxas info    : Function properties for _Z23bicubic_interp2d_kerneliddb15THCDeviceTensorIdLi4Ei16DefaultPtrTraitsES1_\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 64 registers, 432 bytes cmem[0], 24 bytes cmem[2]\r\n```\r\n\r\n`New` uses `124` registers, which is too much for my card:\r\n\r\n```\r\nptxas info    : 77696 bytes gmem, 72 bytes cmem[3]\r\nptxas info    : Compiling entry function '_ZN2at6native76_GLOBAL__N__52_tmpxft_0000626d_00000000_6_UpSampleBicubic2d_cpp1_ii_b4c1e1f328upsample_bicubic2d_out_frameElddbNS_20PackedTensorAccessorIdLm4ENS_16DefaultPtrTraitsElEES4_' for 'sm_61'\r\nptxas info    : Function properties for _ZN2at6native76_GLOBAL__N__52_tmpxft_0000626d_00000000_6_UpSampleBicubic2d_cpp1_ii_b4c1e1f328upsample_bicubic2d_out_frameElddbNS_20PackedTensorAccessorIdLm4ENS_16DefaultPtrTraitsElEES4_\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 124 registers, 496 bytes cmem[0], 24 bytes cmem[2]\r\n```\r\n\r\nWith `__launch_bounds__(1024)`, the code again uses `64` registers.\r\n\r\nIf you use `C10_LAUNCH_BOUNDS_1(1024)` **for both kernels**, the tests pass here.\r\n\r\n\r\nNow *why* is the regcount higher in the new code? It could be `PackedTensorAccessor`, it could be the fact that many instances of `int` have been changed to `int64_t`. :)\r\n\r\nYou could experiment or just use the launch bounds.  Other code in `native/cuda` seems to use lower bounds for `blockDim`, too.  1024 seems to be an outlier.\n<issue_comment>username_0: @username_2 \r\n\r\nit seems it worked locally! thank you so much!  I really appreciate that!\n<issue_comment>username_0: thanks @username_1 and @username_2 for all the help!\r\n\r\n@username_3 it is done for review!\n<issue_comment>username_3: CircleCI runs on AWS, and the I'm fairly sure we've got one of the g3 types, either g3.4xlarge or g3.8xlarge\n<issue_comment>username_3: cc @ngimel, if you want to take a look at this (I'm planning to do a review too)\n<issue_comment>username_0: @username_4 could it be addressed in another PR?\n<issue_comment>username_4: @username_0 Sure, it's definitely better to rename both cpu & gpu impls together in a separate PR. Just want to make sure we have the correct one after porting.\n<issue_comment>username_3: Just as a clarification: I'm OK with having changes which are improvements over the old kernels come in latter patches.\n<issue_comment>username_3: I finished reviewing one of the kernels, and did a cursory scan of the others. In general the patch looks like it's in good shape. I'd suggest that we fix up the major correctness issues (PackedTensorAccessor ref, and int32 versus int64 indexing) and anything small that is easy to fix (for example, double zeroing the array), and leave other improvements for follow ups. Should be ready to land soon!\n<issue_comment>username_0: thanks so much @username_3!\r\nI will let you know when it is ready again for a new review! thanks!\n<issue_comment>username_0: @username_2 @username_3 \r\nnot sure but the errors on CI seems to be related to jenkins ... it seems all these jobs that failed (for building) ran by 01h 01min ... not sure if it is a coincidence ...\n<issue_comment>username_1: @username_0 indeed it looks like all jobs were aborted at the same time. no obvious issues in the build log related to your code. Comparing e.g. the `cuda9-cudnn7` build with a successful one from another PR, it takes 1hr 14min there and your build is about at the place where the other one is after an hour.\r\n\r\nI suggest to just push a new commit to rebuild. Probably a temporary CI hiccup.\n<issue_comment>username_3: I accidentally rebooted Jenkins yesterday which is the likely cause, my apologies.\r\n\r\n@pytorchbot retest this please\n<issue_comment>username_0: @username_3 @username_2 @username_1 \r\n\r\nall tests passed except `pr/caffe2-py2-cuda9.0-cudnn7-windows-build`:\r\n \r\n```\r\n14:43:49 Build timed out (after 180 minutes). Marking the build as failed.\r\n14:43:49 Build was aborted\r\n14:43:49 [BFA] Scanning build for known causes...\r\n14:43:49 [BFA] No failure causes found\r\n14:43:49 [BFA] Done. 0s\r\n14:43:49 Finished: FAILURE\r\n```\r\n\r\nnot sure if this timeout means that the code now is slower.\r\n\r\nwhat do you think? do you have any suggestion?\n<issue_comment>username_3: Sometimes the Windows build flakes out like that. It didn't timeout while running a relevant test, so I judge it to be not your problem.\n<issue_comment>username_3: The launch bounds logic is wrong, but I acknowledge that this is a big patch already; just fix it in a follow up. I am going to go ahead and land this.\n<issue_comment>username_0: sounds good @username_3 thanks!","repo":"pytorch/pytorch","issue_id":436372857,"issue_number":19630}
{"question":"What addon in the React library is mentioned in the text, and where can its documentation be found?","answer":"The addon mentioned in the text is ReactWithAddons. Its documentation can be found at http://facebook.github.io/react/docs/addons.html.","id":75,"text":"I didn't realise this was in the addons bundle, would be good to get it added to the docs.\r\n\r\nhttp://facebook.github.io/react/docs/addons.html\r\nhttps://github.com/facebook/react/blob/master/src/browser/ReactWithAddons.js#L40\r\n\r\nWould be good to also include an example of using https://github.com/facebook/react/blob/master/src/addons/ReactRAFBatchingStrategy.js or something similar.","context":"<issue_start><issue_comment>Title: Document React.addons.batchedUpdates\nusername_0: I didn't realise this was in the addons bundle, would be good to get it added to the docs.\r\n\r\nhttp://facebook.github.io/react/docs/addons.html\r\nhttps://github.com/facebook/react/blob/master/src/browser/ReactWithAddons.js#L40\r\n\r\nWould be good to also include an example of using https://github.com/facebook/react/blob/master/src/addons/ReactRAFBatchingStrategy.js or something similar.\n<issue_comment>username_1: rAF batching isn't supported and won't go in the docs, but batchedUpdates should.\n<issue_comment>username_2: There are also known issues with rAF batching (mostly around controlled components).\n<issue_comment>username_3: Should we deprecate and eventually remove the RAF batching?\n<issue_comment>username_1: It's never been supported in any fashion.\n<issue_comment>username_1: (i.e., it's already deprecated?)\n<issue_comment>username_4: shouldn't this file be deleted in that case?\r\nhttps://github.com/facebook/react/blob/master/src/addons/ReactRAFBatchingStrategy.js\n<issue_comment>username_1: That's not exposed as a public API in any of our builds, but sure, we can delete it  PR incoming.\n<issue_comment>username_5: Killed in https://github.com/facebook/react/pull/6016<issue_closed>\n<issue_comment>username_1: We should document ReactDOM.unstable_batchedUpdates.\n<issue_comment>username_1: I didn't realise this was in the addons bundle, would be good to get it added to the docs.\r\n\r\nhttp://facebook.github.io/react/docs/addons.html\r\nhttps://github.com/facebook/react/blob/master/src/browser/ReactWithAddons.js#L40\r\n\r\nWould be good to also include an example of using https://github.com/facebook/react/blob/master/src/addons/ReactRAFBatchingStrategy.js or something similar.\n<issue_comment>username_5: @username_1 Since when did we start documenting `unstable_` features?  Haven't we always left them intentionally undocumented?  We also have unstable_renderSubtree and unstable_handleError and others, but I thought the decision was to not document them until they become stable.  We document them as part of the release process.\n<issue_comment>username_1: Sure.<issue_closed>","repo":"facebook/react","issue_id":65878895,"issue_number":3570}
{"question":"Why are `unstable_` features such as `unstable_renderSubtree` and `unstable_handleError` being documented despite the previous decision to leave them undocumented until they become stable?","answer":"The decision to document `unstable_` features like `unstable_renderSubtree` and `unstable_handleError` is to incorporate them as part of the release process, providing visibility and context for tracking changes and improvements. This allows developers to actively engage with these features, provide feedback, and prepare for their stabilization in future releases.","id":76,"text":"@username_1 Since when did we start documenting `unstable_` features?  Haven't we always left them intentionally undocumented?  We also have unstable_renderSubtree and unstable_handleError and others, but I thought the decision was to not document them until they become stable.  We document them as part of the release process.","context":"<issue_start><issue_comment>Title: Document React.addons.batchedUpdates\nusername_0: I didn't realise this was in the addons bundle, would be good to get it added to the docs.\r\n\r\nhttp://facebook.github.io/react/docs/addons.html\r\nhttps://github.com/facebook/react/blob/master/src/browser/ReactWithAddons.js#L40\r\n\r\nWould be good to also include an example of using https://github.com/facebook/react/blob/master/src/addons/ReactRAFBatchingStrategy.js or something similar.\n<issue_comment>username_1: rAF batching isn't supported and won't go in the docs, but batchedUpdates should.\n<issue_comment>username_2: There are also known issues with rAF batching (mostly around controlled components).\n<issue_comment>username_3: Should we deprecate and eventually remove the RAF batching?\n<issue_comment>username_1: It's never been supported in any fashion.\n<issue_comment>username_1: (i.e., it's already deprecated?)\n<issue_comment>username_4: shouldn't this file be deleted in that case?\r\nhttps://github.com/facebook/react/blob/master/src/addons/ReactRAFBatchingStrategy.js\n<issue_comment>username_1: That's not exposed as a public API in any of our builds, but sure, we can delete it  PR incoming.\n<issue_comment>username_5: Killed in https://github.com/facebook/react/pull/6016<issue_closed>\n<issue_comment>username_1: We should document ReactDOM.unstable_batchedUpdates.\n<issue_comment>username_1: I didn't realise this was in the addons bundle, would be good to get it added to the docs.\r\n\r\nhttp://facebook.github.io/react/docs/addons.html\r\nhttps://github.com/facebook/react/blob/master/src/browser/ReactWithAddons.js#L40\r\n\r\nWould be good to also include an example of using https://github.com/facebook/react/blob/master/src/addons/ReactRAFBatchingStrategy.js or something similar.\n<issue_comment>username_5: @username_1 Since when did we start documenting `unstable_` features?  Haven't we always left them intentionally undocumented?  We also have unstable_renderSubtree and unstable_handleError and others, but I thought the decision was to not document them until they become stable.  We document them as part of the release process.\n<issue_comment>username_1: Sure.<issue_closed>","repo":"facebook/react","issue_id":65878895,"issue_number":3570}
{"question":"What issue is the user reporting regarding the React package version comment at the top of the build?","answer":"The user is reporting that the React 16.6.3 package has a comment at the top claiming to be React 16.6.1, which is incorrect.","id":78,"text":"<!--\r\n  Note: if the issue is about documentation or the website, please file it at:\r\n  https://github.com/reactjs/reactjs.org/issues/new\r\n-->\r\n\r\n**Do you want to request a *feature* or report a *bug*?** bug\r\n\r\n**What is the current behavior?** The react@16.6.3 package has a comment at the top claiming to be React 16.6.1.\r\n\r\n**If the current behavior is a bug, please provide the steps to reproduce and if possible a minimal demo of the problem. Your bug will get fixed much faster if we can run your code and it doesn't have dependencies other than React. Paste the link to your JSFiddle (https://jsfiddle.net/Luktwrdm/) or CodeSandbox (https://codesandbox.io/s/new) example below:**\r\n```shell\r\ncd /tmp\r\nnpm pack react@16.6.3\r\ntar xf react-16.6.3.tgz\r\nhead -1 package/cjs/react.development.js\r\n# output: /** @license React v16.6.1\r\n```\r\n\r\n**What is the expected behavior?**\r\nThe comment at the top of the build indicates the correct version.\r\n\r\n**Which versions of React, and which browser / OS are affected by this issue? Did this work in previous versions of React?**\r\n16.6.2 and 16.6.3 both have the wrong comment. 16.6.1 and 16.6.0 are correct.","context":"<issue_start><issue_comment>Title: Comments in NPM package have wrong version\nusername_0: <!--\r\n  Note: if the issue is about documentation or the website, please file it at:\r\n  https://github.com/reactjs/reactjs.org/issues/new\r\n-->\r\n\r\n**Do you want to request a *feature* or report a *bug*?** bug\r\n\r\n**What is the current behavior?** The react@16.6.3 package has a comment at the top claiming to be React 16.6.1.\r\n\r\n**If the current behavior is a bug, please provide the steps to reproduce and if possible a minimal demo of the problem. Your bug will get fixed much faster if we can run your code and it doesn't have dependencies other than React. Paste the link to your JSFiddle (https://jsfiddle.net/Luktwrdm/) or CodeSandbox (https://codesandbox.io/s/new) example below:**\r\n```shell\r\ncd /tmp\r\nnpm pack react@16.6.3\r\ntar xf react-16.6.3.tgz\r\nhead -1 package/cjs/react.development.js\r\n# output: /** @license React v16.6.1\r\n```\r\n\r\n**What is the expected behavior?**\r\nThe comment at the top of the build indicates the correct version.\r\n\r\n**Which versions of React, and which browser / OS are affected by this issue? Did this work in previous versions of React?**\r\n16.6.2 and 16.6.3 both have the wrong comment. 16.6.1 and 16.6.0 are correct.\n<issue_comment>username_0: Also, https://github.com/facebook/react/blob/v16.6.3/packages/shared/ReactVersion.js claims to be `16.6.1`, but in the actual NPM package ReactVersion is set correctly.\n<issue_comment>username_1: @username_2 should this be handled as part of the release flow you've been working on?<issue_closed>\n<issue_comment>username_2: This wasa transient issue, caused by a somewhat manual release that Andrew ran. (There's no bug in the release script.) This issue should only affect 16.6.2 and 16.6.3. Fortunately it shouldn't cause any observable behavior unless someone reads the comments. ","repo":"facebook/react","issue_id":389079792,"issue_number":14410}
{"question":"What discrepancy is mentioned in the React version between the file on GitHub and the NPM package?","answer":"The React version mentioned in the file on GitHub is claimed to be 16.6.1, but in the actual NPM package, ReactVersion is set correctly.","id":79,"text":"Also, https://github.com/facebook/react/blob/v16.6.3/packages/shared/ReactVersion.js claims to be `16.6.1`, but in the actual NPM package ReactVersion is set correctly.","context":"<issue_start><issue_comment>Title: Comments in NPM package have wrong version\nusername_0: <!--\r\n  Note: if the issue is about documentation or the website, please file it at:\r\n  https://github.com/reactjs/reactjs.org/issues/new\r\n-->\r\n\r\n**Do you want to request a *feature* or report a *bug*?** bug\r\n\r\n**What is the current behavior?** The react@16.6.3 package has a comment at the top claiming to be React 16.6.1.\r\n\r\n**If the current behavior is a bug, please provide the steps to reproduce and if possible a minimal demo of the problem. Your bug will get fixed much faster if we can run your code and it doesn't have dependencies other than React. Paste the link to your JSFiddle (https://jsfiddle.net/Luktwrdm/) or CodeSandbox (https://codesandbox.io/s/new) example below:**\r\n```shell\r\ncd /tmp\r\nnpm pack react@16.6.3\r\ntar xf react-16.6.3.tgz\r\nhead -1 package/cjs/react.development.js\r\n# output: /** @license React v16.6.1\r\n```\r\n\r\n**What is the expected behavior?**\r\nThe comment at the top of the build indicates the correct version.\r\n\r\n**Which versions of React, and which browser / OS are affected by this issue? Did this work in previous versions of React?**\r\n16.6.2 and 16.6.3 both have the wrong comment. 16.6.1 and 16.6.0 are correct.\n<issue_comment>username_0: Also, https://github.com/facebook/react/blob/v16.6.3/packages/shared/ReactVersion.js claims to be `16.6.1`, but in the actual NPM package ReactVersion is set correctly.\n<issue_comment>username_1: @username_2 should this be handled as part of the release flow you've been working on?<issue_closed>\n<issue_comment>username_2: This wasa transient issue, caused by a somewhat manual release that Andrew ran. (There's no bug in the release script.) This issue should only affect 16.6.2 and 16.6.3. Fortunately it shouldn't cause any observable behavior unless someone reads the comments. ","repo":"facebook/react","issue_id":389079792,"issue_number":14410}
{"question":"What caused the transient issue mentioned in the text?","answer":"The transient issue was caused by a manual release that Andrew ran.","id":81,"text":"This wasa transient issue, caused by a somewhat manual release that Andrew ran. (There's no bug in the release script.) This issue should only affect 16.6.2 and 16.6.3. Fortunately it shouldn't cause any observable behavior unless someone reads the comments. ","context":"<issue_start><issue_comment>Title: Comments in NPM package have wrong version\nusername_0: <!--\r\n  Note: if the issue is about documentation or the website, please file it at:\r\n  https://github.com/reactjs/reactjs.org/issues/new\r\n-->\r\n\r\n**Do you want to request a *feature* or report a *bug*?** bug\r\n\r\n**What is the current behavior?** The react@16.6.3 package has a comment at the top claiming to be React 16.6.1.\r\n\r\n**If the current behavior is a bug, please provide the steps to reproduce and if possible a minimal demo of the problem. Your bug will get fixed much faster if we can run your code and it doesn't have dependencies other than React. Paste the link to your JSFiddle (https://jsfiddle.net/Luktwrdm/) or CodeSandbox (https://codesandbox.io/s/new) example below:**\r\n```shell\r\ncd /tmp\r\nnpm pack react@16.6.3\r\ntar xf react-16.6.3.tgz\r\nhead -1 package/cjs/react.development.js\r\n# output: /** @license React v16.6.1\r\n```\r\n\r\n**What is the expected behavior?**\r\nThe comment at the top of the build indicates the correct version.\r\n\r\n**Which versions of React, and which browser / OS are affected by this issue? Did this work in previous versions of React?**\r\n16.6.2 and 16.6.3 both have the wrong comment. 16.6.1 and 16.6.0 are correct.\n<issue_comment>username_0: Also, https://github.com/facebook/react/blob/v16.6.3/packages/shared/ReactVersion.js claims to be `16.6.1`, but in the actual NPM package ReactVersion is set correctly.\n<issue_comment>username_1: @username_2 should this be handled as part of the release flow you've been working on?<issue_closed>\n<issue_comment>username_2: This wasa transient issue, caused by a somewhat manual release that Andrew ran. (There's no bug in the release script.) This issue should only affect 16.6.2 and 16.6.3. Fortunately it shouldn't cause any observable behavior unless someone reads the comments. ","repo":"facebook/react","issue_id":389079792,"issue_number":14410}
{"question":"What issue is encountered when attempting to run PyTorch on 2 GPUs according to the stack trace provided?","answer":"The issue encountered is that it is blocked when using 2 GPUs but runs successfully with 1 GPU.","id":82,"text":"The pytorch version is 1.0.0, when use 2 gpus on one machine it was blocked,but it run successfully when use 1 gpu\r\n\r\nThe call stack in python as follow:\r\n```\r\nFile \"/home/admin/code/PROPAGATE/tools/0_train_base.py\", line 319, in <module>\r\n    process(sys.argv[1], sys.argv[2], int(sys.argv[3]), sys.argv[4])\r\n  File \"/home/admin/code/PROPAGATE/tools/0_train_base.py\", line 241, in process\r\n    solver.solve()\r\n  File \"/home/admin/code/PROPAGATE/tools/0_train_base.py\", line 190, in solve\r\n    loss, accB, accN, lossC = self.iterate(XB, YB, XN, YN, True)\r\n  File \"/home/admin/code/PROPAGATE/tools/0_train_base.py\", line 140, in iterate\r\n    self.optimizer.step()\r\n  File \"/home/admin/code/PROPAGATE/rpm/anaconda/lib/python2.7/site-packages/torch/optim/sgd.py\", line 107, in step\r\n    p.data.add_(-group['lr'], d_p)\r\n  File \"<string>\", line 1, in <module>\r\n  File \"<string>\", line 1, in <module>\r\n```","context":"<issue_start><issue_comment>Title: pytorch was blocked at optimizer.step\nusername_0: The pytorch version is 1.0.0, when use 2 gpus on one machine it was blocked,but it run successfully when use 1 gpu\r\n\r\nThe call stack in python as follow:\r\n```\r\nFile \"/home/admin/code/PROPAGATE/tools/0_train_base.py\", line 319, in <module>\r\n    process(sys.argv[1], sys.argv[2], int(sys.argv[3]), sys.argv[4])\r\n  File \"/home/admin/code/PROPAGATE/tools/0_train_base.py\", line 241, in process\r\n    solver.solve()\r\n  File \"/home/admin/code/PROPAGATE/tools/0_train_base.py\", line 190, in solve\r\n    loss, accB, accN, lossC = self.iterate(XB, YB, XN, YN, True)\r\n  File \"/home/admin/code/PROPAGATE/tools/0_train_base.py\", line 140, in iterate\r\n    self.optimizer.step()\r\n  File \"/home/admin/code/PROPAGATE/rpm/anaconda/lib/python2.7/site-packages/torch/optim/sgd.py\", line 107, in step\r\n    p.data.add_(-group['lr'], d_p)\r\n  File \"<string>\", line 1, in <module>\r\n  File \"<string>\", line 1, in <module>\r\n```\n<issue_comment>username_1: Please provide a minimal repro script. It's hard to debug without an repro here. Thanks!\n<issue_comment>username_2: Closing due to age and lack of repro script<issue_closed>","repo":"pytorch/pytorch","issue_id":408518494,"issue_number":16934}
{"question":"What change is being made to functionals in the ghstack repository according to the text?","answer":"The change involves making all non-input arguments to functionals part of its options.","id":85,"text":"Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#29319 Make all non-input arguments to functionals part of its options**\n* #29318 Pass F::*FuncOptions instead of torch::nn::*Options to functionals, and make F::*FuncOptions a different class when necessary","context":"<issue_start><issue_comment>Title: Make all non-input arguments to functionals part of its options\nusername_0: Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#29319 Make all non-input arguments to functionals part of its options**\n* #29318 Pass F::*FuncOptions instead of torch::nn::*Options to functionals, and make F::*FuncOptions a different class when necessary\n<issue_comment>username_1: ## CircleCI build failures summary\nAs of commit 2e954dd:\n* 0/19 recognized as flaky\n* 19/19 failures introduced in this PR\n\n\n\nHere are the [reasons each build failed](https://dr.pytorch.org/commit-details.html?sha1=2e954dd7243d98f23b016bc55bfaff8aff11dba1).\n\n---\nThis comment was automatically generated by [Dr. CI](https://github.com/username_1/circleci-failure-tracker/tree/master/docs/from-pull-request-comment).\nFollow [this link to opt-out](https://dr.pytorch.org/admin/comments-opt-out.html) of these comments for your Pull Requests.\n\n\nPlease report bugs/suggestions on the [GitHub issue tracker](https://github.com/username_1/circleci-failure-tracker/issues).","repo":"pytorch/pytorch","issue_id":518718409,"issue_number":29319}
{"question":"How many of the 19 builds were recognized as flaky in the CircleCI build failures summary at commit 2e954dd?","answer":"0/19 builds were recognized as flaky in the CircleCI build failures summary at commit 2e954dd.","id":86,"text":"## CircleCI build failures summary\nAs of commit 2e954dd:\n* 0/19 recognized as flaky\n* 19/19 failures introduced in this PR\n\n\n\nHere are the [reasons each build failed](https://dr.pytorch.org/commit-details.html?sha1=2e954dd7243d98f23b016bc55bfaff8aff11dba1).\n\n---\nThis comment was automatically generated by [Dr. CI](https://github.com/username_1/circleci-failure-tracker/tree/master/docs/from-pull-request-comment).\nFollow [this link to opt-out](https://dr.pytorch.org/admin/comments-opt-out.html) of these comments for your Pull Requests.\n\n\nPlease report bugs/suggestions on the [GitHub issue tracker](https://github.com/username_1/circleci-failure-tracker/issues).","context":"<issue_start><issue_comment>Title: Make all non-input arguments to functionals part of its options\nusername_0: Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#29319 Make all non-input arguments to functionals part of its options**\n* #29318 Pass F::*FuncOptions instead of torch::nn::*Options to functionals, and make F::*FuncOptions a different class when necessary\n<issue_comment>username_1: ## CircleCI build failures summary\nAs of commit 2e954dd:\n* 0/19 recognized as flaky\n* 19/19 failures introduced in this PR\n\n\n\nHere are the [reasons each build failed](https://dr.pytorch.org/commit-details.html?sha1=2e954dd7243d98f23b016bc55bfaff8aff11dba1).\n\n---\nThis comment was automatically generated by [Dr. CI](https://github.com/username_1/circleci-failure-tracker/tree/master/docs/from-pull-request-comment).\nFollow [this link to opt-out](https://dr.pytorch.org/admin/comments-opt-out.html) of these comments for your Pull Requests.\n\n\nPlease report bugs/suggestions on the [GitHub issue tracker](https://github.com/username_1/circleci-failure-tracker/issues).","repo":"pytorch/pytorch","issue_id":518718409,"issue_number":29319}
{"question":"What is the purpose of updating the master Persons of Interest to match version v1.2.0 in the PyTorch repository?","answer":"The purpose is to ensure that the list of key individuals remains up-to-date based on the changes and developments in the PyTorch project.","id":87,"text":"Update master Persons of Interest to match that from v1.2.0\r\nhttps://github.com/pytorch/pytorch/blob/v1.2.0/docs/source/community/persons_of_interest.rst","context":"<issue_start><issue_comment>Title: Update master Persons of Interest to match v1.2.0\nusername_0: Update master Persons of Interest to match that from v1.2.0\r\nhttps://github.com/pytorch/pytorch/blob/v1.2.0/docs/source/community/persons_of_interest.rst\n<issue_comment>username_1: yeah, there is definitely a lot going on here. I think it might be better to can this PR and start with a new PR on top what we currently have. It would certainly be much more incremental.","repo":"pytorch/pytorch","issue_id":530629662,"issue_number":30582}
{"question":"What approach does the text recommend for handling the current situation with the pull request (PR)?","answer":"The text recommends cancelling the current PR and starting a new PR on top of the existing work for a more incremental approach.","id":88,"text":"yeah, there is definitely a lot going on here. I think it might be better to can this PR and start with a new PR on top what we currently have. It would certainly be much more incremental.","context":"<issue_start><issue_comment>Title: Update master Persons of Interest to match v1.2.0\nusername_0: Update master Persons of Interest to match that from v1.2.0\r\nhttps://github.com/pytorch/pytorch/blob/v1.2.0/docs/source/community/persons_of_interest.rst\n<issue_comment>username_1: yeah, there is definitely a lot going on here. I think it might be better to can this PR and start with a new PR on top what we currently have. It would certainly be much more incremental.","repo":"pytorch/pytorch","issue_id":530629662,"issue_number":30582}
{"question":"What was the reason for the failure in the test case related to NCCL error handling in PyTorch?","answer":"The reason for the failure in the test case related to NCCL error handling in PyTorch was an assertion error due to the absolute difference between two values not being less than or equal to a specific threshold.","id":89,"text":"https://app.circleci.com/jobs/github/pytorch/pytorch/4155093\r\n\r\n```\r\nJan 07 20:36:55 ======================================================================\r\nJan 07 20:36:55 FAIL [3.205s]: test_nccl_errors_blocking_clean_exit (__main__.NcclErrorHandlingTest)\r\nJan 07 20:36:55 ----------------------------------------------------------------------\r\nJan 07 20:36:55 Traceback (most recent call last):\r\nJan 07 20:36:55   File \"/var/lib/jenkins/workspace/test/common_distributed.py\", line 130, in wrapper\r\nJan 07 20:36:55     self._join_processes(fn)\r\nJan 07 20:36:55   File \"/var/lib/jenkins/workspace/test/common_distributed.py\", line 211, in _join_processes\r\nJan 07 20:36:55     self._check_return_codes(elapsed_time)\r\nJan 07 20:36:55   File \"/var/lib/jenkins/workspace/test/common_distributed.py\", line 231, in _check_return_codes\r\nJan 07 20:36:55     self.assertEqual(p.exitcode, first_process.exitcode)\r\nJan 07 20:36:55   File \"/var/lib/jenkins/workspace/test/common_utils.py\", line 888, in assertEqual\r\nJan 07 20:36:55     super(TestCase, self).assertLessEqual(abs(x - y), prec, message)\r\nJan 07 20:36:55 AssertionError: 11 not less than or equal to 1e-05 : \r\n```","context":"<issue_start><issue_comment>Title: test_nccl_errors_blocking_clean_exit is flaky\nusername_0: https://app.circleci.com/jobs/github/pytorch/pytorch/4155093\r\n\r\n```\r\nJan 07 20:36:55 ======================================================================\r\nJan 07 20:36:55 FAIL [3.205s]: test_nccl_errors_blocking_clean_exit (__main__.NcclErrorHandlingTest)\r\nJan 07 20:36:55 ----------------------------------------------------------------------\r\nJan 07 20:36:55 Traceback (most recent call last):\r\nJan 07 20:36:55   File \"/var/lib/jenkins/workspace/test/common_distributed.py\", line 130, in wrapper\r\nJan 07 20:36:55     self._join_processes(fn)\r\nJan 07 20:36:55   File \"/var/lib/jenkins/workspace/test/common_distributed.py\", line 211, in _join_processes\r\nJan 07 20:36:55     self._check_return_codes(elapsed_time)\r\nJan 07 20:36:55   File \"/var/lib/jenkins/workspace/test/common_distributed.py\", line 231, in _check_return_codes\r\nJan 07 20:36:55     self.assertEqual(p.exitcode, first_process.exitcode)\r\nJan 07 20:36:55   File \"/var/lib/jenkins/workspace/test/common_utils.py\", line 888, in assertEqual\r\nJan 07 20:36:55     super(TestCase, self).assertLessEqual(abs(x - y), prec, message)\r\nJan 07 20:36:55 AssertionError: 11 not less than or equal to 1e-05 : \r\n```\n<issue_comment>username_1: Cannot reproduce this failure after running the same test 100 times on 8 GPUs.<issue_closed>","repo":"pytorch/pytorch","issue_id":546524878,"issue_number":31924}
{"question":"What is the task described in the stack from ghstack that includes \"#32207 [DO NOT MERGE] iOS 1.4.0 binary push\"?","answer":"The task described in the stack is \"#32207 [DO NOT MERGE] iOS 1.4.0 binary push\".","id":91,"text":"Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#32207 [DO NOT MERGE] iOS 1.4.0 binary push**\n* #32147 [iOS] Update Gemfile\n* #31912 [CI]Suppress pip logs","context":"<issue_start><issue_comment>Title: [DO NOT MERGE] iOS 1.4.0 binary push\nusername_0: Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#32207 [DO NOT MERGE] iOS 1.4.0 binary push**\n* #32147 [iOS] Update Gemfile\n* #31912 [CI]Suppress pip logs\n<issue_comment>username_1: ## :pill: CircleCI build failures summary and remediations\nAs of commit 2f788305:\n\n\n* **1/1** failures introduced in this PR\n\n\n## Detailed failure analysis\n\nOne may explore the probable reasons each build failed interactively [on the Dr. CI website](https://dr.pytorch.org/commit-details.html?sha1=2f7883058b67b349112aba3524e2774237bd2e6c).\n\n### :detective: 1 new failure recognized by patterns\nThe following build failures do not appear to be due to upstream breakage:\n#### [![See CircleCI build](https://avatars0.githubusercontent.com/ml/7?s=12)](https://circleci.com/gh/pytorch/pytorch/4249792) binary_ios_upload (1/1)\n**Step:** \"Upload\" ([full log](https://dr.pytorch.org/api/view-log-full?build_id=86265225) | [pattern match details](https://dr.pytorch.org/build-details.html?build_id=86265225))\n<details>\n<summary>\n<code>fatal error: /Applications/Xcode-11.2.1.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/lipo: can't open input file: /Users/distiller/workspace/ios/x86_64/lib/libtorch_cpu.a (No such file or directory)</code>\n</summary>\n\n```\n++ lipo -create /Users/distiller/workspace/ios/x86_64/lib/libcpuinfo.a /Users/distiller/workspace/ios/arm64/lib/libcpuinfo.a -o /Users/distiller/workspace/zip/install/lib/libcpuinfo.a \n++ for lib in '${target_libs[*]}' \n++ libs=(${ARTIFACTS_DIR}/x86_64/lib/${lib} ${ARTIFACTS_DIR}/arm64/lib/${lib}) \n++ lipo -create /Users/distiller/workspace/ios/x86_64/lib/libeigen_blas.a /Users/distiller/workspace/ios/arm64/lib/libeigen_blas.a -o /Users/distiller/workspace/zip/install/lib/libeigen_blas.a \n++ for lib in '${target_libs[*]}' \n++ libs=(${ARTIFACTS_DIR}/x86_64/lib/${lib} ${ARTIFACTS_DIR}/arm64/lib/${lib}) \n++ lipo -create /Users/distiller/workspace/ios/x86_64/lib/libpytorch_qnnpack.a /Users/distiller/workspace/ios/arm64/lib/libpytorch_qnnpack.a -o /Users/distiller/workspace/zip/install/lib/libpytorch_qnnpack.a \n++ for lib in '${target_libs[*]}' \n++ libs=(${ARTIFACTS_DIR}/x86_64/lib/${lib} ${ARTIFACTS_DIR}/arm64/lib/${lib}) \n++ lipo -create /Users/distiller/workspace/ios/x86_64/lib/libtorch_cpu.a /Users/distiller/workspace/ios/arm64/lib/libtorch_cpu.a -o /Users/distiller/workspace/zip/install/lib/libtorch_cpu.a \nfatal error: /Applications/Xcode-11.2.1.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/lipo: can't open input file: /Users/distiller/workspace/ios/x86_64/lib/libtorch_cpu.a (No such file or directory) \n```\n\n</details>\n\n\n\n---\n\n<details><summary>This comment was automatically generated by <a href=\"https://github.com/username_1/circleci-failure-tracker/tree/master/docs/from-pull-request-comment\">Dr. CI</a> (expand for details).</summary>Follow <a href=\"https://dr.pytorch.org/admin/comments-opt-out.html\">this link to opt-out</a> of these comments for your Pull Requests.\n\nPlease report bugs/suggestions on the <a href=\"https://github.com/username_1/circleci-failure-tracker/issues\">GitHub issue tracker</a>.\n</details>","repo":"pytorch/pytorch","issue_id":549943186,"issue_number":32207}
{"question":"What fatal error occurred during the 'binary_ios_upload' step in the build process?","answer":"The fatal error during the 'binary_ios_upload' step was: /Applications/Xcode-11.2.1.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/lipo: can't open input file: /Users/distiller/workspace/ios/x86_64/lib/libtorch_cpu.a (No such file or directory)","id":92,"text":"## :pill: CircleCI build failures summary and remediations\nAs of commit 2f788305:\n\n\n* **1/1** failures introduced in this PR\n\n\n## Detailed failure analysis\n\nOne may explore the probable reasons each build failed interactively [on the Dr. CI website](https://dr.pytorch.org/commit-details.html?sha1=2f7883058b67b349112aba3524e2774237bd2e6c).\n\n### :detective: 1 new failure recognized by patterns\nThe following build failures do not appear to be due to upstream breakage:\n#### [![See CircleCI build](https://avatars0.githubusercontent.com/ml/7?s=12)](https://circleci.com/gh/pytorch/pytorch/4249792) binary_ios_upload (1/1)\n**Step:** \"Upload\" ([full log](https://dr.pytorch.org/api/view-log-full?build_id=86265225) | [pattern match details](https://dr.pytorch.org/build-details.html?build_id=86265225))\n<details>\n<summary>\n<code>fatal error: /Applications/Xcode-11.2.1.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/lipo: can't open input file: /Users/distiller/workspace/ios/x86_64/lib/libtorch_cpu.a (No such file or directory)</code>\n</summary>\n\n```\n++ lipo -create /Users/distiller/workspace/ios/x86_64/lib/libcpuinfo.a /Users/distiller/workspace/ios/arm64/lib/libcpuinfo.a -o /Users/distiller/workspace/zip/install/lib/libcpuinfo.a \n++ for lib in '${target_libs[*]}' \n++ libs=(${ARTIFACTS_DIR}/x86_64/lib/${lib} ${ARTIFACTS_DIR}/arm64/lib/${lib}) \n++ lipo -create /Users/distiller/workspace/ios/x86_64/lib/libeigen_blas.a /Users/distiller/workspace/ios/arm64/lib/libeigen_blas.a -o /Users/distiller/workspace/zip/install/lib/libeigen_blas.a \n++ for lib in '${target_libs[*]}' \n++ libs=(${ARTIFACTS_DIR}/x86_64/lib/${lib} ${ARTIFACTS_DIR}/arm64/lib/${lib}) \n++ lipo -create /Users/distiller/workspace/ios/x86_64/lib/libpytorch_qnnpack.a /Users/distiller/workspace/ios/arm64/lib/libpytorch_qnnpack.a -o /Users/distiller/workspace/zip/install/lib/libpytorch_qnnpack.a \n++ for lib in '${target_libs[*]}' \n++ libs=(${ARTIFACTS_DIR}/x86_64/lib/${lib} ${ARTIFACTS_DIR}/arm64/lib/${lib}) \n++ lipo -create /Users/distiller/workspace/ios/x86_64/lib/libtorch_cpu.a /Users/distiller/workspace/ios/arm64/lib/libtorch_cpu.a -o /Users/distiller/workspace/zip/install/lib/libtorch_cpu.a \nfatal error: /Applications/Xcode-11.2.1.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/lipo: can't open input file: /Users/distiller/workspace/ios/x86_64/lib/libtorch_cpu.a (No such file or directory) \n```\n\n</details>\n\n\n\n---\n\n<details><summary>This comment was automatically generated by <a href=\"https://github.com/username_1/circleci-failure-tracker/tree/master/docs/from-pull-request-comment\">Dr. CI</a> (expand for details).</summary>Follow <a href=\"https://dr.pytorch.org/admin/comments-opt-out.html\">this link to opt-out</a> of these comments for your Pull Requests.\n\nPlease report bugs/suggestions on the <a href=\"https://github.com/username_1/circleci-failure-tracker/issues\">GitHub issue tracker</a>.\n</details>","context":"<issue_start><issue_comment>Title: [DO NOT MERGE] iOS 1.4.0 binary push\nusername_0: Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#32207 [DO NOT MERGE] iOS 1.4.0 binary push**\n* #32147 [iOS] Update Gemfile\n* #31912 [CI]Suppress pip logs\n<issue_comment>username_1: ## :pill: CircleCI build failures summary and remediations\nAs of commit 2f788305:\n\n\n* **1/1** failures introduced in this PR\n\n\n## Detailed failure analysis\n\nOne may explore the probable reasons each build failed interactively [on the Dr. CI website](https://dr.pytorch.org/commit-details.html?sha1=2f7883058b67b349112aba3524e2774237bd2e6c).\n\n### :detective: 1 new failure recognized by patterns\nThe following build failures do not appear to be due to upstream breakage:\n#### [![See CircleCI build](https://avatars0.githubusercontent.com/ml/7?s=12)](https://circleci.com/gh/pytorch/pytorch/4249792) binary_ios_upload (1/1)\n**Step:** \"Upload\" ([full log](https://dr.pytorch.org/api/view-log-full?build_id=86265225) | [pattern match details](https://dr.pytorch.org/build-details.html?build_id=86265225))\n<details>\n<summary>\n<code>fatal error: /Applications/Xcode-11.2.1.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/lipo: can't open input file: /Users/distiller/workspace/ios/x86_64/lib/libtorch_cpu.a (No such file or directory)</code>\n</summary>\n\n```\n++ lipo -create /Users/distiller/workspace/ios/x86_64/lib/libcpuinfo.a /Users/distiller/workspace/ios/arm64/lib/libcpuinfo.a -o /Users/distiller/workspace/zip/install/lib/libcpuinfo.a \n++ for lib in '${target_libs[*]}' \n++ libs=(${ARTIFACTS_DIR}/x86_64/lib/${lib} ${ARTIFACTS_DIR}/arm64/lib/${lib}) \n++ lipo -create /Users/distiller/workspace/ios/x86_64/lib/libeigen_blas.a /Users/distiller/workspace/ios/arm64/lib/libeigen_blas.a -o /Users/distiller/workspace/zip/install/lib/libeigen_blas.a \n++ for lib in '${target_libs[*]}' \n++ libs=(${ARTIFACTS_DIR}/x86_64/lib/${lib} ${ARTIFACTS_DIR}/arm64/lib/${lib}) \n++ lipo -create /Users/distiller/workspace/ios/x86_64/lib/libpytorch_qnnpack.a /Users/distiller/workspace/ios/arm64/lib/libpytorch_qnnpack.a -o /Users/distiller/workspace/zip/install/lib/libpytorch_qnnpack.a \n++ for lib in '${target_libs[*]}' \n++ libs=(${ARTIFACTS_DIR}/x86_64/lib/${lib} ${ARTIFACTS_DIR}/arm64/lib/${lib}) \n++ lipo -create /Users/distiller/workspace/ios/x86_64/lib/libtorch_cpu.a /Users/distiller/workspace/ios/arm64/lib/libtorch_cpu.a -o /Users/distiller/workspace/zip/install/lib/libtorch_cpu.a \nfatal error: /Applications/Xcode-11.2.1.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/lipo: can't open input file: /Users/distiller/workspace/ios/x86_64/lib/libtorch_cpu.a (No such file or directory) \n```\n\n</details>\n\n\n\n---\n\n<details><summary>This comment was automatically generated by <a href=\"https://github.com/username_1/circleci-failure-tracker/tree/master/docs/from-pull-request-comment\">Dr. CI</a> (expand for details).</summary>Follow <a href=\"https://dr.pytorch.org/admin/comments-opt-out.html\">this link to opt-out</a> of these comments for your Pull Requests.\n\nPlease report bugs/suggestions on the <a href=\"https://github.com/username_1/circleci-failure-tracker/issues\">GitHub issue tracker</a>.\n</details>","repo":"pytorch/pytorch","issue_id":549943186,"issue_number":32207}
{"question":"What does the following code snippet do?\nt = torch.empty(... , dtype=torch.int64)\nt.random_(torch.iinfo(torch.int64).min, None)","answer":"The code snippet creates a tensor 't' filled with 64-bit integer values. It uses 'torch.empty' to create an empty tensor and 'random_' to fill it with random values in the range from the minimum 64-bit integer value to the maximum 64-bit integer value.","id":93,"text":"```\r\nt = torch.empty(... , dtype=torch.int64)\r\nt.random_(torch.iinfo(torch.int64).min, None)\r\n```\r\nshould be able to fill tensor with all 64 bit number including `torch.iinfo(torch.int64).min` and `torch.iinfo(torch.int64).max`","context":"<issue_start><issue_comment>Title: Tensor.random_ should be able to generate all 64 bit numbers including min and max value\nusername_0: ```\r\nt = torch.empty(... , dtype=torch.int64)\r\nt.random_(torch.iinfo(torch.int64).min, None)\r\n```\r\nshould be able to fill tensor with all 64 bit number including `torch.iinfo(torch.int64).min` and `torch.iinfo(torch.int64).max`\n<issue_comment>username_1: This is a dupe of #16944\n<issue_comment>username_0: Fixed in https://github.com/pytorch/pytorch/commit/095de1e872671c4da2cc0965976058cf752c3be0\r\nhttps://github.com/pytorch/pytorch/blob/master/aten/src/ATen/test/cpu_rng_test.cpp#L93 demonstrates how to generate uint64_t max value, (2^64 - 1)<issue_closed>\n<issue_comment>username_0: ```\r\nIn [1]: import torch\r\n\r\nIn [2]: \"{0:b}\".format(torch.empty(1, dtype=torch.int64, device='cpu').random_(torch.iinfo(torch.int64).max, None)[0])\r\nOut[2]: '111111111111111111111111111111111111111111111111111111111111111'\r\n\r\nIn [3]: \"{0:b}\".format(torch.empty(1, dtype=torch.int64, device='cuda').random_(torch.iinfo(torch.int64).max, None)[0])\r\nOut[3]: '111111111111111111111111111111111111111111111111111111111111111'\r\n```","repo":"pytorch/pytorch","issue_id":564830266,"issue_number":33299}
{"question":"What does the code snippet in https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/test/cpu_rng_test.cpp#L93 demonstrate?","answer":"The code snippet demonstrates how to generate the maximum uint64_t value, which is (2^64 - 1).","id":95,"text":"Fixed in https://github.com/pytorch/pytorch/commit/095de1e872671c4da2cc0965976058cf752c3be0\r\nhttps://github.com/pytorch/pytorch/blob/master/aten/src/ATen/test/cpu_rng_test.cpp#L93 demonstrates how to generate uint64_t max value, (2^64 - 1)","context":"<issue_start><issue_comment>Title: Tensor.random_ should be able to generate all 64 bit numbers including min and max value\nusername_0: ```\r\nt = torch.empty(... , dtype=torch.int64)\r\nt.random_(torch.iinfo(torch.int64).min, None)\r\n```\r\nshould be able to fill tensor with all 64 bit number including `torch.iinfo(torch.int64).min` and `torch.iinfo(torch.int64).max`\n<issue_comment>username_1: This is a dupe of #16944\n<issue_comment>username_0: Fixed in https://github.com/pytorch/pytorch/commit/095de1e872671c4da2cc0965976058cf752c3be0\r\nhttps://github.com/pytorch/pytorch/blob/master/aten/src/ATen/test/cpu_rng_test.cpp#L93 demonstrates how to generate uint64_t max value, (2^64 - 1)<issue_closed>\n<issue_comment>username_0: ```\r\nIn [1]: import torch\r\n\r\nIn [2]: \"{0:b}\".format(torch.empty(1, dtype=torch.int64, device='cpu').random_(torch.iinfo(torch.int64).max, None)[0])\r\nOut[2]: '111111111111111111111111111111111111111111111111111111111111111'\r\n\r\nIn [3]: \"{0:b}\".format(torch.empty(1, dtype=torch.int64, device='cuda').random_(torch.iinfo(torch.int64).max, None)[0])\r\nOut[3]: '111111111111111111111111111111111111111111111111111111111111111'\r\n```","repo":"pytorch/pytorch","issue_id":564830266,"issue_number":33299}
{"question":"What does the code snippet using the torch library in Python demonstrate?","answer":"The code snippet demonstrates generating a random binary number of length equal to the maximum range of a 64-bit integer using the torch library in Python.","id":96,"text":"```\r\nIn [1]: import torch\r\n\r\nIn [2]: \"{0:b}\".format(torch.empty(1, dtype=torch.int64, device='cpu').random_(torch.iinfo(torch.int64).max, None)[0])\r\nOut[2]: '111111111111111111111111111111111111111111111111111111111111111'\r\n\r\nIn [3]: \"{0:b}\".format(torch.empty(1, dtype=torch.int64, device='cuda').random_(torch.iinfo(torch.int64).max, None)[0])\r\nOut[3]: '111111111111111111111111111111111111111111111111111111111111111'\r\n```","context":"<issue_start><issue_comment>Title: Tensor.random_ should be able to generate all 64 bit numbers including min and max value\nusername_0: ```\r\nt = torch.empty(... , dtype=torch.int64)\r\nt.random_(torch.iinfo(torch.int64).min, None)\r\n```\r\nshould be able to fill tensor with all 64 bit number including `torch.iinfo(torch.int64).min` and `torch.iinfo(torch.int64).max`\n<issue_comment>username_1: This is a dupe of #16944\n<issue_comment>username_0: Fixed in https://github.com/pytorch/pytorch/commit/095de1e872671c4da2cc0965976058cf752c3be0\r\nhttps://github.com/pytorch/pytorch/blob/master/aten/src/ATen/test/cpu_rng_test.cpp#L93 demonstrates how to generate uint64_t max value, (2^64 - 1)<issue_closed>\n<issue_comment>username_0: ```\r\nIn [1]: import torch\r\n\r\nIn [2]: \"{0:b}\".format(torch.empty(1, dtype=torch.int64, device='cpu').random_(torch.iinfo(torch.int64).max, None)[0])\r\nOut[2]: '111111111111111111111111111111111111111111111111111111111111111'\r\n\r\nIn [3]: \"{0:b}\".format(torch.empty(1, dtype=torch.int64, device='cuda').random_(torch.iinfo(torch.int64).max, None)[0])\r\nOut[3]: '111111111111111111111111111111111111111111111111111111111111111'\r\n```","repo":"pytorch/pytorch","issue_id":564830266,"issue_number":33299}
{"question":"What modification was made in test_jit.py according to the text chunk from ghstack?","answer":"Don't change global executor state in test_jit.py - rather do that in setUp and tearDown.","id":97,"text":"Stack from [ghstack](https://github.com/ezyang/ghstack):\n* #34983 Refactor test_jit_fuser legacy tests.\n* **#34982 Don't change global executor state in test_jit.py - rather do that in setUp and tearDown.**\n* #34981 Query graph executor mode in test fixture rather than relying on a global variable.\n* #34980 Fix warnings in test/test_jit_fuser.py\n\n\n\nDifferential Revision: [D20520370](https://our.internmc.facebook.com/intern/diff/D20520370)","context":"<issue_start><issue_comment>Title: Don't change global executor state in test_jit.py - rather do that in setUp and tearDown.\nusername_0: Stack from [ghstack](https://github.com/ezyang/ghstack):\n* #34983 Refactor test_jit_fuser legacy tests.\n* **#34982 Don't change global executor state in test_jit.py - rather do that in setUp and tearDown.**\n* #34981 Query graph executor mode in test fixture rather than relying on a global variable.\n* #34980 Fix warnings in test/test_jit_fuser.py\n\n\n\nDifferential Revision: [D20520370](https://our.internmc.facebook.com/intern/diff/D20520370)","repo":"pytorch/pytorch","issue_id":583995217,"issue_number":34982}
{"question":"What problem was fixed in the pull request on the PyTorch repository mentioned in the text, and how was it addressed?","answer":"The pull request on the PyTorch repository addressed a problem related to a specific issue, and a more robust solution was implemented compared to a previous fix. The differential revision reference provides more details about the specific change made. The exact nature of the problem and the details of the solution would require referencing the PyTorch repository and the pull request itself for a comprehensive answer.","id":98,"text":"Stack from [ghstack](https://github.com/ezyang/ghstack):\n* (to be filled)\n\nAn additional (and hopefully more robust) way of fixing the same problem https://github.com/pytorch/pytorch/pull/58382 fixed.\n\nDifferential Revision: [D28474154](https://our.internmc.facebook.com/intern/diff/D28474154/)","context":"<issue_start><issue_comment>Title: Prevent lock inversions with GIL in Future\nusername_0: Stack from [ghstack](https://github.com/ezyang/ghstack):\n* (to be filled)\n\nAn additional (and hopefully more robust) way of fixing the same problem https://github.com/pytorch/pytorch/pull/58382 fixed.\n\nDifferential Revision: [D28474154](https://our.internmc.facebook.com/intern/diff/D28474154/)","repo":"pytorch/pytorch","issue_id":893273577,"issue_number":58391}
{"question":"What error message does the user encounter when calling `torch.linalg.svd` on a large float64 tensor on the CPU?","answer":"The error message is: RuntimeError: falseINTERNAL ASSERT FAILED at \"/opt/conda/conda-bld/pytorch_1634272068185/work/aten/src/ATen/native/LinearAlgebraUtils.h\":244, please report a bug to PyTorch. svd_cpu: Argument 12 has illegal value. Most certainly there is a bug in the implementation calling the backend library.","id":99,"text":"##  Bug\r\n\r\nCalling `torch.linalg.svd` on a large float64 tensor on the CPU (size 90k x ~34k) leads to this error. `torch.linalg.eigh` on `tensor.T@tensor` seg faults.\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. I don't know. I have a large tensor of type float64. Calling `torch.linalg.svd`on it produces this error.\r\n\r\n```\r\nIntel MKL ERROR: Parameter 12 was incorrect on entry to DGESDD.\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n/tmp/ipykernel_3300/2679862595.py in <module>\r\n----> 1 torch.linalg.svd(tensor)\r\n\r\nRuntimeError: falseINTERNAL ASSERT FAILED at \"/opt/conda/conda-bld/pytorch_1634272068185/work/aten/src/ATen/native/LinearAlgebraUtils.h\":244, please report a bug to PyTorch. svd_cpu: Argument 12 has illegal value. Most certainly there is a bug in the implementation calling the backend library.\r\n```\r\n## Expected behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n## Environment\r\n\r\nPlease copy and paste the output from our\r\n[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)\r\n(or fill out the checklist below manually).\r\n\r\nYou can get the script and run it with:\r\n```\r\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\r\n# For security purposes, please check the contents of collect_env.py before running it.\r\npython collect_env.py\r\n```\r\n\r\n - PyTorch Version (e.g., 1.0): '1.10.0'\r\n - OS (e.g., Linux): Linux\r\n - How you installed PyTorch (`conda`, `pip`, source): conda\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.9.5\r\n - CUDA/cuDNN version:\r\n - GPU models and configuration:On the CPU\r\n - Any other relevant information:\r\n\r\n## Additional context\r\n`torch.linalg.eigh` on `tensor.T@tensor` seg faults.","context":"<issue_start><issue_comment>Title: INTERNAL ASSERT FAILED at \"/opt/conda/conda-bld/pytorch_1634272068185/work/aten/src/ATen/native/LinearAlgebraUtils.h\":244\nusername_0: ##  Bug\r\n\r\nCalling `torch.linalg.svd` on a large float64 tensor on the CPU (size 90k x ~34k) leads to this error. `torch.linalg.eigh` on `tensor.T@tensor` seg faults.\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. I don't know. I have a large tensor of type float64. Calling `torch.linalg.svd`on it produces this error.\r\n\r\n```\r\nIntel MKL ERROR: Parameter 12 was incorrect on entry to DGESDD.\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n/tmp/ipykernel_3300/2679862595.py in <module>\r\n----> 1 torch.linalg.svd(tensor)\r\n\r\nRuntimeError: falseINTERNAL ASSERT FAILED at \"/opt/conda/conda-bld/pytorch_1634272068185/work/aten/src/ATen/native/LinearAlgebraUtils.h\":244, please report a bug to PyTorch. svd_cpu: Argument 12 has illegal value. Most certainly there is a bug in the implementation calling the backend library.\r\n```\r\n## Expected behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n## Environment\r\n\r\nPlease copy and paste the output from our\r\n[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)\r\n(or fill out the checklist below manually).\r\n\r\nYou can get the script and run it with:\r\n```\r\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\r\n# For security purposes, please check the contents of collect_env.py before running it.\r\npython collect_env.py\r\n```\r\n\r\n - PyTorch Version (e.g., 1.0): '1.10.0'\r\n - OS (e.g., Linux): Linux\r\n - How you installed PyTorch (`conda`, `pip`, source): conda\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.9.5\r\n - CUDA/cuDNN version:\r\n - GPU models and configuration:On the CPU\r\n - Any other relevant information:\r\n\r\n## Additional context\r\n`torch.linalg.eigh` on `tensor.T@tensor` seg faults.\n<issue_comment>username_0: I have been trying to figure out what is special about this tensor that is causing the algorithm to fail.\r\n\r\nI note that `torch.linalg.svdvals` is successful.\n<issue_comment>username_0: Ok, I am now able to replicate this problem using just `torch.rand` and `torch.linalg.svd`.\r\n\r\nThe following fails:\r\n```\r\nx = torch.rand(30000,30000)\r\nu,s,v = torch.linalg.svd(x)\r\n```\n<issue_comment>username_1: Thank you for the tiny repro! I can reproduce in master. cc @username_2\n<issue_comment>username_2: The problem is related to https://github.com/pytorch/pytorch/issues/51720.\r\n\r\nThe optimal workspace size for the input of this size is `2.70021e+09` which doesn't fit into `int` type. In PyTorch, we use a 32-bit interface to backend linear algebra libraries.\r\nWe can \"fix\" the internal assert with this patch\r\n```\r\ndiff --git a/aten/src/ATen/native/BatchLinearAlgebra.cpp b/aten/src/ATen/native/BatchLinearAlgebra.cpp\r\nindex 853f5c2cbe..8902958d98 100644\r\n--- a/aten/src/ATen/native/BatchLinearAlgebra.cpp\r\n+++ b/aten/src/ATen/native/BatchLinearAlgebra.cpp\r\n@@ -2984,7 +2984,9 @@ static void apply_svd(Tensor& self, Tensor& U, Tensor& S, Tensor& VT,\r\n   int lwork = -1;\r\n   scalar_t wkopt;\r\n   lapackSvd<scalar_t, value_t>(jobz, m, n, self_data, lda, S_data, U_data, lda, VT_data, ldvt, &wkopt, lwork, rwork_data, iwork_data, &info);\r\n-  lwork = std::max<int>(1, real_impl<scalar_t, value_t>(wkopt));\r\n+  TORCH_CHECK(\r\n+      (real_impl<scalar_t, value_t>(wkopt)) < INT_MAX,\r\n+      \"svd: The optimal workspace size is larger than allowed by 32-bit interface to backend math library.\");\r\n   Tensor work = at::empty({lwork}, self.options());\r\n   auto work_data = work.data_ptr<scalar_t>();\r\n```\r\nThe issue is likely there for other functions as well and we could include checks of the values before casting them to int.\n<issue_comment>username_0: @username_2 will there be any way around this in the future for large SVDs? Or is there an upper size limit due to the 32bit interface?\n<issue_comment>username_2: Unfortunately, there is no workaround available.\r\nFrom the MKL documentation the workspace size (`lwork`) can be computed using these guidelines:\r\n```\r\nFor real flavors:\r\n\r\nIf jobz = 'N', lwork  3*min(m, n) + max (max(m,n), 6*min(m, n));\r\n\r\nIf jobz = 'O', lwork  3*(min(m, n))2 + max (max(m, n), 5*(min(m, n))2 + 4*min(m, n));\r\n\r\nIf jobz = 'S' or 'A', lwork  3*(min(m, n))2 + max (max(m, n), 4*(min(m, n))2 + 4*min(m, n))\r\n\r\nFor complex flavors:\r\n\r\nIf jobz = 'N', lwork  2*min(m, n) + max(m, n);\r\n\r\nIf jobz = 'O', lwork  2*(min(m, n))2 + max(m, n) + 2*min(m, n);\r\n\r\nIf jobz = 'S' or 'A', lwork  (min(m, n))2 + max(m, n) + 2*min(m, n);\r\n```\r\n`lwork` must be smaller than `2147483647` (maximum `int`) in order to not hit the limitation of the 32-bit interface.\n<issue_comment>username_0: @username_2 Any clue as to why `svdvals` and `eigvalsh` work?\n<issue_comment>username_1: For what is worth, `svdvals` and `eigvalsh` may work in master when the input does not require gradients, as we do not compute the `U` and the `Vh` in those cases, so the working space needed is less than for the full SVD / eigendecomposition.","repo":"pytorch/pytorch","issue_id":1052456616,"issue_number":68291}
{"question":"Why is the algorithm failing for the tensor, and what does the successful use of `torch.linalg.svdvals` suggest about the issue?","answer":"The algorithm is failing due to a special property of the tensor, possibly related to singular value decomposition. The successful use of `torch.linalg.svdvals` indicates that the issue may be resolved by analyzing the singular values of the tensor.","id":100,"text":"I have been trying to figure out what is special about this tensor that is causing the algorithm to fail.\r\n\r\nI note that `torch.linalg.svdvals` is successful.","context":"<issue_start><issue_comment>Title: INTERNAL ASSERT FAILED at \"/opt/conda/conda-bld/pytorch_1634272068185/work/aten/src/ATen/native/LinearAlgebraUtils.h\":244\nusername_0: ##  Bug\r\n\r\nCalling `torch.linalg.svd` on a large float64 tensor on the CPU (size 90k x ~34k) leads to this error. `torch.linalg.eigh` on `tensor.T@tensor` seg faults.\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. I don't know. I have a large tensor of type float64. Calling `torch.linalg.svd`on it produces this error.\r\n\r\n```\r\nIntel MKL ERROR: Parameter 12 was incorrect on entry to DGESDD.\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n/tmp/ipykernel_3300/2679862595.py in <module>\r\n----> 1 torch.linalg.svd(tensor)\r\n\r\nRuntimeError: falseINTERNAL ASSERT FAILED at \"/opt/conda/conda-bld/pytorch_1634272068185/work/aten/src/ATen/native/LinearAlgebraUtils.h\":244, please report a bug to PyTorch. svd_cpu: Argument 12 has illegal value. Most certainly there is a bug in the implementation calling the backend library.\r\n```\r\n## Expected behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n## Environment\r\n\r\nPlease copy and paste the output from our\r\n[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)\r\n(or fill out the checklist below manually).\r\n\r\nYou can get the script and run it with:\r\n```\r\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\r\n# For security purposes, please check the contents of collect_env.py before running it.\r\npython collect_env.py\r\n```\r\n\r\n - PyTorch Version (e.g., 1.0): '1.10.0'\r\n - OS (e.g., Linux): Linux\r\n - How you installed PyTorch (`conda`, `pip`, source): conda\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.9.5\r\n - CUDA/cuDNN version:\r\n - GPU models and configuration:On the CPU\r\n - Any other relevant information:\r\n\r\n## Additional context\r\n`torch.linalg.eigh` on `tensor.T@tensor` seg faults.\n<issue_comment>username_0: I have been trying to figure out what is special about this tensor that is causing the algorithm to fail.\r\n\r\nI note that `torch.linalg.svdvals` is successful.\n<issue_comment>username_0: Ok, I am now able to replicate this problem using just `torch.rand` and `torch.linalg.svd`.\r\n\r\nThe following fails:\r\n```\r\nx = torch.rand(30000,30000)\r\nu,s,v = torch.linalg.svd(x)\r\n```\n<issue_comment>username_1: Thank you for the tiny repro! I can reproduce in master. cc @username_2\n<issue_comment>username_2: The problem is related to https://github.com/pytorch/pytorch/issues/51720.\r\n\r\nThe optimal workspace size for the input of this size is `2.70021e+09` which doesn't fit into `int` type. In PyTorch, we use a 32-bit interface to backend linear algebra libraries.\r\nWe can \"fix\" the internal assert with this patch\r\n```\r\ndiff --git a/aten/src/ATen/native/BatchLinearAlgebra.cpp b/aten/src/ATen/native/BatchLinearAlgebra.cpp\r\nindex 853f5c2cbe..8902958d98 100644\r\n--- a/aten/src/ATen/native/BatchLinearAlgebra.cpp\r\n+++ b/aten/src/ATen/native/BatchLinearAlgebra.cpp\r\n@@ -2984,7 +2984,9 @@ static void apply_svd(Tensor& self, Tensor& U, Tensor& S, Tensor& VT,\r\n   int lwork = -1;\r\n   scalar_t wkopt;\r\n   lapackSvd<scalar_t, value_t>(jobz, m, n, self_data, lda, S_data, U_data, lda, VT_data, ldvt, &wkopt, lwork, rwork_data, iwork_data, &info);\r\n-  lwork = std::max<int>(1, real_impl<scalar_t, value_t>(wkopt));\r\n+  TORCH_CHECK(\r\n+      (real_impl<scalar_t, value_t>(wkopt)) < INT_MAX,\r\n+      \"svd: The optimal workspace size is larger than allowed by 32-bit interface to backend math library.\");\r\n   Tensor work = at::empty({lwork}, self.options());\r\n   auto work_data = work.data_ptr<scalar_t>();\r\n```\r\nThe issue is likely there for other functions as well and we could include checks of the values before casting them to int.\n<issue_comment>username_0: @username_2 will there be any way around this in the future for large SVDs? Or is there an upper size limit due to the 32bit interface?\n<issue_comment>username_2: Unfortunately, there is no workaround available.\r\nFrom the MKL documentation the workspace size (`lwork`) can be computed using these guidelines:\r\n```\r\nFor real flavors:\r\n\r\nIf jobz = 'N', lwork  3*min(m, n) + max (max(m,n), 6*min(m, n));\r\n\r\nIf jobz = 'O', lwork  3*(min(m, n))2 + max (max(m, n), 5*(min(m, n))2 + 4*min(m, n));\r\n\r\nIf jobz = 'S' or 'A', lwork  3*(min(m, n))2 + max (max(m, n), 4*(min(m, n))2 + 4*min(m, n))\r\n\r\nFor complex flavors:\r\n\r\nIf jobz = 'N', lwork  2*min(m, n) + max(m, n);\r\n\r\nIf jobz = 'O', lwork  2*(min(m, n))2 + max(m, n) + 2*min(m, n);\r\n\r\nIf jobz = 'S' or 'A', lwork  (min(m, n))2 + max(m, n) + 2*min(m, n);\r\n```\r\n`lwork` must be smaller than `2147483647` (maximum `int`) in order to not hit the limitation of the 32-bit interface.\n<issue_comment>username_0: @username_2 Any clue as to why `svdvals` and `eigvalsh` work?\n<issue_comment>username_1: For what is worth, `svdvals` and `eigvalsh` may work in master when the input does not require gradients, as we do not compute the `U` and the `Vh` in those cases, so the working space needed is less than for the full SVD / eigendecomposition.","repo":"pytorch/pytorch","issue_id":1052456616,"issue_number":68291}
{"question":"Why does the code snippet `x = torch.rand(30000,30000) u,s,v = torch.linalg.svd(x)` fail?","answer":"The code snippet fails because torch.linalg.svd encounters memory allocation issues when dealing with a large random tensor of size 30000x30000 created with torch.rand. The function torch.linalg.svd is unable to handle the memory requirements for such a large tensor, leading to a failure in the decomposition process. This highlights the limitation of torch.linalg.svd when applied to extremely large tensors due to memory constraints.","id":101,"text":"Ok, I am now able to replicate this problem using just `torch.rand` and `torch.linalg.svd`.\r\n\r\nThe following fails:\r\n```\r\nx = torch.rand(30000,30000)\r\nu,s,v = torch.linalg.svd(x)\r\n```","context":"<issue_start><issue_comment>Title: INTERNAL ASSERT FAILED at \"/opt/conda/conda-bld/pytorch_1634272068185/work/aten/src/ATen/native/LinearAlgebraUtils.h\":244\nusername_0: ##  Bug\r\n\r\nCalling `torch.linalg.svd` on a large float64 tensor on the CPU (size 90k x ~34k) leads to this error. `torch.linalg.eigh` on `tensor.T@tensor` seg faults.\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. I don't know. I have a large tensor of type float64. Calling `torch.linalg.svd`on it produces this error.\r\n\r\n```\r\nIntel MKL ERROR: Parameter 12 was incorrect on entry to DGESDD.\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n/tmp/ipykernel_3300/2679862595.py in <module>\r\n----> 1 torch.linalg.svd(tensor)\r\n\r\nRuntimeError: falseINTERNAL ASSERT FAILED at \"/opt/conda/conda-bld/pytorch_1634272068185/work/aten/src/ATen/native/LinearAlgebraUtils.h\":244, please report a bug to PyTorch. svd_cpu: Argument 12 has illegal value. Most certainly there is a bug in the implementation calling the backend library.\r\n```\r\n## Expected behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n## Environment\r\n\r\nPlease copy and paste the output from our\r\n[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)\r\n(or fill out the checklist below manually).\r\n\r\nYou can get the script and run it with:\r\n```\r\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\r\n# For security purposes, please check the contents of collect_env.py before running it.\r\npython collect_env.py\r\n```\r\n\r\n - PyTorch Version (e.g., 1.0): '1.10.0'\r\n - OS (e.g., Linux): Linux\r\n - How you installed PyTorch (`conda`, `pip`, source): conda\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.9.5\r\n - CUDA/cuDNN version:\r\n - GPU models and configuration:On the CPU\r\n - Any other relevant information:\r\n\r\n## Additional context\r\n`torch.linalg.eigh` on `tensor.T@tensor` seg faults.\n<issue_comment>username_0: I have been trying to figure out what is special about this tensor that is causing the algorithm to fail.\r\n\r\nI note that `torch.linalg.svdvals` is successful.\n<issue_comment>username_0: Ok, I am now able to replicate this problem using just `torch.rand` and `torch.linalg.svd`.\r\n\r\nThe following fails:\r\n```\r\nx = torch.rand(30000,30000)\r\nu,s,v = torch.linalg.svd(x)\r\n```\n<issue_comment>username_1: Thank you for the tiny repro! I can reproduce in master. cc @username_2\n<issue_comment>username_2: The problem is related to https://github.com/pytorch/pytorch/issues/51720.\r\n\r\nThe optimal workspace size for the input of this size is `2.70021e+09` which doesn't fit into `int` type. In PyTorch, we use a 32-bit interface to backend linear algebra libraries.\r\nWe can \"fix\" the internal assert with this patch\r\n```\r\ndiff --git a/aten/src/ATen/native/BatchLinearAlgebra.cpp b/aten/src/ATen/native/BatchLinearAlgebra.cpp\r\nindex 853f5c2cbe..8902958d98 100644\r\n--- a/aten/src/ATen/native/BatchLinearAlgebra.cpp\r\n+++ b/aten/src/ATen/native/BatchLinearAlgebra.cpp\r\n@@ -2984,7 +2984,9 @@ static void apply_svd(Tensor& self, Tensor& U, Tensor& S, Tensor& VT,\r\n   int lwork = -1;\r\n   scalar_t wkopt;\r\n   lapackSvd<scalar_t, value_t>(jobz, m, n, self_data, lda, S_data, U_data, lda, VT_data, ldvt, &wkopt, lwork, rwork_data, iwork_data, &info);\r\n-  lwork = std::max<int>(1, real_impl<scalar_t, value_t>(wkopt));\r\n+  TORCH_CHECK(\r\n+      (real_impl<scalar_t, value_t>(wkopt)) < INT_MAX,\r\n+      \"svd: The optimal workspace size is larger than allowed by 32-bit interface to backend math library.\");\r\n   Tensor work = at::empty({lwork}, self.options());\r\n   auto work_data = work.data_ptr<scalar_t>();\r\n```\r\nThe issue is likely there for other functions as well and we could include checks of the values before casting them to int.\n<issue_comment>username_0: @username_2 will there be any way around this in the future for large SVDs? Or is there an upper size limit due to the 32bit interface?\n<issue_comment>username_2: Unfortunately, there is no workaround available.\r\nFrom the MKL documentation the workspace size (`lwork`) can be computed using these guidelines:\r\n```\r\nFor real flavors:\r\n\r\nIf jobz = 'N', lwork  3*min(m, n) + max (max(m,n), 6*min(m, n));\r\n\r\nIf jobz = 'O', lwork  3*(min(m, n))2 + max (max(m, n), 5*(min(m, n))2 + 4*min(m, n));\r\n\r\nIf jobz = 'S' or 'A', lwork  3*(min(m, n))2 + max (max(m, n), 4*(min(m, n))2 + 4*min(m, n))\r\n\r\nFor complex flavors:\r\n\r\nIf jobz = 'N', lwork  2*min(m, n) + max(m, n);\r\n\r\nIf jobz = 'O', lwork  2*(min(m, n))2 + max(m, n) + 2*min(m, n);\r\n\r\nIf jobz = 'S' or 'A', lwork  (min(m, n))2 + max(m, n) + 2*min(m, n);\r\n```\r\n`lwork` must be smaller than `2147483647` (maximum `int`) in order to not hit the limitation of the 32-bit interface.\n<issue_comment>username_0: @username_2 Any clue as to why `svdvals` and `eigvalsh` work?\n<issue_comment>username_1: For what is worth, `svdvals` and `eigvalsh` may work in master when the input does not require gradients, as we do not compute the `U` and the `Vh` in those cases, so the working space needed is less than for the full SVD / eigendecomposition.","repo":"pytorch/pytorch","issue_id":1052456616,"issue_number":68291}
{"question":"What approach can be taken to identify and replace unsupported operations in CycleGAN code that are not supported by the CPU?","answer":"You can try putting debug statements in the CycleGAN code to identify unsupported operations, particularly PIL operations, and then search for replacements for these unsupported operations.","id":128,"text":"@username_2 You can try to put debug statements in CycleGAN code and see which one isn't supported by your CPU (probably some PIL operations), and search for replacements. Sorry I don't have much else to offer.","context":"<issue_start><issue_comment>Title: Problem in DataLoader (same problem with #4643, but not solved by it)\nusername_0: - OS: Ubuntu 16.04\r\n- PyTorch version: torch-0.3.1-cp36-cp36-linux_x86_64.whl\r\n- How you installed PyTorch (conda, pip, source): pip3 from the binary\r\n- Python version: 3.6\r\n- CUDA/cuDNN version: 9.1\r\n- GPU models and configuration: GTX 1070 Ti\r\n\r\nAfter install using the wheel file, I have changed the torch/utils/data/Dataloader.py according to the changes shown at https://github.com/pytorch/pytorch/pull/4643\r\n\r\nBut the same error still happen when I try the tutorial example to load image database:\r\n\r\nException ignored in: <bound method DataLoaderIter.__del__ of <torch.utils.data.dataloader.DataLoaderIter object at 0x56090c8c7348>>\r\nTraceback (most recent call last):\r\n  File \"/home/username_0/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 375, in __del__\r\n  File \"/home/username_0/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 361, in _shutdown_workers\r\n  File \"/home/username_0/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 39, in _get_from_queue\r\n  File \"/usr/pkg/lib/python3.6/multiprocessing/queues.py\", line 337, in get\r\nImportError: sys.meta_path is None, Python is likely shutting down\r\n\r\nSo could anyone help me on that? Or is there any other file I should modify to get it run?\n<issue_comment>username_0: And, I am not sure how can I use the changes in torch/csrc/DataLoader.cpp, should I clone all the source code on GitHub and build the newest binary to fully fix this my problem?\n<issue_comment>username_1: when does this error happen? is it happening when you exit python?\n<issue_comment>username_2: @username_1 and @username_3,\r\nI am running this:\r\n\r\npython train.py --dataroot ./datasets/horse2zebra --name horse2zebra_cyclegan --model cycle_gan --no_dropout --loadSize 64 --nThreads 1\r\n\r\nfrom \r\n\r\nhttps://github.com/junyanz/pytorch-CycleGAN-and-pix2pix\r\n\r\nThe model is succefully created:\r\nmodel [CycleGANModel] was created\r\n\r\nI am on ubuntu 16.04, pytorch 3.1, cuda 9.0 and cuDnn 7.1\r\n\r\nThe error I get is traced below:\r\ncreate web directory ./checkpoints/horse2zebra_cyclegan/web...\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 22, in <module>\r\n    for i, data in enumerate(dataset):\r\n  File \"/home/sam/Documents/ComputerVision/pytorch-CycleGAN-and-pix2pix/data/custom_dataset_data_loader.py\", line 44, in __iter__\r\n    for i, data in enumerate(self.dataloader):\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 275, in __next__\r\n    idx, batch = self._get_batch()\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 254, in _get_batch\r\n    return self.data_queue.get()\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/queues.py\", line 335, in get\r\n    res = self._reader.recv_bytes()\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\r\n    buf = self._recv_bytes(maxlength)\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\r\n    buf = self._recv(4)\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\r\n    chunk = read(handle, remaining)\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 175, in handler\r\n    _error_if_any_worker_fails()\r\nRuntimeError: DataLoader worker (pid 3792) is killed by signal: Illegal instruction.\n<issue_comment>username_3: @username_2 that error is because you are running it on a computer that is too old and does not have SSE4 instruction on CPU.\n<issue_comment>username_1: @username_2 Also, it is more likely due to that your CPU doesn't support the operations done in the dataset.__getitem__.\n<issue_comment>username_2: @username_1 & @username_3,\r\n\r\nSome months ago I made it work in windows10 (I think)\r\nThe my windows machine crashed. So here I am. Sorry\r\n\r\nHere is what I see in cat /proc/cpuinfo:\r\n\r\nprocessor\t: 0\r\nvendor_id\t: AuthenticAMD\r\ncpu family\t: 21\r\nmodel\t\t: 2\r\nmodel name\t: AMD FX(tm)-4300 Quad-Core Processor\r\nstepping\t: 0\r\nmicrocode\t: 0x600081c\r\ncpu MHz\t\t: 1400.000\r\ncache size\t: 2048 KB\r\nphysical id\t: 0\r\nsiblings\t: 4\r\ncore id\t\t: 0\r\ncpu cores\t: 2\r\napicid\t\t: 16\r\ninitial apicid\t: 0\r\nfpu\t\t: yes\r\nfpu_exception\t: yes\r\ncpuid level\t: 13\r\nwp\t\t: yes\r\nflags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf pni pclmulqdq monitor ssse3 fma cx16 **sse4_1 sse4_2** popcnt aes xsave avx f16c lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs xop skinit wdt lwp fma4 tce nodeid_msr tbm topoext perfctr_core perfctr_nb cpb hw_pstate retpoline retpoline_amd rsb_ctxsw vmmcall bmi1 arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold\r\nbugs\t\t: fxsave_leak sysret_ss_attrs null_seg spectre_v1 spectre_v2\r\nbogomips\t: 7634.15\r\nTLB size\t: 1536 4K pages\r\nclflush size\t: 64\r\ncache_alignment\t: 64\r\naddress sizes\t: 48 bits physical, 48 bits virtual\r\npower management: ts ttp tm 100mhzsteps hwpstate cpb eff_freq_ro\r\n\r\n@username_1 how do I check if CPU support dataset.getitem?\n<issue_comment>username_1: @username_2 run with `nThreads=0`\n<issue_comment>username_2: tried it with nThreads=0\r\ndifferent error...\r\nmodel [CycleGANModel] was created\r\ncreate web directory ./checkpoints/horse2zebra_cyclegan/web...\r\nIllegal instruction (core dumped)\n<issue_comment>username_2: I see something I am uneasy about:\r\nsee bold below\r\n\r\nCustomDatasetDataLoader\r\ndataset [UnalignedDataset] was created\r\n/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torchvision-0.2.0-py3.6.egg/torchvision/transforms/transforms.py:156: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\r\n#training images = 1334\r\ncycle_gan\r\n**initialization method [normal]\r\ninitialization method [normal]\r\ninitialization method [normal]\r\ninitialization method [normal]**\n<issue_comment>username_1: @username_2 No it's same error. We just wrap it in a nice way so you can see it even if it's in the workers. \r\n\r\nIsn't that expected? CycleGAN has four networks: D_X D_Y G F .\n<issue_comment>username_1: @username_2 anyways, it confirms my hypothesis that your CPU doesn't support the operation in `dataset.__getitem__`.\n<issue_comment>username_2: Oh... Can you suggest a workaround?\n<issue_comment>username_1: @username_2 You can try to put debug statements in CycleGAN code and see which one isn't supported by your CPU (probably some PIL operations), and search for replacements. Sorry I don't have much else to offer.\n<issue_comment>username_2: @username_1 \r\nThank you sir !<issue_closed>\n<issue_comment>username_1: closed due to inactivity. feel free to reopen if you can reproduce with latest stable (0.4 at the moment of writing).\n<issue_comment>username_4: Still can't get this to work.\r\n\r\nhttps://stackoverflow.com/questions/53449927/runtimeerror-dataloader-worker-is-killed-by-signal-illegal-instruction\r\n\r\nPlease help.","repo":"pytorch/pytorch","issue_id":304991946,"issue_number":5761}
{"question":"What is the significance of the code patch provided in the text regarding workspace size in PyTorch?","answer":"The code patch addresses an internal assert related to workspace size exceeding the `int` type limit by including a check for the optimal workspace size not exceeding `INT_MAX`.","id":103,"text":"The problem is related to https://github.com/pytorch/pytorch/issues/51720.\r\n\r\nThe optimal workspace size for the input of this size is `2.70021e+09` which doesn't fit into `int` type. In PyTorch, we use a 32-bit interface to backend linear algebra libraries.\r\nWe can \"fix\" the internal assert with this patch\r\n```\r\ndiff --git a/aten/src/ATen/native/BatchLinearAlgebra.cpp b/aten/src/ATen/native/BatchLinearAlgebra.cpp\r\nindex 853f5c2cbe..8902958d98 100644\r\n--- a/aten/src/ATen/native/BatchLinearAlgebra.cpp\r\n+++ b/aten/src/ATen/native/BatchLinearAlgebra.cpp\r\n@@ -2984,7 +2984,9 @@ static void apply_svd(Tensor& self, Tensor& U, Tensor& S, Tensor& VT,\r\n   int lwork = -1;\r\n   scalar_t wkopt;\r\n   lapackSvd<scalar_t, value_t>(jobz, m, n, self_data, lda, S_data, U_data, lda, VT_data, ldvt, &wkopt, lwork, rwork_data, iwork_data, &info);\r\n-  lwork = std::max<int>(1, real_impl<scalar_t, value_t>(wkopt));\r\n+  TORCH_CHECK(\r\n+      (real_impl<scalar_t, value_t>(wkopt)) < INT_MAX,\r\n+      \"svd: The optimal workspace size is larger than allowed by 32-bit interface to backend math library.\");\r\n   Tensor work = at::empty({lwork}, self.options());\r\n   auto work_data = work.data_ptr<scalar_t>();\r\n```\r\nThe issue is likely there for other functions as well and we could include checks of the values before casting them to int.","context":"<issue_start><issue_comment>Title: INTERNAL ASSERT FAILED at \"/opt/conda/conda-bld/pytorch_1634272068185/work/aten/src/ATen/native/LinearAlgebraUtils.h\":244\nusername_0: ##  Bug\r\n\r\nCalling `torch.linalg.svd` on a large float64 tensor on the CPU (size 90k x ~34k) leads to this error. `torch.linalg.eigh` on `tensor.T@tensor` seg faults.\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. I don't know. I have a large tensor of type float64. Calling `torch.linalg.svd`on it produces this error.\r\n\r\n```\r\nIntel MKL ERROR: Parameter 12 was incorrect on entry to DGESDD.\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n/tmp/ipykernel_3300/2679862595.py in <module>\r\n----> 1 torch.linalg.svd(tensor)\r\n\r\nRuntimeError: falseINTERNAL ASSERT FAILED at \"/opt/conda/conda-bld/pytorch_1634272068185/work/aten/src/ATen/native/LinearAlgebraUtils.h\":244, please report a bug to PyTorch. svd_cpu: Argument 12 has illegal value. Most certainly there is a bug in the implementation calling the backend library.\r\n```\r\n## Expected behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n## Environment\r\n\r\nPlease copy and paste the output from our\r\n[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)\r\n(or fill out the checklist below manually).\r\n\r\nYou can get the script and run it with:\r\n```\r\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\r\n# For security purposes, please check the contents of collect_env.py before running it.\r\npython collect_env.py\r\n```\r\n\r\n - PyTorch Version (e.g., 1.0): '1.10.0'\r\n - OS (e.g., Linux): Linux\r\n - How you installed PyTorch (`conda`, `pip`, source): conda\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.9.5\r\n - CUDA/cuDNN version:\r\n - GPU models and configuration:On the CPU\r\n - Any other relevant information:\r\n\r\n## Additional context\r\n`torch.linalg.eigh` on `tensor.T@tensor` seg faults.\n<issue_comment>username_0: I have been trying to figure out what is special about this tensor that is causing the algorithm to fail.\r\n\r\nI note that `torch.linalg.svdvals` is successful.\n<issue_comment>username_0: Ok, I am now able to replicate this problem using just `torch.rand` and `torch.linalg.svd`.\r\n\r\nThe following fails:\r\n```\r\nx = torch.rand(30000,30000)\r\nu,s,v = torch.linalg.svd(x)\r\n```\n<issue_comment>username_1: Thank you for the tiny repro! I can reproduce in master. cc @username_2\n<issue_comment>username_2: The problem is related to https://github.com/pytorch/pytorch/issues/51720.\r\n\r\nThe optimal workspace size for the input of this size is `2.70021e+09` which doesn't fit into `int` type. In PyTorch, we use a 32-bit interface to backend linear algebra libraries.\r\nWe can \"fix\" the internal assert with this patch\r\n```\r\ndiff --git a/aten/src/ATen/native/BatchLinearAlgebra.cpp b/aten/src/ATen/native/BatchLinearAlgebra.cpp\r\nindex 853f5c2cbe..8902958d98 100644\r\n--- a/aten/src/ATen/native/BatchLinearAlgebra.cpp\r\n+++ b/aten/src/ATen/native/BatchLinearAlgebra.cpp\r\n@@ -2984,7 +2984,9 @@ static void apply_svd(Tensor& self, Tensor& U, Tensor& S, Tensor& VT,\r\n   int lwork = -1;\r\n   scalar_t wkopt;\r\n   lapackSvd<scalar_t, value_t>(jobz, m, n, self_data, lda, S_data, U_data, lda, VT_data, ldvt, &wkopt, lwork, rwork_data, iwork_data, &info);\r\n-  lwork = std::max<int>(1, real_impl<scalar_t, value_t>(wkopt));\r\n+  TORCH_CHECK(\r\n+      (real_impl<scalar_t, value_t>(wkopt)) < INT_MAX,\r\n+      \"svd: The optimal workspace size is larger than allowed by 32-bit interface to backend math library.\");\r\n   Tensor work = at::empty({lwork}, self.options());\r\n   auto work_data = work.data_ptr<scalar_t>();\r\n```\r\nThe issue is likely there for other functions as well and we could include checks of the values before casting them to int.\n<issue_comment>username_0: @username_2 will there be any way around this in the future for large SVDs? Or is there an upper size limit due to the 32bit interface?\n<issue_comment>username_2: Unfortunately, there is no workaround available.\r\nFrom the MKL documentation the workspace size (`lwork`) can be computed using these guidelines:\r\n```\r\nFor real flavors:\r\n\r\nIf jobz = 'N', lwork  3*min(m, n) + max (max(m,n), 6*min(m, n));\r\n\r\nIf jobz = 'O', lwork  3*(min(m, n))2 + max (max(m, n), 5*(min(m, n))2 + 4*min(m, n));\r\n\r\nIf jobz = 'S' or 'A', lwork  3*(min(m, n))2 + max (max(m, n), 4*(min(m, n))2 + 4*min(m, n))\r\n\r\nFor complex flavors:\r\n\r\nIf jobz = 'N', lwork  2*min(m, n) + max(m, n);\r\n\r\nIf jobz = 'O', lwork  2*(min(m, n))2 + max(m, n) + 2*min(m, n);\r\n\r\nIf jobz = 'S' or 'A', lwork  (min(m, n))2 + max(m, n) + 2*min(m, n);\r\n```\r\n`lwork` must be smaller than `2147483647` (maximum `int`) in order to not hit the limitation of the 32-bit interface.\n<issue_comment>username_0: @username_2 Any clue as to why `svdvals` and `eigvalsh` work?\n<issue_comment>username_1: For what is worth, `svdvals` and `eigvalsh` may work in master when the input does not require gradients, as we do not compute the `U` and the `Vh` in those cases, so the working space needed is less than for the full SVD / eigendecomposition.","repo":"pytorch/pytorch","issue_id":1052456616,"issue_number":68291}
{"question":"How can the workspace size `lwork` be computed according to the MKL documentation for real and complex flavors and different job types?","answer":"The workspace size `lwork` can be computed based on the guidelines provided in the MKL documentation. For real flavors, the computation depends on the job type ('N', 'O', 'S', 'A') and involves specific formulas and conditions for different scenarios. Similarly, for complex flavors, the computation also varies based on the job type. It is crucial to ensure that the computed `lwork` is smaller than `2147483647` to avoid limitations of the 32-bit interface.","id":105,"text":"Unfortunately, there is no workaround available.\r\nFrom the MKL documentation the workspace size (`lwork`) can be computed using these guidelines:\r\n```\r\nFor real flavors:\r\n\r\nIf jobz = 'N', lwork  3*min(m, n) + max (max(m,n), 6*min(m, n));\r\n\r\nIf jobz = 'O', lwork  3*(min(m, n))2 + max (max(m, n), 5*(min(m, n))2 + 4*min(m, n));\r\n\r\nIf jobz = 'S' or 'A', lwork  3*(min(m, n))2 + max (max(m, n), 4*(min(m, n))2 + 4*min(m, n))\r\n\r\nFor complex flavors:\r\n\r\nIf jobz = 'N', lwork  2*min(m, n) + max(m, n);\r\n\r\nIf jobz = 'O', lwork  2*(min(m, n))2 + max(m, n) + 2*min(m, n);\r\n\r\nIf jobz = 'S' or 'A', lwork  (min(m, n))2 + max(m, n) + 2*min(m, n);\r\n```\r\n`lwork` must be smaller than `2147483647` (maximum `int`) in order to not hit the limitation of the 32-bit interface.","context":"<issue_start><issue_comment>Title: INTERNAL ASSERT FAILED at \"/opt/conda/conda-bld/pytorch_1634272068185/work/aten/src/ATen/native/LinearAlgebraUtils.h\":244\nusername_0: ##  Bug\r\n\r\nCalling `torch.linalg.svd` on a large float64 tensor on the CPU (size 90k x ~34k) leads to this error. `torch.linalg.eigh` on `tensor.T@tensor` seg faults.\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. I don't know. I have a large tensor of type float64. Calling `torch.linalg.svd`on it produces this error.\r\n\r\n```\r\nIntel MKL ERROR: Parameter 12 was incorrect on entry to DGESDD.\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n/tmp/ipykernel_3300/2679862595.py in <module>\r\n----> 1 torch.linalg.svd(tensor)\r\n\r\nRuntimeError: falseINTERNAL ASSERT FAILED at \"/opt/conda/conda-bld/pytorch_1634272068185/work/aten/src/ATen/native/LinearAlgebraUtils.h\":244, please report a bug to PyTorch. svd_cpu: Argument 12 has illegal value. Most certainly there is a bug in the implementation calling the backend library.\r\n```\r\n## Expected behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n## Environment\r\n\r\nPlease copy and paste the output from our\r\n[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)\r\n(or fill out the checklist below manually).\r\n\r\nYou can get the script and run it with:\r\n```\r\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\r\n# For security purposes, please check the contents of collect_env.py before running it.\r\npython collect_env.py\r\n```\r\n\r\n - PyTorch Version (e.g., 1.0): '1.10.0'\r\n - OS (e.g., Linux): Linux\r\n - How you installed PyTorch (`conda`, `pip`, source): conda\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.9.5\r\n - CUDA/cuDNN version:\r\n - GPU models and configuration:On the CPU\r\n - Any other relevant information:\r\n\r\n## Additional context\r\n`torch.linalg.eigh` on `tensor.T@tensor` seg faults.\n<issue_comment>username_0: I have been trying to figure out what is special about this tensor that is causing the algorithm to fail.\r\n\r\nI note that `torch.linalg.svdvals` is successful.\n<issue_comment>username_0: Ok, I am now able to replicate this problem using just `torch.rand` and `torch.linalg.svd`.\r\n\r\nThe following fails:\r\n```\r\nx = torch.rand(30000,30000)\r\nu,s,v = torch.linalg.svd(x)\r\n```\n<issue_comment>username_1: Thank you for the tiny repro! I can reproduce in master. cc @username_2\n<issue_comment>username_2: The problem is related to https://github.com/pytorch/pytorch/issues/51720.\r\n\r\nThe optimal workspace size for the input of this size is `2.70021e+09` which doesn't fit into `int` type. In PyTorch, we use a 32-bit interface to backend linear algebra libraries.\r\nWe can \"fix\" the internal assert with this patch\r\n```\r\ndiff --git a/aten/src/ATen/native/BatchLinearAlgebra.cpp b/aten/src/ATen/native/BatchLinearAlgebra.cpp\r\nindex 853f5c2cbe..8902958d98 100644\r\n--- a/aten/src/ATen/native/BatchLinearAlgebra.cpp\r\n+++ b/aten/src/ATen/native/BatchLinearAlgebra.cpp\r\n@@ -2984,7 +2984,9 @@ static void apply_svd(Tensor& self, Tensor& U, Tensor& S, Tensor& VT,\r\n   int lwork = -1;\r\n   scalar_t wkopt;\r\n   lapackSvd<scalar_t, value_t>(jobz, m, n, self_data, lda, S_data, U_data, lda, VT_data, ldvt, &wkopt, lwork, rwork_data, iwork_data, &info);\r\n-  lwork = std::max<int>(1, real_impl<scalar_t, value_t>(wkopt));\r\n+  TORCH_CHECK(\r\n+      (real_impl<scalar_t, value_t>(wkopt)) < INT_MAX,\r\n+      \"svd: The optimal workspace size is larger than allowed by 32-bit interface to backend math library.\");\r\n   Tensor work = at::empty({lwork}, self.options());\r\n   auto work_data = work.data_ptr<scalar_t>();\r\n```\r\nThe issue is likely there for other functions as well and we could include checks of the values before casting them to int.\n<issue_comment>username_0: @username_2 will there be any way around this in the future for large SVDs? Or is there an upper size limit due to the 32bit interface?\n<issue_comment>username_2: Unfortunately, there is no workaround available.\r\nFrom the MKL documentation the workspace size (`lwork`) can be computed using these guidelines:\r\n```\r\nFor real flavors:\r\n\r\nIf jobz = 'N', lwork  3*min(m, n) + max (max(m,n), 6*min(m, n));\r\n\r\nIf jobz = 'O', lwork  3*(min(m, n))2 + max (max(m, n), 5*(min(m, n))2 + 4*min(m, n));\r\n\r\nIf jobz = 'S' or 'A', lwork  3*(min(m, n))2 + max (max(m, n), 4*(min(m, n))2 + 4*min(m, n))\r\n\r\nFor complex flavors:\r\n\r\nIf jobz = 'N', lwork  2*min(m, n) + max(m, n);\r\n\r\nIf jobz = 'O', lwork  2*(min(m, n))2 + max(m, n) + 2*min(m, n);\r\n\r\nIf jobz = 'S' or 'A', lwork  (min(m, n))2 + max(m, n) + 2*min(m, n);\r\n```\r\n`lwork` must be smaller than `2147483647` (maximum `int`) in order to not hit the limitation of the 32-bit interface.\n<issue_comment>username_0: @username_2 Any clue as to why `svdvals` and `eigvalsh` work?\n<issue_comment>username_1: For what is worth, `svdvals` and `eigvalsh` may work in master when the input does not require gradients, as we do not compute the `U` and the `Vh` in those cases, so the working space needed is less than for the full SVD / eigendecomposition.","repo":"pytorch/pytorch","issue_id":1052456616,"issue_number":68291}
{"question":"In what cases do `svdvals` and `eigvalsh` work in master, according to the text?","answer":"`svdvals` and `eigvalsh` may work in master when the input does not require gradients, as they do not compute the `U` and the `Vh` in those cases, resulting in less working space needed compared to the full SVD / eigendecomposition.","id":107,"text":"For what is worth, `svdvals` and `eigvalsh` may work in master when the input does not require gradients, as we do not compute the `U` and the `Vh` in those cases, so the working space needed is less than for the full SVD / eigendecomposition.","context":"<issue_start><issue_comment>Title: INTERNAL ASSERT FAILED at \"/opt/conda/conda-bld/pytorch_1634272068185/work/aten/src/ATen/native/LinearAlgebraUtils.h\":244\nusername_0: ##  Bug\r\n\r\nCalling `torch.linalg.svd` on a large float64 tensor on the CPU (size 90k x ~34k) leads to this error. `torch.linalg.eigh` on `tensor.T@tensor` seg faults.\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. I don't know. I have a large tensor of type float64. Calling `torch.linalg.svd`on it produces this error.\r\n\r\n```\r\nIntel MKL ERROR: Parameter 12 was incorrect on entry to DGESDD.\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n/tmp/ipykernel_3300/2679862595.py in <module>\r\n----> 1 torch.linalg.svd(tensor)\r\n\r\nRuntimeError: falseINTERNAL ASSERT FAILED at \"/opt/conda/conda-bld/pytorch_1634272068185/work/aten/src/ATen/native/LinearAlgebraUtils.h\":244, please report a bug to PyTorch. svd_cpu: Argument 12 has illegal value. Most certainly there is a bug in the implementation calling the backend library.\r\n```\r\n## Expected behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n## Environment\r\n\r\nPlease copy and paste the output from our\r\n[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)\r\n(or fill out the checklist below manually).\r\n\r\nYou can get the script and run it with:\r\n```\r\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\r\n# For security purposes, please check the contents of collect_env.py before running it.\r\npython collect_env.py\r\n```\r\n\r\n - PyTorch Version (e.g., 1.0): '1.10.0'\r\n - OS (e.g., Linux): Linux\r\n - How you installed PyTorch (`conda`, `pip`, source): conda\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.9.5\r\n - CUDA/cuDNN version:\r\n - GPU models and configuration:On the CPU\r\n - Any other relevant information:\r\n\r\n## Additional context\r\n`torch.linalg.eigh` on `tensor.T@tensor` seg faults.\n<issue_comment>username_0: I have been trying to figure out what is special about this tensor that is causing the algorithm to fail.\r\n\r\nI note that `torch.linalg.svdvals` is successful.\n<issue_comment>username_0: Ok, I am now able to replicate this problem using just `torch.rand` and `torch.linalg.svd`.\r\n\r\nThe following fails:\r\n```\r\nx = torch.rand(30000,30000)\r\nu,s,v = torch.linalg.svd(x)\r\n```\n<issue_comment>username_1: Thank you for the tiny repro! I can reproduce in master. cc @username_2\n<issue_comment>username_2: The problem is related to https://github.com/pytorch/pytorch/issues/51720.\r\n\r\nThe optimal workspace size for the input of this size is `2.70021e+09` which doesn't fit into `int` type. In PyTorch, we use a 32-bit interface to backend linear algebra libraries.\r\nWe can \"fix\" the internal assert with this patch\r\n```\r\ndiff --git a/aten/src/ATen/native/BatchLinearAlgebra.cpp b/aten/src/ATen/native/BatchLinearAlgebra.cpp\r\nindex 853f5c2cbe..8902958d98 100644\r\n--- a/aten/src/ATen/native/BatchLinearAlgebra.cpp\r\n+++ b/aten/src/ATen/native/BatchLinearAlgebra.cpp\r\n@@ -2984,7 +2984,9 @@ static void apply_svd(Tensor& self, Tensor& U, Tensor& S, Tensor& VT,\r\n   int lwork = -1;\r\n   scalar_t wkopt;\r\n   lapackSvd<scalar_t, value_t>(jobz, m, n, self_data, lda, S_data, U_data, lda, VT_data, ldvt, &wkopt, lwork, rwork_data, iwork_data, &info);\r\n-  lwork = std::max<int>(1, real_impl<scalar_t, value_t>(wkopt));\r\n+  TORCH_CHECK(\r\n+      (real_impl<scalar_t, value_t>(wkopt)) < INT_MAX,\r\n+      \"svd: The optimal workspace size is larger than allowed by 32-bit interface to backend math library.\");\r\n   Tensor work = at::empty({lwork}, self.options());\r\n   auto work_data = work.data_ptr<scalar_t>();\r\n```\r\nThe issue is likely there for other functions as well and we could include checks of the values before casting them to int.\n<issue_comment>username_0: @username_2 will there be any way around this in the future for large SVDs? Or is there an upper size limit due to the 32bit interface?\n<issue_comment>username_2: Unfortunately, there is no workaround available.\r\nFrom the MKL documentation the workspace size (`lwork`) can be computed using these guidelines:\r\n```\r\nFor real flavors:\r\n\r\nIf jobz = 'N', lwork  3*min(m, n) + max (max(m,n), 6*min(m, n));\r\n\r\nIf jobz = 'O', lwork  3*(min(m, n))2 + max (max(m, n), 5*(min(m, n))2 + 4*min(m, n));\r\n\r\nIf jobz = 'S' or 'A', lwork  3*(min(m, n))2 + max (max(m, n), 4*(min(m, n))2 + 4*min(m, n))\r\n\r\nFor complex flavors:\r\n\r\nIf jobz = 'N', lwork  2*min(m, n) + max(m, n);\r\n\r\nIf jobz = 'O', lwork  2*(min(m, n))2 + max(m, n) + 2*min(m, n);\r\n\r\nIf jobz = 'S' or 'A', lwork  (min(m, n))2 + max(m, n) + 2*min(m, n);\r\n```\r\n`lwork` must be smaller than `2147483647` (maximum `int`) in order to not hit the limitation of the 32-bit interface.\n<issue_comment>username_0: @username_2 Any clue as to why `svdvals` and `eigvalsh` work?\n<issue_comment>username_1: For what is worth, `svdvals` and `eigvalsh` may work in master when the input does not require gradients, as we do not compute the `U` and the `Vh` in those cases, so the working space needed is less than for the full SVD / eigendecomposition.","repo":"pytorch/pytorch","issue_id":1052456616,"issue_number":68291}
{"question":"What is the purpose of ensuring that methods with large bodies are placed in `.cpp` (source) files instead of `.h` (header) files?","answer":"The purpose is to prevent slowing down compilation speed by avoiding large method definitions in header files, as mentioned in the text.","id":108,"text":"Stack from [ghstack](https://github.com/username_1/ghstack):\n* (to be filled)\n\nSimple refactoring change to ensure that methods with large bodies don't show up in `.h` (header) files. Instead they should be in `.cpp` (source) files to prevent slowing down compilation speed.\n\n#accept2ship\n\nDifferential Revision: [D34432507](https://our.internmc.facebook.com/intern/diff/D34432507/)","context":"<issue_start><issue_comment>Title: [PyTorch] [Model Tracer] Refactor method bodies in various record function handlers into source (.cpp files)\nusername_0: Stack from [ghstack](https://github.com/username_1/ghstack):\n* (to be filled)\n\nSimple refactoring change to ensure that methods with large bodies don't show up in `.h` (header) files. Instead they should be in `.cpp` (source) files to prevent slowing down compilation speed.\n\n#accept2ship\n\nDifferential Revision: [D34432507](https://our.internmc.facebook.com/intern/diff/D34432507/)\n<issue_comment>username_1: @username_1 has imported this pull request.  If you are a Facebook employee, you can view this diff [on Phabricator](https://www.internalfb.com/diff/D34432507).","repo":"pytorch/pytorch","issue_id":1148648561,"issue_number":73321}
{"question":"Who has imported this pull request according to the text?","answer":"@username_1","id":109,"text":"@username_1 has imported this pull request.  If you are a Facebook employee, you can view this diff [on Phabricator](https://www.internalfb.com/diff/D34432507).","context":"<issue_start><issue_comment>Title: [PyTorch] [Model Tracer] Refactor method bodies in various record function handlers into source (.cpp files)\nusername_0: Stack from [ghstack](https://github.com/username_1/ghstack):\n* (to be filled)\n\nSimple refactoring change to ensure that methods with large bodies don't show up in `.h` (header) files. Instead they should be in `.cpp` (source) files to prevent slowing down compilation speed.\n\n#accept2ship\n\nDifferential Revision: [D34432507](https://our.internmc.facebook.com/intern/diff/D34432507/)\n<issue_comment>username_1: @username_1 has imported this pull request.  If you are a Facebook employee, you can view this diff [on Phabricator](https://www.internalfb.com/diff/D34432507).","repo":"pytorch/pytorch","issue_id":1148648561,"issue_number":73321}
{"question":"What adjustment was made to the version to accommodate a release with only semver-patch & semver-exempt changes?","answer":"The version was bumped back to semver-minor level.","id":111,"text":"(Not technically semver-minor on its own but bumping back to that since it otherwise thwarts my attempt to run a release with only semver-patch & semver-exempt changes)","context":"<issue_start><issue_comment>Title: Fix tests from #6158 to use Jasmine 2\nusername_0: When merging, I didnt notice it uses old test format.\r\nFixing these to unbreak master.\n<issue_comment>username_1: (Not technically semver-minor on its own but bumping back to that since it otherwise thwarts my attempt to run a release with only semver-patch & semver-exempt changes)","repo":"facebook/react","issue_id":162356676,"issue_number":7126}
{"question":"What are the steps to be completed before submitting a pull request to the React repository on GitHub?","answer":"The steps to be completed before submitting a pull request to the React repository on GitHub include: 1. Forking the repository and creating a branch from `master`, 2. Adding tests if new code is added, 3. Updating the documentation if APIs are changed, 4. Ensuring the test suite passes using `npm test`, 5. Formatting the code with prettier using `npm run prettier`, 6. Running lint checks with `npm run lint`, 7. Performing Flow typechecks with `npm run flow`, and 8. Completing the CLA if not done already.","id":112,"text":"**Before submitting a pull request,** please make sure the following is done:\r\n\r\n1. Fork [the repository](https://github.com/facebook/react) and create your branch from `master`.\r\n2. If you've added code that should be tested, add tests!\r\n3. If you've changed APIs, update the documentation.\r\n4. Ensure the test suite passes (`npm test`).\r\n5. Format your code with [prettier](https://github.com/prettier/prettier) (`npm run prettier`).\r\n6. Make sure your code lints (`npm run lint`).\r\n7. Run the [Flow](https://flowtype.org/) typechecks (`npm run flow`).\r\n8. If you haven't already, complete the CLA.","context":"<issue_start><issue_comment>Title: Update conferences\nusername_0: **Before submitting a pull request,** please make sure the following is done:\r\n\r\n1. Fork [the repository](https://github.com/facebook/react) and create your branch from `master`.\r\n2. If you've added code that should be tested, add tests!\r\n3. If you've changed APIs, update the documentation.\r\n4. Ensure the test suite passes (`npm test`).\r\n5. Format your code with [prettier](https://github.com/prettier/prettier) (`npm run prettier`).\r\n6. Make sure your code lints (`npm run lint`).\r\n7. Run the [Flow](https://flowtype.org/) typechecks (`npm run flow`).\r\n8. If you haven't already, complete the CLA.\n<issue_comment>username_1: React Boston and React Alicante also ended.\n<issue_comment>username_0: Thanks!","repo":"facebook/react","issue_id":259881446,"issue_number":10781}
{"question":"What error message did the user encounter when trying to load an image database after making changes to DataLoader.py and installing PyTorch via a wheel file?","answer":"The error message was: ImportError: sys.meta_path is None, Python is likely shutting down.","id":115,"text":"- OS: Ubuntu 16.04\r\n- PyTorch version: torch-0.3.1-cp36-cp36-linux_x86_64.whl\r\n- How you installed PyTorch (conda, pip, source): pip3 from the binary\r\n- Python version: 3.6\r\n- CUDA/cuDNN version: 9.1\r\n- GPU models and configuration: GTX 1070 Ti\r\n\r\nAfter install using the wheel file, I have changed the torch/utils/data/Dataloader.py according to the changes shown at https://github.com/pytorch/pytorch/pull/4643\r\n\r\nBut the same error still happen when I try the tutorial example to load image database:\r\n\r\nException ignored in: <bound method DataLoaderIter.__del__ of <torch.utils.data.dataloader.DataLoaderIter object at 0x56090c8c7348>>\r\nTraceback (most recent call last):\r\n  File \"/home/username_0/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 375, in __del__\r\n  File \"/home/username_0/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 361, in _shutdown_workers\r\n  File \"/home/username_0/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 39, in _get_from_queue\r\n  File \"/usr/pkg/lib/python3.6/multiprocessing/queues.py\", line 337, in get\r\nImportError: sys.meta_path is None, Python is likely shutting down\r\n\r\nSo could anyone help me on that? Or is there any other file I should modify to get it run?","context":"<issue_start><issue_comment>Title: Problem in DataLoader (same problem with #4643, but not solved by it)\nusername_0: - OS: Ubuntu 16.04\r\n- PyTorch version: torch-0.3.1-cp36-cp36-linux_x86_64.whl\r\n- How you installed PyTorch (conda, pip, source): pip3 from the binary\r\n- Python version: 3.6\r\n- CUDA/cuDNN version: 9.1\r\n- GPU models and configuration: GTX 1070 Ti\r\n\r\nAfter install using the wheel file, I have changed the torch/utils/data/Dataloader.py according to the changes shown at https://github.com/pytorch/pytorch/pull/4643\r\n\r\nBut the same error still happen when I try the tutorial example to load image database:\r\n\r\nException ignored in: <bound method DataLoaderIter.__del__ of <torch.utils.data.dataloader.DataLoaderIter object at 0x56090c8c7348>>\r\nTraceback (most recent call last):\r\n  File \"/home/username_0/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 375, in __del__\r\n  File \"/home/username_0/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 361, in _shutdown_workers\r\n  File \"/home/username_0/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 39, in _get_from_queue\r\n  File \"/usr/pkg/lib/python3.6/multiprocessing/queues.py\", line 337, in get\r\nImportError: sys.meta_path is None, Python is likely shutting down\r\n\r\nSo could anyone help me on that? Or is there any other file I should modify to get it run?\n<issue_comment>username_0: And, I am not sure how can I use the changes in torch/csrc/DataLoader.cpp, should I clone all the source code on GitHub and build the newest binary to fully fix this my problem?\n<issue_comment>username_1: when does this error happen? is it happening when you exit python?\n<issue_comment>username_2: @username_1 and @username_3,\r\nI am running this:\r\n\r\npython train.py --dataroot ./datasets/horse2zebra --name horse2zebra_cyclegan --model cycle_gan --no_dropout --loadSize 64 --nThreads 1\r\n\r\nfrom \r\n\r\nhttps://github.com/junyanz/pytorch-CycleGAN-and-pix2pix\r\n\r\nThe model is succefully created:\r\nmodel [CycleGANModel] was created\r\n\r\nI am on ubuntu 16.04, pytorch 3.1, cuda 9.0 and cuDnn 7.1\r\n\r\nThe error I get is traced below:\r\ncreate web directory ./checkpoints/horse2zebra_cyclegan/web...\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 22, in <module>\r\n    for i, data in enumerate(dataset):\r\n  File \"/home/sam/Documents/ComputerVision/pytorch-CycleGAN-and-pix2pix/data/custom_dataset_data_loader.py\", line 44, in __iter__\r\n    for i, data in enumerate(self.dataloader):\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 275, in __next__\r\n    idx, batch = self._get_batch()\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 254, in _get_batch\r\n    return self.data_queue.get()\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/queues.py\", line 335, in get\r\n    res = self._reader.recv_bytes()\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\r\n    buf = self._recv_bytes(maxlength)\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\r\n    buf = self._recv(4)\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\r\n    chunk = read(handle, remaining)\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 175, in handler\r\n    _error_if_any_worker_fails()\r\nRuntimeError: DataLoader worker (pid 3792) is killed by signal: Illegal instruction.\n<issue_comment>username_3: @username_2 that error is because you are running it on a computer that is too old and does not have SSE4 instruction on CPU.\n<issue_comment>username_1: @username_2 Also, it is more likely due to that your CPU doesn't support the operations done in the dataset.__getitem__.\n<issue_comment>username_2: @username_1 & @username_3,\r\n\r\nSome months ago I made it work in windows10 (I think)\r\nThe my windows machine crashed. So here I am. Sorry\r\n\r\nHere is what I see in cat /proc/cpuinfo:\r\n\r\nprocessor\t: 0\r\nvendor_id\t: AuthenticAMD\r\ncpu family\t: 21\r\nmodel\t\t: 2\r\nmodel name\t: AMD FX(tm)-4300 Quad-Core Processor\r\nstepping\t: 0\r\nmicrocode\t: 0x600081c\r\ncpu MHz\t\t: 1400.000\r\ncache size\t: 2048 KB\r\nphysical id\t: 0\r\nsiblings\t: 4\r\ncore id\t\t: 0\r\ncpu cores\t: 2\r\napicid\t\t: 16\r\ninitial apicid\t: 0\r\nfpu\t\t: yes\r\nfpu_exception\t: yes\r\ncpuid level\t: 13\r\nwp\t\t: yes\r\nflags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf pni pclmulqdq monitor ssse3 fma cx16 **sse4_1 sse4_2** popcnt aes xsave avx f16c lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs xop skinit wdt lwp fma4 tce nodeid_msr tbm topoext perfctr_core perfctr_nb cpb hw_pstate retpoline retpoline_amd rsb_ctxsw vmmcall bmi1 arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold\r\nbugs\t\t: fxsave_leak sysret_ss_attrs null_seg spectre_v1 spectre_v2\r\nbogomips\t: 7634.15\r\nTLB size\t: 1536 4K pages\r\nclflush size\t: 64\r\ncache_alignment\t: 64\r\naddress sizes\t: 48 bits physical, 48 bits virtual\r\npower management: ts ttp tm 100mhzsteps hwpstate cpb eff_freq_ro\r\n\r\n@username_1 how do I check if CPU support dataset.getitem?\n<issue_comment>username_1: @username_2 run with `nThreads=0`\n<issue_comment>username_2: tried it with nThreads=0\r\ndifferent error...\r\nmodel [CycleGANModel] was created\r\ncreate web directory ./checkpoints/horse2zebra_cyclegan/web...\r\nIllegal instruction (core dumped)\n<issue_comment>username_2: I see something I am uneasy about:\r\nsee bold below\r\n\r\nCustomDatasetDataLoader\r\ndataset [UnalignedDataset] was created\r\n/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torchvision-0.2.0-py3.6.egg/torchvision/transforms/transforms.py:156: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\r\n#training images = 1334\r\ncycle_gan\r\n**initialization method [normal]\r\ninitialization method [normal]\r\ninitialization method [normal]\r\ninitialization method [normal]**\n<issue_comment>username_1: @username_2 No it's same error. We just wrap it in a nice way so you can see it even if it's in the workers. \r\n\r\nIsn't that expected? CycleGAN has four networks: D_X D_Y G F .\n<issue_comment>username_1: @username_2 anyways, it confirms my hypothesis that your CPU doesn't support the operation in `dataset.__getitem__`.\n<issue_comment>username_2: Oh... Can you suggest a workaround?\n<issue_comment>username_1: @username_2 You can try to put debug statements in CycleGAN code and see which one isn't supported by your CPU (probably some PIL operations), and search for replacements. Sorry I don't have much else to offer.\n<issue_comment>username_2: @username_1 \r\nThank you sir !<issue_closed>\n<issue_comment>username_1: closed due to inactivity. feel free to reopen if you can reproduce with latest stable (0.4 at the moment of writing).\n<issue_comment>username_4: Still can't get this to work.\r\n\r\nhttps://stackoverflow.com/questions/53449927/runtimeerror-dataloader-worker-is-killed-by-signal-illegal-instruction\r\n\r\nPlease help.","repo":"pytorch/pytorch","issue_id":304991946,"issue_number":5761}
{"question":"How can one effectively utilize the changes in torch/csrc/DataLoader.cpp to address the stated problem?","answer":"To effectively utilize the changes in torch/csrc/DataLoader.cpp, one does not necessarily need to clone the entire source code from GitHub and build a new binary. Instead, the specific changes in DataLoader.cpp can be identified and integrated into the existing codebase to address the problem. This can be achieved by carefully reviewing the changes, understanding their impact, and selectively applying them to the relevant sections of the codebase.","id":116,"text":"And, I am not sure how can I use the changes in torch/csrc/DataLoader.cpp, should I clone all the source code on GitHub and build the newest binary to fully fix this my problem?","context":"<issue_start><issue_comment>Title: Problem in DataLoader (same problem with #4643, but not solved by it)\nusername_0: - OS: Ubuntu 16.04\r\n- PyTorch version: torch-0.3.1-cp36-cp36-linux_x86_64.whl\r\n- How you installed PyTorch (conda, pip, source): pip3 from the binary\r\n- Python version: 3.6\r\n- CUDA/cuDNN version: 9.1\r\n- GPU models and configuration: GTX 1070 Ti\r\n\r\nAfter install using the wheel file, I have changed the torch/utils/data/Dataloader.py according to the changes shown at https://github.com/pytorch/pytorch/pull/4643\r\n\r\nBut the same error still happen when I try the tutorial example to load image database:\r\n\r\nException ignored in: <bound method DataLoaderIter.__del__ of <torch.utils.data.dataloader.DataLoaderIter object at 0x56090c8c7348>>\r\nTraceback (most recent call last):\r\n  File \"/home/username_0/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 375, in __del__\r\n  File \"/home/username_0/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 361, in _shutdown_workers\r\n  File \"/home/username_0/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 39, in _get_from_queue\r\n  File \"/usr/pkg/lib/python3.6/multiprocessing/queues.py\", line 337, in get\r\nImportError: sys.meta_path is None, Python is likely shutting down\r\n\r\nSo could anyone help me on that? Or is there any other file I should modify to get it run?\n<issue_comment>username_0: And, I am not sure how can I use the changes in torch/csrc/DataLoader.cpp, should I clone all the source code on GitHub and build the newest binary to fully fix this my problem?\n<issue_comment>username_1: when does this error happen? is it happening when you exit python?\n<issue_comment>username_2: @username_1 and @username_3,\r\nI am running this:\r\n\r\npython train.py --dataroot ./datasets/horse2zebra --name horse2zebra_cyclegan --model cycle_gan --no_dropout --loadSize 64 --nThreads 1\r\n\r\nfrom \r\n\r\nhttps://github.com/junyanz/pytorch-CycleGAN-and-pix2pix\r\n\r\nThe model is succefully created:\r\nmodel [CycleGANModel] was created\r\n\r\nI am on ubuntu 16.04, pytorch 3.1, cuda 9.0 and cuDnn 7.1\r\n\r\nThe error I get is traced below:\r\ncreate web directory ./checkpoints/horse2zebra_cyclegan/web...\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 22, in <module>\r\n    for i, data in enumerate(dataset):\r\n  File \"/home/sam/Documents/ComputerVision/pytorch-CycleGAN-and-pix2pix/data/custom_dataset_data_loader.py\", line 44, in __iter__\r\n    for i, data in enumerate(self.dataloader):\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 275, in __next__\r\n    idx, batch = self._get_batch()\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 254, in _get_batch\r\n    return self.data_queue.get()\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/queues.py\", line 335, in get\r\n    res = self._reader.recv_bytes()\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\r\n    buf = self._recv_bytes(maxlength)\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\r\n    buf = self._recv(4)\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\r\n    chunk = read(handle, remaining)\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 175, in handler\r\n    _error_if_any_worker_fails()\r\nRuntimeError: DataLoader worker (pid 3792) is killed by signal: Illegal instruction.\n<issue_comment>username_3: @username_2 that error is because you are running it on a computer that is too old and does not have SSE4 instruction on CPU.\n<issue_comment>username_1: @username_2 Also, it is more likely due to that your CPU doesn't support the operations done in the dataset.__getitem__.\n<issue_comment>username_2: @username_1 & @username_3,\r\n\r\nSome months ago I made it work in windows10 (I think)\r\nThe my windows machine crashed. So here I am. Sorry\r\n\r\nHere is what I see in cat /proc/cpuinfo:\r\n\r\nprocessor\t: 0\r\nvendor_id\t: AuthenticAMD\r\ncpu family\t: 21\r\nmodel\t\t: 2\r\nmodel name\t: AMD FX(tm)-4300 Quad-Core Processor\r\nstepping\t: 0\r\nmicrocode\t: 0x600081c\r\ncpu MHz\t\t: 1400.000\r\ncache size\t: 2048 KB\r\nphysical id\t: 0\r\nsiblings\t: 4\r\ncore id\t\t: 0\r\ncpu cores\t: 2\r\napicid\t\t: 16\r\ninitial apicid\t: 0\r\nfpu\t\t: yes\r\nfpu_exception\t: yes\r\ncpuid level\t: 13\r\nwp\t\t: yes\r\nflags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf pni pclmulqdq monitor ssse3 fma cx16 **sse4_1 sse4_2** popcnt aes xsave avx f16c lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs xop skinit wdt lwp fma4 tce nodeid_msr tbm topoext perfctr_core perfctr_nb cpb hw_pstate retpoline retpoline_amd rsb_ctxsw vmmcall bmi1 arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold\r\nbugs\t\t: fxsave_leak sysret_ss_attrs null_seg spectre_v1 spectre_v2\r\nbogomips\t: 7634.15\r\nTLB size\t: 1536 4K pages\r\nclflush size\t: 64\r\ncache_alignment\t: 64\r\naddress sizes\t: 48 bits physical, 48 bits virtual\r\npower management: ts ttp tm 100mhzsteps hwpstate cpb eff_freq_ro\r\n\r\n@username_1 how do I check if CPU support dataset.getitem?\n<issue_comment>username_1: @username_2 run with `nThreads=0`\n<issue_comment>username_2: tried it with nThreads=0\r\ndifferent error...\r\nmodel [CycleGANModel] was created\r\ncreate web directory ./checkpoints/horse2zebra_cyclegan/web...\r\nIllegal instruction (core dumped)\n<issue_comment>username_2: I see something I am uneasy about:\r\nsee bold below\r\n\r\nCustomDatasetDataLoader\r\ndataset [UnalignedDataset] was created\r\n/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torchvision-0.2.0-py3.6.egg/torchvision/transforms/transforms.py:156: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\r\n#training images = 1334\r\ncycle_gan\r\n**initialization method [normal]\r\ninitialization method [normal]\r\ninitialization method [normal]\r\ninitialization method [normal]**\n<issue_comment>username_1: @username_2 No it's same error. We just wrap it in a nice way so you can see it even if it's in the workers. \r\n\r\nIsn't that expected? CycleGAN has four networks: D_X D_Y G F .\n<issue_comment>username_1: @username_2 anyways, it confirms my hypothesis that your CPU doesn't support the operation in `dataset.__getitem__`.\n<issue_comment>username_2: Oh... Can you suggest a workaround?\n<issue_comment>username_1: @username_2 You can try to put debug statements in CycleGAN code and see which one isn't supported by your CPU (probably some PIL operations), and search for replacements. Sorry I don't have much else to offer.\n<issue_comment>username_2: @username_1 \r\nThank you sir !<issue_closed>\n<issue_comment>username_1: closed due to inactivity. feel free to reopen if you can reproduce with latest stable (0.4 at the moment of writing).\n<issue_comment>username_4: Still can't get this to work.\r\n\r\nhttps://stackoverflow.com/questions/53449927/runtimeerror-dataloader-worker-is-killed-by-signal-illegal-instruction\r\n\r\nPlease help.","repo":"pytorch/pytorch","issue_id":304991946,"issue_number":5761}
{"question":"What is the specific error message that the user encountered while running the CycleGAN model on Ubuntu 16.04 with PyTorch 0.3.1, CUDA 9.0, and cuDNN 7.1?","answer":"The specific error message is 'RuntimeError: DataLoader worker (pid 3792) is killed by signal: Illegal instruction.'","id":118,"text":"@username_1 and @username_3,\r\nI am running this:\r\n\r\npython train.py --dataroot ./datasets/horse2zebra --name horse2zebra_cyclegan --model cycle_gan --no_dropout --loadSize 64 --nThreads 1\r\n\r\nfrom \r\n\r\nhttps://github.com/junyanz/pytorch-CycleGAN-and-pix2pix\r\n\r\nThe model is succefully created:\r\nmodel [CycleGANModel] was created\r\n\r\nI am on ubuntu 16.04, pytorch 3.1, cuda 9.0 and cuDnn 7.1\r\n\r\nThe error I get is traced below:\r\ncreate web directory ./checkpoints/horse2zebra_cyclegan/web...\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 22, in <module>\r\n    for i, data in enumerate(dataset):\r\n  File \"/home/sam/Documents/ComputerVision/pytorch-CycleGAN-and-pix2pix/data/custom_dataset_data_loader.py\", line 44, in __iter__\r\n    for i, data in enumerate(self.dataloader):\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 275, in __next__\r\n    idx, batch = self._get_batch()\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 254, in _get_batch\r\n    return self.data_queue.get()\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/queues.py\", line 335, in get\r\n    res = self._reader.recv_bytes()\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\r\n    buf = self._recv_bytes(maxlength)\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\r\n    buf = self._recv(4)\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\r\n    chunk = read(handle, remaining)\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 175, in handler\r\n    _error_if_any_worker_fails()\r\nRuntimeError: DataLoader worker (pid 3792) is killed by signal: Illegal instruction.","context":"<issue_start><issue_comment>Title: Problem in DataLoader (same problem with #4643, but not solved by it)\nusername_0: - OS: Ubuntu 16.04\r\n- PyTorch version: torch-0.3.1-cp36-cp36-linux_x86_64.whl\r\n- How you installed PyTorch (conda, pip, source): pip3 from the binary\r\n- Python version: 3.6\r\n- CUDA/cuDNN version: 9.1\r\n- GPU models and configuration: GTX 1070 Ti\r\n\r\nAfter install using the wheel file, I have changed the torch/utils/data/Dataloader.py according to the changes shown at https://github.com/pytorch/pytorch/pull/4643\r\n\r\nBut the same error still happen when I try the tutorial example to load image database:\r\n\r\nException ignored in: <bound method DataLoaderIter.__del__ of <torch.utils.data.dataloader.DataLoaderIter object at 0x56090c8c7348>>\r\nTraceback (most recent call last):\r\n  File \"/home/username_0/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 375, in __del__\r\n  File \"/home/username_0/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 361, in _shutdown_workers\r\n  File \"/home/username_0/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 39, in _get_from_queue\r\n  File \"/usr/pkg/lib/python3.6/multiprocessing/queues.py\", line 337, in get\r\nImportError: sys.meta_path is None, Python is likely shutting down\r\n\r\nSo could anyone help me on that? Or is there any other file I should modify to get it run?\n<issue_comment>username_0: And, I am not sure how can I use the changes in torch/csrc/DataLoader.cpp, should I clone all the source code on GitHub and build the newest binary to fully fix this my problem?\n<issue_comment>username_1: when does this error happen? is it happening when you exit python?\n<issue_comment>username_2: @username_1 and @username_3,\r\nI am running this:\r\n\r\npython train.py --dataroot ./datasets/horse2zebra --name horse2zebra_cyclegan --model cycle_gan --no_dropout --loadSize 64 --nThreads 1\r\n\r\nfrom \r\n\r\nhttps://github.com/junyanz/pytorch-CycleGAN-and-pix2pix\r\n\r\nThe model is succefully created:\r\nmodel [CycleGANModel] was created\r\n\r\nI am on ubuntu 16.04, pytorch 3.1, cuda 9.0 and cuDnn 7.1\r\n\r\nThe error I get is traced below:\r\ncreate web directory ./checkpoints/horse2zebra_cyclegan/web...\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 22, in <module>\r\n    for i, data in enumerate(dataset):\r\n  File \"/home/sam/Documents/ComputerVision/pytorch-CycleGAN-and-pix2pix/data/custom_dataset_data_loader.py\", line 44, in __iter__\r\n    for i, data in enumerate(self.dataloader):\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 275, in __next__\r\n    idx, batch = self._get_batch()\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 254, in _get_batch\r\n    return self.data_queue.get()\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/queues.py\", line 335, in get\r\n    res = self._reader.recv_bytes()\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\r\n    buf = self._recv_bytes(maxlength)\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\r\n    buf = self._recv(4)\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\r\n    chunk = read(handle, remaining)\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 175, in handler\r\n    _error_if_any_worker_fails()\r\nRuntimeError: DataLoader worker (pid 3792) is killed by signal: Illegal instruction.\n<issue_comment>username_3: @username_2 that error is because you are running it on a computer that is too old and does not have SSE4 instruction on CPU.\n<issue_comment>username_1: @username_2 Also, it is more likely due to that your CPU doesn't support the operations done in the dataset.__getitem__.\n<issue_comment>username_2: @username_1 & @username_3,\r\n\r\nSome months ago I made it work in windows10 (I think)\r\nThe my windows machine crashed. So here I am. Sorry\r\n\r\nHere is what I see in cat /proc/cpuinfo:\r\n\r\nprocessor\t: 0\r\nvendor_id\t: AuthenticAMD\r\ncpu family\t: 21\r\nmodel\t\t: 2\r\nmodel name\t: AMD FX(tm)-4300 Quad-Core Processor\r\nstepping\t: 0\r\nmicrocode\t: 0x600081c\r\ncpu MHz\t\t: 1400.000\r\ncache size\t: 2048 KB\r\nphysical id\t: 0\r\nsiblings\t: 4\r\ncore id\t\t: 0\r\ncpu cores\t: 2\r\napicid\t\t: 16\r\ninitial apicid\t: 0\r\nfpu\t\t: yes\r\nfpu_exception\t: yes\r\ncpuid level\t: 13\r\nwp\t\t: yes\r\nflags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf pni pclmulqdq monitor ssse3 fma cx16 **sse4_1 sse4_2** popcnt aes xsave avx f16c lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs xop skinit wdt lwp fma4 tce nodeid_msr tbm topoext perfctr_core perfctr_nb cpb hw_pstate retpoline retpoline_amd rsb_ctxsw vmmcall bmi1 arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold\r\nbugs\t\t: fxsave_leak sysret_ss_attrs null_seg spectre_v1 spectre_v2\r\nbogomips\t: 7634.15\r\nTLB size\t: 1536 4K pages\r\nclflush size\t: 64\r\ncache_alignment\t: 64\r\naddress sizes\t: 48 bits physical, 48 bits virtual\r\npower management: ts ttp tm 100mhzsteps hwpstate cpb eff_freq_ro\r\n\r\n@username_1 how do I check if CPU support dataset.getitem?\n<issue_comment>username_1: @username_2 run with `nThreads=0`\n<issue_comment>username_2: tried it with nThreads=0\r\ndifferent error...\r\nmodel [CycleGANModel] was created\r\ncreate web directory ./checkpoints/horse2zebra_cyclegan/web...\r\nIllegal instruction (core dumped)\n<issue_comment>username_2: I see something I am uneasy about:\r\nsee bold below\r\n\r\nCustomDatasetDataLoader\r\ndataset [UnalignedDataset] was created\r\n/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torchvision-0.2.0-py3.6.egg/torchvision/transforms/transforms.py:156: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\r\n#training images = 1334\r\ncycle_gan\r\n**initialization method [normal]\r\ninitialization method [normal]\r\ninitialization method [normal]\r\ninitialization method [normal]**\n<issue_comment>username_1: @username_2 No it's same error. We just wrap it in a nice way so you can see it even if it's in the workers. \r\n\r\nIsn't that expected? CycleGAN has four networks: D_X D_Y G F .\n<issue_comment>username_1: @username_2 anyways, it confirms my hypothesis that your CPU doesn't support the operation in `dataset.__getitem__`.\n<issue_comment>username_2: Oh... Can you suggest a workaround?\n<issue_comment>username_1: @username_2 You can try to put debug statements in CycleGAN code and see which one isn't supported by your CPU (probably some PIL operations), and search for replacements. Sorry I don't have much else to offer.\n<issue_comment>username_2: @username_1 \r\nThank you sir !<issue_closed>\n<issue_comment>username_1: closed due to inactivity. feel free to reopen if you can reproduce with latest stable (0.4 at the moment of writing).\n<issue_comment>username_4: Still can't get this to work.\r\n\r\nhttps://stackoverflow.com/questions/53449927/runtimeerror-dataloader-worker-is-killed-by-signal-illegal-instruction\r\n\r\nPlease help.","repo":"pytorch/pytorch","issue_id":304991946,"issue_number":5761}
{"question":"How can you determine if a CPU supports the dataset.getitem function based on the information provided in the text chunk?","answer":"To determine if a CPU supports the dataset.getitem function based on the information provided in the text chunk, you need to check if the CPU model supports the SSE4.1 instruction set. This can be done by examining the flags section of the CPU information.","id":121,"text":"@username_1 & @username_3,\r\n\r\nSome months ago I made it work in windows10 (I think)\r\nThe my windows machine crashed. So here I am. Sorry\r\n\r\nHere is what I see in cat /proc/cpuinfo:\r\n\r\nprocessor\t: 0\r\nvendor_id\t: AuthenticAMD\r\ncpu family\t: 21\r\nmodel\t\t: 2\r\nmodel name\t: AMD FX(tm)-4300 Quad-Core Processor\r\nstepping\t: 0\r\nmicrocode\t: 0x600081c\r\ncpu MHz\t\t: 1400.000\r\ncache size\t: 2048 KB\r\nphysical id\t: 0\r\nsiblings\t: 4\r\ncore id\t\t: 0\r\ncpu cores\t: 2\r\napicid\t\t: 16\r\ninitial apicid\t: 0\r\nfpu\t\t: yes\r\nfpu_exception\t: yes\r\ncpuid level\t: 13\r\nwp\t\t: yes\r\nflags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf pni pclmulqdq monitor ssse3 fma cx16 **sse4_1 sse4_2** popcnt aes xsave avx f16c lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs xop skinit wdt lwp fma4 tce nodeid_msr tbm topoext perfctr_core perfctr_nb cpb hw_pstate retpoline retpoline_amd rsb_ctxsw vmmcall bmi1 arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold\r\nbugs\t\t: fxsave_leak sysret_ss_attrs null_seg spectre_v1 spectre_v2\r\nbogomips\t: 7634.15\r\nTLB size\t: 1536 4K pages\r\nclflush size\t: 64\r\ncache_alignment\t: 64\r\naddress sizes\t: 48 bits physical, 48 bits virtual\r\npower management: ts ttp tm 100mhzsteps hwpstate cpb eff_freq_ro\r\n\r\n@username_1 how do I check if CPU support dataset.getitem?","context":"<issue_start><issue_comment>Title: Problem in DataLoader (same problem with #4643, but not solved by it)\nusername_0: - OS: Ubuntu 16.04\r\n- PyTorch version: torch-0.3.1-cp36-cp36-linux_x86_64.whl\r\n- How you installed PyTorch (conda, pip, source): pip3 from the binary\r\n- Python version: 3.6\r\n- CUDA/cuDNN version: 9.1\r\n- GPU models and configuration: GTX 1070 Ti\r\n\r\nAfter install using the wheel file, I have changed the torch/utils/data/Dataloader.py according to the changes shown at https://github.com/pytorch/pytorch/pull/4643\r\n\r\nBut the same error still happen when I try the tutorial example to load image database:\r\n\r\nException ignored in: <bound method DataLoaderIter.__del__ of <torch.utils.data.dataloader.DataLoaderIter object at 0x56090c8c7348>>\r\nTraceback (most recent call last):\r\n  File \"/home/username_0/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 375, in __del__\r\n  File \"/home/username_0/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 361, in _shutdown_workers\r\n  File \"/home/username_0/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 39, in _get_from_queue\r\n  File \"/usr/pkg/lib/python3.6/multiprocessing/queues.py\", line 337, in get\r\nImportError: sys.meta_path is None, Python is likely shutting down\r\n\r\nSo could anyone help me on that? Or is there any other file I should modify to get it run?\n<issue_comment>username_0: And, I am not sure how can I use the changes in torch/csrc/DataLoader.cpp, should I clone all the source code on GitHub and build the newest binary to fully fix this my problem?\n<issue_comment>username_1: when does this error happen? is it happening when you exit python?\n<issue_comment>username_2: @username_1 and @username_3,\r\nI am running this:\r\n\r\npython train.py --dataroot ./datasets/horse2zebra --name horse2zebra_cyclegan --model cycle_gan --no_dropout --loadSize 64 --nThreads 1\r\n\r\nfrom \r\n\r\nhttps://github.com/junyanz/pytorch-CycleGAN-and-pix2pix\r\n\r\nThe model is succefully created:\r\nmodel [CycleGANModel] was created\r\n\r\nI am on ubuntu 16.04, pytorch 3.1, cuda 9.0 and cuDnn 7.1\r\n\r\nThe error I get is traced below:\r\ncreate web directory ./checkpoints/horse2zebra_cyclegan/web...\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 22, in <module>\r\n    for i, data in enumerate(dataset):\r\n  File \"/home/sam/Documents/ComputerVision/pytorch-CycleGAN-and-pix2pix/data/custom_dataset_data_loader.py\", line 44, in __iter__\r\n    for i, data in enumerate(self.dataloader):\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 275, in __next__\r\n    idx, batch = self._get_batch()\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 254, in _get_batch\r\n    return self.data_queue.get()\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/queues.py\", line 335, in get\r\n    res = self._reader.recv_bytes()\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\r\n    buf = self._recv_bytes(maxlength)\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\r\n    buf = self._recv(4)\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\r\n    chunk = read(handle, remaining)\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 175, in handler\r\n    _error_if_any_worker_fails()\r\nRuntimeError: DataLoader worker (pid 3792) is killed by signal: Illegal instruction.\n<issue_comment>username_3: @username_2 that error is because you are running it on a computer that is too old and does not have SSE4 instruction on CPU.\n<issue_comment>username_1: @username_2 Also, it is more likely due to that your CPU doesn't support the operations done in the dataset.__getitem__.\n<issue_comment>username_2: @username_1 & @username_3,\r\n\r\nSome months ago I made it work in windows10 (I think)\r\nThe my windows machine crashed. So here I am. Sorry\r\n\r\nHere is what I see in cat /proc/cpuinfo:\r\n\r\nprocessor\t: 0\r\nvendor_id\t: AuthenticAMD\r\ncpu family\t: 21\r\nmodel\t\t: 2\r\nmodel name\t: AMD FX(tm)-4300 Quad-Core Processor\r\nstepping\t: 0\r\nmicrocode\t: 0x600081c\r\ncpu MHz\t\t: 1400.000\r\ncache size\t: 2048 KB\r\nphysical id\t: 0\r\nsiblings\t: 4\r\ncore id\t\t: 0\r\ncpu cores\t: 2\r\napicid\t\t: 16\r\ninitial apicid\t: 0\r\nfpu\t\t: yes\r\nfpu_exception\t: yes\r\ncpuid level\t: 13\r\nwp\t\t: yes\r\nflags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf pni pclmulqdq monitor ssse3 fma cx16 **sse4_1 sse4_2** popcnt aes xsave avx f16c lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs xop skinit wdt lwp fma4 tce nodeid_msr tbm topoext perfctr_core perfctr_nb cpb hw_pstate retpoline retpoline_amd rsb_ctxsw vmmcall bmi1 arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold\r\nbugs\t\t: fxsave_leak sysret_ss_attrs null_seg spectre_v1 spectre_v2\r\nbogomips\t: 7634.15\r\nTLB size\t: 1536 4K pages\r\nclflush size\t: 64\r\ncache_alignment\t: 64\r\naddress sizes\t: 48 bits physical, 48 bits virtual\r\npower management: ts ttp tm 100mhzsteps hwpstate cpb eff_freq_ro\r\n\r\n@username_1 how do I check if CPU support dataset.getitem?\n<issue_comment>username_1: @username_2 run with `nThreads=0`\n<issue_comment>username_2: tried it with nThreads=0\r\ndifferent error...\r\nmodel [CycleGANModel] was created\r\ncreate web directory ./checkpoints/horse2zebra_cyclegan/web...\r\nIllegal instruction (core dumped)\n<issue_comment>username_2: I see something I am uneasy about:\r\nsee bold below\r\n\r\nCustomDatasetDataLoader\r\ndataset [UnalignedDataset] was created\r\n/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torchvision-0.2.0-py3.6.egg/torchvision/transforms/transforms.py:156: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\r\n#training images = 1334\r\ncycle_gan\r\n**initialization method [normal]\r\ninitialization method [normal]\r\ninitialization method [normal]\r\ninitialization method [normal]**\n<issue_comment>username_1: @username_2 No it's same error. We just wrap it in a nice way so you can see it even if it's in the workers. \r\n\r\nIsn't that expected? CycleGAN has four networks: D_X D_Y G F .\n<issue_comment>username_1: @username_2 anyways, it confirms my hypothesis that your CPU doesn't support the operation in `dataset.__getitem__`.\n<issue_comment>username_2: Oh... Can you suggest a workaround?\n<issue_comment>username_1: @username_2 You can try to put debug statements in CycleGAN code and see which one isn't supported by your CPU (probably some PIL operations), and search for replacements. Sorry I don't have much else to offer.\n<issue_comment>username_2: @username_1 \r\nThank you sir !<issue_closed>\n<issue_comment>username_1: closed due to inactivity. feel free to reopen if you can reproduce with latest stable (0.4 at the moment of writing).\n<issue_comment>username_4: Still can't get this to work.\r\n\r\nhttps://stackoverflow.com/questions/53449927/runtimeerror-dataloader-worker-is-killed-by-signal-illegal-instruction\r\n\r\nPlease help.","repo":"pytorch/pytorch","issue_id":304991946,"issue_number":5761}
{"question":"What could have caused the 'Illegal instruction' error when attempting to create a CycleGAN model with nThreads=0?","answer":"The 'Illegal instruction' error could be caused by an issue with the compilation or execution environment, such as incompatible instructions for the processor architecture.","id":123,"text":"tried it with nThreads=0\r\ndifferent error...\r\nmodel [CycleGANModel] was created\r\ncreate web directory ./checkpoints/horse2zebra_cyclegan/web...\r\nIllegal instruction (core dumped)","context":"<issue_start><issue_comment>Title: Problem in DataLoader (same problem with #4643, but not solved by it)\nusername_0: - OS: Ubuntu 16.04\r\n- PyTorch version: torch-0.3.1-cp36-cp36-linux_x86_64.whl\r\n- How you installed PyTorch (conda, pip, source): pip3 from the binary\r\n- Python version: 3.6\r\n- CUDA/cuDNN version: 9.1\r\n- GPU models and configuration: GTX 1070 Ti\r\n\r\nAfter install using the wheel file, I have changed the torch/utils/data/Dataloader.py according to the changes shown at https://github.com/pytorch/pytorch/pull/4643\r\n\r\nBut the same error still happen when I try the tutorial example to load image database:\r\n\r\nException ignored in: <bound method DataLoaderIter.__del__ of <torch.utils.data.dataloader.DataLoaderIter object at 0x56090c8c7348>>\r\nTraceback (most recent call last):\r\n  File \"/home/username_0/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 375, in __del__\r\n  File \"/home/username_0/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 361, in _shutdown_workers\r\n  File \"/home/username_0/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 39, in _get_from_queue\r\n  File \"/usr/pkg/lib/python3.6/multiprocessing/queues.py\", line 337, in get\r\nImportError: sys.meta_path is None, Python is likely shutting down\r\n\r\nSo could anyone help me on that? Or is there any other file I should modify to get it run?\n<issue_comment>username_0: And, I am not sure how can I use the changes in torch/csrc/DataLoader.cpp, should I clone all the source code on GitHub and build the newest binary to fully fix this my problem?\n<issue_comment>username_1: when does this error happen? is it happening when you exit python?\n<issue_comment>username_2: @username_1 and @username_3,\r\nI am running this:\r\n\r\npython train.py --dataroot ./datasets/horse2zebra --name horse2zebra_cyclegan --model cycle_gan --no_dropout --loadSize 64 --nThreads 1\r\n\r\nfrom \r\n\r\nhttps://github.com/junyanz/pytorch-CycleGAN-and-pix2pix\r\n\r\nThe model is succefully created:\r\nmodel [CycleGANModel] was created\r\n\r\nI am on ubuntu 16.04, pytorch 3.1, cuda 9.0 and cuDnn 7.1\r\n\r\nThe error I get is traced below:\r\ncreate web directory ./checkpoints/horse2zebra_cyclegan/web...\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 22, in <module>\r\n    for i, data in enumerate(dataset):\r\n  File \"/home/sam/Documents/ComputerVision/pytorch-CycleGAN-and-pix2pix/data/custom_dataset_data_loader.py\", line 44, in __iter__\r\n    for i, data in enumerate(self.dataloader):\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 275, in __next__\r\n    idx, batch = self._get_batch()\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 254, in _get_batch\r\n    return self.data_queue.get()\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/queues.py\", line 335, in get\r\n    res = self._reader.recv_bytes()\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\r\n    buf = self._recv_bytes(maxlength)\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\r\n    buf = self._recv(4)\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\r\n    chunk = read(handle, remaining)\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 175, in handler\r\n    _error_if_any_worker_fails()\r\nRuntimeError: DataLoader worker (pid 3792) is killed by signal: Illegal instruction.\n<issue_comment>username_3: @username_2 that error is because you are running it on a computer that is too old and does not have SSE4 instruction on CPU.\n<issue_comment>username_1: @username_2 Also, it is more likely due to that your CPU doesn't support the operations done in the dataset.__getitem__.\n<issue_comment>username_2: @username_1 & @username_3,\r\n\r\nSome months ago I made it work in windows10 (I think)\r\nThe my windows machine crashed. So here I am. Sorry\r\n\r\nHere is what I see in cat /proc/cpuinfo:\r\n\r\nprocessor\t: 0\r\nvendor_id\t: AuthenticAMD\r\ncpu family\t: 21\r\nmodel\t\t: 2\r\nmodel name\t: AMD FX(tm)-4300 Quad-Core Processor\r\nstepping\t: 0\r\nmicrocode\t: 0x600081c\r\ncpu MHz\t\t: 1400.000\r\ncache size\t: 2048 KB\r\nphysical id\t: 0\r\nsiblings\t: 4\r\ncore id\t\t: 0\r\ncpu cores\t: 2\r\napicid\t\t: 16\r\ninitial apicid\t: 0\r\nfpu\t\t: yes\r\nfpu_exception\t: yes\r\ncpuid level\t: 13\r\nwp\t\t: yes\r\nflags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf pni pclmulqdq monitor ssse3 fma cx16 **sse4_1 sse4_2** popcnt aes xsave avx f16c lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs xop skinit wdt lwp fma4 tce nodeid_msr tbm topoext perfctr_core perfctr_nb cpb hw_pstate retpoline retpoline_amd rsb_ctxsw vmmcall bmi1 arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold\r\nbugs\t\t: fxsave_leak sysret_ss_attrs null_seg spectre_v1 spectre_v2\r\nbogomips\t: 7634.15\r\nTLB size\t: 1536 4K pages\r\nclflush size\t: 64\r\ncache_alignment\t: 64\r\naddress sizes\t: 48 bits physical, 48 bits virtual\r\npower management: ts ttp tm 100mhzsteps hwpstate cpb eff_freq_ro\r\n\r\n@username_1 how do I check if CPU support dataset.getitem?\n<issue_comment>username_1: @username_2 run with `nThreads=0`\n<issue_comment>username_2: tried it with nThreads=0\r\ndifferent error...\r\nmodel [CycleGANModel] was created\r\ncreate web directory ./checkpoints/horse2zebra_cyclegan/web...\r\nIllegal instruction (core dumped)\n<issue_comment>username_2: I see something I am uneasy about:\r\nsee bold below\r\n\r\nCustomDatasetDataLoader\r\ndataset [UnalignedDataset] was created\r\n/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torchvision-0.2.0-py3.6.egg/torchvision/transforms/transforms.py:156: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\r\n#training images = 1334\r\ncycle_gan\r\n**initialization method [normal]\r\ninitialization method [normal]\r\ninitialization method [normal]\r\ninitialization method [normal]**\n<issue_comment>username_1: @username_2 No it's same error. We just wrap it in a nice way so you can see it even if it's in the workers. \r\n\r\nIsn't that expected? CycleGAN has four networks: D_X D_Y G F .\n<issue_comment>username_1: @username_2 anyways, it confirms my hypothesis that your CPU doesn't support the operation in `dataset.__getitem__`.\n<issue_comment>username_2: Oh... Can you suggest a workaround?\n<issue_comment>username_1: @username_2 You can try to put debug statements in CycleGAN code and see which one isn't supported by your CPU (probably some PIL operations), and search for replacements. Sorry I don't have much else to offer.\n<issue_comment>username_2: @username_1 \r\nThank you sir !<issue_closed>\n<issue_comment>username_1: closed due to inactivity. feel free to reopen if you can reproduce with latest stable (0.4 at the moment of writing).\n<issue_comment>username_4: Still can't get this to work.\r\n\r\nhttps://stackoverflow.com/questions/53449927/runtimeerror-dataloader-worker-is-killed-by-signal-illegal-instruction\r\n\r\nPlease help.","repo":"pytorch/pytorch","issue_id":304991946,"issue_number":5761}
{"question":"What initialization method was used in the context of cycle_gan?","answer":"The initialization method used in the context of cycle_gan was 'normal'.","id":124,"text":"I see something I am uneasy about:\r\nsee bold below\r\n\r\nCustomDatasetDataLoader\r\ndataset [UnalignedDataset] was created\r\n/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torchvision-0.2.0-py3.6.egg/torchvision/transforms/transforms.py:156: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\r\n#training images = 1334\r\ncycle_gan\r\n**initialization method [normal]\r\ninitialization method [normal]\r\ninitialization method [normal]\r\ninitialization method [normal]**","context":"<issue_start><issue_comment>Title: Problem in DataLoader (same problem with #4643, but not solved by it)\nusername_0: - OS: Ubuntu 16.04\r\n- PyTorch version: torch-0.3.1-cp36-cp36-linux_x86_64.whl\r\n- How you installed PyTorch (conda, pip, source): pip3 from the binary\r\n- Python version: 3.6\r\n- CUDA/cuDNN version: 9.1\r\n- GPU models and configuration: GTX 1070 Ti\r\n\r\nAfter install using the wheel file, I have changed the torch/utils/data/Dataloader.py according to the changes shown at https://github.com/pytorch/pytorch/pull/4643\r\n\r\nBut the same error still happen when I try the tutorial example to load image database:\r\n\r\nException ignored in: <bound method DataLoaderIter.__del__ of <torch.utils.data.dataloader.DataLoaderIter object at 0x56090c8c7348>>\r\nTraceback (most recent call last):\r\n  File \"/home/username_0/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 375, in __del__\r\n  File \"/home/username_0/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 361, in _shutdown_workers\r\n  File \"/home/username_0/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 39, in _get_from_queue\r\n  File \"/usr/pkg/lib/python3.6/multiprocessing/queues.py\", line 337, in get\r\nImportError: sys.meta_path is None, Python is likely shutting down\r\n\r\nSo could anyone help me on that? Or is there any other file I should modify to get it run?\n<issue_comment>username_0: And, I am not sure how can I use the changes in torch/csrc/DataLoader.cpp, should I clone all the source code on GitHub and build the newest binary to fully fix this my problem?\n<issue_comment>username_1: when does this error happen? is it happening when you exit python?\n<issue_comment>username_2: @username_1 and @username_3,\r\nI am running this:\r\n\r\npython train.py --dataroot ./datasets/horse2zebra --name horse2zebra_cyclegan --model cycle_gan --no_dropout --loadSize 64 --nThreads 1\r\n\r\nfrom \r\n\r\nhttps://github.com/junyanz/pytorch-CycleGAN-and-pix2pix\r\n\r\nThe model is succefully created:\r\nmodel [CycleGANModel] was created\r\n\r\nI am on ubuntu 16.04, pytorch 3.1, cuda 9.0 and cuDnn 7.1\r\n\r\nThe error I get is traced below:\r\ncreate web directory ./checkpoints/horse2zebra_cyclegan/web...\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 22, in <module>\r\n    for i, data in enumerate(dataset):\r\n  File \"/home/sam/Documents/ComputerVision/pytorch-CycleGAN-and-pix2pix/data/custom_dataset_data_loader.py\", line 44, in __iter__\r\n    for i, data in enumerate(self.dataloader):\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 275, in __next__\r\n    idx, batch = self._get_batch()\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 254, in _get_batch\r\n    return self.data_queue.get()\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/queues.py\", line 335, in get\r\n    res = self._reader.recv_bytes()\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\r\n    buf = self._recv_bytes(maxlength)\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\r\n    buf = self._recv(4)\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\r\n    chunk = read(handle, remaining)\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 175, in handler\r\n    _error_if_any_worker_fails()\r\nRuntimeError: DataLoader worker (pid 3792) is killed by signal: Illegal instruction.\n<issue_comment>username_3: @username_2 that error is because you are running it on a computer that is too old and does not have SSE4 instruction on CPU.\n<issue_comment>username_1: @username_2 Also, it is more likely due to that your CPU doesn't support the operations done in the dataset.__getitem__.\n<issue_comment>username_2: @username_1 & @username_3,\r\n\r\nSome months ago I made it work in windows10 (I think)\r\nThe my windows machine crashed. So here I am. Sorry\r\n\r\nHere is what I see in cat /proc/cpuinfo:\r\n\r\nprocessor\t: 0\r\nvendor_id\t: AuthenticAMD\r\ncpu family\t: 21\r\nmodel\t\t: 2\r\nmodel name\t: AMD FX(tm)-4300 Quad-Core Processor\r\nstepping\t: 0\r\nmicrocode\t: 0x600081c\r\ncpu MHz\t\t: 1400.000\r\ncache size\t: 2048 KB\r\nphysical id\t: 0\r\nsiblings\t: 4\r\ncore id\t\t: 0\r\ncpu cores\t: 2\r\napicid\t\t: 16\r\ninitial apicid\t: 0\r\nfpu\t\t: yes\r\nfpu_exception\t: yes\r\ncpuid level\t: 13\r\nwp\t\t: yes\r\nflags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf pni pclmulqdq monitor ssse3 fma cx16 **sse4_1 sse4_2** popcnt aes xsave avx f16c lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs xop skinit wdt lwp fma4 tce nodeid_msr tbm topoext perfctr_core perfctr_nb cpb hw_pstate retpoline retpoline_amd rsb_ctxsw vmmcall bmi1 arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold\r\nbugs\t\t: fxsave_leak sysret_ss_attrs null_seg spectre_v1 spectre_v2\r\nbogomips\t: 7634.15\r\nTLB size\t: 1536 4K pages\r\nclflush size\t: 64\r\ncache_alignment\t: 64\r\naddress sizes\t: 48 bits physical, 48 bits virtual\r\npower management: ts ttp tm 100mhzsteps hwpstate cpb eff_freq_ro\r\n\r\n@username_1 how do I check if CPU support dataset.getitem?\n<issue_comment>username_1: @username_2 run with `nThreads=0`\n<issue_comment>username_2: tried it with nThreads=0\r\ndifferent error...\r\nmodel [CycleGANModel] was created\r\ncreate web directory ./checkpoints/horse2zebra_cyclegan/web...\r\nIllegal instruction (core dumped)\n<issue_comment>username_2: I see something I am uneasy about:\r\nsee bold below\r\n\r\nCustomDatasetDataLoader\r\ndataset [UnalignedDataset] was created\r\n/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torchvision-0.2.0-py3.6.egg/torchvision/transforms/transforms.py:156: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\r\n#training images = 1334\r\ncycle_gan\r\n**initialization method [normal]\r\ninitialization method [normal]\r\ninitialization method [normal]\r\ninitialization method [normal]**\n<issue_comment>username_1: @username_2 No it's same error. We just wrap it in a nice way so you can see it even if it's in the workers. \r\n\r\nIsn't that expected? CycleGAN has four networks: D_X D_Y G F .\n<issue_comment>username_1: @username_2 anyways, it confirms my hypothesis that your CPU doesn't support the operation in `dataset.__getitem__`.\n<issue_comment>username_2: Oh... Can you suggest a workaround?\n<issue_comment>username_1: @username_2 You can try to put debug statements in CycleGAN code and see which one isn't supported by your CPU (probably some PIL operations), and search for replacements. Sorry I don't have much else to offer.\n<issue_comment>username_2: @username_1 \r\nThank you sir !<issue_closed>\n<issue_comment>username_1: closed due to inactivity. feel free to reopen if you can reproduce with latest stable (0.4 at the moment of writing).\n<issue_comment>username_4: Still can't get this to work.\r\n\r\nhttps://stackoverflow.com/questions/53449927/runtimeerror-dataloader-worker-is-killed-by-signal-illegal-instruction\r\n\r\nPlease help.","repo":"pytorch/pytorch","issue_id":304991946,"issue_number":5761}
{"question":"How many networks are involved in CycleGAN?","answer":"CycleGAN involves four networks: D_X, D_Y, G, and F.","id":125,"text":"@username_2 No it's same error. We just wrap it in a nice way so you can see it even if it's in the workers. \r\n\r\nIsn't that expected? CycleGAN has four networks: D_X D_Y G F .","context":"<issue_start><issue_comment>Title: Problem in DataLoader (same problem with #4643, but not solved by it)\nusername_0: - OS: Ubuntu 16.04\r\n- PyTorch version: torch-0.3.1-cp36-cp36-linux_x86_64.whl\r\n- How you installed PyTorch (conda, pip, source): pip3 from the binary\r\n- Python version: 3.6\r\n- CUDA/cuDNN version: 9.1\r\n- GPU models and configuration: GTX 1070 Ti\r\n\r\nAfter install using the wheel file, I have changed the torch/utils/data/Dataloader.py according to the changes shown at https://github.com/pytorch/pytorch/pull/4643\r\n\r\nBut the same error still happen when I try the tutorial example to load image database:\r\n\r\nException ignored in: <bound method DataLoaderIter.__del__ of <torch.utils.data.dataloader.DataLoaderIter object at 0x56090c8c7348>>\r\nTraceback (most recent call last):\r\n  File \"/home/username_0/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 375, in __del__\r\n  File \"/home/username_0/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 361, in _shutdown_workers\r\n  File \"/home/username_0/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 39, in _get_from_queue\r\n  File \"/usr/pkg/lib/python3.6/multiprocessing/queues.py\", line 337, in get\r\nImportError: sys.meta_path is None, Python is likely shutting down\r\n\r\nSo could anyone help me on that? Or is there any other file I should modify to get it run?\n<issue_comment>username_0: And, I am not sure how can I use the changes in torch/csrc/DataLoader.cpp, should I clone all the source code on GitHub and build the newest binary to fully fix this my problem?\n<issue_comment>username_1: when does this error happen? is it happening when you exit python?\n<issue_comment>username_2: @username_1 and @username_3,\r\nI am running this:\r\n\r\npython train.py --dataroot ./datasets/horse2zebra --name horse2zebra_cyclegan --model cycle_gan --no_dropout --loadSize 64 --nThreads 1\r\n\r\nfrom \r\n\r\nhttps://github.com/junyanz/pytorch-CycleGAN-and-pix2pix\r\n\r\nThe model is succefully created:\r\nmodel [CycleGANModel] was created\r\n\r\nI am on ubuntu 16.04, pytorch 3.1, cuda 9.0 and cuDnn 7.1\r\n\r\nThe error I get is traced below:\r\ncreate web directory ./checkpoints/horse2zebra_cyclegan/web...\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 22, in <module>\r\n    for i, data in enumerate(dataset):\r\n  File \"/home/sam/Documents/ComputerVision/pytorch-CycleGAN-and-pix2pix/data/custom_dataset_data_loader.py\", line 44, in __iter__\r\n    for i, data in enumerate(self.dataloader):\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 275, in __next__\r\n    idx, batch = self._get_batch()\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 254, in _get_batch\r\n    return self.data_queue.get()\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/queues.py\", line 335, in get\r\n    res = self._reader.recv_bytes()\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\r\n    buf = self._recv_bytes(maxlength)\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\r\n    buf = self._recv(4)\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\r\n    chunk = read(handle, remaining)\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 175, in handler\r\n    _error_if_any_worker_fails()\r\nRuntimeError: DataLoader worker (pid 3792) is killed by signal: Illegal instruction.\n<issue_comment>username_3: @username_2 that error is because you are running it on a computer that is too old and does not have SSE4 instruction on CPU.\n<issue_comment>username_1: @username_2 Also, it is more likely due to that your CPU doesn't support the operations done in the dataset.__getitem__.\n<issue_comment>username_2: @username_1 & @username_3,\r\n\r\nSome months ago I made it work in windows10 (I think)\r\nThe my windows machine crashed. So here I am. Sorry\r\n\r\nHere is what I see in cat /proc/cpuinfo:\r\n\r\nprocessor\t: 0\r\nvendor_id\t: AuthenticAMD\r\ncpu family\t: 21\r\nmodel\t\t: 2\r\nmodel name\t: AMD FX(tm)-4300 Quad-Core Processor\r\nstepping\t: 0\r\nmicrocode\t: 0x600081c\r\ncpu MHz\t\t: 1400.000\r\ncache size\t: 2048 KB\r\nphysical id\t: 0\r\nsiblings\t: 4\r\ncore id\t\t: 0\r\ncpu cores\t: 2\r\napicid\t\t: 16\r\ninitial apicid\t: 0\r\nfpu\t\t: yes\r\nfpu_exception\t: yes\r\ncpuid level\t: 13\r\nwp\t\t: yes\r\nflags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf pni pclmulqdq monitor ssse3 fma cx16 **sse4_1 sse4_2** popcnt aes xsave avx f16c lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs xop skinit wdt lwp fma4 tce nodeid_msr tbm topoext perfctr_core perfctr_nb cpb hw_pstate retpoline retpoline_amd rsb_ctxsw vmmcall bmi1 arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold\r\nbugs\t\t: fxsave_leak sysret_ss_attrs null_seg spectre_v1 spectre_v2\r\nbogomips\t: 7634.15\r\nTLB size\t: 1536 4K pages\r\nclflush size\t: 64\r\ncache_alignment\t: 64\r\naddress sizes\t: 48 bits physical, 48 bits virtual\r\npower management: ts ttp tm 100mhzsteps hwpstate cpb eff_freq_ro\r\n\r\n@username_1 how do I check if CPU support dataset.getitem?\n<issue_comment>username_1: @username_2 run with `nThreads=0`\n<issue_comment>username_2: tried it with nThreads=0\r\ndifferent error...\r\nmodel [CycleGANModel] was created\r\ncreate web directory ./checkpoints/horse2zebra_cyclegan/web...\r\nIllegal instruction (core dumped)\n<issue_comment>username_2: I see something I am uneasy about:\r\nsee bold below\r\n\r\nCustomDatasetDataLoader\r\ndataset [UnalignedDataset] was created\r\n/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torchvision-0.2.0-py3.6.egg/torchvision/transforms/transforms.py:156: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\r\n#training images = 1334\r\ncycle_gan\r\n**initialization method [normal]\r\ninitialization method [normal]\r\ninitialization method [normal]\r\ninitialization method [normal]**\n<issue_comment>username_1: @username_2 No it's same error. We just wrap it in a nice way so you can see it even if it's in the workers. \r\n\r\nIsn't that expected? CycleGAN has four networks: D_X D_Y G F .\n<issue_comment>username_1: @username_2 anyways, it confirms my hypothesis that your CPU doesn't support the operation in `dataset.__getitem__`.\n<issue_comment>username_2: Oh... Can you suggest a workaround?\n<issue_comment>username_1: @username_2 You can try to put debug statements in CycleGAN code and see which one isn't supported by your CPU (probably some PIL operations), and search for replacements. Sorry I don't have much else to offer.\n<issue_comment>username_2: @username_1 \r\nThank you sir !<issue_closed>\n<issue_comment>username_1: closed due to inactivity. feel free to reopen if you can reproduce with latest stable (0.4 at the moment of writing).\n<issue_comment>username_4: Still can't get this to work.\r\n\r\nhttps://stackoverflow.com/questions/53449927/runtimeerror-dataloader-worker-is-killed-by-signal-illegal-instruction\r\n\r\nPlease help.","repo":"pytorch/pytorch","issue_id":304991946,"issue_number":5761}
{"question":"What is the specific error message encountered during the project generation process?","answer":"The error message is: fatal error LNK1136:  [E:\\SOFTWARE\\pytorch\\build\\caffe2\\caffe2_gpu.vcxproj].","id":132,"text":"E:\\SOFTWARE\\pytorch\\build\\caffe2\\torch\\lib\\libshm_windows\\shm.vcxproj()\r\n\r\nE:\\SOFTWARE\\pytorch\\build\\ALL_BUILD.vcxproj() - \r\n\r\nE:\\SOFTWARE\\pytorch\\build\\INSTALL.vcxproj() - \r\n\r\n\r\n\r\n\r\nE:\\SOFTWARE\\pytorch\\build\\INSTALL.vcxproj() (1) ->\r\nE:\\SOFTWARE\\pytorch\\build\\ALL_BUILD.vcxproj() (3) ->\r\nE:\\SOFTWARE\\pytorch\\build\\caffe2\\AlgorithmsTest.vcxproj() (4) ->\r\nE:\\SOFTWARE\\pytorch\\build\\caffe2\\caffe2_gpu.vcxproj() (22) ->\r\n(Link ) ->\r\n  C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\lib\\x64.obj : fatal error LNK1136:  [E:\\SOFTWARE\\pyt\r\norch\\build\\caffe2\\caffe2_gpu.vcxproj]\r\n\r\n    0 \r\n    1 \r\n\r\n 00:00:16.23\r\n\r\nE:\\SOFTWARE\\pytorch\\build>IF ERRORLEVEL 1 exit 1\r\nFailed to run 'tools\\build_pytorch_libs.bat --use-cuda --use-fbgemm --use-nnpack --use-mkldnn --use-qnnpack caffe2'\r\n\r\npytorch versionstable1.0\r\nosWindows 10 Pro Version 1803\r\npython version:3.7\r\nCUDA:10\r\ncudnn:7.4.1\r\nVisual Studio version: Microsoft Visual Studio 2017 Community Edition\r\n\r\nCan anyone help me solve this problem? Thanks !","context":"<issue_start><issue_comment>Title: Can't open x64.obj while installing pytorch with win10 \nusername_0: E:\\SOFTWARE\\pytorch\\build\\caffe2\\torch\\lib\\libshm_windows\\shm.vcxproj()\r\n\r\nE:\\SOFTWARE\\pytorch\\build\\ALL_BUILD.vcxproj() - \r\n\r\nE:\\SOFTWARE\\pytorch\\build\\INSTALL.vcxproj() - \r\n\r\n\r\n\r\n\r\nE:\\SOFTWARE\\pytorch\\build\\INSTALL.vcxproj() (1) ->\r\nE:\\SOFTWARE\\pytorch\\build\\ALL_BUILD.vcxproj() (3) ->\r\nE:\\SOFTWARE\\pytorch\\build\\caffe2\\AlgorithmsTest.vcxproj() (4) ->\r\nE:\\SOFTWARE\\pytorch\\build\\caffe2\\caffe2_gpu.vcxproj() (22) ->\r\n(Link ) ->\r\n  C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\lib\\x64.obj : fatal error LNK1136:  [E:\\SOFTWARE\\pyt\r\norch\\build\\caffe2\\caffe2_gpu.vcxproj]\r\n\r\n    0 \r\n    1 \r\n\r\n 00:00:16.23\r\n\r\nE:\\SOFTWARE\\pytorch\\build>IF ERRORLEVEL 1 exit 1\r\nFailed to run 'tools\\build_pytorch_libs.bat --use-cuda --use-fbgemm --use-nnpack --use-mkldnn --use-qnnpack caffe2'\r\n\r\npytorch versionstable1.0\r\nosWindows 10 Pro Version 1803\r\npython version:3.7\r\nCUDA:10\r\ncudnn:7.4.1\r\nVisual Studio version: Microsoft Visual Studio 2017 Community Edition\r\n\r\nCan anyone help me solve this problem? Thanks !\n<issue_comment>username_1: Please provide the full log.\n<issue_comment>username_0: the full log\r\n\r\nBuilding wheel torch-1.0.0a0+df61437\r\nrunning install\r\nsetup.py::run()\r\nrunning build_deps\r\nsetup.py::build_deps::run()\r\n\r\nE:\\SOFTWARE\\pytorch>cd \"E:\\SOFTWARE\\pytorch\\tools\\\\..\"\r\n\r\nE:\\SOFTWARE\\pytorch>set PATH=\\bin;E:\\SOFTWARE\\Anaconda\\DLLs;E:\\SOFTWARE\\Visual Studio\\VC\\Tools\\MSVC\\14.11.25503\\bin\\HostX64\\x64;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\VC\\VCPackages;C:\\Program Files (x86)\\Microsoft SDKs\\TypeScript\\3.1;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\TestWindow;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\TeamFoundation\\Team Explorer;E:\\SOFTWARE\\Visual Studio\\MSBuild\\15.0\\bin\\Roslyn;E:\\SOFTWARE\\Visual Studio\\Team Tools\\Performance Tools\\x64;E:\\SOFTWARE\\Visual Studio\\Team Tools\\Performance Tools;E:\\SOFTWARE\\Visual Studio\\share\\Common\\VSPerfCollectionTools\\\\x64;E:\\SOFTWARE\\Visual Studio\\share\\Common\\VSPerfCollectionTools\\;C:\\Program Files (x86)\\Microsoft SDKs\\Windows\\v10.0A\\bin\\NETFX 4.6.1 Tools\\x64\\;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\10.0.17763.0\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x64;E:\\SOFTWARE\\Visual Studio\\\\MSBuild\\15.0\\bin;C:\\WINDOWS\\Microsoft.NET\\Framework64\\v4.0.30319;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\;E:\\SOFTWARE\\Visual Studio\\Common7\\Tools\\;E:\\SOFTWARE\\Anaconda;E:\\SOFTWARE\\Anaconda\\Library\\mingw-w64\\bin;E:\\SOFTWARE\\Anaconda\\Library\\usr\\bin;E:\\SOFTWARE\\Anaconda\\Library\\bin;E:\\SOFTWARE\\Anaconda\\Scripts;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\bin;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\libnvvp;C:\\WINDOWS\\system32;C:\\WINDOWS;C:\\WINDOWS\\System32\\Wbem;C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0\\;C:\\WINDOWS\\System32\\OpenSSH\\;E:\\\\Git\\cmd;E:\\SOFTWARE\\cmake\\bin;C:\\Program Files (x86)\\Windows Kits\\8.1\\Windows Performance Toolkit\\;C:\\Program Files (x86)\\NVIDIA Corporation\\PhysX\\Common;C:\\Program Files\\dotnet\\;C:\\Program Files\\NVIDIA Corporation\\NVIDIA NvDLISR;C:\\Windows\\Microsoft.NET\\Framework\\v4.0.30319;C:\\Windows\\Microsoft.NET\\Framework64\\v4.0.30319;C:\\Users\\11838\\AppData\\Local\\Microsoft\\WindowsApps;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\CMake\\bin;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\Ninja\r\n\r\nE:\\SOFTWARE\\pytorch>set BASE_DIR=E:/SOFTWARE/pytorch\r\n\r\nE:\\SOFTWARE\\pytorch>set TORCH_LIB_DIR=E:/SOFTWARE/pytorch/torch/lib\r\n\r\nE:\\SOFTWARE\\pytorch>set THIRD_PARTY_DIR=E:/SOFTWARE/pytorch/third_party\r\n\r\nE:\\SOFTWARE\\pytorch>set BASIC_C_FLAGS=\r\n\r\nE:\\SOFTWARE\\pytorch>set BASIC_CUDA_FLAGS=\r\n\r\nE:\\SOFTWARE\\pytorch>IF NOT DEFINED INSTALL_DIR (set \"INSTALL_DIR=E:/SOFTWARE/pytorch/torch/lib/tmp_install\" )  ELSE (set \"INSTALL_DIR=\\=/\" )\r\n\r\nE:\\SOFTWARE\\pytorch>set LDFLAGS=/LIBPATH:E:/SOFTWARE/pytorch/torch/lib/tmp_install/lib\r\n\r\nE:\\SOFTWARE\\pytorch>set C_FLAGS= /D_WIN32 /Z7 /EHa /DNOMINMAX\r\n\r\nE:\\SOFTWARE\\pytorch>set LINK_FLAGS=/DEBUG:FULL\r\n\r\nE:\\SOFTWARE\\pytorch>if not exist torch\\lib\\tmp_install mkdir torch\\lib\\tmp_install\r\n\r\nE:\\SOFTWARE\\pytorch>set /a USE_CUDA=0\r\n\r\nE:\\SOFTWARE\\pytorch>set /a USE_FBGEMM=1\r\n\r\nE:\\SOFTWARE\\pytorch>set /a USE_ROCM=0\r\n\r\nE:\\SOFTWARE\\pytorch>set /a USE_NNPACK=0\r\n\r\nE:\\SOFTWARE\\pytorch>set /a USE_QNNPACK=0\r\n\r\nE:\\SOFTWARE\\pytorch>set /a USE_GLOO_IBVERBS=0\r\n\r\nE:\\SOFTWARE\\pytorch>set /a USE_MKLDNN=0\r\n\r\nE:\\SOFTWARE\\pytorch>set _BUILD_ARGS=\r\n\r\nE:\\SOFTWARE\\pytorch>if \"--use-cuda\" == \"\" (goto :process_args_exit )\r\n\r\nE:\\SOFTWARE\\pytorch>if \"--use-cuda\" == \"--use-cuda\" (\r\nset /a USE_CUDA=1\r\n goto :process_args_processed\r\n)\r\n\r\nE:\\SOFTWARE\\pytorch>shift\r\n\r\nE:\\SOFTWARE\\pytorch>goto :process_args\r\n\r\nE:\\SOFTWARE\\pytorch>if \"--use-fbgemm\" == \"\" (goto :process_args_exit )\r\n\r\nE:\\SOFTWARE\\pytorch>if \"--use-fbgemm\" == \"--use-cuda\" (\r\nset /a USE_CUDA=1\r\n goto :process_args_processed\r\n)\r\n\r\nE:\\SOFTWARE\\pytorch>if \"--use-fbgemm\" == \"--use-fbgemm\" (\r\nset /a USE_FBGEMM=1\r\n goto :process_args_processed\r\n)\r\n\r\nE:\\SOFTWARE\\pytorch>shift\r\n\r\nE:\\SOFTWARE\\pytorch>goto :process_args\r\n\r\nE:\\SOFTWARE\\pytorch>if \"--use-nnpack\" == \"\" (goto :process_args_exit )\r\n\r\nE:\\SOFTWARE\\pytorch>if \"--use-nnpack\" == \"--use-cuda\" (\r\nset /a USE_CUDA=1\r\n goto :process_args_processed\r\n[Truncated]\nDone Building Project \"E:\\SOFTWARE\\pytorch\\build\\INSTALL.vcxproj\" (default targets) -- FAILED.\r\n\r\n\r\nBuild FAILED.\r\n\r\n\"E:\\SOFTWARE\\pytorch\\build\\INSTALL.vcxproj\" (default target) (1) ->\r\n\"E:\\SOFTWARE\\pytorch\\build\\ALL_BUILD.vcxproj\" (default target) (3) ->\r\n\"E:\\SOFTWARE\\pytorch\\build\\caffe2\\AlgorithmsTest.vcxproj\" (default target) (4) ->\r\n\"E:\\SOFTWARE\\pytorch\\build\\caffe2\\caffe2_gpu.vcxproj\" (default target) (22) ->\r\n(Link target) ->\r\n  LINK : fatal error LNK1181: C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\lib\\x64.obj [E:\\SOFTWA\r\nRE\\pytorch\\build\\caffe2\\caffe2_gpu.vcxproj]\r\n\r\n    0 Warning(s)\r\n    1 Error(s)\r\n\r\nTime Elapsed 00:00:22.22\r\n\r\nE:\\SOFTWARE\\pytorch\\build>IF ERRORLEVEL 1 exit 1\r\nFailed to run 'tools\\build_pytorch_libs.bat --use-cuda --use-fbgemm --use-nnpack --use-mkldnn --use-qnnpack caffe2'\n<issue_comment>username_1: The error is caused by the misconfiguration of the flag `CUDNN_LIBRARY`. Could you please run `tools/setup_helpers/cudnn.py` and see which code path it goes through?\n<issue_comment>username_0: My environmental variables is :\r\nCUDA_PATH=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\r\nCUDA_PATH_V10_0=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\r\nCUDNN_INCLUDE_DIR=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\include\r\nCUDNN_LIBRARY=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\lib\\x64\r\nNVCUDASAMPLES10_0_ROOT=C:\\ProgramData\\NVIDIA Corporation\\CUDA Samples\\v10.0\r\nNVCUDASAMPLES_ROOT=C:\\ProgramData\\NVIDIA Corporation\\CUDA Samples\\v10.0\r\nNVTOOLSEXT_PATH=C:\\Program Files\\NVIDIA Corporation\\NvToolsExt\\\r\nPath=E:\\SOFTWARE\\Visual Studio\\VC\\Tools\\MSVC\\14.11.25503\\bin\\HostX64\\x64;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\VC\\VCPackages;C:\\Program Files (x86)\\Microsoft SDKs\\TypeScript\\3.1;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\TestWindow;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\TeamFoundation\\Team Explorer;E:\\SOFTWARE\\Visual Studio\\MSBuild\\15.0\\bin\\Roslyn;E:\\SOFTWARE\\Visual Studio\\Team Tools\\Performance Tools\\x64;E:\\SOFTWARE\\Visual Studio\\Team Tools\\Performance Tools;E:\\SOFTWARE\\Visual Studio\\share\\Common\\VSPerfCollectionTools\\\\x64;E:\\SOFTWARE\\Visual Studio\\share\\Common\\VSPerfCollectionTools\\;C:\\Program Files (x86)\\Microsoft SDKs\\Windows\\v10.0A\\bin\\NETFX 4.6.1 Tools\\x64\\;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\10.0.17763.0\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x64;E:\\SOFTWARE\\Visual Studio\\\\MSBuild\\15.0\\bin;C:\\WINDOWS\\Microsoft.NET\\Framework64\\v4.0.30319;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\;E:\\SOFTWARE\\Visual Studio\\Common7\\Tools\\;E:\\SOFTWARE\\Anaconda;E:\\SOFTWARE\\Anaconda\\Library\\mingw-w64\\bin;E:\\SOFTWARE\\Anaconda\\Library\\usr\\bin;E:\\SOFTWARE\\Anaconda\\Library\\bin;E:\\SOFTWARE\\Anaconda\\Scripts;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\bin;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\libnvvp;C:\\WINDOWS\\system32;C:\\WINDOWS;C:\\WINDOWS\\System32\\Wbem;C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0\\;C:\\WINDOWS\\System32\\OpenSSH\\;E:\\\\Git\\cmd;E:\\SOFTWARE\\cmake\\bin;C:\\Program Files (x86)\\Windows Kits\\8.1\\Windows Performance Toolkit\\;C:\\Program Files (x86)\\NVIDIA Corporation\\PhysX\\Common;C:\\Program Files\\dotnet\\;C:\\Program Files\\NVIDIA Corporation\\NVIDIA NvDLISR;C:\\Windows\\Microsoft.NET\\Framework\\v4.0.30319;C:\\Windows\\Microsoft.NET\\Framework64\\v4.0.30319;C:\\Users\\11838\\AppData\\Local\\Microsoft\\WindowsApps;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\CMake\\bin;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\Ninja\r\n\r\nAND the cudnn.py shows:\r\nimport os\r\nimport glob\r\n\r\nfrom .env import IS_WINDOWS, IS_CONDA, CONDA_DIR, check_negative_env_flag, gather_paths, lib_paths_from_base\r\nfrom .cuda import USE_CUDA, CUDA_HOME\r\n\r\n\r\nUSE_CUDNN = False\r\nCUDNN_LIB_DIR = None\r\nCUDNN_INCLUDE_DIR = None\r\nCUDNN_LIBRARY = None\r\nWITH_STATIC_CUDNN = os.getenv(\"USE_STATIC_CUDNN\")\r\n\r\nif USE_CUDA and not check_negative_env_flag('USE_CUDNN'):\r\n    lib_paths = list(filter(bool, [\r\n        os.getenv('CUDNN_LIB_DIR')\r\n    ] + lib_paths_from_base(CUDA_HOME) + [\r\n        '/usr/lib/x86_64-linux-gnu/',\r\n        '/usr/lib/powerpc64le-linux-gnu/',\r\n        '/usr/lib/aarch64-linux-gnu/',\r\n    ] + gather_paths([\r\n        'LIBRARY_PATH',\r\n    ]) + gather_paths([\r\n        'LD_LIBRARY_PATH',\r\n    ])))\r\n    include_paths = list(filter(bool, [\r\n        os.getenv('CUDNN_INCLUDE_DIR'),\r\n        os.path.join(CUDA_HOME, 'include'),\r\n        '/usr/include/',\r\n    ] + gather_paths([\r\n        'CPATH',\r\n        'C_INCLUDE_PATH',\r\n        'CPLUS_INCLUDE_PATH',\r\n    ])))\r\n    # Add CUDA related dirs to candidate list\r\n    if IS_CONDA:\r\n        lib_paths.append(os.path.join(CONDA_DIR, 'lib'))\r\n        include_paths.append(os.path.join(CONDA_DIR, 'include'))\r\n    for path in include_paths:\r\n        if path is None or not os.path.exists(path):\r\n            continue\r\n        include_file_path = os.path.join(path, 'cudnn.h')\r\n        CUDNN_INCLUDE_VERSION = None\r\n        if os.path.exists(include_file_path):\r\n            CUDNN_INCLUDE_DIR = path\r\n            with open(include_file_path) as f:\r\n                for line in f:\r\n                    if \"#define CUDNN_MAJOR\" in line:\r\n                        CUDNN_INCLUDE_VERSION = int(line.split()[-1])\r\n                        break\r\n            if CUDNN_INCLUDE_VERSION is None:\r\n                raise AssertionError(\"Could not find #define CUDNN_MAJOR in \" + include_file_path)\r\n            break\r\n\r\n    if CUDNN_INCLUDE_VERSION is None:\r\n        pass\r\n\r\n    # Check for standalone cuDNN libraries\r\n    if CUDNN_INCLUDE_DIR is not None:\r\n        cudnn_path = os.path.join(os.path.dirname(CUDNN_INCLUDE_DIR))\r\n        cudnn_lib_paths = lib_paths_from_base(cudnn_path)\r\n        lib_paths.extend(cudnn_lib_paths)\r\n\r\n    for path in lib_paths:\r\n        if path is None or not os.path.exists(path):\r\n            continue\r\n        if IS_WINDOWS:\r\n            library = os.path.join(path, 'cudnn.lib')\r\n            if os.path.exists(library):\r\n[Truncated]\n                search_name = 'libcudnn*' + str(CUDNN_INCLUDE_VERSION) + \"*\"\r\n            libraries = sorted(glob.glob(os.path.join(path, search_name)))\r\n            if libraries:\r\n                CUDNN_LIBRARY = libraries[0]\r\n                CUDNN_LIB_DIR = path\r\n                break\r\n    # Specifying the library directly will overwrite the lib directory\r\n    library = os.getenv('CUDNN_LIBRARY')\r\n    if library is not None and os.path.exists(library):\r\n        CUDNN_LIBRARY = library\r\n        CUDNN_LIB_DIR = os.path.dirname(CUDNN_LIBRARY)\r\n\r\n    if not all([CUDNN_LIBRARY, CUDNN_LIB_DIR, CUDNN_INCLUDE_DIR]):\r\n        CUDNN_LIBRARY = CUDNN_LIB_DIR = CUDNN_INCLUDE_DIR = None\r\n    else:\r\n        real_cudnn_library = os.path.realpath(CUDNN_LIBRARY)\r\n        real_cudnn_lib_dir = os.path.realpath(CUDNN_LIB_DIR)\r\n        assert os.path.dirname(real_cudnn_library) == real_cudnn_lib_dir, (\r\n            'cudnn library and lib_dir must agree')\r\n        USE_CUDNN = True\n<issue_comment>username_1: Please remove `CUDNN_INCLUDE_DIR` and `CUDNN_LIBRARY` by using the following commands.\r\n```cmd\r\nset CUDNN_INCLUDE_DIR=\r\nset CUDNN_LIBRARY=\r\n```\n<issue_comment>username_0: It works! Thanks a lot!<issue_closed>","repo":"pytorch/pytorch","issue_id":391131022,"issue_number":15222}
{"question":"What was the fatal error encountered during the build process in the log provided?","answer":"The fatal error encountered during the build process in the log provided was 'LNK1181: Cannot open input file \"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\lib\\x64.obj\".'","id":134,"text":"the full log\r\n\r\nBuilding wheel torch-1.0.0a0+df61437\r\nrunning install\r\nsetup.py::run()\r\nrunning build_deps\r\nsetup.py::build_deps::run()\r\n\r\nE:\\SOFTWARE\\pytorch>cd \"E:\\SOFTWARE\\pytorch\\tools\\\\..\"\r\n\r\nE:\\SOFTWARE\\pytorch>set PATH=\\bin;E:\\SOFTWARE\\Anaconda\\DLLs;E:\\SOFTWARE\\Visual Studio\\VC\\Tools\\MSVC\\14.11.25503\\bin\\HostX64\\x64;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\VC\\VCPackages;C:\\Program Files (x86)\\Microsoft SDKs\\TypeScript\\3.1;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\TestWindow;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\TeamFoundation\\Team Explorer;E:\\SOFTWARE\\Visual Studio\\MSBuild\\15.0\\bin\\Roslyn;E:\\SOFTWARE\\Visual Studio\\Team Tools\\Performance Tools\\x64;E:\\SOFTWARE\\Visual Studio\\Team Tools\\Performance Tools;E:\\SOFTWARE\\Visual Studio\\share\\Common\\VSPerfCollectionTools\\\\x64;E:\\SOFTWARE\\Visual Studio\\share\\Common\\VSPerfCollectionTools\\;C:\\Program Files (x86)\\Microsoft SDKs\\Windows\\v10.0A\\bin\\NETFX 4.6.1 Tools\\x64\\;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\10.0.17763.0\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x64;E:\\SOFTWARE\\Visual Studio\\\\MSBuild\\15.0\\bin;C:\\WINDOWS\\Microsoft.NET\\Framework64\\v4.0.30319;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\;E:\\SOFTWARE\\Visual Studio\\Common7\\Tools\\;E:\\SOFTWARE\\Anaconda;E:\\SOFTWARE\\Anaconda\\Library\\mingw-w64\\bin;E:\\SOFTWARE\\Anaconda\\Library\\usr\\bin;E:\\SOFTWARE\\Anaconda\\Library\\bin;E:\\SOFTWARE\\Anaconda\\Scripts;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\bin;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\libnvvp;C:\\WINDOWS\\system32;C:\\WINDOWS;C:\\WINDOWS\\System32\\Wbem;C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0\\;C:\\WINDOWS\\System32\\OpenSSH\\;E:\\\\Git\\cmd;E:\\SOFTWARE\\cmake\\bin;C:\\Program Files (x86)\\Windows Kits\\8.1\\Windows Performance Toolkit\\;C:\\Program Files (x86)\\NVIDIA Corporation\\PhysX\\Common;C:\\Program Files\\dotnet\\;C:\\Program Files\\NVIDIA Corporation\\NVIDIA NvDLISR;C:\\Windows\\Microsoft.NET\\Framework\\v4.0.30319;C:\\Windows\\Microsoft.NET\\Framework64\\v4.0.30319;C:\\Users\\11838\\AppData\\Local\\Microsoft\\WindowsApps;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\CMake\\bin;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\Ninja\r\n\r\nE:\\SOFTWARE\\pytorch>set BASE_DIR=E:/SOFTWARE/pytorch\r\n\r\nE:\\SOFTWARE\\pytorch>set TORCH_LIB_DIR=E:/SOFTWARE/pytorch/torch/lib\r\n\r\nE:\\SOFTWARE\\pytorch>set THIRD_PARTY_DIR=E:/SOFTWARE/pytorch/third_party\r\n\r\nE:\\SOFTWARE\\pytorch>set BASIC_C_FLAGS=\r\n\r\nE:\\SOFTWARE\\pytorch>set BASIC_CUDA_FLAGS=\r\n\r\nE:\\SOFTWARE\\pytorch>IF NOT DEFINED INSTALL_DIR (set \"INSTALL_DIR=E:/SOFTWARE/pytorch/torch/lib/tmp_install\" )  ELSE (set \"INSTALL_DIR=\\=/\" )\r\n\r\nE:\\SOFTWARE\\pytorch>set LDFLAGS=/LIBPATH:E:/SOFTWARE/pytorch/torch/lib/tmp_install/lib\r\n\r\nE:\\SOFTWARE\\pytorch>set C_FLAGS= /D_WIN32 /Z7 /EHa /DNOMINMAX\r\n\r\nE:\\SOFTWARE\\pytorch>set LINK_FLAGS=/DEBUG:FULL\r\n\r\nE:\\SOFTWARE\\pytorch>if not exist torch\\lib\\tmp_install mkdir torch\\lib\\tmp_install\r\n\r\nE:\\SOFTWARE\\pytorch>set /a USE_CUDA=0\r\n\r\nE:\\SOFTWARE\\pytorch>set /a USE_FBGEMM=1\r\n\r\nE:\\SOFTWARE\\pytorch>set /a USE_ROCM=0\r\n\r\nE:\\SOFTWARE\\pytorch>set /a USE_NNPACK=0\r\n\r\nE:\\SOFTWARE\\pytorch>set /a USE_QNNPACK=0\r\n\r\nE:\\SOFTWARE\\pytorch>set /a USE_GLOO_IBVERBS=0\r\n\r\nE:\\SOFTWARE\\pytorch>set /a USE_MKLDNN=0\r\n\r\nE:\\SOFTWARE\\pytorch>set _BUILD_ARGS=\r\n\r\nE:\\SOFTWARE\\pytorch>if \"--use-cuda\" == \"\" (goto :process_args_exit )\r\n\r\nE:\\SOFTWARE\\pytorch>if \"--use-cuda\" == \"--use-cuda\" (\r\nset /a USE_CUDA=1\r\n goto :process_args_processed\r\n)\r\n\r\nE:\\SOFTWARE\\pytorch>shift\r\n\r\nE:\\SOFTWARE\\pytorch>goto :process_args\r\n\r\nE:\\SOFTWARE\\pytorch>if \"--use-fbgemm\" == \"\" (goto :process_args_exit )\r\n\r\nE:\\SOFTWARE\\pytorch>if \"--use-fbgemm\" == \"--use-cuda\" (\r\nset /a USE_CUDA=1\r\n goto :process_args_processed\r\n)\r\n\r\nE:\\SOFTWARE\\pytorch>if \"--use-fbgemm\" == \"--use-fbgemm\" (\r\nset /a USE_FBGEMM=1\r\n goto :process_args_processed\r\n)\r\n\r\nE:\\SOFTWARE\\pytorch>shift\r\n\r\nE:\\SOFTWARE\\pytorch>goto :process_args\r\n\r\nE:\\SOFTWARE\\pytorch>if \"--use-nnpack\" == \"\" (goto :process_args_exit )\r\n\r\nE:\\SOFTWARE\\pytorch>if \"--use-nnpack\" == \"--use-cuda\" (\r\nset /a USE_CUDA=1\r\n goto :process_args_processed\r\n[Truncated]\nDone Building Project \"E:\\SOFTWARE\\pytorch\\build\\INSTALL.vcxproj\" (default targets) -- FAILED.\r\n\r\n\r\nBuild FAILED.\r\n\r\n\"E:\\SOFTWARE\\pytorch\\build\\INSTALL.vcxproj\" (default target) (1) ->\r\n\"E:\\SOFTWARE\\pytorch\\build\\ALL_BUILD.vcxproj\" (default target) (3) ->\r\n\"E:\\SOFTWARE\\pytorch\\build\\caffe2\\AlgorithmsTest.vcxproj\" (default target) (4) ->\r\n\"E:\\SOFTWARE\\pytorch\\build\\caffe2\\caffe2_gpu.vcxproj\" (default target) (22) ->\r\n(Link target) ->\r\n  LINK : fatal error LNK1181: C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\lib\\x64.obj [E:\\SOFTWA\r\nRE\\pytorch\\build\\caffe2\\caffe2_gpu.vcxproj]\r\n\r\n    0 Warning(s)\r\n    1 Error(s)\r\n\r\nTime Elapsed 00:00:22.22\r\n\r\nE:\\SOFTWARE\\pytorch\\build>IF ERRORLEVEL 1 exit 1\r\nFailed to run 'tools\\build_pytorch_libs.bat --use-cuda --use-fbgemm --use-nnpack --use-mkldnn --use-qnnpack caffe2'","context":"<issue_start><issue_comment>Title: Can't open x64.obj while installing pytorch with win10 \nusername_0: E:\\SOFTWARE\\pytorch\\build\\caffe2\\torch\\lib\\libshm_windows\\shm.vcxproj()\r\n\r\nE:\\SOFTWARE\\pytorch\\build\\ALL_BUILD.vcxproj() - \r\n\r\nE:\\SOFTWARE\\pytorch\\build\\INSTALL.vcxproj() - \r\n\r\n\r\n\r\n\r\nE:\\SOFTWARE\\pytorch\\build\\INSTALL.vcxproj() (1) ->\r\nE:\\SOFTWARE\\pytorch\\build\\ALL_BUILD.vcxproj() (3) ->\r\nE:\\SOFTWARE\\pytorch\\build\\caffe2\\AlgorithmsTest.vcxproj() (4) ->\r\nE:\\SOFTWARE\\pytorch\\build\\caffe2\\caffe2_gpu.vcxproj() (22) ->\r\n(Link ) ->\r\n  C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\lib\\x64.obj : fatal error LNK1136:  [E:\\SOFTWARE\\pyt\r\norch\\build\\caffe2\\caffe2_gpu.vcxproj]\r\n\r\n    0 \r\n    1 \r\n\r\n 00:00:16.23\r\n\r\nE:\\SOFTWARE\\pytorch\\build>IF ERRORLEVEL 1 exit 1\r\nFailed to run 'tools\\build_pytorch_libs.bat --use-cuda --use-fbgemm --use-nnpack --use-mkldnn --use-qnnpack caffe2'\r\n\r\npytorch versionstable1.0\r\nosWindows 10 Pro Version 1803\r\npython version:3.7\r\nCUDA:10\r\ncudnn:7.4.1\r\nVisual Studio version: Microsoft Visual Studio 2017 Community Edition\r\n\r\nCan anyone help me solve this problem? Thanks !\n<issue_comment>username_1: Please provide the full log.\n<issue_comment>username_0: the full log\r\n\r\nBuilding wheel torch-1.0.0a0+df61437\r\nrunning install\r\nsetup.py::run()\r\nrunning build_deps\r\nsetup.py::build_deps::run()\r\n\r\nE:\\SOFTWARE\\pytorch>cd \"E:\\SOFTWARE\\pytorch\\tools\\\\..\"\r\n\r\nE:\\SOFTWARE\\pytorch>set PATH=\\bin;E:\\SOFTWARE\\Anaconda\\DLLs;E:\\SOFTWARE\\Visual Studio\\VC\\Tools\\MSVC\\14.11.25503\\bin\\HostX64\\x64;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\VC\\VCPackages;C:\\Program Files (x86)\\Microsoft SDKs\\TypeScript\\3.1;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\TestWindow;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\TeamFoundation\\Team Explorer;E:\\SOFTWARE\\Visual Studio\\MSBuild\\15.0\\bin\\Roslyn;E:\\SOFTWARE\\Visual Studio\\Team Tools\\Performance Tools\\x64;E:\\SOFTWARE\\Visual Studio\\Team Tools\\Performance Tools;E:\\SOFTWARE\\Visual Studio\\share\\Common\\VSPerfCollectionTools\\\\x64;E:\\SOFTWARE\\Visual Studio\\share\\Common\\VSPerfCollectionTools\\;C:\\Program Files (x86)\\Microsoft SDKs\\Windows\\v10.0A\\bin\\NETFX 4.6.1 Tools\\x64\\;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\10.0.17763.0\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x64;E:\\SOFTWARE\\Visual Studio\\\\MSBuild\\15.0\\bin;C:\\WINDOWS\\Microsoft.NET\\Framework64\\v4.0.30319;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\;E:\\SOFTWARE\\Visual Studio\\Common7\\Tools\\;E:\\SOFTWARE\\Anaconda;E:\\SOFTWARE\\Anaconda\\Library\\mingw-w64\\bin;E:\\SOFTWARE\\Anaconda\\Library\\usr\\bin;E:\\SOFTWARE\\Anaconda\\Library\\bin;E:\\SOFTWARE\\Anaconda\\Scripts;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\bin;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\libnvvp;C:\\WINDOWS\\system32;C:\\WINDOWS;C:\\WINDOWS\\System32\\Wbem;C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0\\;C:\\WINDOWS\\System32\\OpenSSH\\;E:\\\\Git\\cmd;E:\\SOFTWARE\\cmake\\bin;C:\\Program Files (x86)\\Windows Kits\\8.1\\Windows Performance Toolkit\\;C:\\Program Files (x86)\\NVIDIA Corporation\\PhysX\\Common;C:\\Program Files\\dotnet\\;C:\\Program Files\\NVIDIA Corporation\\NVIDIA NvDLISR;C:\\Windows\\Microsoft.NET\\Framework\\v4.0.30319;C:\\Windows\\Microsoft.NET\\Framework64\\v4.0.30319;C:\\Users\\11838\\AppData\\Local\\Microsoft\\WindowsApps;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\CMake\\bin;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\Ninja\r\n\r\nE:\\SOFTWARE\\pytorch>set BASE_DIR=E:/SOFTWARE/pytorch\r\n\r\nE:\\SOFTWARE\\pytorch>set TORCH_LIB_DIR=E:/SOFTWARE/pytorch/torch/lib\r\n\r\nE:\\SOFTWARE\\pytorch>set THIRD_PARTY_DIR=E:/SOFTWARE/pytorch/third_party\r\n\r\nE:\\SOFTWARE\\pytorch>set BASIC_C_FLAGS=\r\n\r\nE:\\SOFTWARE\\pytorch>set BASIC_CUDA_FLAGS=\r\n\r\nE:\\SOFTWARE\\pytorch>IF NOT DEFINED INSTALL_DIR (set \"INSTALL_DIR=E:/SOFTWARE/pytorch/torch/lib/tmp_install\" )  ELSE (set \"INSTALL_DIR=\\=/\" )\r\n\r\nE:\\SOFTWARE\\pytorch>set LDFLAGS=/LIBPATH:E:/SOFTWARE/pytorch/torch/lib/tmp_install/lib\r\n\r\nE:\\SOFTWARE\\pytorch>set C_FLAGS= /D_WIN32 /Z7 /EHa /DNOMINMAX\r\n\r\nE:\\SOFTWARE\\pytorch>set LINK_FLAGS=/DEBUG:FULL\r\n\r\nE:\\SOFTWARE\\pytorch>if not exist torch\\lib\\tmp_install mkdir torch\\lib\\tmp_install\r\n\r\nE:\\SOFTWARE\\pytorch>set /a USE_CUDA=0\r\n\r\nE:\\SOFTWARE\\pytorch>set /a USE_FBGEMM=1\r\n\r\nE:\\SOFTWARE\\pytorch>set /a USE_ROCM=0\r\n\r\nE:\\SOFTWARE\\pytorch>set /a USE_NNPACK=0\r\n\r\nE:\\SOFTWARE\\pytorch>set /a USE_QNNPACK=0\r\n\r\nE:\\SOFTWARE\\pytorch>set /a USE_GLOO_IBVERBS=0\r\n\r\nE:\\SOFTWARE\\pytorch>set /a USE_MKLDNN=0\r\n\r\nE:\\SOFTWARE\\pytorch>set _BUILD_ARGS=\r\n\r\nE:\\SOFTWARE\\pytorch>if \"--use-cuda\" == \"\" (goto :process_args_exit )\r\n\r\nE:\\SOFTWARE\\pytorch>if \"--use-cuda\" == \"--use-cuda\" (\r\nset /a USE_CUDA=1\r\n goto :process_args_processed\r\n)\r\n\r\nE:\\SOFTWARE\\pytorch>shift\r\n\r\nE:\\SOFTWARE\\pytorch>goto :process_args\r\n\r\nE:\\SOFTWARE\\pytorch>if \"--use-fbgemm\" == \"\" (goto :process_args_exit )\r\n\r\nE:\\SOFTWARE\\pytorch>if \"--use-fbgemm\" == \"--use-cuda\" (\r\nset /a USE_CUDA=1\r\n goto :process_args_processed\r\n)\r\n\r\nE:\\SOFTWARE\\pytorch>if \"--use-fbgemm\" == \"--use-fbgemm\" (\r\nset /a USE_FBGEMM=1\r\n goto :process_args_processed\r\n)\r\n\r\nE:\\SOFTWARE\\pytorch>shift\r\n\r\nE:\\SOFTWARE\\pytorch>goto :process_args\r\n\r\nE:\\SOFTWARE\\pytorch>if \"--use-nnpack\" == \"\" (goto :process_args_exit )\r\n\r\nE:\\SOFTWARE\\pytorch>if \"--use-nnpack\" == \"--use-cuda\" (\r\nset /a USE_CUDA=1\r\n goto :process_args_processed\r\n[Truncated]\nDone Building Project \"E:\\SOFTWARE\\pytorch\\build\\INSTALL.vcxproj\" (default targets) -- FAILED.\r\n\r\n\r\nBuild FAILED.\r\n\r\n\"E:\\SOFTWARE\\pytorch\\build\\INSTALL.vcxproj\" (default target) (1) ->\r\n\"E:\\SOFTWARE\\pytorch\\build\\ALL_BUILD.vcxproj\" (default target) (3) ->\r\n\"E:\\SOFTWARE\\pytorch\\build\\caffe2\\AlgorithmsTest.vcxproj\" (default target) (4) ->\r\n\"E:\\SOFTWARE\\pytorch\\build\\caffe2\\caffe2_gpu.vcxproj\" (default target) (22) ->\r\n(Link target) ->\r\n  LINK : fatal error LNK1181: C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\lib\\x64.obj [E:\\SOFTWA\r\nRE\\pytorch\\build\\caffe2\\caffe2_gpu.vcxproj]\r\n\r\n    0 Warning(s)\r\n    1 Error(s)\r\n\r\nTime Elapsed 00:00:22.22\r\n\r\nE:\\SOFTWARE\\pytorch\\build>IF ERRORLEVEL 1 exit 1\r\nFailed to run 'tools\\build_pytorch_libs.bat --use-cuda --use-fbgemm --use-nnpack --use-mkldnn --use-qnnpack caffe2'\n<issue_comment>username_1: The error is caused by the misconfiguration of the flag `CUDNN_LIBRARY`. Could you please run `tools/setup_helpers/cudnn.py` and see which code path it goes through?\n<issue_comment>username_0: My environmental variables is :\r\nCUDA_PATH=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\r\nCUDA_PATH_V10_0=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\r\nCUDNN_INCLUDE_DIR=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\include\r\nCUDNN_LIBRARY=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\lib\\x64\r\nNVCUDASAMPLES10_0_ROOT=C:\\ProgramData\\NVIDIA Corporation\\CUDA Samples\\v10.0\r\nNVCUDASAMPLES_ROOT=C:\\ProgramData\\NVIDIA Corporation\\CUDA Samples\\v10.0\r\nNVTOOLSEXT_PATH=C:\\Program Files\\NVIDIA Corporation\\NvToolsExt\\\r\nPath=E:\\SOFTWARE\\Visual Studio\\VC\\Tools\\MSVC\\14.11.25503\\bin\\HostX64\\x64;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\VC\\VCPackages;C:\\Program Files (x86)\\Microsoft SDKs\\TypeScript\\3.1;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\TestWindow;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\TeamFoundation\\Team Explorer;E:\\SOFTWARE\\Visual Studio\\MSBuild\\15.0\\bin\\Roslyn;E:\\SOFTWARE\\Visual Studio\\Team Tools\\Performance Tools\\x64;E:\\SOFTWARE\\Visual Studio\\Team Tools\\Performance Tools;E:\\SOFTWARE\\Visual Studio\\share\\Common\\VSPerfCollectionTools\\\\x64;E:\\SOFTWARE\\Visual Studio\\share\\Common\\VSPerfCollectionTools\\;C:\\Program Files (x86)\\Microsoft SDKs\\Windows\\v10.0A\\bin\\NETFX 4.6.1 Tools\\x64\\;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\10.0.17763.0\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x64;E:\\SOFTWARE\\Visual Studio\\\\MSBuild\\15.0\\bin;C:\\WINDOWS\\Microsoft.NET\\Framework64\\v4.0.30319;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\;E:\\SOFTWARE\\Visual Studio\\Common7\\Tools\\;E:\\SOFTWARE\\Anaconda;E:\\SOFTWARE\\Anaconda\\Library\\mingw-w64\\bin;E:\\SOFTWARE\\Anaconda\\Library\\usr\\bin;E:\\SOFTWARE\\Anaconda\\Library\\bin;E:\\SOFTWARE\\Anaconda\\Scripts;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\bin;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\libnvvp;C:\\WINDOWS\\system32;C:\\WINDOWS;C:\\WINDOWS\\System32\\Wbem;C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0\\;C:\\WINDOWS\\System32\\OpenSSH\\;E:\\\\Git\\cmd;E:\\SOFTWARE\\cmake\\bin;C:\\Program Files (x86)\\Windows Kits\\8.1\\Windows Performance Toolkit\\;C:\\Program Files (x86)\\NVIDIA Corporation\\PhysX\\Common;C:\\Program Files\\dotnet\\;C:\\Program Files\\NVIDIA Corporation\\NVIDIA NvDLISR;C:\\Windows\\Microsoft.NET\\Framework\\v4.0.30319;C:\\Windows\\Microsoft.NET\\Framework64\\v4.0.30319;C:\\Users\\11838\\AppData\\Local\\Microsoft\\WindowsApps;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\CMake\\bin;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\Ninja\r\n\r\nAND the cudnn.py shows:\r\nimport os\r\nimport glob\r\n\r\nfrom .env import IS_WINDOWS, IS_CONDA, CONDA_DIR, check_negative_env_flag, gather_paths, lib_paths_from_base\r\nfrom .cuda import USE_CUDA, CUDA_HOME\r\n\r\n\r\nUSE_CUDNN = False\r\nCUDNN_LIB_DIR = None\r\nCUDNN_INCLUDE_DIR = None\r\nCUDNN_LIBRARY = None\r\nWITH_STATIC_CUDNN = os.getenv(\"USE_STATIC_CUDNN\")\r\n\r\nif USE_CUDA and not check_negative_env_flag('USE_CUDNN'):\r\n    lib_paths = list(filter(bool, [\r\n        os.getenv('CUDNN_LIB_DIR')\r\n    ] + lib_paths_from_base(CUDA_HOME) + [\r\n        '/usr/lib/x86_64-linux-gnu/',\r\n        '/usr/lib/powerpc64le-linux-gnu/',\r\n        '/usr/lib/aarch64-linux-gnu/',\r\n    ] + gather_paths([\r\n        'LIBRARY_PATH',\r\n    ]) + gather_paths([\r\n        'LD_LIBRARY_PATH',\r\n    ])))\r\n    include_paths = list(filter(bool, [\r\n        os.getenv('CUDNN_INCLUDE_DIR'),\r\n        os.path.join(CUDA_HOME, 'include'),\r\n        '/usr/include/',\r\n    ] + gather_paths([\r\n        'CPATH',\r\n        'C_INCLUDE_PATH',\r\n        'CPLUS_INCLUDE_PATH',\r\n    ])))\r\n    # Add CUDA related dirs to candidate list\r\n    if IS_CONDA:\r\n        lib_paths.append(os.path.join(CONDA_DIR, 'lib'))\r\n        include_paths.append(os.path.join(CONDA_DIR, 'include'))\r\n    for path in include_paths:\r\n        if path is None or not os.path.exists(path):\r\n            continue\r\n        include_file_path = os.path.join(path, 'cudnn.h')\r\n        CUDNN_INCLUDE_VERSION = None\r\n        if os.path.exists(include_file_path):\r\n            CUDNN_INCLUDE_DIR = path\r\n            with open(include_file_path) as f:\r\n                for line in f:\r\n                    if \"#define CUDNN_MAJOR\" in line:\r\n                        CUDNN_INCLUDE_VERSION = int(line.split()[-1])\r\n                        break\r\n            if CUDNN_INCLUDE_VERSION is None:\r\n                raise AssertionError(\"Could not find #define CUDNN_MAJOR in \" + include_file_path)\r\n            break\r\n\r\n    if CUDNN_INCLUDE_VERSION is None:\r\n        pass\r\n\r\n    # Check for standalone cuDNN libraries\r\n    if CUDNN_INCLUDE_DIR is not None:\r\n        cudnn_path = os.path.join(os.path.dirname(CUDNN_INCLUDE_DIR))\r\n        cudnn_lib_paths = lib_paths_from_base(cudnn_path)\r\n        lib_paths.extend(cudnn_lib_paths)\r\n\r\n    for path in lib_paths:\r\n        if path is None or not os.path.exists(path):\r\n            continue\r\n        if IS_WINDOWS:\r\n            library = os.path.join(path, 'cudnn.lib')\r\n            if os.path.exists(library):\r\n[Truncated]\n                search_name = 'libcudnn*' + str(CUDNN_INCLUDE_VERSION) + \"*\"\r\n            libraries = sorted(glob.glob(os.path.join(path, search_name)))\r\n            if libraries:\r\n                CUDNN_LIBRARY = libraries[0]\r\n                CUDNN_LIB_DIR = path\r\n                break\r\n    # Specifying the library directly will overwrite the lib directory\r\n    library = os.getenv('CUDNN_LIBRARY')\r\n    if library is not None and os.path.exists(library):\r\n        CUDNN_LIBRARY = library\r\n        CUDNN_LIB_DIR = os.path.dirname(CUDNN_LIBRARY)\r\n\r\n    if not all([CUDNN_LIBRARY, CUDNN_LIB_DIR, CUDNN_INCLUDE_DIR]):\r\n        CUDNN_LIBRARY = CUDNN_LIB_DIR = CUDNN_INCLUDE_DIR = None\r\n    else:\r\n        real_cudnn_library = os.path.realpath(CUDNN_LIBRARY)\r\n        real_cudnn_lib_dir = os.path.realpath(CUDNN_LIB_DIR)\r\n        assert os.path.dirname(real_cudnn_library) == real_cudnn_lib_dir, (\r\n            'cudnn library and lib_dir must agree')\r\n        USE_CUDNN = True\n<issue_comment>username_1: Please remove `CUDNN_INCLUDE_DIR` and `CUDNN_LIBRARY` by using the following commands.\r\n```cmd\r\nset CUDNN_INCLUDE_DIR=\r\nset CUDNN_LIBRARY=\r\n```\n<issue_comment>username_0: It works! Thanks a lot!<issue_closed>","repo":"pytorch/pytorch","issue_id":391131022,"issue_number":15222}
{"question":"What action should be taken to address the error caused by the misconfiguration of the `CUDNN_LIBRARY` flag?","answer":"To address the error, the `CUDNN_LIBRARY` flag misconfiguration can be resolved by running `tools/setup_helpers/cudnn.py` to determine the code path being used.","id":135,"text":"The error is caused by the misconfiguration of the flag `CUDNN_LIBRARY`. Could you please run `tools/setup_helpers/cudnn.py` and see which code path it goes through?","context":"<issue_start><issue_comment>Title: Can't open x64.obj while installing pytorch with win10 \nusername_0: E:\\SOFTWARE\\pytorch\\build\\caffe2\\torch\\lib\\libshm_windows\\shm.vcxproj()\r\n\r\nE:\\SOFTWARE\\pytorch\\build\\ALL_BUILD.vcxproj() - \r\n\r\nE:\\SOFTWARE\\pytorch\\build\\INSTALL.vcxproj() - \r\n\r\n\r\n\r\n\r\nE:\\SOFTWARE\\pytorch\\build\\INSTALL.vcxproj() (1) ->\r\nE:\\SOFTWARE\\pytorch\\build\\ALL_BUILD.vcxproj() (3) ->\r\nE:\\SOFTWARE\\pytorch\\build\\caffe2\\AlgorithmsTest.vcxproj() (4) ->\r\nE:\\SOFTWARE\\pytorch\\build\\caffe2\\caffe2_gpu.vcxproj() (22) ->\r\n(Link ) ->\r\n  C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\lib\\x64.obj : fatal error LNK1136:  [E:\\SOFTWARE\\pyt\r\norch\\build\\caffe2\\caffe2_gpu.vcxproj]\r\n\r\n    0 \r\n    1 \r\n\r\n 00:00:16.23\r\n\r\nE:\\SOFTWARE\\pytorch\\build>IF ERRORLEVEL 1 exit 1\r\nFailed to run 'tools\\build_pytorch_libs.bat --use-cuda --use-fbgemm --use-nnpack --use-mkldnn --use-qnnpack caffe2'\r\n\r\npytorch versionstable1.0\r\nosWindows 10 Pro Version 1803\r\npython version:3.7\r\nCUDA:10\r\ncudnn:7.4.1\r\nVisual Studio version: Microsoft Visual Studio 2017 Community Edition\r\n\r\nCan anyone help me solve this problem? Thanks !\n<issue_comment>username_1: Please provide the full log.\n<issue_comment>username_0: the full log\r\n\r\nBuilding wheel torch-1.0.0a0+df61437\r\nrunning install\r\nsetup.py::run()\r\nrunning build_deps\r\nsetup.py::build_deps::run()\r\n\r\nE:\\SOFTWARE\\pytorch>cd \"E:\\SOFTWARE\\pytorch\\tools\\\\..\"\r\n\r\nE:\\SOFTWARE\\pytorch>set PATH=\\bin;E:\\SOFTWARE\\Anaconda\\DLLs;E:\\SOFTWARE\\Visual Studio\\VC\\Tools\\MSVC\\14.11.25503\\bin\\HostX64\\x64;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\VC\\VCPackages;C:\\Program Files (x86)\\Microsoft SDKs\\TypeScript\\3.1;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\TestWindow;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\TeamFoundation\\Team Explorer;E:\\SOFTWARE\\Visual Studio\\MSBuild\\15.0\\bin\\Roslyn;E:\\SOFTWARE\\Visual Studio\\Team Tools\\Performance Tools\\x64;E:\\SOFTWARE\\Visual Studio\\Team Tools\\Performance Tools;E:\\SOFTWARE\\Visual Studio\\share\\Common\\VSPerfCollectionTools\\\\x64;E:\\SOFTWARE\\Visual Studio\\share\\Common\\VSPerfCollectionTools\\;C:\\Program Files (x86)\\Microsoft SDKs\\Windows\\v10.0A\\bin\\NETFX 4.6.1 Tools\\x64\\;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\10.0.17763.0\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x64;E:\\SOFTWARE\\Visual Studio\\\\MSBuild\\15.0\\bin;C:\\WINDOWS\\Microsoft.NET\\Framework64\\v4.0.30319;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\;E:\\SOFTWARE\\Visual Studio\\Common7\\Tools\\;E:\\SOFTWARE\\Anaconda;E:\\SOFTWARE\\Anaconda\\Library\\mingw-w64\\bin;E:\\SOFTWARE\\Anaconda\\Library\\usr\\bin;E:\\SOFTWARE\\Anaconda\\Library\\bin;E:\\SOFTWARE\\Anaconda\\Scripts;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\bin;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\libnvvp;C:\\WINDOWS\\system32;C:\\WINDOWS;C:\\WINDOWS\\System32\\Wbem;C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0\\;C:\\WINDOWS\\System32\\OpenSSH\\;E:\\\\Git\\cmd;E:\\SOFTWARE\\cmake\\bin;C:\\Program Files (x86)\\Windows Kits\\8.1\\Windows Performance Toolkit\\;C:\\Program Files (x86)\\NVIDIA Corporation\\PhysX\\Common;C:\\Program Files\\dotnet\\;C:\\Program Files\\NVIDIA Corporation\\NVIDIA NvDLISR;C:\\Windows\\Microsoft.NET\\Framework\\v4.0.30319;C:\\Windows\\Microsoft.NET\\Framework64\\v4.0.30319;C:\\Users\\11838\\AppData\\Local\\Microsoft\\WindowsApps;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\CMake\\bin;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\Ninja\r\n\r\nE:\\SOFTWARE\\pytorch>set BASE_DIR=E:/SOFTWARE/pytorch\r\n\r\nE:\\SOFTWARE\\pytorch>set TORCH_LIB_DIR=E:/SOFTWARE/pytorch/torch/lib\r\n\r\nE:\\SOFTWARE\\pytorch>set THIRD_PARTY_DIR=E:/SOFTWARE/pytorch/third_party\r\n\r\nE:\\SOFTWARE\\pytorch>set BASIC_C_FLAGS=\r\n\r\nE:\\SOFTWARE\\pytorch>set BASIC_CUDA_FLAGS=\r\n\r\nE:\\SOFTWARE\\pytorch>IF NOT DEFINED INSTALL_DIR (set \"INSTALL_DIR=E:/SOFTWARE/pytorch/torch/lib/tmp_install\" )  ELSE (set \"INSTALL_DIR=\\=/\" )\r\n\r\nE:\\SOFTWARE\\pytorch>set LDFLAGS=/LIBPATH:E:/SOFTWARE/pytorch/torch/lib/tmp_install/lib\r\n\r\nE:\\SOFTWARE\\pytorch>set C_FLAGS= /D_WIN32 /Z7 /EHa /DNOMINMAX\r\n\r\nE:\\SOFTWARE\\pytorch>set LINK_FLAGS=/DEBUG:FULL\r\n\r\nE:\\SOFTWARE\\pytorch>if not exist torch\\lib\\tmp_install mkdir torch\\lib\\tmp_install\r\n\r\nE:\\SOFTWARE\\pytorch>set /a USE_CUDA=0\r\n\r\nE:\\SOFTWARE\\pytorch>set /a USE_FBGEMM=1\r\n\r\nE:\\SOFTWARE\\pytorch>set /a USE_ROCM=0\r\n\r\nE:\\SOFTWARE\\pytorch>set /a USE_NNPACK=0\r\n\r\nE:\\SOFTWARE\\pytorch>set /a USE_QNNPACK=0\r\n\r\nE:\\SOFTWARE\\pytorch>set /a USE_GLOO_IBVERBS=0\r\n\r\nE:\\SOFTWARE\\pytorch>set /a USE_MKLDNN=0\r\n\r\nE:\\SOFTWARE\\pytorch>set _BUILD_ARGS=\r\n\r\nE:\\SOFTWARE\\pytorch>if \"--use-cuda\" == \"\" (goto :process_args_exit )\r\n\r\nE:\\SOFTWARE\\pytorch>if \"--use-cuda\" == \"--use-cuda\" (\r\nset /a USE_CUDA=1\r\n goto :process_args_processed\r\n)\r\n\r\nE:\\SOFTWARE\\pytorch>shift\r\n\r\nE:\\SOFTWARE\\pytorch>goto :process_args\r\n\r\nE:\\SOFTWARE\\pytorch>if \"--use-fbgemm\" == \"\" (goto :process_args_exit )\r\n\r\nE:\\SOFTWARE\\pytorch>if \"--use-fbgemm\" == \"--use-cuda\" (\r\nset /a USE_CUDA=1\r\n goto :process_args_processed\r\n)\r\n\r\nE:\\SOFTWARE\\pytorch>if \"--use-fbgemm\" == \"--use-fbgemm\" (\r\nset /a USE_FBGEMM=1\r\n goto :process_args_processed\r\n)\r\n\r\nE:\\SOFTWARE\\pytorch>shift\r\n\r\nE:\\SOFTWARE\\pytorch>goto :process_args\r\n\r\nE:\\SOFTWARE\\pytorch>if \"--use-nnpack\" == \"\" (goto :process_args_exit )\r\n\r\nE:\\SOFTWARE\\pytorch>if \"--use-nnpack\" == \"--use-cuda\" (\r\nset /a USE_CUDA=1\r\n goto :process_args_processed\r\n[Truncated]\nDone Building Project \"E:\\SOFTWARE\\pytorch\\build\\INSTALL.vcxproj\" (default targets) -- FAILED.\r\n\r\n\r\nBuild FAILED.\r\n\r\n\"E:\\SOFTWARE\\pytorch\\build\\INSTALL.vcxproj\" (default target) (1) ->\r\n\"E:\\SOFTWARE\\pytorch\\build\\ALL_BUILD.vcxproj\" (default target) (3) ->\r\n\"E:\\SOFTWARE\\pytorch\\build\\caffe2\\AlgorithmsTest.vcxproj\" (default target) (4) ->\r\n\"E:\\SOFTWARE\\pytorch\\build\\caffe2\\caffe2_gpu.vcxproj\" (default target) (22) ->\r\n(Link target) ->\r\n  LINK : fatal error LNK1181: C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\lib\\x64.obj [E:\\SOFTWA\r\nRE\\pytorch\\build\\caffe2\\caffe2_gpu.vcxproj]\r\n\r\n    0 Warning(s)\r\n    1 Error(s)\r\n\r\nTime Elapsed 00:00:22.22\r\n\r\nE:\\SOFTWARE\\pytorch\\build>IF ERRORLEVEL 1 exit 1\r\nFailed to run 'tools\\build_pytorch_libs.bat --use-cuda --use-fbgemm --use-nnpack --use-mkldnn --use-qnnpack caffe2'\n<issue_comment>username_1: The error is caused by the misconfiguration of the flag `CUDNN_LIBRARY`. Could you please run `tools/setup_helpers/cudnn.py` and see which code path it goes through?\n<issue_comment>username_0: My environmental variables is :\r\nCUDA_PATH=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\r\nCUDA_PATH_V10_0=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\r\nCUDNN_INCLUDE_DIR=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\include\r\nCUDNN_LIBRARY=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\lib\\x64\r\nNVCUDASAMPLES10_0_ROOT=C:\\ProgramData\\NVIDIA Corporation\\CUDA Samples\\v10.0\r\nNVCUDASAMPLES_ROOT=C:\\ProgramData\\NVIDIA Corporation\\CUDA Samples\\v10.0\r\nNVTOOLSEXT_PATH=C:\\Program Files\\NVIDIA Corporation\\NvToolsExt\\\r\nPath=E:\\SOFTWARE\\Visual Studio\\VC\\Tools\\MSVC\\14.11.25503\\bin\\HostX64\\x64;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\VC\\VCPackages;C:\\Program Files (x86)\\Microsoft SDKs\\TypeScript\\3.1;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\TestWindow;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\TeamFoundation\\Team Explorer;E:\\SOFTWARE\\Visual Studio\\MSBuild\\15.0\\bin\\Roslyn;E:\\SOFTWARE\\Visual Studio\\Team Tools\\Performance Tools\\x64;E:\\SOFTWARE\\Visual Studio\\Team Tools\\Performance Tools;E:\\SOFTWARE\\Visual Studio\\share\\Common\\VSPerfCollectionTools\\\\x64;E:\\SOFTWARE\\Visual Studio\\share\\Common\\VSPerfCollectionTools\\;C:\\Program Files (x86)\\Microsoft SDKs\\Windows\\v10.0A\\bin\\NETFX 4.6.1 Tools\\x64\\;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\10.0.17763.0\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x64;E:\\SOFTWARE\\Visual Studio\\\\MSBuild\\15.0\\bin;C:\\WINDOWS\\Microsoft.NET\\Framework64\\v4.0.30319;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\;E:\\SOFTWARE\\Visual Studio\\Common7\\Tools\\;E:\\SOFTWARE\\Anaconda;E:\\SOFTWARE\\Anaconda\\Library\\mingw-w64\\bin;E:\\SOFTWARE\\Anaconda\\Library\\usr\\bin;E:\\SOFTWARE\\Anaconda\\Library\\bin;E:\\SOFTWARE\\Anaconda\\Scripts;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\bin;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\libnvvp;C:\\WINDOWS\\system32;C:\\WINDOWS;C:\\WINDOWS\\System32\\Wbem;C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0\\;C:\\WINDOWS\\System32\\OpenSSH\\;E:\\\\Git\\cmd;E:\\SOFTWARE\\cmake\\bin;C:\\Program Files (x86)\\Windows Kits\\8.1\\Windows Performance Toolkit\\;C:\\Program Files (x86)\\NVIDIA Corporation\\PhysX\\Common;C:\\Program Files\\dotnet\\;C:\\Program Files\\NVIDIA Corporation\\NVIDIA NvDLISR;C:\\Windows\\Microsoft.NET\\Framework\\v4.0.30319;C:\\Windows\\Microsoft.NET\\Framework64\\v4.0.30319;C:\\Users\\11838\\AppData\\Local\\Microsoft\\WindowsApps;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\CMake\\bin;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\Ninja\r\n\r\nAND the cudnn.py shows:\r\nimport os\r\nimport glob\r\n\r\nfrom .env import IS_WINDOWS, IS_CONDA, CONDA_DIR, check_negative_env_flag, gather_paths, lib_paths_from_base\r\nfrom .cuda import USE_CUDA, CUDA_HOME\r\n\r\n\r\nUSE_CUDNN = False\r\nCUDNN_LIB_DIR = None\r\nCUDNN_INCLUDE_DIR = None\r\nCUDNN_LIBRARY = None\r\nWITH_STATIC_CUDNN = os.getenv(\"USE_STATIC_CUDNN\")\r\n\r\nif USE_CUDA and not check_negative_env_flag('USE_CUDNN'):\r\n    lib_paths = list(filter(bool, [\r\n        os.getenv('CUDNN_LIB_DIR')\r\n    ] + lib_paths_from_base(CUDA_HOME) + [\r\n        '/usr/lib/x86_64-linux-gnu/',\r\n        '/usr/lib/powerpc64le-linux-gnu/',\r\n        '/usr/lib/aarch64-linux-gnu/',\r\n    ] + gather_paths([\r\n        'LIBRARY_PATH',\r\n    ]) + gather_paths([\r\n        'LD_LIBRARY_PATH',\r\n    ])))\r\n    include_paths = list(filter(bool, [\r\n        os.getenv('CUDNN_INCLUDE_DIR'),\r\n        os.path.join(CUDA_HOME, 'include'),\r\n        '/usr/include/',\r\n    ] + gather_paths([\r\n        'CPATH',\r\n        'C_INCLUDE_PATH',\r\n        'CPLUS_INCLUDE_PATH',\r\n    ])))\r\n    # Add CUDA related dirs to candidate list\r\n    if IS_CONDA:\r\n        lib_paths.append(os.path.join(CONDA_DIR, 'lib'))\r\n        include_paths.append(os.path.join(CONDA_DIR, 'include'))\r\n    for path in include_paths:\r\n        if path is None or not os.path.exists(path):\r\n            continue\r\n        include_file_path = os.path.join(path, 'cudnn.h')\r\n        CUDNN_INCLUDE_VERSION = None\r\n        if os.path.exists(include_file_path):\r\n            CUDNN_INCLUDE_DIR = path\r\n            with open(include_file_path) as f:\r\n                for line in f:\r\n                    if \"#define CUDNN_MAJOR\" in line:\r\n                        CUDNN_INCLUDE_VERSION = int(line.split()[-1])\r\n                        break\r\n            if CUDNN_INCLUDE_VERSION is None:\r\n                raise AssertionError(\"Could not find #define CUDNN_MAJOR in \" + include_file_path)\r\n            break\r\n\r\n    if CUDNN_INCLUDE_VERSION is None:\r\n        pass\r\n\r\n    # Check for standalone cuDNN libraries\r\n    if CUDNN_INCLUDE_DIR is not None:\r\n        cudnn_path = os.path.join(os.path.dirname(CUDNN_INCLUDE_DIR))\r\n        cudnn_lib_paths = lib_paths_from_base(cudnn_path)\r\n        lib_paths.extend(cudnn_lib_paths)\r\n\r\n    for path in lib_paths:\r\n        if path is None or not os.path.exists(path):\r\n            continue\r\n        if IS_WINDOWS:\r\n            library = os.path.join(path, 'cudnn.lib')\r\n            if os.path.exists(library):\r\n[Truncated]\n                search_name = 'libcudnn*' + str(CUDNN_INCLUDE_VERSION) + \"*\"\r\n            libraries = sorted(glob.glob(os.path.join(path, search_name)))\r\n            if libraries:\r\n                CUDNN_LIBRARY = libraries[0]\r\n                CUDNN_LIB_DIR = path\r\n                break\r\n    # Specifying the library directly will overwrite the lib directory\r\n    library = os.getenv('CUDNN_LIBRARY')\r\n    if library is not None and os.path.exists(library):\r\n        CUDNN_LIBRARY = library\r\n        CUDNN_LIB_DIR = os.path.dirname(CUDNN_LIBRARY)\r\n\r\n    if not all([CUDNN_LIBRARY, CUDNN_LIB_DIR, CUDNN_INCLUDE_DIR]):\r\n        CUDNN_LIBRARY = CUDNN_LIB_DIR = CUDNN_INCLUDE_DIR = None\r\n    else:\r\n        real_cudnn_library = os.path.realpath(CUDNN_LIBRARY)\r\n        real_cudnn_lib_dir = os.path.realpath(CUDNN_LIB_DIR)\r\n        assert os.path.dirname(real_cudnn_library) == real_cudnn_lib_dir, (\r\n            'cudnn library and lib_dir must agree')\r\n        USE_CUDNN = True\n<issue_comment>username_1: Please remove `CUDNN_INCLUDE_DIR` and `CUDNN_LIBRARY` by using the following commands.\r\n```cmd\r\nset CUDNN_INCLUDE_DIR=\r\nset CUDNN_LIBRARY=\r\n```\n<issue_comment>username_0: It works! Thanks a lot!<issue_closed>","repo":"pytorch/pytorch","issue_id":391131022,"issue_number":15222}
{"question":"How does the Python script in cudnn.py determine the location of cuDNN libraries based on the environment variables?","answer":"The Python script in cudnn.py determines the location of cuDNN libraries by checking and using the environment variables like 'CUDNN_LIB_DIR', 'CUDNN_INCLUDE_DIR', and 'CUDNN_LIBRARY'. It filters and gathers paths from various sources, including CUDA related directories, to find the libraries and directories. Additionally, it checks for the existence of 'libcudnn' files with specific versions and sets the 'CUDNN_LIBRARY', 'CUDNN_LIB_DIR', and 'CUDNN_INCLUDE_DIR' variables accordingly. If these variables are not found, they are set to None.","id":136,"text":"My environmental variables is :\r\nCUDA_PATH=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\r\nCUDA_PATH_V10_0=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\r\nCUDNN_INCLUDE_DIR=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\include\r\nCUDNN_LIBRARY=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\lib\\x64\r\nNVCUDASAMPLES10_0_ROOT=C:\\ProgramData\\NVIDIA Corporation\\CUDA Samples\\v10.0\r\nNVCUDASAMPLES_ROOT=C:\\ProgramData\\NVIDIA Corporation\\CUDA Samples\\v10.0\r\nNVTOOLSEXT_PATH=C:\\Program Files\\NVIDIA Corporation\\NvToolsExt\\\r\nPath=E:\\SOFTWARE\\Visual Studio\\VC\\Tools\\MSVC\\14.11.25503\\bin\\HostX64\\x64;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\VC\\VCPackages;C:\\Program Files (x86)\\Microsoft SDKs\\TypeScript\\3.1;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\TestWindow;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\TeamFoundation\\Team Explorer;E:\\SOFTWARE\\Visual Studio\\MSBuild\\15.0\\bin\\Roslyn;E:\\SOFTWARE\\Visual Studio\\Team Tools\\Performance Tools\\x64;E:\\SOFTWARE\\Visual Studio\\Team Tools\\Performance Tools;E:\\SOFTWARE\\Visual Studio\\share\\Common\\VSPerfCollectionTools\\\\x64;E:\\SOFTWARE\\Visual Studio\\share\\Common\\VSPerfCollectionTools\\;C:\\Program Files (x86)\\Microsoft SDKs\\Windows\\v10.0A\\bin\\NETFX 4.6.1 Tools\\x64\\;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\10.0.17763.0\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x64;E:\\SOFTWARE\\Visual Studio\\\\MSBuild\\15.0\\bin;C:\\WINDOWS\\Microsoft.NET\\Framework64\\v4.0.30319;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\;E:\\SOFTWARE\\Visual Studio\\Common7\\Tools\\;E:\\SOFTWARE\\Anaconda;E:\\SOFTWARE\\Anaconda\\Library\\mingw-w64\\bin;E:\\SOFTWARE\\Anaconda\\Library\\usr\\bin;E:\\SOFTWARE\\Anaconda\\Library\\bin;E:\\SOFTWARE\\Anaconda\\Scripts;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\bin;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\libnvvp;C:\\WINDOWS\\system32;C:\\WINDOWS;C:\\WINDOWS\\System32\\Wbem;C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0\\;C:\\WINDOWS\\System32\\OpenSSH\\;E:\\\\Git\\cmd;E:\\SOFTWARE\\cmake\\bin;C:\\Program Files (x86)\\Windows Kits\\8.1\\Windows Performance Toolkit\\;C:\\Program Files (x86)\\NVIDIA Corporation\\PhysX\\Common;C:\\Program Files\\dotnet\\;C:\\Program Files\\NVIDIA Corporation\\NVIDIA NvDLISR;C:\\Windows\\Microsoft.NET\\Framework\\v4.0.30319;C:\\Windows\\Microsoft.NET\\Framework64\\v4.0.30319;C:\\Users\\11838\\AppData\\Local\\Microsoft\\WindowsApps;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\CMake\\bin;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\Ninja\r\n\r\nAND the cudnn.py shows:\r\nimport os\r\nimport glob\r\n\r\nfrom .env import IS_WINDOWS, IS_CONDA, CONDA_DIR, check_negative_env_flag, gather_paths, lib_paths_from_base\r\nfrom .cuda import USE_CUDA, CUDA_HOME\r\n\r\n\r\nUSE_CUDNN = False\r\nCUDNN_LIB_DIR = None\r\nCUDNN_INCLUDE_DIR = None\r\nCUDNN_LIBRARY = None\r\nWITH_STATIC_CUDNN = os.getenv(\"USE_STATIC_CUDNN\")\r\n\r\nif USE_CUDA and not check_negative_env_flag('USE_CUDNN'):\r\n    lib_paths = list(filter(bool, [\r\n        os.getenv('CUDNN_LIB_DIR')\r\n    ] + lib_paths_from_base(CUDA_HOME) + [\r\n        '/usr/lib/x86_64-linux-gnu/',\r\n        '/usr/lib/powerpc64le-linux-gnu/',\r\n        '/usr/lib/aarch64-linux-gnu/',\r\n    ] + gather_paths([\r\n        'LIBRARY_PATH',\r\n    ]) + gather_paths([\r\n        'LD_LIBRARY_PATH',\r\n    ])))\r\n    include_paths = list(filter(bool, [\r\n        os.getenv('CUDNN_INCLUDE_DIR'),\r\n        os.path.join(CUDA_HOME, 'include'),\r\n        '/usr/include/',\r\n    ] + gather_paths([\r\n        'CPATH',\r\n        'C_INCLUDE_PATH',\r\n        'CPLUS_INCLUDE_PATH',\r\n    ])))\r\n    # Add CUDA related dirs to candidate list\r\n    if IS_CONDA:\r\n        lib_paths.append(os.path.join(CONDA_DIR, 'lib'))\r\n        include_paths.append(os.path.join(CONDA_DIR, 'include'))\r\n    for path in include_paths:\r\n        if path is None or not os.path.exists(path):\r\n            continue\r\n        include_file_path = os.path.join(path, 'cudnn.h')\r\n        CUDNN_INCLUDE_VERSION = None\r\n        if os.path.exists(include_file_path):\r\n            CUDNN_INCLUDE_DIR = path\r\n            with open(include_file_path) as f:\r\n                for line in f:\r\n                    if \"#define CUDNN_MAJOR\" in line:\r\n                        CUDNN_INCLUDE_VERSION = int(line.split()[-1])\r\n                        break\r\n            if CUDNN_INCLUDE_VERSION is None:\r\n                raise AssertionError(\"Could not find #define CUDNN_MAJOR in \" + include_file_path)\r\n            break\r\n\r\n    if CUDNN_INCLUDE_VERSION is None:\r\n        pass\r\n\r\n    # Check for standalone cuDNN libraries\r\n    if CUDNN_INCLUDE_DIR is not None:\r\n        cudnn_path = os.path.join(os.path.dirname(CUDNN_INCLUDE_DIR))\r\n        cudnn_lib_paths = lib_paths_from_base(cudnn_path)\r\n        lib_paths.extend(cudnn_lib_paths)\r\n\r\n    for path in lib_paths:\r\n        if path is None or not os.path.exists(path):\r\n            continue\r\n        if IS_WINDOWS:\r\n            library = os.path.join(path, 'cudnn.lib')\r\n            if os.path.exists(library):\r\n[Truncated]\n                search_name = 'libcudnn*' + str(CUDNN_INCLUDE_VERSION) + \"*\"\r\n            libraries = sorted(glob.glob(os.path.join(path, search_name)))\r\n            if libraries:\r\n                CUDNN_LIBRARY = libraries[0]\r\n                CUDNN_LIB_DIR = path\r\n                break\r\n    # Specifying the library directly will overwrite the lib directory\r\n    library = os.getenv('CUDNN_LIBRARY')\r\n    if library is not None and os.path.exists(library):\r\n        CUDNN_LIBRARY = library\r\n        CUDNN_LIB_DIR = os.path.dirname(CUDNN_LIBRARY)\r\n\r\n    if not all([CUDNN_LIBRARY, CUDNN_LIB_DIR, CUDNN_INCLUDE_DIR]):\r\n        CUDNN_LIBRARY = CUDNN_LIB_DIR = CUDNN_INCLUDE_DIR = None\r\n    else:\r\n        real_cudnn_library = os.path.realpath(CUDNN_LIBRARY)\r\n        real_cudnn_lib_dir = os.path.realpath(CUDNN_LIB_DIR)\r\n        assert os.path.dirname(real_cudnn_library) == real_cudnn_lib_dir, (\r\n            'cudnn library and lib_dir must agree')\r\n        USE_CUDNN = True","context":"<issue_start><issue_comment>Title: Can't open x64.obj while installing pytorch with win10 \nusername_0: E:\\SOFTWARE\\pytorch\\build\\caffe2\\torch\\lib\\libshm_windows\\shm.vcxproj()\r\n\r\nE:\\SOFTWARE\\pytorch\\build\\ALL_BUILD.vcxproj() - \r\n\r\nE:\\SOFTWARE\\pytorch\\build\\INSTALL.vcxproj() - \r\n\r\n\r\n\r\n\r\nE:\\SOFTWARE\\pytorch\\build\\INSTALL.vcxproj() (1) ->\r\nE:\\SOFTWARE\\pytorch\\build\\ALL_BUILD.vcxproj() (3) ->\r\nE:\\SOFTWARE\\pytorch\\build\\caffe2\\AlgorithmsTest.vcxproj() (4) ->\r\nE:\\SOFTWARE\\pytorch\\build\\caffe2\\caffe2_gpu.vcxproj() (22) ->\r\n(Link ) ->\r\n  C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\lib\\x64.obj : fatal error LNK1136:  [E:\\SOFTWARE\\pyt\r\norch\\build\\caffe2\\caffe2_gpu.vcxproj]\r\n\r\n    0 \r\n    1 \r\n\r\n 00:00:16.23\r\n\r\nE:\\SOFTWARE\\pytorch\\build>IF ERRORLEVEL 1 exit 1\r\nFailed to run 'tools\\build_pytorch_libs.bat --use-cuda --use-fbgemm --use-nnpack --use-mkldnn --use-qnnpack caffe2'\r\n\r\npytorch versionstable1.0\r\nosWindows 10 Pro Version 1803\r\npython version:3.7\r\nCUDA:10\r\ncudnn:7.4.1\r\nVisual Studio version: Microsoft Visual Studio 2017 Community Edition\r\n\r\nCan anyone help me solve this problem? Thanks !\n<issue_comment>username_1: Please provide the full log.\n<issue_comment>username_0: the full log\r\n\r\nBuilding wheel torch-1.0.0a0+df61437\r\nrunning install\r\nsetup.py::run()\r\nrunning build_deps\r\nsetup.py::build_deps::run()\r\n\r\nE:\\SOFTWARE\\pytorch>cd \"E:\\SOFTWARE\\pytorch\\tools\\\\..\"\r\n\r\nE:\\SOFTWARE\\pytorch>set PATH=\\bin;E:\\SOFTWARE\\Anaconda\\DLLs;E:\\SOFTWARE\\Visual Studio\\VC\\Tools\\MSVC\\14.11.25503\\bin\\HostX64\\x64;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\VC\\VCPackages;C:\\Program Files (x86)\\Microsoft SDKs\\TypeScript\\3.1;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\TestWindow;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\TeamFoundation\\Team Explorer;E:\\SOFTWARE\\Visual Studio\\MSBuild\\15.0\\bin\\Roslyn;E:\\SOFTWARE\\Visual Studio\\Team Tools\\Performance Tools\\x64;E:\\SOFTWARE\\Visual Studio\\Team Tools\\Performance Tools;E:\\SOFTWARE\\Visual Studio\\share\\Common\\VSPerfCollectionTools\\\\x64;E:\\SOFTWARE\\Visual Studio\\share\\Common\\VSPerfCollectionTools\\;C:\\Program Files (x86)\\Microsoft SDKs\\Windows\\v10.0A\\bin\\NETFX 4.6.1 Tools\\x64\\;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\10.0.17763.0\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x64;E:\\SOFTWARE\\Visual Studio\\\\MSBuild\\15.0\\bin;C:\\WINDOWS\\Microsoft.NET\\Framework64\\v4.0.30319;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\;E:\\SOFTWARE\\Visual Studio\\Common7\\Tools\\;E:\\SOFTWARE\\Anaconda;E:\\SOFTWARE\\Anaconda\\Library\\mingw-w64\\bin;E:\\SOFTWARE\\Anaconda\\Library\\usr\\bin;E:\\SOFTWARE\\Anaconda\\Library\\bin;E:\\SOFTWARE\\Anaconda\\Scripts;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\bin;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\libnvvp;C:\\WINDOWS\\system32;C:\\WINDOWS;C:\\WINDOWS\\System32\\Wbem;C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0\\;C:\\WINDOWS\\System32\\OpenSSH\\;E:\\\\Git\\cmd;E:\\SOFTWARE\\cmake\\bin;C:\\Program Files (x86)\\Windows Kits\\8.1\\Windows Performance Toolkit\\;C:\\Program Files (x86)\\NVIDIA Corporation\\PhysX\\Common;C:\\Program Files\\dotnet\\;C:\\Program Files\\NVIDIA Corporation\\NVIDIA NvDLISR;C:\\Windows\\Microsoft.NET\\Framework\\v4.0.30319;C:\\Windows\\Microsoft.NET\\Framework64\\v4.0.30319;C:\\Users\\11838\\AppData\\Local\\Microsoft\\WindowsApps;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\CMake\\bin;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\Ninja\r\n\r\nE:\\SOFTWARE\\pytorch>set BASE_DIR=E:/SOFTWARE/pytorch\r\n\r\nE:\\SOFTWARE\\pytorch>set TORCH_LIB_DIR=E:/SOFTWARE/pytorch/torch/lib\r\n\r\nE:\\SOFTWARE\\pytorch>set THIRD_PARTY_DIR=E:/SOFTWARE/pytorch/third_party\r\n\r\nE:\\SOFTWARE\\pytorch>set BASIC_C_FLAGS=\r\n\r\nE:\\SOFTWARE\\pytorch>set BASIC_CUDA_FLAGS=\r\n\r\nE:\\SOFTWARE\\pytorch>IF NOT DEFINED INSTALL_DIR (set \"INSTALL_DIR=E:/SOFTWARE/pytorch/torch/lib/tmp_install\" )  ELSE (set \"INSTALL_DIR=\\=/\" )\r\n\r\nE:\\SOFTWARE\\pytorch>set LDFLAGS=/LIBPATH:E:/SOFTWARE/pytorch/torch/lib/tmp_install/lib\r\n\r\nE:\\SOFTWARE\\pytorch>set C_FLAGS= /D_WIN32 /Z7 /EHa /DNOMINMAX\r\n\r\nE:\\SOFTWARE\\pytorch>set LINK_FLAGS=/DEBUG:FULL\r\n\r\nE:\\SOFTWARE\\pytorch>if not exist torch\\lib\\tmp_install mkdir torch\\lib\\tmp_install\r\n\r\nE:\\SOFTWARE\\pytorch>set /a USE_CUDA=0\r\n\r\nE:\\SOFTWARE\\pytorch>set /a USE_FBGEMM=1\r\n\r\nE:\\SOFTWARE\\pytorch>set /a USE_ROCM=0\r\n\r\nE:\\SOFTWARE\\pytorch>set /a USE_NNPACK=0\r\n\r\nE:\\SOFTWARE\\pytorch>set /a USE_QNNPACK=0\r\n\r\nE:\\SOFTWARE\\pytorch>set /a USE_GLOO_IBVERBS=0\r\n\r\nE:\\SOFTWARE\\pytorch>set /a USE_MKLDNN=0\r\n\r\nE:\\SOFTWARE\\pytorch>set _BUILD_ARGS=\r\n\r\nE:\\SOFTWARE\\pytorch>if \"--use-cuda\" == \"\" (goto :process_args_exit )\r\n\r\nE:\\SOFTWARE\\pytorch>if \"--use-cuda\" == \"--use-cuda\" (\r\nset /a USE_CUDA=1\r\n goto :process_args_processed\r\n)\r\n\r\nE:\\SOFTWARE\\pytorch>shift\r\n\r\nE:\\SOFTWARE\\pytorch>goto :process_args\r\n\r\nE:\\SOFTWARE\\pytorch>if \"--use-fbgemm\" == \"\" (goto :process_args_exit )\r\n\r\nE:\\SOFTWARE\\pytorch>if \"--use-fbgemm\" == \"--use-cuda\" (\r\nset /a USE_CUDA=1\r\n goto :process_args_processed\r\n)\r\n\r\nE:\\SOFTWARE\\pytorch>if \"--use-fbgemm\" == \"--use-fbgemm\" (\r\nset /a USE_FBGEMM=1\r\n goto :process_args_processed\r\n)\r\n\r\nE:\\SOFTWARE\\pytorch>shift\r\n\r\nE:\\SOFTWARE\\pytorch>goto :process_args\r\n\r\nE:\\SOFTWARE\\pytorch>if \"--use-nnpack\" == \"\" (goto :process_args_exit )\r\n\r\nE:\\SOFTWARE\\pytorch>if \"--use-nnpack\" == \"--use-cuda\" (\r\nset /a USE_CUDA=1\r\n goto :process_args_processed\r\n[Truncated]\nDone Building Project \"E:\\SOFTWARE\\pytorch\\build\\INSTALL.vcxproj\" (default targets) -- FAILED.\r\n\r\n\r\nBuild FAILED.\r\n\r\n\"E:\\SOFTWARE\\pytorch\\build\\INSTALL.vcxproj\" (default target) (1) ->\r\n\"E:\\SOFTWARE\\pytorch\\build\\ALL_BUILD.vcxproj\" (default target) (3) ->\r\n\"E:\\SOFTWARE\\pytorch\\build\\caffe2\\AlgorithmsTest.vcxproj\" (default target) (4) ->\r\n\"E:\\SOFTWARE\\pytorch\\build\\caffe2\\caffe2_gpu.vcxproj\" (default target) (22) ->\r\n(Link target) ->\r\n  LINK : fatal error LNK1181: C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\lib\\x64.obj [E:\\SOFTWA\r\nRE\\pytorch\\build\\caffe2\\caffe2_gpu.vcxproj]\r\n\r\n    0 Warning(s)\r\n    1 Error(s)\r\n\r\nTime Elapsed 00:00:22.22\r\n\r\nE:\\SOFTWARE\\pytorch\\build>IF ERRORLEVEL 1 exit 1\r\nFailed to run 'tools\\build_pytorch_libs.bat --use-cuda --use-fbgemm --use-nnpack --use-mkldnn --use-qnnpack caffe2'\n<issue_comment>username_1: The error is caused by the misconfiguration of the flag `CUDNN_LIBRARY`. Could you please run `tools/setup_helpers/cudnn.py` and see which code path it goes through?\n<issue_comment>username_0: My environmental variables is :\r\nCUDA_PATH=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\r\nCUDA_PATH_V10_0=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\r\nCUDNN_INCLUDE_DIR=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\include\r\nCUDNN_LIBRARY=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\lib\\x64\r\nNVCUDASAMPLES10_0_ROOT=C:\\ProgramData\\NVIDIA Corporation\\CUDA Samples\\v10.0\r\nNVCUDASAMPLES_ROOT=C:\\ProgramData\\NVIDIA Corporation\\CUDA Samples\\v10.0\r\nNVTOOLSEXT_PATH=C:\\Program Files\\NVIDIA Corporation\\NvToolsExt\\\r\nPath=E:\\SOFTWARE\\Visual Studio\\VC\\Tools\\MSVC\\14.11.25503\\bin\\HostX64\\x64;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\VC\\VCPackages;C:\\Program Files (x86)\\Microsoft SDKs\\TypeScript\\3.1;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\TestWindow;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\TeamFoundation\\Team Explorer;E:\\SOFTWARE\\Visual Studio\\MSBuild\\15.0\\bin\\Roslyn;E:\\SOFTWARE\\Visual Studio\\Team Tools\\Performance Tools\\x64;E:\\SOFTWARE\\Visual Studio\\Team Tools\\Performance Tools;E:\\SOFTWARE\\Visual Studio\\share\\Common\\VSPerfCollectionTools\\\\x64;E:\\SOFTWARE\\Visual Studio\\share\\Common\\VSPerfCollectionTools\\;C:\\Program Files (x86)\\Microsoft SDKs\\Windows\\v10.0A\\bin\\NETFX 4.6.1 Tools\\x64\\;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\10.0.17763.0\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x64;E:\\SOFTWARE\\Visual Studio\\\\MSBuild\\15.0\\bin;C:\\WINDOWS\\Microsoft.NET\\Framework64\\v4.0.30319;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\;E:\\SOFTWARE\\Visual Studio\\Common7\\Tools\\;E:\\SOFTWARE\\Anaconda;E:\\SOFTWARE\\Anaconda\\Library\\mingw-w64\\bin;E:\\SOFTWARE\\Anaconda\\Library\\usr\\bin;E:\\SOFTWARE\\Anaconda\\Library\\bin;E:\\SOFTWARE\\Anaconda\\Scripts;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\bin;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\libnvvp;C:\\WINDOWS\\system32;C:\\WINDOWS;C:\\WINDOWS\\System32\\Wbem;C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0\\;C:\\WINDOWS\\System32\\OpenSSH\\;E:\\\\Git\\cmd;E:\\SOFTWARE\\cmake\\bin;C:\\Program Files (x86)\\Windows Kits\\8.1\\Windows Performance Toolkit\\;C:\\Program Files (x86)\\NVIDIA Corporation\\PhysX\\Common;C:\\Program Files\\dotnet\\;C:\\Program Files\\NVIDIA Corporation\\NVIDIA NvDLISR;C:\\Windows\\Microsoft.NET\\Framework\\v4.0.30319;C:\\Windows\\Microsoft.NET\\Framework64\\v4.0.30319;C:\\Users\\11838\\AppData\\Local\\Microsoft\\WindowsApps;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\CMake\\bin;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\Ninja\r\n\r\nAND the cudnn.py shows:\r\nimport os\r\nimport glob\r\n\r\nfrom .env import IS_WINDOWS, IS_CONDA, CONDA_DIR, check_negative_env_flag, gather_paths, lib_paths_from_base\r\nfrom .cuda import USE_CUDA, CUDA_HOME\r\n\r\n\r\nUSE_CUDNN = False\r\nCUDNN_LIB_DIR = None\r\nCUDNN_INCLUDE_DIR = None\r\nCUDNN_LIBRARY = None\r\nWITH_STATIC_CUDNN = os.getenv(\"USE_STATIC_CUDNN\")\r\n\r\nif USE_CUDA and not check_negative_env_flag('USE_CUDNN'):\r\n    lib_paths = list(filter(bool, [\r\n        os.getenv('CUDNN_LIB_DIR')\r\n    ] + lib_paths_from_base(CUDA_HOME) + [\r\n        '/usr/lib/x86_64-linux-gnu/',\r\n        '/usr/lib/powerpc64le-linux-gnu/',\r\n        '/usr/lib/aarch64-linux-gnu/',\r\n    ] + gather_paths([\r\n        'LIBRARY_PATH',\r\n    ]) + gather_paths([\r\n        'LD_LIBRARY_PATH',\r\n    ])))\r\n    include_paths = list(filter(bool, [\r\n        os.getenv('CUDNN_INCLUDE_DIR'),\r\n        os.path.join(CUDA_HOME, 'include'),\r\n        '/usr/include/',\r\n    ] + gather_paths([\r\n        'CPATH',\r\n        'C_INCLUDE_PATH',\r\n        'CPLUS_INCLUDE_PATH',\r\n    ])))\r\n    # Add CUDA related dirs to candidate list\r\n    if IS_CONDA:\r\n        lib_paths.append(os.path.join(CONDA_DIR, 'lib'))\r\n        include_paths.append(os.path.join(CONDA_DIR, 'include'))\r\n    for path in include_paths:\r\n        if path is None or not os.path.exists(path):\r\n            continue\r\n        include_file_path = os.path.join(path, 'cudnn.h')\r\n        CUDNN_INCLUDE_VERSION = None\r\n        if os.path.exists(include_file_path):\r\n            CUDNN_INCLUDE_DIR = path\r\n            with open(include_file_path) as f:\r\n                for line in f:\r\n                    if \"#define CUDNN_MAJOR\" in line:\r\n                        CUDNN_INCLUDE_VERSION = int(line.split()[-1])\r\n                        break\r\n            if CUDNN_INCLUDE_VERSION is None:\r\n                raise AssertionError(\"Could not find #define CUDNN_MAJOR in \" + include_file_path)\r\n            break\r\n\r\n    if CUDNN_INCLUDE_VERSION is None:\r\n        pass\r\n\r\n    # Check for standalone cuDNN libraries\r\n    if CUDNN_INCLUDE_DIR is not None:\r\n        cudnn_path = os.path.join(os.path.dirname(CUDNN_INCLUDE_DIR))\r\n        cudnn_lib_paths = lib_paths_from_base(cudnn_path)\r\n        lib_paths.extend(cudnn_lib_paths)\r\n\r\n    for path in lib_paths:\r\n        if path is None or not os.path.exists(path):\r\n            continue\r\n        if IS_WINDOWS:\r\n            library = os.path.join(path, 'cudnn.lib')\r\n            if os.path.exists(library):\r\n[Truncated]\n                search_name = 'libcudnn*' + str(CUDNN_INCLUDE_VERSION) + \"*\"\r\n            libraries = sorted(glob.glob(os.path.join(path, search_name)))\r\n            if libraries:\r\n                CUDNN_LIBRARY = libraries[0]\r\n                CUDNN_LIB_DIR = path\r\n                break\r\n    # Specifying the library directly will overwrite the lib directory\r\n    library = os.getenv('CUDNN_LIBRARY')\r\n    if library is not None and os.path.exists(library):\r\n        CUDNN_LIBRARY = library\r\n        CUDNN_LIB_DIR = os.path.dirname(CUDNN_LIBRARY)\r\n\r\n    if not all([CUDNN_LIBRARY, CUDNN_LIB_DIR, CUDNN_INCLUDE_DIR]):\r\n        CUDNN_LIBRARY = CUDNN_LIB_DIR = CUDNN_INCLUDE_DIR = None\r\n    else:\r\n        real_cudnn_library = os.path.realpath(CUDNN_LIBRARY)\r\n        real_cudnn_lib_dir = os.path.realpath(CUDNN_LIB_DIR)\r\n        assert os.path.dirname(real_cudnn_library) == real_cudnn_lib_dir, (\r\n            'cudnn library and lib_dir must agree')\r\n        USE_CUDNN = True\n<issue_comment>username_1: Please remove `CUDNN_INCLUDE_DIR` and `CUDNN_LIBRARY` by using the following commands.\r\n```cmd\r\nset CUDNN_INCLUDE_DIR=\r\nset CUDNN_LIBRARY=\r\n```\n<issue_comment>username_0: It works! Thanks a lot!<issue_closed>","repo":"pytorch/pytorch","issue_id":391131022,"issue_number":15222}
{"question":"What caused the reduction in errors from 8 to 4 with the same message in the provided text?","answer":"The reduction in errors from 8 to 4 was caused by addressing the CUDA error related to too many resources requested for launch, as documented in the existing problem labeled as gh-8103.","id":140,"text":"Fewer errors than before, 4 instead of 8 with the same message:\r\n```\r\nRuntimeError: CUDA error: too many resources requested for launch\r\n```\r\nThis seems to be an existing problem: gh-8103.\r\n\r\n@username_2 could you have a look at this and tell us what you think?","context":"<issue_start><issue_comment>Title: [WIP] UpSample GPU Porting \nusername_0: resolves #16158\n<issue_comment>username_1: Fewer errors than before, 4 instead of 8 with the same message:\r\n```\r\nRuntimeError: CUDA error: too many resources requested for launch\r\n```\r\nThis seems to be an existing problem: gh-8103.\r\n\r\n@username_2 could you have a look at this and tell us what you think?\n<issue_comment>username_2: Looking at it very briefly, the `Jetson TX` referenced in the issue only has 256 cores.\r\n\r\nSo while the issue looks the same, I'd probably not expect it on the CI platforms.  FWIW, some recent CI tests in other issues are green.\n<issue_comment>username_2: @pytorchbot retest this please.\n<issue_comment>username_1: same failures.\n<issue_comment>username_2: Is this rebased on the latest master (you can also ask the bot to rebase)?\n<issue_comment>username_0: I rebased that yesterday ... also needed to resolve conflicts because 7 days ago upsample files were changed.\n<issue_comment>username_0: @username_2 do you have any idea about what could be this problem? or a way to debug that?\r\nalso it seems some jobs is taking a lot of time to build, for example, for some job the estimate time is 19h .. not sure if it the regular estimate ..\n<issue_comment>username_2: @username_0 If you can't reproduce it at home it seems hard to debug other than reading at the diffs again.  I can take a look on Monday, it's getting a bit late here.\n<issue_comment>username_2: Also, has anyone found a way to show the actual hardware used on the CI in detail?\n<issue_comment>username_0: @username_2 I am working in parallel on a paperspace environment .. it tooks a lot of time it is running with 8 cpu cores.\n<issue_comment>username_2: But have you ever been able to reproduce this issue on paperspace?\n<issue_comment>username_1: I can reproduce it locally: Arch Linux, CUDA 10.0, RTX2070 GPU. Given the CI failures, I think it should be reproducible for multiple CUDA and GPU versions. I just haven't worked on any CUDA code before, and am short on time, so I'd rather not dig too deep.\n<issue_comment>username_0: not yet .. my last building was with a my previous commit ... I will work again in this task in some minutes :)\n<issue_comment>username_1: @username_0 your previous commit had the same failure though (except for the less clear exception message), and it seems quite reproducible. So I think you'll see it now.\n<issue_comment>username_2: Ah, thanks.  @username_0, then I guess just compiling with \"DEBUG=1\" and stepping through the offending code with gdb may be the fastest way.\n<issue_comment>username_0: @username_2 thanks! I will try that!\n<issue_comment>username_0: it seems just UpSampleBicubic2d is using upsample_get_value_bounded (https://github.com/pytorch/pytorch/pull/19630/files#diff-5092da792c30694ee4adf0d0ae2a37c6R171) and upsample_increment_value_bounded (https://github.com/pytorch/pytorch/pull/19630/files#diff-5092da792c30694ee4adf0d0ae2a37c6R191)\r\n\r\nmaybe the problem could be inside one of these functions ... maybe related to the order of indexes (x, y) ...\r\n\r\nbut the problem seems to be related to cuda block/threads ... so not sure if it is really related to these functions.\n<issue_comment>username_2: The new code uses far more registers than the existing one. I verified that the both versions actually call the offending test case with `1024` in `blockDim`.  So it's very likely a register issue.\r\n\r\n`Existing` uses `64` registers, which seems to be optimal or my card:\r\n\r\n```\r\nptxas info    : 77696 bytes gmem, 72 bytes cmem[3]\r\nptxas info    : Compiling entry function '_Z23bicubic_interp2d_kerneliddb15THCDeviceTensorIdLi4Ei16DefaultPtrTraitsES1_' for 'sm_61'\r\nptxas info    : Function properties for _Z23bicubic_interp2d_kerneliddb15THCDeviceTensorIdLi4Ei16DefaultPtrTraitsES1_\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 64 registers, 432 bytes cmem[0], 24 bytes cmem[2]\r\n```\r\n\r\n`New` uses `124` registers, which is too much for my card:\r\n\r\n```\r\nptxas info    : 77696 bytes gmem, 72 bytes cmem[3]\r\nptxas info    : Compiling entry function '_ZN2at6native76_GLOBAL__N__52_tmpxft_0000626d_00000000_6_UpSampleBicubic2d_cpp1_ii_b4c1e1f328upsample_bicubic2d_out_frameElddbNS_20PackedTensorAccessorIdLm4ENS_16DefaultPtrTraitsElEES4_' for 'sm_61'\r\nptxas info    : Function properties for _ZN2at6native76_GLOBAL__N__52_tmpxft_0000626d_00000000_6_UpSampleBicubic2d_cpp1_ii_b4c1e1f328upsample_bicubic2d_out_frameElddbNS_20PackedTensorAccessorIdLm4ENS_16DefaultPtrTraitsElEES4_\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 124 registers, 496 bytes cmem[0], 24 bytes cmem[2]\r\n```\r\n\r\nWith `__launch_bounds__(1024)`, the code again uses `64` registers.\r\n\r\nIf you use `C10_LAUNCH_BOUNDS_1(1024)` **for both kernels**, the tests pass here.\r\n\r\n\r\nNow *why* is the regcount higher in the new code? It could be `PackedTensorAccessor`, it could be the fact that many instances of `int` have been changed to `int64_t`. :)\r\n\r\nYou could experiment or just use the launch bounds.  Other code in `native/cuda` seems to use lower bounds for `blockDim`, too.  1024 seems to be an outlier.\n<issue_comment>username_0: @username_2 \r\n\r\nit seems it worked locally! thank you so much!  I really appreciate that!\n<issue_comment>username_0: thanks @username_1 and @username_2 for all the help!\r\n\r\n@username_3 it is done for review!\n<issue_comment>username_3: CircleCI runs on AWS, and the I'm fairly sure we've got one of the g3 types, either g3.4xlarge or g3.8xlarge\n<issue_comment>username_3: cc @ngimel, if you want to take a look at this (I'm planning to do a review too)\n<issue_comment>username_0: @username_4 could it be addressed in another PR?\n<issue_comment>username_4: @username_0 Sure, it's definitely better to rename both cpu & gpu impls together in a separate PR. Just want to make sure we have the correct one after porting.\n<issue_comment>username_3: Just as a clarification: I'm OK with having changes which are improvements over the old kernels come in latter patches.\n<issue_comment>username_3: I finished reviewing one of the kernels, and did a cursory scan of the others. In general the patch looks like it's in good shape. I'd suggest that we fix up the major correctness issues (PackedTensorAccessor ref, and int32 versus int64 indexing) and anything small that is easy to fix (for example, double zeroing the array), and leave other improvements for follow ups. Should be ready to land soon!\n<issue_comment>username_0: thanks so much @username_3!\r\nI will let you know when it is ready again for a new review! thanks!\n<issue_comment>username_0: @username_2 @username_3 \r\nnot sure but the errors on CI seems to be related to jenkins ... it seems all these jobs that failed (for building) ran by 01h 01min ... not sure if it is a coincidence ...\n<issue_comment>username_1: @username_0 indeed it looks like all jobs were aborted at the same time. no obvious issues in the build log related to your code. Comparing e.g. the `cuda9-cudnn7` build with a successful one from another PR, it takes 1hr 14min there and your build is about at the place where the other one is after an hour.\r\n\r\nI suggest to just push a new commit to rebuild. Probably a temporary CI hiccup.\n<issue_comment>username_3: I accidentally rebooted Jenkins yesterday which is the likely cause, my apologies.\r\n\r\n@pytorchbot retest this please\n<issue_comment>username_0: @username_3 @username_2 @username_1 \r\n\r\nall tests passed except `pr/caffe2-py2-cuda9.0-cudnn7-windows-build`:\r\n \r\n```\r\n14:43:49 Build timed out (after 180 minutes). Marking the build as failed.\r\n14:43:49 Build was aborted\r\n14:43:49 [BFA] Scanning build for known causes...\r\n14:43:49 [BFA] No failure causes found\r\n14:43:49 [BFA] Done. 0s\r\n14:43:49 Finished: FAILURE\r\n```\r\n\r\nnot sure if this timeout means that the code now is slower.\r\n\r\nwhat do you think? do you have any suggestion?\n<issue_comment>username_3: Sometimes the Windows build flakes out like that. It didn't timeout while running a relevant test, so I judge it to be not your problem.\n<issue_comment>username_3: The launch bounds logic is wrong, but I acknowledge that this is a big patch already; just fix it in a follow up. I am going to go ahead and land this.\n<issue_comment>username_0: sounds good @username_3 thanks!","repo":"pytorch/pytorch","issue_id":436372857,"issue_number":19630}
{"question":"How many cores does the 'Jetson TX' referenced in the issue have?","answer":"256 cores","id":141,"text":"Looking at it very briefly, the `Jetson TX` referenced in the issue only has 256 cores.\r\n\r\nSo while the issue looks the same, I'd probably not expect it on the CI platforms.  FWIW, some recent CI tests in other issues are green.","context":"<issue_start><issue_comment>Title: [WIP] UpSample GPU Porting \nusername_0: resolves #16158\n<issue_comment>username_1: Fewer errors than before, 4 instead of 8 with the same message:\r\n```\r\nRuntimeError: CUDA error: too many resources requested for launch\r\n```\r\nThis seems to be an existing problem: gh-8103.\r\n\r\n@username_2 could you have a look at this and tell us what you think?\n<issue_comment>username_2: Looking at it very briefly, the `Jetson TX` referenced in the issue only has 256 cores.\r\n\r\nSo while the issue looks the same, I'd probably not expect it on the CI platforms.  FWIW, some recent CI tests in other issues are green.\n<issue_comment>username_2: @pytorchbot retest this please.\n<issue_comment>username_1: same failures.\n<issue_comment>username_2: Is this rebased on the latest master (you can also ask the bot to rebase)?\n<issue_comment>username_0: I rebased that yesterday ... also needed to resolve conflicts because 7 days ago upsample files were changed.\n<issue_comment>username_0: @username_2 do you have any idea about what could be this problem? or a way to debug that?\r\nalso it seems some jobs is taking a lot of time to build, for example, for some job the estimate time is 19h .. not sure if it the regular estimate ..\n<issue_comment>username_2: @username_0 If you can't reproduce it at home it seems hard to debug other than reading at the diffs again.  I can take a look on Monday, it's getting a bit late here.\n<issue_comment>username_2: Also, has anyone found a way to show the actual hardware used on the CI in detail?\n<issue_comment>username_0: @username_2 I am working in parallel on a paperspace environment .. it tooks a lot of time it is running with 8 cpu cores.\n<issue_comment>username_2: But have you ever been able to reproduce this issue on paperspace?\n<issue_comment>username_1: I can reproduce it locally: Arch Linux, CUDA 10.0, RTX2070 GPU. Given the CI failures, I think it should be reproducible for multiple CUDA and GPU versions. I just haven't worked on any CUDA code before, and am short on time, so I'd rather not dig too deep.\n<issue_comment>username_0: not yet .. my last building was with a my previous commit ... I will work again in this task in some minutes :)\n<issue_comment>username_1: @username_0 your previous commit had the same failure though (except for the less clear exception message), and it seems quite reproducible. So I think you'll see it now.\n<issue_comment>username_2: Ah, thanks.  @username_0, then I guess just compiling with \"DEBUG=1\" and stepping through the offending code with gdb may be the fastest way.\n<issue_comment>username_0: @username_2 thanks! I will try that!\n<issue_comment>username_0: it seems just UpSampleBicubic2d is using upsample_get_value_bounded (https://github.com/pytorch/pytorch/pull/19630/files#diff-5092da792c30694ee4adf0d0ae2a37c6R171) and upsample_increment_value_bounded (https://github.com/pytorch/pytorch/pull/19630/files#diff-5092da792c30694ee4adf0d0ae2a37c6R191)\r\n\r\nmaybe the problem could be inside one of these functions ... maybe related to the order of indexes (x, y) ...\r\n\r\nbut the problem seems to be related to cuda block/threads ... so not sure if it is really related to these functions.\n<issue_comment>username_2: The new code uses far more registers than the existing one. I verified that the both versions actually call the offending test case with `1024` in `blockDim`.  So it's very likely a register issue.\r\n\r\n`Existing` uses `64` registers, which seems to be optimal or my card:\r\n\r\n```\r\nptxas info    : 77696 bytes gmem, 72 bytes cmem[3]\r\nptxas info    : Compiling entry function '_Z23bicubic_interp2d_kerneliddb15THCDeviceTensorIdLi4Ei16DefaultPtrTraitsES1_' for 'sm_61'\r\nptxas info    : Function properties for _Z23bicubic_interp2d_kerneliddb15THCDeviceTensorIdLi4Ei16DefaultPtrTraitsES1_\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 64 registers, 432 bytes cmem[0], 24 bytes cmem[2]\r\n```\r\n\r\n`New` uses `124` registers, which is too much for my card:\r\n\r\n```\r\nptxas info    : 77696 bytes gmem, 72 bytes cmem[3]\r\nptxas info    : Compiling entry function '_ZN2at6native76_GLOBAL__N__52_tmpxft_0000626d_00000000_6_UpSampleBicubic2d_cpp1_ii_b4c1e1f328upsample_bicubic2d_out_frameElddbNS_20PackedTensorAccessorIdLm4ENS_16DefaultPtrTraitsElEES4_' for 'sm_61'\r\nptxas info    : Function properties for _ZN2at6native76_GLOBAL__N__52_tmpxft_0000626d_00000000_6_UpSampleBicubic2d_cpp1_ii_b4c1e1f328upsample_bicubic2d_out_frameElddbNS_20PackedTensorAccessorIdLm4ENS_16DefaultPtrTraitsElEES4_\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 124 registers, 496 bytes cmem[0], 24 bytes cmem[2]\r\n```\r\n\r\nWith `__launch_bounds__(1024)`, the code again uses `64` registers.\r\n\r\nIf you use `C10_LAUNCH_BOUNDS_1(1024)` **for both kernels**, the tests pass here.\r\n\r\n\r\nNow *why* is the regcount higher in the new code? It could be `PackedTensorAccessor`, it could be the fact that many instances of `int` have been changed to `int64_t`. :)\r\n\r\nYou could experiment or just use the launch bounds.  Other code in `native/cuda` seems to use lower bounds for `blockDim`, too.  1024 seems to be an outlier.\n<issue_comment>username_0: @username_2 \r\n\r\nit seems it worked locally! thank you so much!  I really appreciate that!\n<issue_comment>username_0: thanks @username_1 and @username_2 for all the help!\r\n\r\n@username_3 it is done for review!\n<issue_comment>username_3: CircleCI runs on AWS, and the I'm fairly sure we've got one of the g3 types, either g3.4xlarge or g3.8xlarge\n<issue_comment>username_3: cc @ngimel, if you want to take a look at this (I'm planning to do a review too)\n<issue_comment>username_0: @username_4 could it be addressed in another PR?\n<issue_comment>username_4: @username_0 Sure, it's definitely better to rename both cpu & gpu impls together in a separate PR. Just want to make sure we have the correct one after porting.\n<issue_comment>username_3: Just as a clarification: I'm OK with having changes which are improvements over the old kernels come in latter patches.\n<issue_comment>username_3: I finished reviewing one of the kernels, and did a cursory scan of the others. In general the patch looks like it's in good shape. I'd suggest that we fix up the major correctness issues (PackedTensorAccessor ref, and int32 versus int64 indexing) and anything small that is easy to fix (for example, double zeroing the array), and leave other improvements for follow ups. Should be ready to land soon!\n<issue_comment>username_0: thanks so much @username_3!\r\nI will let you know when it is ready again for a new review! thanks!\n<issue_comment>username_0: @username_2 @username_3 \r\nnot sure but the errors on CI seems to be related to jenkins ... it seems all these jobs that failed (for building) ran by 01h 01min ... not sure if it is a coincidence ...\n<issue_comment>username_1: @username_0 indeed it looks like all jobs were aborted at the same time. no obvious issues in the build log related to your code. Comparing e.g. the `cuda9-cudnn7` build with a successful one from another PR, it takes 1hr 14min there and your build is about at the place where the other one is after an hour.\r\n\r\nI suggest to just push a new commit to rebuild. Probably a temporary CI hiccup.\n<issue_comment>username_3: I accidentally rebooted Jenkins yesterday which is the likely cause, my apologies.\r\n\r\n@pytorchbot retest this please\n<issue_comment>username_0: @username_3 @username_2 @username_1 \r\n\r\nall tests passed except `pr/caffe2-py2-cuda9.0-cudnn7-windows-build`:\r\n \r\n```\r\n14:43:49 Build timed out (after 180 minutes). Marking the build as failed.\r\n14:43:49 Build was aborted\r\n14:43:49 [BFA] Scanning build for known causes...\r\n14:43:49 [BFA] No failure causes found\r\n14:43:49 [BFA] Done. 0s\r\n14:43:49 Finished: FAILURE\r\n```\r\n\r\nnot sure if this timeout means that the code now is slower.\r\n\r\nwhat do you think? do you have any suggestion?\n<issue_comment>username_3: Sometimes the Windows build flakes out like that. It didn't timeout while running a relevant test, so I judge it to be not your problem.\n<issue_comment>username_3: The launch bounds logic is wrong, but I acknowledge that this is a big patch already; just fix it in a follow up. I am going to go ahead and land this.\n<issue_comment>username_0: sounds good @username_3 thanks!","repo":"pytorch/pytorch","issue_id":436372857,"issue_number":19630}
{"question":"What could be the potential reasons for jobs taking a long time to build, such as the example of a job with an estimated time of 19 hours?","answer":"The potential reasons for jobs taking a long time to build could include issues with slow job execution due to heavy resource usage, misconfigurations in the build process leading to inefficiencies, or bottleneck in the infrastructure causing delays.","id":146,"text":"@username_2 do you have any idea about what could be this problem? or a way to debug that?\r\nalso it seems some jobs is taking a lot of time to build, for example, for some job the estimate time is 19h .. not sure if it the regular estimate ..","context":"<issue_start><issue_comment>Title: [WIP] UpSample GPU Porting \nusername_0: resolves #16158\n<issue_comment>username_1: Fewer errors than before, 4 instead of 8 with the same message:\r\n```\r\nRuntimeError: CUDA error: too many resources requested for launch\r\n```\r\nThis seems to be an existing problem: gh-8103.\r\n\r\n@username_2 could you have a look at this and tell us what you think?\n<issue_comment>username_2: Looking at it very briefly, the `Jetson TX` referenced in the issue only has 256 cores.\r\n\r\nSo while the issue looks the same, I'd probably not expect it on the CI platforms.  FWIW, some recent CI tests in other issues are green.\n<issue_comment>username_2: @pytorchbot retest this please.\n<issue_comment>username_1: same failures.\n<issue_comment>username_2: Is this rebased on the latest master (you can also ask the bot to rebase)?\n<issue_comment>username_0: I rebased that yesterday ... also needed to resolve conflicts because 7 days ago upsample files were changed.\n<issue_comment>username_0: @username_2 do you have any idea about what could be this problem? or a way to debug that?\r\nalso it seems some jobs is taking a lot of time to build, for example, for some job the estimate time is 19h .. not sure if it the regular estimate ..\n<issue_comment>username_2: @username_0 If you can't reproduce it at home it seems hard to debug other than reading at the diffs again.  I can take a look on Monday, it's getting a bit late here.\n<issue_comment>username_2: Also, has anyone found a way to show the actual hardware used on the CI in detail?\n<issue_comment>username_0: @username_2 I am working in parallel on a paperspace environment .. it tooks a lot of time it is running with 8 cpu cores.\n<issue_comment>username_2: But have you ever been able to reproduce this issue on paperspace?\n<issue_comment>username_1: I can reproduce it locally: Arch Linux, CUDA 10.0, RTX2070 GPU. Given the CI failures, I think it should be reproducible for multiple CUDA and GPU versions. I just haven't worked on any CUDA code before, and am short on time, so I'd rather not dig too deep.\n<issue_comment>username_0: not yet .. my last building was with a my previous commit ... I will work again in this task in some minutes :)\n<issue_comment>username_1: @username_0 your previous commit had the same failure though (except for the less clear exception message), and it seems quite reproducible. So I think you'll see it now.\n<issue_comment>username_2: Ah, thanks.  @username_0, then I guess just compiling with \"DEBUG=1\" and stepping through the offending code with gdb may be the fastest way.\n<issue_comment>username_0: @username_2 thanks! I will try that!\n<issue_comment>username_0: it seems just UpSampleBicubic2d is using upsample_get_value_bounded (https://github.com/pytorch/pytorch/pull/19630/files#diff-5092da792c30694ee4adf0d0ae2a37c6R171) and upsample_increment_value_bounded (https://github.com/pytorch/pytorch/pull/19630/files#diff-5092da792c30694ee4adf0d0ae2a37c6R191)\r\n\r\nmaybe the problem could be inside one of these functions ... maybe related to the order of indexes (x, y) ...\r\n\r\nbut the problem seems to be related to cuda block/threads ... so not sure if it is really related to these functions.\n<issue_comment>username_2: The new code uses far more registers than the existing one. I verified that the both versions actually call the offending test case with `1024` in `blockDim`.  So it's very likely a register issue.\r\n\r\n`Existing` uses `64` registers, which seems to be optimal or my card:\r\n\r\n```\r\nptxas info    : 77696 bytes gmem, 72 bytes cmem[3]\r\nptxas info    : Compiling entry function '_Z23bicubic_interp2d_kerneliddb15THCDeviceTensorIdLi4Ei16DefaultPtrTraitsES1_' for 'sm_61'\r\nptxas info    : Function properties for _Z23bicubic_interp2d_kerneliddb15THCDeviceTensorIdLi4Ei16DefaultPtrTraitsES1_\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 64 registers, 432 bytes cmem[0], 24 bytes cmem[2]\r\n```\r\n\r\n`New` uses `124` registers, which is too much for my card:\r\n\r\n```\r\nptxas info    : 77696 bytes gmem, 72 bytes cmem[3]\r\nptxas info    : Compiling entry function '_ZN2at6native76_GLOBAL__N__52_tmpxft_0000626d_00000000_6_UpSampleBicubic2d_cpp1_ii_b4c1e1f328upsample_bicubic2d_out_frameElddbNS_20PackedTensorAccessorIdLm4ENS_16DefaultPtrTraitsElEES4_' for 'sm_61'\r\nptxas info    : Function properties for _ZN2at6native76_GLOBAL__N__52_tmpxft_0000626d_00000000_6_UpSampleBicubic2d_cpp1_ii_b4c1e1f328upsample_bicubic2d_out_frameElddbNS_20PackedTensorAccessorIdLm4ENS_16DefaultPtrTraitsElEES4_\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 124 registers, 496 bytes cmem[0], 24 bytes cmem[2]\r\n```\r\n\r\nWith `__launch_bounds__(1024)`, the code again uses `64` registers.\r\n\r\nIf you use `C10_LAUNCH_BOUNDS_1(1024)` **for both kernels**, the tests pass here.\r\n\r\n\r\nNow *why* is the regcount higher in the new code? It could be `PackedTensorAccessor`, it could be the fact that many instances of `int` have been changed to `int64_t`. :)\r\n\r\nYou could experiment or just use the launch bounds.  Other code in `native/cuda` seems to use lower bounds for `blockDim`, too.  1024 seems to be an outlier.\n<issue_comment>username_0: @username_2 \r\n\r\nit seems it worked locally! thank you so much!  I really appreciate that!\n<issue_comment>username_0: thanks @username_1 and @username_2 for all the help!\r\n\r\n@username_3 it is done for review!\n<issue_comment>username_3: CircleCI runs on AWS, and the I'm fairly sure we've got one of the g3 types, either g3.4xlarge or g3.8xlarge\n<issue_comment>username_3: cc @ngimel, if you want to take a look at this (I'm planning to do a review too)\n<issue_comment>username_0: @username_4 could it be addressed in another PR?\n<issue_comment>username_4: @username_0 Sure, it's definitely better to rename both cpu & gpu impls together in a separate PR. Just want to make sure we have the correct one after porting.\n<issue_comment>username_3: Just as a clarification: I'm OK with having changes which are improvements over the old kernels come in latter patches.\n<issue_comment>username_3: I finished reviewing one of the kernels, and did a cursory scan of the others. In general the patch looks like it's in good shape. I'd suggest that we fix up the major correctness issues (PackedTensorAccessor ref, and int32 versus int64 indexing) and anything small that is easy to fix (for example, double zeroing the array), and leave other improvements for follow ups. Should be ready to land soon!\n<issue_comment>username_0: thanks so much @username_3!\r\nI will let you know when it is ready again for a new review! thanks!\n<issue_comment>username_0: @username_2 @username_3 \r\nnot sure but the errors on CI seems to be related to jenkins ... it seems all these jobs that failed (for building) ran by 01h 01min ... not sure if it is a coincidence ...\n<issue_comment>username_1: @username_0 indeed it looks like all jobs were aborted at the same time. no obvious issues in the build log related to your code. Comparing e.g. the `cuda9-cudnn7` build with a successful one from another PR, it takes 1hr 14min there and your build is about at the place where the other one is after an hour.\r\n\r\nI suggest to just push a new commit to rebuild. Probably a temporary CI hiccup.\n<issue_comment>username_3: I accidentally rebooted Jenkins yesterday which is the likely cause, my apologies.\r\n\r\n@pytorchbot retest this please\n<issue_comment>username_0: @username_3 @username_2 @username_1 \r\n\r\nall tests passed except `pr/caffe2-py2-cuda9.0-cudnn7-windows-build`:\r\n \r\n```\r\n14:43:49 Build timed out (after 180 minutes). Marking the build as failed.\r\n14:43:49 Build was aborted\r\n14:43:49 [BFA] Scanning build for known causes...\r\n14:43:49 [BFA] No failure causes found\r\n14:43:49 [BFA] Done. 0s\r\n14:43:49 Finished: FAILURE\r\n```\r\n\r\nnot sure if this timeout means that the code now is slower.\r\n\r\nwhat do you think? do you have any suggestion?\n<issue_comment>username_3: Sometimes the Windows build flakes out like that. It didn't timeout while running a relevant test, so I judge it to be not your problem.\n<issue_comment>username_3: The launch bounds logic is wrong, but I acknowledge that this is a big patch already; just fix it in a follow up. I am going to go ahead and land this.\n<issue_comment>username_0: sounds good @username_3 thanks!","repo":"pytorch/pytorch","issue_id":436372857,"issue_number":19630}
{"question":"Why is it challenging to debug the issue mentioned in the text?","answer":"It is difficult to debug the issue because it cannot be reproduced at home, making it hard to troubleshoot effectively without the ability to replicate the problem.","id":147,"text":"@username_0 If you can't reproduce it at home it seems hard to debug other than reading at the diffs again.  I can take a look on Monday, it's getting a bit late here.","context":"<issue_start><issue_comment>Title: [WIP] UpSample GPU Porting \nusername_0: resolves #16158\n<issue_comment>username_1: Fewer errors than before, 4 instead of 8 with the same message:\r\n```\r\nRuntimeError: CUDA error: too many resources requested for launch\r\n```\r\nThis seems to be an existing problem: gh-8103.\r\n\r\n@username_2 could you have a look at this and tell us what you think?\n<issue_comment>username_2: Looking at it very briefly, the `Jetson TX` referenced in the issue only has 256 cores.\r\n\r\nSo while the issue looks the same, I'd probably not expect it on the CI platforms.  FWIW, some recent CI tests in other issues are green.\n<issue_comment>username_2: @pytorchbot retest this please.\n<issue_comment>username_1: same failures.\n<issue_comment>username_2: Is this rebased on the latest master (you can also ask the bot to rebase)?\n<issue_comment>username_0: I rebased that yesterday ... also needed to resolve conflicts because 7 days ago upsample files were changed.\n<issue_comment>username_0: @username_2 do you have any idea about what could be this problem? or a way to debug that?\r\nalso it seems some jobs is taking a lot of time to build, for example, for some job the estimate time is 19h .. not sure if it the regular estimate ..\n<issue_comment>username_2: @username_0 If you can't reproduce it at home it seems hard to debug other than reading at the diffs again.  I can take a look on Monday, it's getting a bit late here.\n<issue_comment>username_2: Also, has anyone found a way to show the actual hardware used on the CI in detail?\n<issue_comment>username_0: @username_2 I am working in parallel on a paperspace environment .. it tooks a lot of time it is running with 8 cpu cores.\n<issue_comment>username_2: But have you ever been able to reproduce this issue on paperspace?\n<issue_comment>username_1: I can reproduce it locally: Arch Linux, CUDA 10.0, RTX2070 GPU. Given the CI failures, I think it should be reproducible for multiple CUDA and GPU versions. I just haven't worked on any CUDA code before, and am short on time, so I'd rather not dig too deep.\n<issue_comment>username_0: not yet .. my last building was with a my previous commit ... I will work again in this task in some minutes :)\n<issue_comment>username_1: @username_0 your previous commit had the same failure though (except for the less clear exception message), and it seems quite reproducible. So I think you'll see it now.\n<issue_comment>username_2: Ah, thanks.  @username_0, then I guess just compiling with \"DEBUG=1\" and stepping through the offending code with gdb may be the fastest way.\n<issue_comment>username_0: @username_2 thanks! I will try that!\n<issue_comment>username_0: it seems just UpSampleBicubic2d is using upsample_get_value_bounded (https://github.com/pytorch/pytorch/pull/19630/files#diff-5092da792c30694ee4adf0d0ae2a37c6R171) and upsample_increment_value_bounded (https://github.com/pytorch/pytorch/pull/19630/files#diff-5092da792c30694ee4adf0d0ae2a37c6R191)\r\n\r\nmaybe the problem could be inside one of these functions ... maybe related to the order of indexes (x, y) ...\r\n\r\nbut the problem seems to be related to cuda block/threads ... so not sure if it is really related to these functions.\n<issue_comment>username_2: The new code uses far more registers than the existing one. I verified that the both versions actually call the offending test case with `1024` in `blockDim`.  So it's very likely a register issue.\r\n\r\n`Existing` uses `64` registers, which seems to be optimal or my card:\r\n\r\n```\r\nptxas info    : 77696 bytes gmem, 72 bytes cmem[3]\r\nptxas info    : Compiling entry function '_Z23bicubic_interp2d_kerneliddb15THCDeviceTensorIdLi4Ei16DefaultPtrTraitsES1_' for 'sm_61'\r\nptxas info    : Function properties for _Z23bicubic_interp2d_kerneliddb15THCDeviceTensorIdLi4Ei16DefaultPtrTraitsES1_\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 64 registers, 432 bytes cmem[0], 24 bytes cmem[2]\r\n```\r\n\r\n`New` uses `124` registers, which is too much for my card:\r\n\r\n```\r\nptxas info    : 77696 bytes gmem, 72 bytes cmem[3]\r\nptxas info    : Compiling entry function '_ZN2at6native76_GLOBAL__N__52_tmpxft_0000626d_00000000_6_UpSampleBicubic2d_cpp1_ii_b4c1e1f328upsample_bicubic2d_out_frameElddbNS_20PackedTensorAccessorIdLm4ENS_16DefaultPtrTraitsElEES4_' for 'sm_61'\r\nptxas info    : Function properties for _ZN2at6native76_GLOBAL__N__52_tmpxft_0000626d_00000000_6_UpSampleBicubic2d_cpp1_ii_b4c1e1f328upsample_bicubic2d_out_frameElddbNS_20PackedTensorAccessorIdLm4ENS_16DefaultPtrTraitsElEES4_\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 124 registers, 496 bytes cmem[0], 24 bytes cmem[2]\r\n```\r\n\r\nWith `__launch_bounds__(1024)`, the code again uses `64` registers.\r\n\r\nIf you use `C10_LAUNCH_BOUNDS_1(1024)` **for both kernels**, the tests pass here.\r\n\r\n\r\nNow *why* is the regcount higher in the new code? It could be `PackedTensorAccessor`, it could be the fact that many instances of `int` have been changed to `int64_t`. :)\r\n\r\nYou could experiment or just use the launch bounds.  Other code in `native/cuda` seems to use lower bounds for `blockDim`, too.  1024 seems to be an outlier.\n<issue_comment>username_0: @username_2 \r\n\r\nit seems it worked locally! thank you so much!  I really appreciate that!\n<issue_comment>username_0: thanks @username_1 and @username_2 for all the help!\r\n\r\n@username_3 it is done for review!\n<issue_comment>username_3: CircleCI runs on AWS, and the I'm fairly sure we've got one of the g3 types, either g3.4xlarge or g3.8xlarge\n<issue_comment>username_3: cc @ngimel, if you want to take a look at this (I'm planning to do a review too)\n<issue_comment>username_0: @username_4 could it be addressed in another PR?\n<issue_comment>username_4: @username_0 Sure, it's definitely better to rename both cpu & gpu impls together in a separate PR. Just want to make sure we have the correct one after porting.\n<issue_comment>username_3: Just as a clarification: I'm OK with having changes which are improvements over the old kernels come in latter patches.\n<issue_comment>username_3: I finished reviewing one of the kernels, and did a cursory scan of the others. In general the patch looks like it's in good shape. I'd suggest that we fix up the major correctness issues (PackedTensorAccessor ref, and int32 versus int64 indexing) and anything small that is easy to fix (for example, double zeroing the array), and leave other improvements for follow ups. Should be ready to land soon!\n<issue_comment>username_0: thanks so much @username_3!\r\nI will let you know when it is ready again for a new review! thanks!\n<issue_comment>username_0: @username_2 @username_3 \r\nnot sure but the errors on CI seems to be related to jenkins ... it seems all these jobs that failed (for building) ran by 01h 01min ... not sure if it is a coincidence ...\n<issue_comment>username_1: @username_0 indeed it looks like all jobs were aborted at the same time. no obvious issues in the build log related to your code. Comparing e.g. the `cuda9-cudnn7` build with a successful one from another PR, it takes 1hr 14min there and your build is about at the place where the other one is after an hour.\r\n\r\nI suggest to just push a new commit to rebuild. Probably a temporary CI hiccup.\n<issue_comment>username_3: I accidentally rebooted Jenkins yesterday which is the likely cause, my apologies.\r\n\r\n@pytorchbot retest this please\n<issue_comment>username_0: @username_3 @username_2 @username_1 \r\n\r\nall tests passed except `pr/caffe2-py2-cuda9.0-cudnn7-windows-build`:\r\n \r\n```\r\n14:43:49 Build timed out (after 180 minutes). Marking the build as failed.\r\n14:43:49 Build was aborted\r\n14:43:49 [BFA] Scanning build for known causes...\r\n14:43:49 [BFA] No failure causes found\r\n14:43:49 [BFA] Done. 0s\r\n14:43:49 Finished: FAILURE\r\n```\r\n\r\nnot sure if this timeout means that the code now is slower.\r\n\r\nwhat do you think? do you have any suggestion?\n<issue_comment>username_3: Sometimes the Windows build flakes out like that. It didn't timeout while running a relevant test, so I judge it to be not your problem.\n<issue_comment>username_3: The launch bounds logic is wrong, but I acknowledge that this is a big patch already; just fix it in a follow up. I am going to go ahead and land this.\n<issue_comment>username_0: sounds good @username_3 thanks!","repo":"pytorch/pytorch","issue_id":436372857,"issue_number":19630}
{"question":"What setup does the author use to reproduce the issue locally?","answer":"Arch Linux, CUDA 10.0, RTX2070 GPU","id":151,"text":"I can reproduce it locally: Arch Linux, CUDA 10.0, RTX2070 GPU. Given the CI failures, I think it should be reproducible for multiple CUDA and GPU versions. I just haven't worked on any CUDA code before, and am short on time, so I'd rather not dig too deep.","context":"<issue_start><issue_comment>Title: [WIP] UpSample GPU Porting \nusername_0: resolves #16158\n<issue_comment>username_1: Fewer errors than before, 4 instead of 8 with the same message:\r\n```\r\nRuntimeError: CUDA error: too many resources requested for launch\r\n```\r\nThis seems to be an existing problem: gh-8103.\r\n\r\n@username_2 could you have a look at this and tell us what you think?\n<issue_comment>username_2: Looking at it very briefly, the `Jetson TX` referenced in the issue only has 256 cores.\r\n\r\nSo while the issue looks the same, I'd probably not expect it on the CI platforms.  FWIW, some recent CI tests in other issues are green.\n<issue_comment>username_2: @pytorchbot retest this please.\n<issue_comment>username_1: same failures.\n<issue_comment>username_2: Is this rebased on the latest master (you can also ask the bot to rebase)?\n<issue_comment>username_0: I rebased that yesterday ... also needed to resolve conflicts because 7 days ago upsample files were changed.\n<issue_comment>username_0: @username_2 do you have any idea about what could be this problem? or a way to debug that?\r\nalso it seems some jobs is taking a lot of time to build, for example, for some job the estimate time is 19h .. not sure if it the regular estimate ..\n<issue_comment>username_2: @username_0 If you can't reproduce it at home it seems hard to debug other than reading at the diffs again.  I can take a look on Monday, it's getting a bit late here.\n<issue_comment>username_2: Also, has anyone found a way to show the actual hardware used on the CI in detail?\n<issue_comment>username_0: @username_2 I am working in parallel on a paperspace environment .. it tooks a lot of time it is running with 8 cpu cores.\n<issue_comment>username_2: But have you ever been able to reproduce this issue on paperspace?\n<issue_comment>username_1: I can reproduce it locally: Arch Linux, CUDA 10.0, RTX2070 GPU. Given the CI failures, I think it should be reproducible for multiple CUDA and GPU versions. I just haven't worked on any CUDA code before, and am short on time, so I'd rather not dig too deep.\n<issue_comment>username_0: not yet .. my last building was with a my previous commit ... I will work again in this task in some minutes :)\n<issue_comment>username_1: @username_0 your previous commit had the same failure though (except for the less clear exception message), and it seems quite reproducible. So I think you'll see it now.\n<issue_comment>username_2: Ah, thanks.  @username_0, then I guess just compiling with \"DEBUG=1\" and stepping through the offending code with gdb may be the fastest way.\n<issue_comment>username_0: @username_2 thanks! I will try that!\n<issue_comment>username_0: it seems just UpSampleBicubic2d is using upsample_get_value_bounded (https://github.com/pytorch/pytorch/pull/19630/files#diff-5092da792c30694ee4adf0d0ae2a37c6R171) and upsample_increment_value_bounded (https://github.com/pytorch/pytorch/pull/19630/files#diff-5092da792c30694ee4adf0d0ae2a37c6R191)\r\n\r\nmaybe the problem could be inside one of these functions ... maybe related to the order of indexes (x, y) ...\r\n\r\nbut the problem seems to be related to cuda block/threads ... so not sure if it is really related to these functions.\n<issue_comment>username_2: The new code uses far more registers than the existing one. I verified that the both versions actually call the offending test case with `1024` in `blockDim`.  So it's very likely a register issue.\r\n\r\n`Existing` uses `64` registers, which seems to be optimal or my card:\r\n\r\n```\r\nptxas info    : 77696 bytes gmem, 72 bytes cmem[3]\r\nptxas info    : Compiling entry function '_Z23bicubic_interp2d_kerneliddb15THCDeviceTensorIdLi4Ei16DefaultPtrTraitsES1_' for 'sm_61'\r\nptxas info    : Function properties for _Z23bicubic_interp2d_kerneliddb15THCDeviceTensorIdLi4Ei16DefaultPtrTraitsES1_\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 64 registers, 432 bytes cmem[0], 24 bytes cmem[2]\r\n```\r\n\r\n`New` uses `124` registers, which is too much for my card:\r\n\r\n```\r\nptxas info    : 77696 bytes gmem, 72 bytes cmem[3]\r\nptxas info    : Compiling entry function '_ZN2at6native76_GLOBAL__N__52_tmpxft_0000626d_00000000_6_UpSampleBicubic2d_cpp1_ii_b4c1e1f328upsample_bicubic2d_out_frameElddbNS_20PackedTensorAccessorIdLm4ENS_16DefaultPtrTraitsElEES4_' for 'sm_61'\r\nptxas info    : Function properties for _ZN2at6native76_GLOBAL__N__52_tmpxft_0000626d_00000000_6_UpSampleBicubic2d_cpp1_ii_b4c1e1f328upsample_bicubic2d_out_frameElddbNS_20PackedTensorAccessorIdLm4ENS_16DefaultPtrTraitsElEES4_\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 124 registers, 496 bytes cmem[0], 24 bytes cmem[2]\r\n```\r\n\r\nWith `__launch_bounds__(1024)`, the code again uses `64` registers.\r\n\r\nIf you use `C10_LAUNCH_BOUNDS_1(1024)` **for both kernels**, the tests pass here.\r\n\r\n\r\nNow *why* is the regcount higher in the new code? It could be `PackedTensorAccessor`, it could be the fact that many instances of `int` have been changed to `int64_t`. :)\r\n\r\nYou could experiment or just use the launch bounds.  Other code in `native/cuda` seems to use lower bounds for `blockDim`, too.  1024 seems to be an outlier.\n<issue_comment>username_0: @username_2 \r\n\r\nit seems it worked locally! thank you so much!  I really appreciate that!\n<issue_comment>username_0: thanks @username_1 and @username_2 for all the help!\r\n\r\n@username_3 it is done for review!\n<issue_comment>username_3: CircleCI runs on AWS, and the I'm fairly sure we've got one of the g3 types, either g3.4xlarge or g3.8xlarge\n<issue_comment>username_3: cc @ngimel, if you want to take a look at this (I'm planning to do a review too)\n<issue_comment>username_0: @username_4 could it be addressed in another PR?\n<issue_comment>username_4: @username_0 Sure, it's definitely better to rename both cpu & gpu impls together in a separate PR. Just want to make sure we have the correct one after porting.\n<issue_comment>username_3: Just as a clarification: I'm OK with having changes which are improvements over the old kernels come in latter patches.\n<issue_comment>username_3: I finished reviewing one of the kernels, and did a cursory scan of the others. In general the patch looks like it's in good shape. I'd suggest that we fix up the major correctness issues (PackedTensorAccessor ref, and int32 versus int64 indexing) and anything small that is easy to fix (for example, double zeroing the array), and leave other improvements for follow ups. Should be ready to land soon!\n<issue_comment>username_0: thanks so much @username_3!\r\nI will let you know when it is ready again for a new review! thanks!\n<issue_comment>username_0: @username_2 @username_3 \r\nnot sure but the errors on CI seems to be related to jenkins ... it seems all these jobs that failed (for building) ran by 01h 01min ... not sure if it is a coincidence ...\n<issue_comment>username_1: @username_0 indeed it looks like all jobs were aborted at the same time. no obvious issues in the build log related to your code. Comparing e.g. the `cuda9-cudnn7` build with a successful one from another PR, it takes 1hr 14min there and your build is about at the place where the other one is after an hour.\r\n\r\nI suggest to just push a new commit to rebuild. Probably a temporary CI hiccup.\n<issue_comment>username_3: I accidentally rebooted Jenkins yesterday which is the likely cause, my apologies.\r\n\r\n@pytorchbot retest this please\n<issue_comment>username_0: @username_3 @username_2 @username_1 \r\n\r\nall tests passed except `pr/caffe2-py2-cuda9.0-cudnn7-windows-build`:\r\n \r\n```\r\n14:43:49 Build timed out (after 180 minutes). Marking the build as failed.\r\n14:43:49 Build was aborted\r\n14:43:49 [BFA] Scanning build for known causes...\r\n14:43:49 [BFA] No failure causes found\r\n14:43:49 [BFA] Done. 0s\r\n14:43:49 Finished: FAILURE\r\n```\r\n\r\nnot sure if this timeout means that the code now is slower.\r\n\r\nwhat do you think? do you have any suggestion?\n<issue_comment>username_3: Sometimes the Windows build flakes out like that. It didn't timeout while running a relevant test, so I judge it to be not your problem.\n<issue_comment>username_3: The launch bounds logic is wrong, but I acknowledge that this is a big patch already; just fix it in a follow up. I am going to go ahead and land this.\n<issue_comment>username_0: sounds good @username_3 thanks!","repo":"pytorch/pytorch","issue_id":436372857,"issue_number":19630}
{"question":"What is the similarity between the failure in the previous commit and the current one mentioned in the text?","answer":"The failure in the previous commit was similar to the current one, except for a less clear exception message, and it was reproducible.","id":153,"text":"@username_0 your previous commit had the same failure though (except for the less clear exception message), and it seems quite reproducible. So I think you'll see it now.","context":"<issue_start><issue_comment>Title: [WIP] UpSample GPU Porting \nusername_0: resolves #16158\n<issue_comment>username_1: Fewer errors than before, 4 instead of 8 with the same message:\r\n```\r\nRuntimeError: CUDA error: too many resources requested for launch\r\n```\r\nThis seems to be an existing problem: gh-8103.\r\n\r\n@username_2 could you have a look at this and tell us what you think?\n<issue_comment>username_2: Looking at it very briefly, the `Jetson TX` referenced in the issue only has 256 cores.\r\n\r\nSo while the issue looks the same, I'd probably not expect it on the CI platforms.  FWIW, some recent CI tests in other issues are green.\n<issue_comment>username_2: @pytorchbot retest this please.\n<issue_comment>username_1: same failures.\n<issue_comment>username_2: Is this rebased on the latest master (you can also ask the bot to rebase)?\n<issue_comment>username_0: I rebased that yesterday ... also needed to resolve conflicts because 7 days ago upsample files were changed.\n<issue_comment>username_0: @username_2 do you have any idea about what could be this problem? or a way to debug that?\r\nalso it seems some jobs is taking a lot of time to build, for example, for some job the estimate time is 19h .. not sure if it the regular estimate ..\n<issue_comment>username_2: @username_0 If you can't reproduce it at home it seems hard to debug other than reading at the diffs again.  I can take a look on Monday, it's getting a bit late here.\n<issue_comment>username_2: Also, has anyone found a way to show the actual hardware used on the CI in detail?\n<issue_comment>username_0: @username_2 I am working in parallel on a paperspace environment .. it tooks a lot of time it is running with 8 cpu cores.\n<issue_comment>username_2: But have you ever been able to reproduce this issue on paperspace?\n<issue_comment>username_1: I can reproduce it locally: Arch Linux, CUDA 10.0, RTX2070 GPU. Given the CI failures, I think it should be reproducible for multiple CUDA and GPU versions. I just haven't worked on any CUDA code before, and am short on time, so I'd rather not dig too deep.\n<issue_comment>username_0: not yet .. my last building was with a my previous commit ... I will work again in this task in some minutes :)\n<issue_comment>username_1: @username_0 your previous commit had the same failure though (except for the less clear exception message), and it seems quite reproducible. So I think you'll see it now.\n<issue_comment>username_2: Ah, thanks.  @username_0, then I guess just compiling with \"DEBUG=1\" and stepping through the offending code with gdb may be the fastest way.\n<issue_comment>username_0: @username_2 thanks! I will try that!\n<issue_comment>username_0: it seems just UpSampleBicubic2d is using upsample_get_value_bounded (https://github.com/pytorch/pytorch/pull/19630/files#diff-5092da792c30694ee4adf0d0ae2a37c6R171) and upsample_increment_value_bounded (https://github.com/pytorch/pytorch/pull/19630/files#diff-5092da792c30694ee4adf0d0ae2a37c6R191)\r\n\r\nmaybe the problem could be inside one of these functions ... maybe related to the order of indexes (x, y) ...\r\n\r\nbut the problem seems to be related to cuda block/threads ... so not sure if it is really related to these functions.\n<issue_comment>username_2: The new code uses far more registers than the existing one. I verified that the both versions actually call the offending test case with `1024` in `blockDim`.  So it's very likely a register issue.\r\n\r\n`Existing` uses `64` registers, which seems to be optimal or my card:\r\n\r\n```\r\nptxas info    : 77696 bytes gmem, 72 bytes cmem[3]\r\nptxas info    : Compiling entry function '_Z23bicubic_interp2d_kerneliddb15THCDeviceTensorIdLi4Ei16DefaultPtrTraitsES1_' for 'sm_61'\r\nptxas info    : Function properties for _Z23bicubic_interp2d_kerneliddb15THCDeviceTensorIdLi4Ei16DefaultPtrTraitsES1_\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 64 registers, 432 bytes cmem[0], 24 bytes cmem[2]\r\n```\r\n\r\n`New` uses `124` registers, which is too much for my card:\r\n\r\n```\r\nptxas info    : 77696 bytes gmem, 72 bytes cmem[3]\r\nptxas info    : Compiling entry function '_ZN2at6native76_GLOBAL__N__52_tmpxft_0000626d_00000000_6_UpSampleBicubic2d_cpp1_ii_b4c1e1f328upsample_bicubic2d_out_frameElddbNS_20PackedTensorAccessorIdLm4ENS_16DefaultPtrTraitsElEES4_' for 'sm_61'\r\nptxas info    : Function properties for _ZN2at6native76_GLOBAL__N__52_tmpxft_0000626d_00000000_6_UpSampleBicubic2d_cpp1_ii_b4c1e1f328upsample_bicubic2d_out_frameElddbNS_20PackedTensorAccessorIdLm4ENS_16DefaultPtrTraitsElEES4_\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 124 registers, 496 bytes cmem[0], 24 bytes cmem[2]\r\n```\r\n\r\nWith `__launch_bounds__(1024)`, the code again uses `64` registers.\r\n\r\nIf you use `C10_LAUNCH_BOUNDS_1(1024)` **for both kernels**, the tests pass here.\r\n\r\n\r\nNow *why* is the regcount higher in the new code? It could be `PackedTensorAccessor`, it could be the fact that many instances of `int` have been changed to `int64_t`. :)\r\n\r\nYou could experiment or just use the launch bounds.  Other code in `native/cuda` seems to use lower bounds for `blockDim`, too.  1024 seems to be an outlier.\n<issue_comment>username_0: @username_2 \r\n\r\nit seems it worked locally! thank you so much!  I really appreciate that!\n<issue_comment>username_0: thanks @username_1 and @username_2 for all the help!\r\n\r\n@username_3 it is done for review!\n<issue_comment>username_3: CircleCI runs on AWS, and the I'm fairly sure we've got one of the g3 types, either g3.4xlarge or g3.8xlarge\n<issue_comment>username_3: cc @ngimel, if you want to take a look at this (I'm planning to do a review too)\n<issue_comment>username_0: @username_4 could it be addressed in another PR?\n<issue_comment>username_4: @username_0 Sure, it's definitely better to rename both cpu & gpu impls together in a separate PR. Just want to make sure we have the correct one after porting.\n<issue_comment>username_3: Just as a clarification: I'm OK with having changes which are improvements over the old kernels come in latter patches.\n<issue_comment>username_3: I finished reviewing one of the kernels, and did a cursory scan of the others. In general the patch looks like it's in good shape. I'd suggest that we fix up the major correctness issues (PackedTensorAccessor ref, and int32 versus int64 indexing) and anything small that is easy to fix (for example, double zeroing the array), and leave other improvements for follow ups. Should be ready to land soon!\n<issue_comment>username_0: thanks so much @username_3!\r\nI will let you know when it is ready again for a new review! thanks!\n<issue_comment>username_0: @username_2 @username_3 \r\nnot sure but the errors on CI seems to be related to jenkins ... it seems all these jobs that failed (for building) ran by 01h 01min ... not sure if it is a coincidence ...\n<issue_comment>username_1: @username_0 indeed it looks like all jobs were aborted at the same time. no obvious issues in the build log related to your code. Comparing e.g. the `cuda9-cudnn7` build with a successful one from another PR, it takes 1hr 14min there and your build is about at the place where the other one is after an hour.\r\n\r\nI suggest to just push a new commit to rebuild. Probably a temporary CI hiccup.\n<issue_comment>username_3: I accidentally rebooted Jenkins yesterday which is the likely cause, my apologies.\r\n\r\n@pytorchbot retest this please\n<issue_comment>username_0: @username_3 @username_2 @username_1 \r\n\r\nall tests passed except `pr/caffe2-py2-cuda9.0-cudnn7-windows-build`:\r\n \r\n```\r\n14:43:49 Build timed out (after 180 minutes). Marking the build as failed.\r\n14:43:49 Build was aborted\r\n14:43:49 [BFA] Scanning build for known causes...\r\n14:43:49 [BFA] No failure causes found\r\n14:43:49 [BFA] Done. 0s\r\n14:43:49 Finished: FAILURE\r\n```\r\n\r\nnot sure if this timeout means that the code now is slower.\r\n\r\nwhat do you think? do you have any suggestion?\n<issue_comment>username_3: Sometimes the Windows build flakes out like that. It didn't timeout while running a relevant test, so I judge it to be not your problem.\n<issue_comment>username_3: The launch bounds logic is wrong, but I acknowledge that this is a big patch already; just fix it in a follow up. I am going to go ahead and land this.\n<issue_comment>username_0: sounds good @username_3 thanks!","repo":"pytorch/pytorch","issue_id":436372857,"issue_number":19630}
{"question":"Why is the regcount higher in the new code compared to the existing one?","answer":"The regcount is higher in the new code due to factors such as the use of `PackedTensorAccessor` and the change from `int` to `int64_t`, which increases the register usage.","id":157,"text":"The new code uses far more registers than the existing one. I verified that the both versions actually call the offending test case with `1024` in `blockDim`.  So it's very likely a register issue.\r\n\r\n`Existing` uses `64` registers, which seems to be optimal or my card:\r\n\r\n```\r\nptxas info    : 77696 bytes gmem, 72 bytes cmem[3]\r\nptxas info    : Compiling entry function '_Z23bicubic_interp2d_kerneliddb15THCDeviceTensorIdLi4Ei16DefaultPtrTraitsES1_' for 'sm_61'\r\nptxas info    : Function properties for _Z23bicubic_interp2d_kerneliddb15THCDeviceTensorIdLi4Ei16DefaultPtrTraitsES1_\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 64 registers, 432 bytes cmem[0], 24 bytes cmem[2]\r\n```\r\n\r\n`New` uses `124` registers, which is too much for my card:\r\n\r\n```\r\nptxas info    : 77696 bytes gmem, 72 bytes cmem[3]\r\nptxas info    : Compiling entry function '_ZN2at6native76_GLOBAL__N__52_tmpxft_0000626d_00000000_6_UpSampleBicubic2d_cpp1_ii_b4c1e1f328upsample_bicubic2d_out_frameElddbNS_20PackedTensorAccessorIdLm4ENS_16DefaultPtrTraitsElEES4_' for 'sm_61'\r\nptxas info    : Function properties for _ZN2at6native76_GLOBAL__N__52_tmpxft_0000626d_00000000_6_UpSampleBicubic2d_cpp1_ii_b4c1e1f328upsample_bicubic2d_out_frameElddbNS_20PackedTensorAccessorIdLm4ENS_16DefaultPtrTraitsElEES4_\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 124 registers, 496 bytes cmem[0], 24 bytes cmem[2]\r\n```\r\n\r\nWith `__launch_bounds__(1024)`, the code again uses `64` registers.\r\n\r\nIf you use `C10_LAUNCH_BOUNDS_1(1024)` **for both kernels**, the tests pass here.\r\n\r\n\r\nNow *why* is the regcount higher in the new code? It could be `PackedTensorAccessor`, it could be the fact that many instances of `int` have been changed to `int64_t`. :)\r\n\r\nYou could experiment or just use the launch bounds.  Other code in `native/cuda` seems to use lower bounds for `blockDim`, too.  1024 seems to be an outlier.","context":"<issue_start><issue_comment>Title: [WIP] UpSample GPU Porting \nusername_0: resolves #16158\n<issue_comment>username_1: Fewer errors than before, 4 instead of 8 with the same message:\r\n```\r\nRuntimeError: CUDA error: too many resources requested for launch\r\n```\r\nThis seems to be an existing problem: gh-8103.\r\n\r\n@username_2 could you have a look at this and tell us what you think?\n<issue_comment>username_2: Looking at it very briefly, the `Jetson TX` referenced in the issue only has 256 cores.\r\n\r\nSo while the issue looks the same, I'd probably not expect it on the CI platforms.  FWIW, some recent CI tests in other issues are green.\n<issue_comment>username_2: @pytorchbot retest this please.\n<issue_comment>username_1: same failures.\n<issue_comment>username_2: Is this rebased on the latest master (you can also ask the bot to rebase)?\n<issue_comment>username_0: I rebased that yesterday ... also needed to resolve conflicts because 7 days ago upsample files were changed.\n<issue_comment>username_0: @username_2 do you have any idea about what could be this problem? or a way to debug that?\r\nalso it seems some jobs is taking a lot of time to build, for example, for some job the estimate time is 19h .. not sure if it the regular estimate ..\n<issue_comment>username_2: @username_0 If you can't reproduce it at home it seems hard to debug other than reading at the diffs again.  I can take a look on Monday, it's getting a bit late here.\n<issue_comment>username_2: Also, has anyone found a way to show the actual hardware used on the CI in detail?\n<issue_comment>username_0: @username_2 I am working in parallel on a paperspace environment .. it tooks a lot of time it is running with 8 cpu cores.\n<issue_comment>username_2: But have you ever been able to reproduce this issue on paperspace?\n<issue_comment>username_1: I can reproduce it locally: Arch Linux, CUDA 10.0, RTX2070 GPU. Given the CI failures, I think it should be reproducible for multiple CUDA and GPU versions. I just haven't worked on any CUDA code before, and am short on time, so I'd rather not dig too deep.\n<issue_comment>username_0: not yet .. my last building was with a my previous commit ... I will work again in this task in some minutes :)\n<issue_comment>username_1: @username_0 your previous commit had the same failure though (except for the less clear exception message), and it seems quite reproducible. So I think you'll see it now.\n<issue_comment>username_2: Ah, thanks.  @username_0, then I guess just compiling with \"DEBUG=1\" and stepping through the offending code with gdb may be the fastest way.\n<issue_comment>username_0: @username_2 thanks! I will try that!\n<issue_comment>username_0: it seems just UpSampleBicubic2d is using upsample_get_value_bounded (https://github.com/pytorch/pytorch/pull/19630/files#diff-5092da792c30694ee4adf0d0ae2a37c6R171) and upsample_increment_value_bounded (https://github.com/pytorch/pytorch/pull/19630/files#diff-5092da792c30694ee4adf0d0ae2a37c6R191)\r\n\r\nmaybe the problem could be inside one of these functions ... maybe related to the order of indexes (x, y) ...\r\n\r\nbut the problem seems to be related to cuda block/threads ... so not sure if it is really related to these functions.\n<issue_comment>username_2: The new code uses far more registers than the existing one. I verified that the both versions actually call the offending test case with `1024` in `blockDim`.  So it's very likely a register issue.\r\n\r\n`Existing` uses `64` registers, which seems to be optimal or my card:\r\n\r\n```\r\nptxas info    : 77696 bytes gmem, 72 bytes cmem[3]\r\nptxas info    : Compiling entry function '_Z23bicubic_interp2d_kerneliddb15THCDeviceTensorIdLi4Ei16DefaultPtrTraitsES1_' for 'sm_61'\r\nptxas info    : Function properties for _Z23bicubic_interp2d_kerneliddb15THCDeviceTensorIdLi4Ei16DefaultPtrTraitsES1_\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 64 registers, 432 bytes cmem[0], 24 bytes cmem[2]\r\n```\r\n\r\n`New` uses `124` registers, which is too much for my card:\r\n\r\n```\r\nptxas info    : 77696 bytes gmem, 72 bytes cmem[3]\r\nptxas info    : Compiling entry function '_ZN2at6native76_GLOBAL__N__52_tmpxft_0000626d_00000000_6_UpSampleBicubic2d_cpp1_ii_b4c1e1f328upsample_bicubic2d_out_frameElddbNS_20PackedTensorAccessorIdLm4ENS_16DefaultPtrTraitsElEES4_' for 'sm_61'\r\nptxas info    : Function properties for _ZN2at6native76_GLOBAL__N__52_tmpxft_0000626d_00000000_6_UpSampleBicubic2d_cpp1_ii_b4c1e1f328upsample_bicubic2d_out_frameElddbNS_20PackedTensorAccessorIdLm4ENS_16DefaultPtrTraitsElEES4_\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 124 registers, 496 bytes cmem[0], 24 bytes cmem[2]\r\n```\r\n\r\nWith `__launch_bounds__(1024)`, the code again uses `64` registers.\r\n\r\nIf you use `C10_LAUNCH_BOUNDS_1(1024)` **for both kernels**, the tests pass here.\r\n\r\n\r\nNow *why* is the regcount higher in the new code? It could be `PackedTensorAccessor`, it could be the fact that many instances of `int` have been changed to `int64_t`. :)\r\n\r\nYou could experiment or just use the launch bounds.  Other code in `native/cuda` seems to use lower bounds for `blockDim`, too.  1024 seems to be an outlier.\n<issue_comment>username_0: @username_2 \r\n\r\nit seems it worked locally! thank you so much!  I really appreciate that!\n<issue_comment>username_0: thanks @username_1 and @username_2 for all the help!\r\n\r\n@username_3 it is done for review!\n<issue_comment>username_3: CircleCI runs on AWS, and the I'm fairly sure we've got one of the g3 types, either g3.4xlarge or g3.8xlarge\n<issue_comment>username_3: cc @ngimel, if you want to take a look at this (I'm planning to do a review too)\n<issue_comment>username_0: @username_4 could it be addressed in another PR?\n<issue_comment>username_4: @username_0 Sure, it's definitely better to rename both cpu & gpu impls together in a separate PR. Just want to make sure we have the correct one after porting.\n<issue_comment>username_3: Just as a clarification: I'm OK with having changes which are improvements over the old kernels come in latter patches.\n<issue_comment>username_3: I finished reviewing one of the kernels, and did a cursory scan of the others. In general the patch looks like it's in good shape. I'd suggest that we fix up the major correctness issues (PackedTensorAccessor ref, and int32 versus int64 indexing) and anything small that is easy to fix (for example, double zeroing the array), and leave other improvements for follow ups. Should be ready to land soon!\n<issue_comment>username_0: thanks so much @username_3!\r\nI will let you know when it is ready again for a new review! thanks!\n<issue_comment>username_0: @username_2 @username_3 \r\nnot sure but the errors on CI seems to be related to jenkins ... it seems all these jobs that failed (for building) ran by 01h 01min ... not sure if it is a coincidence ...\n<issue_comment>username_1: @username_0 indeed it looks like all jobs were aborted at the same time. no obvious issues in the build log related to your code. Comparing e.g. the `cuda9-cudnn7` build with a successful one from another PR, it takes 1hr 14min there and your build is about at the place where the other one is after an hour.\r\n\r\nI suggest to just push a new commit to rebuild. Probably a temporary CI hiccup.\n<issue_comment>username_3: I accidentally rebooted Jenkins yesterday which is the likely cause, my apologies.\r\n\r\n@pytorchbot retest this please\n<issue_comment>username_0: @username_3 @username_2 @username_1 \r\n\r\nall tests passed except `pr/caffe2-py2-cuda9.0-cudnn7-windows-build`:\r\n \r\n```\r\n14:43:49 Build timed out (after 180 minutes). Marking the build as failed.\r\n14:43:49 Build was aborted\r\n14:43:49 [BFA] Scanning build for known causes...\r\n14:43:49 [BFA] No failure causes found\r\n14:43:49 [BFA] Done. 0s\r\n14:43:49 Finished: FAILURE\r\n```\r\n\r\nnot sure if this timeout means that the code now is slower.\r\n\r\nwhat do you think? do you have any suggestion?\n<issue_comment>username_3: Sometimes the Windows build flakes out like that. It didn't timeout while running a relevant test, so I judge it to be not your problem.\n<issue_comment>username_3: The launch bounds logic is wrong, but I acknowledge that this is a big patch already; just fix it in a follow up. I am going to go ahead and land this.\n<issue_comment>username_0: sounds good @username_3 thanks!","repo":"pytorch/pytorch","issue_id":436372857,"issue_number":19630}
{"question":"Why is it recommended to rename both CPU and GPU implementations together in a separate PR after porting?","answer":"It is recommended to rename both CPU and GPU implementations together in a separate PR after porting to ensure the correctness of the changes and to maintain consistency between the two implementations.","id":163,"text":"@username_0 Sure, it's definitely better to rename both cpu & gpu impls together in a separate PR. Just want to make sure we have the correct one after porting.","context":"<issue_start><issue_comment>Title: [WIP] UpSample GPU Porting \nusername_0: resolves #16158\n<issue_comment>username_1: Fewer errors than before, 4 instead of 8 with the same message:\r\n```\r\nRuntimeError: CUDA error: too many resources requested for launch\r\n```\r\nThis seems to be an existing problem: gh-8103.\r\n\r\n@username_2 could you have a look at this and tell us what you think?\n<issue_comment>username_2: Looking at it very briefly, the `Jetson TX` referenced in the issue only has 256 cores.\r\n\r\nSo while the issue looks the same, I'd probably not expect it on the CI platforms.  FWIW, some recent CI tests in other issues are green.\n<issue_comment>username_2: @pytorchbot retest this please.\n<issue_comment>username_1: same failures.\n<issue_comment>username_2: Is this rebased on the latest master (you can also ask the bot to rebase)?\n<issue_comment>username_0: I rebased that yesterday ... also needed to resolve conflicts because 7 days ago upsample files were changed.\n<issue_comment>username_0: @username_2 do you have any idea about what could be this problem? or a way to debug that?\r\nalso it seems some jobs is taking a lot of time to build, for example, for some job the estimate time is 19h .. not sure if it the regular estimate ..\n<issue_comment>username_2: @username_0 If you can't reproduce it at home it seems hard to debug other than reading at the diffs again.  I can take a look on Monday, it's getting a bit late here.\n<issue_comment>username_2: Also, has anyone found a way to show the actual hardware used on the CI in detail?\n<issue_comment>username_0: @username_2 I am working in parallel on a paperspace environment .. it tooks a lot of time it is running with 8 cpu cores.\n<issue_comment>username_2: But have you ever been able to reproduce this issue on paperspace?\n<issue_comment>username_1: I can reproduce it locally: Arch Linux, CUDA 10.0, RTX2070 GPU. Given the CI failures, I think it should be reproducible for multiple CUDA and GPU versions. I just haven't worked on any CUDA code before, and am short on time, so I'd rather not dig too deep.\n<issue_comment>username_0: not yet .. my last building was with a my previous commit ... I will work again in this task in some minutes :)\n<issue_comment>username_1: @username_0 your previous commit had the same failure though (except for the less clear exception message), and it seems quite reproducible. So I think you'll see it now.\n<issue_comment>username_2: Ah, thanks.  @username_0, then I guess just compiling with \"DEBUG=1\" and stepping through the offending code with gdb may be the fastest way.\n<issue_comment>username_0: @username_2 thanks! I will try that!\n<issue_comment>username_0: it seems just UpSampleBicubic2d is using upsample_get_value_bounded (https://github.com/pytorch/pytorch/pull/19630/files#diff-5092da792c30694ee4adf0d0ae2a37c6R171) and upsample_increment_value_bounded (https://github.com/pytorch/pytorch/pull/19630/files#diff-5092da792c30694ee4adf0d0ae2a37c6R191)\r\n\r\nmaybe the problem could be inside one of these functions ... maybe related to the order of indexes (x, y) ...\r\n\r\nbut the problem seems to be related to cuda block/threads ... so not sure if it is really related to these functions.\n<issue_comment>username_2: The new code uses far more registers than the existing one. I verified that the both versions actually call the offending test case with `1024` in `blockDim`.  So it's very likely a register issue.\r\n\r\n`Existing` uses `64` registers, which seems to be optimal or my card:\r\n\r\n```\r\nptxas info    : 77696 bytes gmem, 72 bytes cmem[3]\r\nptxas info    : Compiling entry function '_Z23bicubic_interp2d_kerneliddb15THCDeviceTensorIdLi4Ei16DefaultPtrTraitsES1_' for 'sm_61'\r\nptxas info    : Function properties for _Z23bicubic_interp2d_kerneliddb15THCDeviceTensorIdLi4Ei16DefaultPtrTraitsES1_\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 64 registers, 432 bytes cmem[0], 24 bytes cmem[2]\r\n```\r\n\r\n`New` uses `124` registers, which is too much for my card:\r\n\r\n```\r\nptxas info    : 77696 bytes gmem, 72 bytes cmem[3]\r\nptxas info    : Compiling entry function '_ZN2at6native76_GLOBAL__N__52_tmpxft_0000626d_00000000_6_UpSampleBicubic2d_cpp1_ii_b4c1e1f328upsample_bicubic2d_out_frameElddbNS_20PackedTensorAccessorIdLm4ENS_16DefaultPtrTraitsElEES4_' for 'sm_61'\r\nptxas info    : Function properties for _ZN2at6native76_GLOBAL__N__52_tmpxft_0000626d_00000000_6_UpSampleBicubic2d_cpp1_ii_b4c1e1f328upsample_bicubic2d_out_frameElddbNS_20PackedTensorAccessorIdLm4ENS_16DefaultPtrTraitsElEES4_\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 124 registers, 496 bytes cmem[0], 24 bytes cmem[2]\r\n```\r\n\r\nWith `__launch_bounds__(1024)`, the code again uses `64` registers.\r\n\r\nIf you use `C10_LAUNCH_BOUNDS_1(1024)` **for both kernels**, the tests pass here.\r\n\r\n\r\nNow *why* is the regcount higher in the new code? It could be `PackedTensorAccessor`, it could be the fact that many instances of `int` have been changed to `int64_t`. :)\r\n\r\nYou could experiment or just use the launch bounds.  Other code in `native/cuda` seems to use lower bounds for `blockDim`, too.  1024 seems to be an outlier.\n<issue_comment>username_0: @username_2 \r\n\r\nit seems it worked locally! thank you so much!  I really appreciate that!\n<issue_comment>username_0: thanks @username_1 and @username_2 for all the help!\r\n\r\n@username_3 it is done for review!\n<issue_comment>username_3: CircleCI runs on AWS, and the I'm fairly sure we've got one of the g3 types, either g3.4xlarge or g3.8xlarge\n<issue_comment>username_3: cc @ngimel, if you want to take a look at this (I'm planning to do a review too)\n<issue_comment>username_0: @username_4 could it be addressed in another PR?\n<issue_comment>username_4: @username_0 Sure, it's definitely better to rename both cpu & gpu impls together in a separate PR. Just want to make sure we have the correct one after porting.\n<issue_comment>username_3: Just as a clarification: I'm OK with having changes which are improvements over the old kernels come in latter patches.\n<issue_comment>username_3: I finished reviewing one of the kernels, and did a cursory scan of the others. In general the patch looks like it's in good shape. I'd suggest that we fix up the major correctness issues (PackedTensorAccessor ref, and int32 versus int64 indexing) and anything small that is easy to fix (for example, double zeroing the array), and leave other improvements for follow ups. Should be ready to land soon!\n<issue_comment>username_0: thanks so much @username_3!\r\nI will let you know when it is ready again for a new review! thanks!\n<issue_comment>username_0: @username_2 @username_3 \r\nnot sure but the errors on CI seems to be related to jenkins ... it seems all these jobs that failed (for building) ran by 01h 01min ... not sure if it is a coincidence ...\n<issue_comment>username_1: @username_0 indeed it looks like all jobs were aborted at the same time. no obvious issues in the build log related to your code. Comparing e.g. the `cuda9-cudnn7` build with a successful one from another PR, it takes 1hr 14min there and your build is about at the place where the other one is after an hour.\r\n\r\nI suggest to just push a new commit to rebuild. Probably a temporary CI hiccup.\n<issue_comment>username_3: I accidentally rebooted Jenkins yesterday which is the likely cause, my apologies.\r\n\r\n@pytorchbot retest this please\n<issue_comment>username_0: @username_3 @username_2 @username_1 \r\n\r\nall tests passed except `pr/caffe2-py2-cuda9.0-cudnn7-windows-build`:\r\n \r\n```\r\n14:43:49 Build timed out (after 180 minutes). Marking the build as failed.\r\n14:43:49 Build was aborted\r\n14:43:49 [BFA] Scanning build for known causes...\r\n14:43:49 [BFA] No failure causes found\r\n14:43:49 [BFA] Done. 0s\r\n14:43:49 Finished: FAILURE\r\n```\r\n\r\nnot sure if this timeout means that the code now is slower.\r\n\r\nwhat do you think? do you have any suggestion?\n<issue_comment>username_3: Sometimes the Windows build flakes out like that. It didn't timeout while running a relevant test, so I judge it to be not your problem.\n<issue_comment>username_3: The launch bounds logic is wrong, but I acknowledge that this is a big patch already; just fix it in a follow up. I am going to go ahead and land this.\n<issue_comment>username_0: sounds good @username_3 thanks!","repo":"pytorch/pytorch","issue_id":436372857,"issue_number":19630}
{"question":"What suggestions are provided in the text for fixing up the kernel patch before landing it?","answer":"The text suggests fixing up major correctness issues such as PackedTensorAccessor ref and int32 versus int64 indexing, as well as addressing small easy-to-fix issues like double zeroing the array, and leaving other improvements for follow ups.","id":165,"text":"I finished reviewing one of the kernels, and did a cursory scan of the others. In general the patch looks like it's in good shape. I'd suggest that we fix up the major correctness issues (PackedTensorAccessor ref, and int32 versus int64 indexing) and anything small that is easy to fix (for example, double zeroing the array), and leave other improvements for follow ups. Should be ready to land soon!","context":"<issue_start><issue_comment>Title: [WIP] UpSample GPU Porting \nusername_0: resolves #16158\n<issue_comment>username_1: Fewer errors than before, 4 instead of 8 with the same message:\r\n```\r\nRuntimeError: CUDA error: too many resources requested for launch\r\n```\r\nThis seems to be an existing problem: gh-8103.\r\n\r\n@username_2 could you have a look at this and tell us what you think?\n<issue_comment>username_2: Looking at it very briefly, the `Jetson TX` referenced in the issue only has 256 cores.\r\n\r\nSo while the issue looks the same, I'd probably not expect it on the CI platforms.  FWIW, some recent CI tests in other issues are green.\n<issue_comment>username_2: @pytorchbot retest this please.\n<issue_comment>username_1: same failures.\n<issue_comment>username_2: Is this rebased on the latest master (you can also ask the bot to rebase)?\n<issue_comment>username_0: I rebased that yesterday ... also needed to resolve conflicts because 7 days ago upsample files were changed.\n<issue_comment>username_0: @username_2 do you have any idea about what could be this problem? or a way to debug that?\r\nalso it seems some jobs is taking a lot of time to build, for example, for some job the estimate time is 19h .. not sure if it the regular estimate ..\n<issue_comment>username_2: @username_0 If you can't reproduce it at home it seems hard to debug other than reading at the diffs again.  I can take a look on Monday, it's getting a bit late here.\n<issue_comment>username_2: Also, has anyone found a way to show the actual hardware used on the CI in detail?\n<issue_comment>username_0: @username_2 I am working in parallel on a paperspace environment .. it tooks a lot of time it is running with 8 cpu cores.\n<issue_comment>username_2: But have you ever been able to reproduce this issue on paperspace?\n<issue_comment>username_1: I can reproduce it locally: Arch Linux, CUDA 10.0, RTX2070 GPU. Given the CI failures, I think it should be reproducible for multiple CUDA and GPU versions. I just haven't worked on any CUDA code before, and am short on time, so I'd rather not dig too deep.\n<issue_comment>username_0: not yet .. my last building was with a my previous commit ... I will work again in this task in some minutes :)\n<issue_comment>username_1: @username_0 your previous commit had the same failure though (except for the less clear exception message), and it seems quite reproducible. So I think you'll see it now.\n<issue_comment>username_2: Ah, thanks.  @username_0, then I guess just compiling with \"DEBUG=1\" and stepping through the offending code with gdb may be the fastest way.\n<issue_comment>username_0: @username_2 thanks! I will try that!\n<issue_comment>username_0: it seems just UpSampleBicubic2d is using upsample_get_value_bounded (https://github.com/pytorch/pytorch/pull/19630/files#diff-5092da792c30694ee4adf0d0ae2a37c6R171) and upsample_increment_value_bounded (https://github.com/pytorch/pytorch/pull/19630/files#diff-5092da792c30694ee4adf0d0ae2a37c6R191)\r\n\r\nmaybe the problem could be inside one of these functions ... maybe related to the order of indexes (x, y) ...\r\n\r\nbut the problem seems to be related to cuda block/threads ... so not sure if it is really related to these functions.\n<issue_comment>username_2: The new code uses far more registers than the existing one. I verified that the both versions actually call the offending test case with `1024` in `blockDim`.  So it's very likely a register issue.\r\n\r\n`Existing` uses `64` registers, which seems to be optimal or my card:\r\n\r\n```\r\nptxas info    : 77696 bytes gmem, 72 bytes cmem[3]\r\nptxas info    : Compiling entry function '_Z23bicubic_interp2d_kerneliddb15THCDeviceTensorIdLi4Ei16DefaultPtrTraitsES1_' for 'sm_61'\r\nptxas info    : Function properties for _Z23bicubic_interp2d_kerneliddb15THCDeviceTensorIdLi4Ei16DefaultPtrTraitsES1_\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 64 registers, 432 bytes cmem[0], 24 bytes cmem[2]\r\n```\r\n\r\n`New` uses `124` registers, which is too much for my card:\r\n\r\n```\r\nptxas info    : 77696 bytes gmem, 72 bytes cmem[3]\r\nptxas info    : Compiling entry function '_ZN2at6native76_GLOBAL__N__52_tmpxft_0000626d_00000000_6_UpSampleBicubic2d_cpp1_ii_b4c1e1f328upsample_bicubic2d_out_frameElddbNS_20PackedTensorAccessorIdLm4ENS_16DefaultPtrTraitsElEES4_' for 'sm_61'\r\nptxas info    : Function properties for _ZN2at6native76_GLOBAL__N__52_tmpxft_0000626d_00000000_6_UpSampleBicubic2d_cpp1_ii_b4c1e1f328upsample_bicubic2d_out_frameElddbNS_20PackedTensorAccessorIdLm4ENS_16DefaultPtrTraitsElEES4_\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 124 registers, 496 bytes cmem[0], 24 bytes cmem[2]\r\n```\r\n\r\nWith `__launch_bounds__(1024)`, the code again uses `64` registers.\r\n\r\nIf you use `C10_LAUNCH_BOUNDS_1(1024)` **for both kernels**, the tests pass here.\r\n\r\n\r\nNow *why* is the regcount higher in the new code? It could be `PackedTensorAccessor`, it could be the fact that many instances of `int` have been changed to `int64_t`. :)\r\n\r\nYou could experiment or just use the launch bounds.  Other code in `native/cuda` seems to use lower bounds for `blockDim`, too.  1024 seems to be an outlier.\n<issue_comment>username_0: @username_2 \r\n\r\nit seems it worked locally! thank you so much!  I really appreciate that!\n<issue_comment>username_0: thanks @username_1 and @username_2 for all the help!\r\n\r\n@username_3 it is done for review!\n<issue_comment>username_3: CircleCI runs on AWS, and the I'm fairly sure we've got one of the g3 types, either g3.4xlarge or g3.8xlarge\n<issue_comment>username_3: cc @ngimel, if you want to take a look at this (I'm planning to do a review too)\n<issue_comment>username_0: @username_4 could it be addressed in another PR?\n<issue_comment>username_4: @username_0 Sure, it's definitely better to rename both cpu & gpu impls together in a separate PR. Just want to make sure we have the correct one after porting.\n<issue_comment>username_3: Just as a clarification: I'm OK with having changes which are improvements over the old kernels come in latter patches.\n<issue_comment>username_3: I finished reviewing one of the kernels, and did a cursory scan of the others. In general the patch looks like it's in good shape. I'd suggest that we fix up the major correctness issues (PackedTensorAccessor ref, and int32 versus int64 indexing) and anything small that is easy to fix (for example, double zeroing the array), and leave other improvements for follow ups. Should be ready to land soon!\n<issue_comment>username_0: thanks so much @username_3!\r\nI will let you know when it is ready again for a new review! thanks!\n<issue_comment>username_0: @username_2 @username_3 \r\nnot sure but the errors on CI seems to be related to jenkins ... it seems all these jobs that failed (for building) ran by 01h 01min ... not sure if it is a coincidence ...\n<issue_comment>username_1: @username_0 indeed it looks like all jobs were aborted at the same time. no obvious issues in the build log related to your code. Comparing e.g. the `cuda9-cudnn7` build with a successful one from another PR, it takes 1hr 14min there and your build is about at the place where the other one is after an hour.\r\n\r\nI suggest to just push a new commit to rebuild. Probably a temporary CI hiccup.\n<issue_comment>username_3: I accidentally rebooted Jenkins yesterday which is the likely cause, my apologies.\r\n\r\n@pytorchbot retest this please\n<issue_comment>username_0: @username_3 @username_2 @username_1 \r\n\r\nall tests passed except `pr/caffe2-py2-cuda9.0-cudnn7-windows-build`:\r\n \r\n```\r\n14:43:49 Build timed out (after 180 minutes). Marking the build as failed.\r\n14:43:49 Build was aborted\r\n14:43:49 [BFA] Scanning build for known causes...\r\n14:43:49 [BFA] No failure causes found\r\n14:43:49 [BFA] Done. 0s\r\n14:43:49 Finished: FAILURE\r\n```\r\n\r\nnot sure if this timeout means that the code now is slower.\r\n\r\nwhat do you think? do you have any suggestion?\n<issue_comment>username_3: Sometimes the Windows build flakes out like that. It didn't timeout while running a relevant test, so I judge it to be not your problem.\n<issue_comment>username_3: The launch bounds logic is wrong, but I acknowledge that this is a big patch already; just fix it in a follow up. I am going to go ahead and land this.\n<issue_comment>username_0: sounds good @username_3 thanks!","repo":"pytorch/pytorch","issue_id":436372857,"issue_number":19630}
{"question":"Is there a pattern in the duration of 01 hour and 01 minute for job failures on CI related to Jenkins, or is it random?","answer":"The duration of 01 hour and 01 minute for job failures on CI related to Jenkins is not a coincidence and may indicate a recurring issue or problem with the jobs at that specific time.","id":167,"text":"@username_2 @username_3 \r\nnot sure but the errors on CI seems to be related to jenkins ... it seems all these jobs that failed (for building) ran by 01h 01min ... not sure if it is a coincidence ...","context":"<issue_start><issue_comment>Title: [WIP] UpSample GPU Porting \nusername_0: resolves #16158\n<issue_comment>username_1: Fewer errors than before, 4 instead of 8 with the same message:\r\n```\r\nRuntimeError: CUDA error: too many resources requested for launch\r\n```\r\nThis seems to be an existing problem: gh-8103.\r\n\r\n@username_2 could you have a look at this and tell us what you think?\n<issue_comment>username_2: Looking at it very briefly, the `Jetson TX` referenced in the issue only has 256 cores.\r\n\r\nSo while the issue looks the same, I'd probably not expect it on the CI platforms.  FWIW, some recent CI tests in other issues are green.\n<issue_comment>username_2: @pytorchbot retest this please.\n<issue_comment>username_1: same failures.\n<issue_comment>username_2: Is this rebased on the latest master (you can also ask the bot to rebase)?\n<issue_comment>username_0: I rebased that yesterday ... also needed to resolve conflicts because 7 days ago upsample files were changed.\n<issue_comment>username_0: @username_2 do you have any idea about what could be this problem? or a way to debug that?\r\nalso it seems some jobs is taking a lot of time to build, for example, for some job the estimate time is 19h .. not sure if it the regular estimate ..\n<issue_comment>username_2: @username_0 If you can't reproduce it at home it seems hard to debug other than reading at the diffs again.  I can take a look on Monday, it's getting a bit late here.\n<issue_comment>username_2: Also, has anyone found a way to show the actual hardware used on the CI in detail?\n<issue_comment>username_0: @username_2 I am working in parallel on a paperspace environment .. it tooks a lot of time it is running with 8 cpu cores.\n<issue_comment>username_2: But have you ever been able to reproduce this issue on paperspace?\n<issue_comment>username_1: I can reproduce it locally: Arch Linux, CUDA 10.0, RTX2070 GPU. Given the CI failures, I think it should be reproducible for multiple CUDA and GPU versions. I just haven't worked on any CUDA code before, and am short on time, so I'd rather not dig too deep.\n<issue_comment>username_0: not yet .. my last building was with a my previous commit ... I will work again in this task in some minutes :)\n<issue_comment>username_1: @username_0 your previous commit had the same failure though (except for the less clear exception message), and it seems quite reproducible. So I think you'll see it now.\n<issue_comment>username_2: Ah, thanks.  @username_0, then I guess just compiling with \"DEBUG=1\" and stepping through the offending code with gdb may be the fastest way.\n<issue_comment>username_0: @username_2 thanks! I will try that!\n<issue_comment>username_0: it seems just UpSampleBicubic2d is using upsample_get_value_bounded (https://github.com/pytorch/pytorch/pull/19630/files#diff-5092da792c30694ee4adf0d0ae2a37c6R171) and upsample_increment_value_bounded (https://github.com/pytorch/pytorch/pull/19630/files#diff-5092da792c30694ee4adf0d0ae2a37c6R191)\r\n\r\nmaybe the problem could be inside one of these functions ... maybe related to the order of indexes (x, y) ...\r\n\r\nbut the problem seems to be related to cuda block/threads ... so not sure if it is really related to these functions.\n<issue_comment>username_2: The new code uses far more registers than the existing one. I verified that the both versions actually call the offending test case with `1024` in `blockDim`.  So it's very likely a register issue.\r\n\r\n`Existing` uses `64` registers, which seems to be optimal or my card:\r\n\r\n```\r\nptxas info    : 77696 bytes gmem, 72 bytes cmem[3]\r\nptxas info    : Compiling entry function '_Z23bicubic_interp2d_kerneliddb15THCDeviceTensorIdLi4Ei16DefaultPtrTraitsES1_' for 'sm_61'\r\nptxas info    : Function properties for _Z23bicubic_interp2d_kerneliddb15THCDeviceTensorIdLi4Ei16DefaultPtrTraitsES1_\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 64 registers, 432 bytes cmem[0], 24 bytes cmem[2]\r\n```\r\n\r\n`New` uses `124` registers, which is too much for my card:\r\n\r\n```\r\nptxas info    : 77696 bytes gmem, 72 bytes cmem[3]\r\nptxas info    : Compiling entry function '_ZN2at6native76_GLOBAL__N__52_tmpxft_0000626d_00000000_6_UpSampleBicubic2d_cpp1_ii_b4c1e1f328upsample_bicubic2d_out_frameElddbNS_20PackedTensorAccessorIdLm4ENS_16DefaultPtrTraitsElEES4_' for 'sm_61'\r\nptxas info    : Function properties for _ZN2at6native76_GLOBAL__N__52_tmpxft_0000626d_00000000_6_UpSampleBicubic2d_cpp1_ii_b4c1e1f328upsample_bicubic2d_out_frameElddbNS_20PackedTensorAccessorIdLm4ENS_16DefaultPtrTraitsElEES4_\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 124 registers, 496 bytes cmem[0], 24 bytes cmem[2]\r\n```\r\n\r\nWith `__launch_bounds__(1024)`, the code again uses `64` registers.\r\n\r\nIf you use `C10_LAUNCH_BOUNDS_1(1024)` **for both kernels**, the tests pass here.\r\n\r\n\r\nNow *why* is the regcount higher in the new code? It could be `PackedTensorAccessor`, it could be the fact that many instances of `int` have been changed to `int64_t`. :)\r\n\r\nYou could experiment or just use the launch bounds.  Other code in `native/cuda` seems to use lower bounds for `blockDim`, too.  1024 seems to be an outlier.\n<issue_comment>username_0: @username_2 \r\n\r\nit seems it worked locally! thank you so much!  I really appreciate that!\n<issue_comment>username_0: thanks @username_1 and @username_2 for all the help!\r\n\r\n@username_3 it is done for review!\n<issue_comment>username_3: CircleCI runs on AWS, and the I'm fairly sure we've got one of the g3 types, either g3.4xlarge or g3.8xlarge\n<issue_comment>username_3: cc @ngimel, if you want to take a look at this (I'm planning to do a review too)\n<issue_comment>username_0: @username_4 could it be addressed in another PR?\n<issue_comment>username_4: @username_0 Sure, it's definitely better to rename both cpu & gpu impls together in a separate PR. Just want to make sure we have the correct one after porting.\n<issue_comment>username_3: Just as a clarification: I'm OK with having changes which are improvements over the old kernels come in latter patches.\n<issue_comment>username_3: I finished reviewing one of the kernels, and did a cursory scan of the others. In general the patch looks like it's in good shape. I'd suggest that we fix up the major correctness issues (PackedTensorAccessor ref, and int32 versus int64 indexing) and anything small that is easy to fix (for example, double zeroing the array), and leave other improvements for follow ups. Should be ready to land soon!\n<issue_comment>username_0: thanks so much @username_3!\r\nI will let you know when it is ready again for a new review! thanks!\n<issue_comment>username_0: @username_2 @username_3 \r\nnot sure but the errors on CI seems to be related to jenkins ... it seems all these jobs that failed (for building) ran by 01h 01min ... not sure if it is a coincidence ...\n<issue_comment>username_1: @username_0 indeed it looks like all jobs were aborted at the same time. no obvious issues in the build log related to your code. Comparing e.g. the `cuda9-cudnn7` build with a successful one from another PR, it takes 1hr 14min there and your build is about at the place where the other one is after an hour.\r\n\r\nI suggest to just push a new commit to rebuild. Probably a temporary CI hiccup.\n<issue_comment>username_3: I accidentally rebooted Jenkins yesterday which is the likely cause, my apologies.\r\n\r\n@pytorchbot retest this please\n<issue_comment>username_0: @username_3 @username_2 @username_1 \r\n\r\nall tests passed except `pr/caffe2-py2-cuda9.0-cudnn7-windows-build`:\r\n \r\n```\r\n14:43:49 Build timed out (after 180 minutes). Marking the build as failed.\r\n14:43:49 Build was aborted\r\n14:43:49 [BFA] Scanning build for known causes...\r\n14:43:49 [BFA] No failure causes found\r\n14:43:49 [BFA] Done. 0s\r\n14:43:49 Finished: FAILURE\r\n```\r\n\r\nnot sure if this timeout means that the code now is slower.\r\n\r\nwhat do you think? do you have any suggestion?\n<issue_comment>username_3: Sometimes the Windows build flakes out like that. It didn't timeout while running a relevant test, so I judge it to be not your problem.\n<issue_comment>username_3: The launch bounds logic is wrong, but I acknowledge that this is a big patch already; just fix it in a follow up. I am going to go ahead and land this.\n<issue_comment>username_0: sounds good @username_3 thanks!","repo":"pytorch/pytorch","issue_id":436372857,"issue_number":19630}
{"question":"What suggestion was given to address the issue of aborted jobs in the build process?","answer":"The suggestion given was to push a new commit to trigger a rebuild, as it was believed to be a temporary CI hiccup.","id":168,"text":"@username_0 indeed it looks like all jobs were aborted at the same time. no obvious issues in the build log related to your code. Comparing e.g. the `cuda9-cudnn7` build with a successful one from another PR, it takes 1hr 14min there and your build is about at the place where the other one is after an hour.\r\n\r\nI suggest to just push a new commit to rebuild. Probably a temporary CI hiccup.","context":"<issue_start><issue_comment>Title: [WIP] UpSample GPU Porting \nusername_0: resolves #16158\n<issue_comment>username_1: Fewer errors than before, 4 instead of 8 with the same message:\r\n```\r\nRuntimeError: CUDA error: too many resources requested for launch\r\n```\r\nThis seems to be an existing problem: gh-8103.\r\n\r\n@username_2 could you have a look at this and tell us what you think?\n<issue_comment>username_2: Looking at it very briefly, the `Jetson TX` referenced in the issue only has 256 cores.\r\n\r\nSo while the issue looks the same, I'd probably not expect it on the CI platforms.  FWIW, some recent CI tests in other issues are green.\n<issue_comment>username_2: @pytorchbot retest this please.\n<issue_comment>username_1: same failures.\n<issue_comment>username_2: Is this rebased on the latest master (you can also ask the bot to rebase)?\n<issue_comment>username_0: I rebased that yesterday ... also needed to resolve conflicts because 7 days ago upsample files were changed.\n<issue_comment>username_0: @username_2 do you have any idea about what could be this problem? or a way to debug that?\r\nalso it seems some jobs is taking a lot of time to build, for example, for some job the estimate time is 19h .. not sure if it the regular estimate ..\n<issue_comment>username_2: @username_0 If you can't reproduce it at home it seems hard to debug other than reading at the diffs again.  I can take a look on Monday, it's getting a bit late here.\n<issue_comment>username_2: Also, has anyone found a way to show the actual hardware used on the CI in detail?\n<issue_comment>username_0: @username_2 I am working in parallel on a paperspace environment .. it tooks a lot of time it is running with 8 cpu cores.\n<issue_comment>username_2: But have you ever been able to reproduce this issue on paperspace?\n<issue_comment>username_1: I can reproduce it locally: Arch Linux, CUDA 10.0, RTX2070 GPU. Given the CI failures, I think it should be reproducible for multiple CUDA and GPU versions. I just haven't worked on any CUDA code before, and am short on time, so I'd rather not dig too deep.\n<issue_comment>username_0: not yet .. my last building was with a my previous commit ... I will work again in this task in some minutes :)\n<issue_comment>username_1: @username_0 your previous commit had the same failure though (except for the less clear exception message), and it seems quite reproducible. So I think you'll see it now.\n<issue_comment>username_2: Ah, thanks.  @username_0, then I guess just compiling with \"DEBUG=1\" and stepping through the offending code with gdb may be the fastest way.\n<issue_comment>username_0: @username_2 thanks! I will try that!\n<issue_comment>username_0: it seems just UpSampleBicubic2d is using upsample_get_value_bounded (https://github.com/pytorch/pytorch/pull/19630/files#diff-5092da792c30694ee4adf0d0ae2a37c6R171) and upsample_increment_value_bounded (https://github.com/pytorch/pytorch/pull/19630/files#diff-5092da792c30694ee4adf0d0ae2a37c6R191)\r\n\r\nmaybe the problem could be inside one of these functions ... maybe related to the order of indexes (x, y) ...\r\n\r\nbut the problem seems to be related to cuda block/threads ... so not sure if it is really related to these functions.\n<issue_comment>username_2: The new code uses far more registers than the existing one. I verified that the both versions actually call the offending test case with `1024` in `blockDim`.  So it's very likely a register issue.\r\n\r\n`Existing` uses `64` registers, which seems to be optimal or my card:\r\n\r\n```\r\nptxas info    : 77696 bytes gmem, 72 bytes cmem[3]\r\nptxas info    : Compiling entry function '_Z23bicubic_interp2d_kerneliddb15THCDeviceTensorIdLi4Ei16DefaultPtrTraitsES1_' for 'sm_61'\r\nptxas info    : Function properties for _Z23bicubic_interp2d_kerneliddb15THCDeviceTensorIdLi4Ei16DefaultPtrTraitsES1_\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 64 registers, 432 bytes cmem[0], 24 bytes cmem[2]\r\n```\r\n\r\n`New` uses `124` registers, which is too much for my card:\r\n\r\n```\r\nptxas info    : 77696 bytes gmem, 72 bytes cmem[3]\r\nptxas info    : Compiling entry function '_ZN2at6native76_GLOBAL__N__52_tmpxft_0000626d_00000000_6_UpSampleBicubic2d_cpp1_ii_b4c1e1f328upsample_bicubic2d_out_frameElddbNS_20PackedTensorAccessorIdLm4ENS_16DefaultPtrTraitsElEES4_' for 'sm_61'\r\nptxas info    : Function properties for _ZN2at6native76_GLOBAL__N__52_tmpxft_0000626d_00000000_6_UpSampleBicubic2d_cpp1_ii_b4c1e1f328upsample_bicubic2d_out_frameElddbNS_20PackedTensorAccessorIdLm4ENS_16DefaultPtrTraitsElEES4_\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 124 registers, 496 bytes cmem[0], 24 bytes cmem[2]\r\n```\r\n\r\nWith `__launch_bounds__(1024)`, the code again uses `64` registers.\r\n\r\nIf you use `C10_LAUNCH_BOUNDS_1(1024)` **for both kernels**, the tests pass here.\r\n\r\n\r\nNow *why* is the regcount higher in the new code? It could be `PackedTensorAccessor`, it could be the fact that many instances of `int` have been changed to `int64_t`. :)\r\n\r\nYou could experiment or just use the launch bounds.  Other code in `native/cuda` seems to use lower bounds for `blockDim`, too.  1024 seems to be an outlier.\n<issue_comment>username_0: @username_2 \r\n\r\nit seems it worked locally! thank you so much!  I really appreciate that!\n<issue_comment>username_0: thanks @username_1 and @username_2 for all the help!\r\n\r\n@username_3 it is done for review!\n<issue_comment>username_3: CircleCI runs on AWS, and the I'm fairly sure we've got one of the g3 types, either g3.4xlarge or g3.8xlarge\n<issue_comment>username_3: cc @ngimel, if you want to take a look at this (I'm planning to do a review too)\n<issue_comment>username_0: @username_4 could it be addressed in another PR?\n<issue_comment>username_4: @username_0 Sure, it's definitely better to rename both cpu & gpu impls together in a separate PR. Just want to make sure we have the correct one after porting.\n<issue_comment>username_3: Just as a clarification: I'm OK with having changes which are improvements over the old kernels come in latter patches.\n<issue_comment>username_3: I finished reviewing one of the kernels, and did a cursory scan of the others. In general the patch looks like it's in good shape. I'd suggest that we fix up the major correctness issues (PackedTensorAccessor ref, and int32 versus int64 indexing) and anything small that is easy to fix (for example, double zeroing the array), and leave other improvements for follow ups. Should be ready to land soon!\n<issue_comment>username_0: thanks so much @username_3!\r\nI will let you know when it is ready again for a new review! thanks!\n<issue_comment>username_0: @username_2 @username_3 \r\nnot sure but the errors on CI seems to be related to jenkins ... it seems all these jobs that failed (for building) ran by 01h 01min ... not sure if it is a coincidence ...\n<issue_comment>username_1: @username_0 indeed it looks like all jobs were aborted at the same time. no obvious issues in the build log related to your code. Comparing e.g. the `cuda9-cudnn7` build with a successful one from another PR, it takes 1hr 14min there and your build is about at the place where the other one is after an hour.\r\n\r\nI suggest to just push a new commit to rebuild. Probably a temporary CI hiccup.\n<issue_comment>username_3: I accidentally rebooted Jenkins yesterday which is the likely cause, my apologies.\r\n\r\n@pytorchbot retest this please\n<issue_comment>username_0: @username_3 @username_2 @username_1 \r\n\r\nall tests passed except `pr/caffe2-py2-cuda9.0-cudnn7-windows-build`:\r\n \r\n```\r\n14:43:49 Build timed out (after 180 minutes). Marking the build as failed.\r\n14:43:49 Build was aborted\r\n14:43:49 [BFA] Scanning build for known causes...\r\n14:43:49 [BFA] No failure causes found\r\n14:43:49 [BFA] Done. 0s\r\n14:43:49 Finished: FAILURE\r\n```\r\n\r\nnot sure if this timeout means that the code now is slower.\r\n\r\nwhat do you think? do you have any suggestion?\n<issue_comment>username_3: Sometimes the Windows build flakes out like that. It didn't timeout while running a relevant test, so I judge it to be not your problem.\n<issue_comment>username_3: The launch bounds logic is wrong, but I acknowledge that this is a big patch already; just fix it in a follow up. I am going to go ahead and land this.\n<issue_comment>username_0: sounds good @username_3 thanks!","repo":"pytorch/pytorch","issue_id":436372857,"issue_number":19630}
{"question":"What was the outcome of the build for 'pr/caffe2-py2-cuda9.0-cudnn7-windows-build' test?","answer":"The build timed out after 180 minutes, resulting in it being marked as failed. The build was then aborted with no specific failure causes detected, and the final status was 'FAILURE'.","id":170,"text":"@username_3 @username_2 @username_1 \r\n\r\nall tests passed except `pr/caffe2-py2-cuda9.0-cudnn7-windows-build`:\r\n \r\n```\r\n14:43:49 Build timed out (after 180 minutes). Marking the build as failed.\r\n14:43:49 Build was aborted\r\n14:43:49 [BFA] Scanning build for known causes...\r\n14:43:49 [BFA] No failure causes found\r\n14:43:49 [BFA] Done. 0s\r\n14:43:49 Finished: FAILURE\r\n```\r\n\r\nnot sure if this timeout means that the code now is slower.\r\n\r\nwhat do you think? do you have any suggestion?","context":"<issue_start><issue_comment>Title: [WIP] UpSample GPU Porting \nusername_0: resolves #16158\n<issue_comment>username_1: Fewer errors than before, 4 instead of 8 with the same message:\r\n```\r\nRuntimeError: CUDA error: too many resources requested for launch\r\n```\r\nThis seems to be an existing problem: gh-8103.\r\n\r\n@username_2 could you have a look at this and tell us what you think?\n<issue_comment>username_2: Looking at it very briefly, the `Jetson TX` referenced in the issue only has 256 cores.\r\n\r\nSo while the issue looks the same, I'd probably not expect it on the CI platforms.  FWIW, some recent CI tests in other issues are green.\n<issue_comment>username_2: @pytorchbot retest this please.\n<issue_comment>username_1: same failures.\n<issue_comment>username_2: Is this rebased on the latest master (you can also ask the bot to rebase)?\n<issue_comment>username_0: I rebased that yesterday ... also needed to resolve conflicts because 7 days ago upsample files were changed.\n<issue_comment>username_0: @username_2 do you have any idea about what could be this problem? or a way to debug that?\r\nalso it seems some jobs is taking a lot of time to build, for example, for some job the estimate time is 19h .. not sure if it the regular estimate ..\n<issue_comment>username_2: @username_0 If you can't reproduce it at home it seems hard to debug other than reading at the diffs again.  I can take a look on Monday, it's getting a bit late here.\n<issue_comment>username_2: Also, has anyone found a way to show the actual hardware used on the CI in detail?\n<issue_comment>username_0: @username_2 I am working in parallel on a paperspace environment .. it tooks a lot of time it is running with 8 cpu cores.\n<issue_comment>username_2: But have you ever been able to reproduce this issue on paperspace?\n<issue_comment>username_1: I can reproduce it locally: Arch Linux, CUDA 10.0, RTX2070 GPU. Given the CI failures, I think it should be reproducible for multiple CUDA and GPU versions. I just haven't worked on any CUDA code before, and am short on time, so I'd rather not dig too deep.\n<issue_comment>username_0: not yet .. my last building was with a my previous commit ... I will work again in this task in some minutes :)\n<issue_comment>username_1: @username_0 your previous commit had the same failure though (except for the less clear exception message), and it seems quite reproducible. So I think you'll see it now.\n<issue_comment>username_2: Ah, thanks.  @username_0, then I guess just compiling with \"DEBUG=1\" and stepping through the offending code with gdb may be the fastest way.\n<issue_comment>username_0: @username_2 thanks! I will try that!\n<issue_comment>username_0: it seems just UpSampleBicubic2d is using upsample_get_value_bounded (https://github.com/pytorch/pytorch/pull/19630/files#diff-5092da792c30694ee4adf0d0ae2a37c6R171) and upsample_increment_value_bounded (https://github.com/pytorch/pytorch/pull/19630/files#diff-5092da792c30694ee4adf0d0ae2a37c6R191)\r\n\r\nmaybe the problem could be inside one of these functions ... maybe related to the order of indexes (x, y) ...\r\n\r\nbut the problem seems to be related to cuda block/threads ... so not sure if it is really related to these functions.\n<issue_comment>username_2: The new code uses far more registers than the existing one. I verified that the both versions actually call the offending test case with `1024` in `blockDim`.  So it's very likely a register issue.\r\n\r\n`Existing` uses `64` registers, which seems to be optimal or my card:\r\n\r\n```\r\nptxas info    : 77696 bytes gmem, 72 bytes cmem[3]\r\nptxas info    : Compiling entry function '_Z23bicubic_interp2d_kerneliddb15THCDeviceTensorIdLi4Ei16DefaultPtrTraitsES1_' for 'sm_61'\r\nptxas info    : Function properties for _Z23bicubic_interp2d_kerneliddb15THCDeviceTensorIdLi4Ei16DefaultPtrTraitsES1_\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 64 registers, 432 bytes cmem[0], 24 bytes cmem[2]\r\n```\r\n\r\n`New` uses `124` registers, which is too much for my card:\r\n\r\n```\r\nptxas info    : 77696 bytes gmem, 72 bytes cmem[3]\r\nptxas info    : Compiling entry function '_ZN2at6native76_GLOBAL__N__52_tmpxft_0000626d_00000000_6_UpSampleBicubic2d_cpp1_ii_b4c1e1f328upsample_bicubic2d_out_frameElddbNS_20PackedTensorAccessorIdLm4ENS_16DefaultPtrTraitsElEES4_' for 'sm_61'\r\nptxas info    : Function properties for _ZN2at6native76_GLOBAL__N__52_tmpxft_0000626d_00000000_6_UpSampleBicubic2d_cpp1_ii_b4c1e1f328upsample_bicubic2d_out_frameElddbNS_20PackedTensorAccessorIdLm4ENS_16DefaultPtrTraitsElEES4_\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 124 registers, 496 bytes cmem[0], 24 bytes cmem[2]\r\n```\r\n\r\nWith `__launch_bounds__(1024)`, the code again uses `64` registers.\r\n\r\nIf you use `C10_LAUNCH_BOUNDS_1(1024)` **for both kernels**, the tests pass here.\r\n\r\n\r\nNow *why* is the regcount higher in the new code? It could be `PackedTensorAccessor`, it could be the fact that many instances of `int` have been changed to `int64_t`. :)\r\n\r\nYou could experiment or just use the launch bounds.  Other code in `native/cuda` seems to use lower bounds for `blockDim`, too.  1024 seems to be an outlier.\n<issue_comment>username_0: @username_2 \r\n\r\nit seems it worked locally! thank you so much!  I really appreciate that!\n<issue_comment>username_0: thanks @username_1 and @username_2 for all the help!\r\n\r\n@username_3 it is done for review!\n<issue_comment>username_3: CircleCI runs on AWS, and the I'm fairly sure we've got one of the g3 types, either g3.4xlarge or g3.8xlarge\n<issue_comment>username_3: cc @ngimel, if you want to take a look at this (I'm planning to do a review too)\n<issue_comment>username_0: @username_4 could it be addressed in another PR?\n<issue_comment>username_4: @username_0 Sure, it's definitely better to rename both cpu & gpu impls together in a separate PR. Just want to make sure we have the correct one after porting.\n<issue_comment>username_3: Just as a clarification: I'm OK with having changes which are improvements over the old kernels come in latter patches.\n<issue_comment>username_3: I finished reviewing one of the kernels, and did a cursory scan of the others. In general the patch looks like it's in good shape. I'd suggest that we fix up the major correctness issues (PackedTensorAccessor ref, and int32 versus int64 indexing) and anything small that is easy to fix (for example, double zeroing the array), and leave other improvements for follow ups. Should be ready to land soon!\n<issue_comment>username_0: thanks so much @username_3!\r\nI will let you know when it is ready again for a new review! thanks!\n<issue_comment>username_0: @username_2 @username_3 \r\nnot sure but the errors on CI seems to be related to jenkins ... it seems all these jobs that failed (for building) ran by 01h 01min ... not sure if it is a coincidence ...\n<issue_comment>username_1: @username_0 indeed it looks like all jobs were aborted at the same time. no obvious issues in the build log related to your code. Comparing e.g. the `cuda9-cudnn7` build with a successful one from another PR, it takes 1hr 14min there and your build is about at the place where the other one is after an hour.\r\n\r\nI suggest to just push a new commit to rebuild. Probably a temporary CI hiccup.\n<issue_comment>username_3: I accidentally rebooted Jenkins yesterday which is the likely cause, my apologies.\r\n\r\n@pytorchbot retest this please\n<issue_comment>username_0: @username_3 @username_2 @username_1 \r\n\r\nall tests passed except `pr/caffe2-py2-cuda9.0-cudnn7-windows-build`:\r\n \r\n```\r\n14:43:49 Build timed out (after 180 minutes). Marking the build as failed.\r\n14:43:49 Build was aborted\r\n14:43:49 [BFA] Scanning build for known causes...\r\n14:43:49 [BFA] No failure causes found\r\n14:43:49 [BFA] Done. 0s\r\n14:43:49 Finished: FAILURE\r\n```\r\n\r\nnot sure if this timeout means that the code now is slower.\r\n\r\nwhat do you think? do you have any suggestion?\n<issue_comment>username_3: Sometimes the Windows build flakes out like that. It didn't timeout while running a relevant test, so I judge it to be not your problem.\n<issue_comment>username_3: The launch bounds logic is wrong, but I acknowledge that this is a big patch already; just fix it in a follow up. I am going to go ahead and land this.\n<issue_comment>username_0: sounds good @username_3 thanks!","repo":"pytorch/pytorch","issue_id":436372857,"issue_number":19630}
{"question":"What is the author's approach to the incorrect launch bounds logic in the patch?","answer":"The author acknowledges the issue with the launch bounds logic but plans to fix it in a follow-up patch while proceeding to land the current patch.","id":172,"text":"The launch bounds logic is wrong, but I acknowledge that this is a big patch already; just fix it in a follow up. I am going to go ahead and land this.","context":"<issue_start><issue_comment>Title: [WIP] UpSample GPU Porting \nusername_0: resolves #16158\n<issue_comment>username_1: Fewer errors than before, 4 instead of 8 with the same message:\r\n```\r\nRuntimeError: CUDA error: too many resources requested for launch\r\n```\r\nThis seems to be an existing problem: gh-8103.\r\n\r\n@username_2 could you have a look at this and tell us what you think?\n<issue_comment>username_2: Looking at it very briefly, the `Jetson TX` referenced in the issue only has 256 cores.\r\n\r\nSo while the issue looks the same, I'd probably not expect it on the CI platforms.  FWIW, some recent CI tests in other issues are green.\n<issue_comment>username_2: @pytorchbot retest this please.\n<issue_comment>username_1: same failures.\n<issue_comment>username_2: Is this rebased on the latest master (you can also ask the bot to rebase)?\n<issue_comment>username_0: I rebased that yesterday ... also needed to resolve conflicts because 7 days ago upsample files were changed.\n<issue_comment>username_0: @username_2 do you have any idea about what could be this problem? or a way to debug that?\r\nalso it seems some jobs is taking a lot of time to build, for example, for some job the estimate time is 19h .. not sure if it the regular estimate ..\n<issue_comment>username_2: @username_0 If you can't reproduce it at home it seems hard to debug other than reading at the diffs again.  I can take a look on Monday, it's getting a bit late here.\n<issue_comment>username_2: Also, has anyone found a way to show the actual hardware used on the CI in detail?\n<issue_comment>username_0: @username_2 I am working in parallel on a paperspace environment .. it tooks a lot of time it is running with 8 cpu cores.\n<issue_comment>username_2: But have you ever been able to reproduce this issue on paperspace?\n<issue_comment>username_1: I can reproduce it locally: Arch Linux, CUDA 10.0, RTX2070 GPU. Given the CI failures, I think it should be reproducible for multiple CUDA and GPU versions. I just haven't worked on any CUDA code before, and am short on time, so I'd rather not dig too deep.\n<issue_comment>username_0: not yet .. my last building was with a my previous commit ... I will work again in this task in some minutes :)\n<issue_comment>username_1: @username_0 your previous commit had the same failure though (except for the less clear exception message), and it seems quite reproducible. So I think you'll see it now.\n<issue_comment>username_2: Ah, thanks.  @username_0, then I guess just compiling with \"DEBUG=1\" and stepping through the offending code with gdb may be the fastest way.\n<issue_comment>username_0: @username_2 thanks! I will try that!\n<issue_comment>username_0: it seems just UpSampleBicubic2d is using upsample_get_value_bounded (https://github.com/pytorch/pytorch/pull/19630/files#diff-5092da792c30694ee4adf0d0ae2a37c6R171) and upsample_increment_value_bounded (https://github.com/pytorch/pytorch/pull/19630/files#diff-5092da792c30694ee4adf0d0ae2a37c6R191)\r\n\r\nmaybe the problem could be inside one of these functions ... maybe related to the order of indexes (x, y) ...\r\n\r\nbut the problem seems to be related to cuda block/threads ... so not sure if it is really related to these functions.\n<issue_comment>username_2: The new code uses far more registers than the existing one. I verified that the both versions actually call the offending test case with `1024` in `blockDim`.  So it's very likely a register issue.\r\n\r\n`Existing` uses `64` registers, which seems to be optimal or my card:\r\n\r\n```\r\nptxas info    : 77696 bytes gmem, 72 bytes cmem[3]\r\nptxas info    : Compiling entry function '_Z23bicubic_interp2d_kerneliddb15THCDeviceTensorIdLi4Ei16DefaultPtrTraitsES1_' for 'sm_61'\r\nptxas info    : Function properties for _Z23bicubic_interp2d_kerneliddb15THCDeviceTensorIdLi4Ei16DefaultPtrTraitsES1_\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 64 registers, 432 bytes cmem[0], 24 bytes cmem[2]\r\n```\r\n\r\n`New` uses `124` registers, which is too much for my card:\r\n\r\n```\r\nptxas info    : 77696 bytes gmem, 72 bytes cmem[3]\r\nptxas info    : Compiling entry function '_ZN2at6native76_GLOBAL__N__52_tmpxft_0000626d_00000000_6_UpSampleBicubic2d_cpp1_ii_b4c1e1f328upsample_bicubic2d_out_frameElddbNS_20PackedTensorAccessorIdLm4ENS_16DefaultPtrTraitsElEES4_' for 'sm_61'\r\nptxas info    : Function properties for _ZN2at6native76_GLOBAL__N__52_tmpxft_0000626d_00000000_6_UpSampleBicubic2d_cpp1_ii_b4c1e1f328upsample_bicubic2d_out_frameElddbNS_20PackedTensorAccessorIdLm4ENS_16DefaultPtrTraitsElEES4_\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 124 registers, 496 bytes cmem[0], 24 bytes cmem[2]\r\n```\r\n\r\nWith `__launch_bounds__(1024)`, the code again uses `64` registers.\r\n\r\nIf you use `C10_LAUNCH_BOUNDS_1(1024)` **for both kernels**, the tests pass here.\r\n\r\n\r\nNow *why* is the regcount higher in the new code? It could be `PackedTensorAccessor`, it could be the fact that many instances of `int` have been changed to `int64_t`. :)\r\n\r\nYou could experiment or just use the launch bounds.  Other code in `native/cuda` seems to use lower bounds for `blockDim`, too.  1024 seems to be an outlier.\n<issue_comment>username_0: @username_2 \r\n\r\nit seems it worked locally! thank you so much!  I really appreciate that!\n<issue_comment>username_0: thanks @username_1 and @username_2 for all the help!\r\n\r\n@username_3 it is done for review!\n<issue_comment>username_3: CircleCI runs on AWS, and the I'm fairly sure we've got one of the g3 types, either g3.4xlarge or g3.8xlarge\n<issue_comment>username_3: cc @ngimel, if you want to take a look at this (I'm planning to do a review too)\n<issue_comment>username_0: @username_4 could it be addressed in another PR?\n<issue_comment>username_4: @username_0 Sure, it's definitely better to rename both cpu & gpu impls together in a separate PR. Just want to make sure we have the correct one after porting.\n<issue_comment>username_3: Just as a clarification: I'm OK with having changes which are improvements over the old kernels come in latter patches.\n<issue_comment>username_3: I finished reviewing one of the kernels, and did a cursory scan of the others. In general the patch looks like it's in good shape. I'd suggest that we fix up the major correctness issues (PackedTensorAccessor ref, and int32 versus int64 indexing) and anything small that is easy to fix (for example, double zeroing the array), and leave other improvements for follow ups. Should be ready to land soon!\n<issue_comment>username_0: thanks so much @username_3!\r\nI will let you know when it is ready again for a new review! thanks!\n<issue_comment>username_0: @username_2 @username_3 \r\nnot sure but the errors on CI seems to be related to jenkins ... it seems all these jobs that failed (for building) ran by 01h 01min ... not sure if it is a coincidence ...\n<issue_comment>username_1: @username_0 indeed it looks like all jobs were aborted at the same time. no obvious issues in the build log related to your code. Comparing e.g. the `cuda9-cudnn7` build with a successful one from another PR, it takes 1hr 14min there and your build is about at the place where the other one is after an hour.\r\n\r\nI suggest to just push a new commit to rebuild. Probably a temporary CI hiccup.\n<issue_comment>username_3: I accidentally rebooted Jenkins yesterday which is the likely cause, my apologies.\r\n\r\n@pytorchbot retest this please\n<issue_comment>username_0: @username_3 @username_2 @username_1 \r\n\r\nall tests passed except `pr/caffe2-py2-cuda9.0-cudnn7-windows-build`:\r\n \r\n```\r\n14:43:49 Build timed out (after 180 minutes). Marking the build as failed.\r\n14:43:49 Build was aborted\r\n14:43:49 [BFA] Scanning build for known causes...\r\n14:43:49 [BFA] No failure causes found\r\n14:43:49 [BFA] Done. 0s\r\n14:43:49 Finished: FAILURE\r\n```\r\n\r\nnot sure if this timeout means that the code now is slower.\r\n\r\nwhat do you think? do you have any suggestion?\n<issue_comment>username_3: Sometimes the Windows build flakes out like that. It didn't timeout while running a relevant test, so I judge it to be not your problem.\n<issue_comment>username_3: The launch bounds logic is wrong, but I acknowledge that this is a big patch already; just fix it in a follow up. I am going to go ahead and land this.\n<issue_comment>username_0: sounds good @username_3 thanks!","repo":"pytorch/pytorch","issue_id":436372857,"issue_number":19630}
{"question":"Why is the performance of TorchScript handling the computation in the Adam Optimizer worse than using native PyTorch operations on CUDA?","answer":"The performance of TorchScript handling the computation in the Adam Optimizer is worse than using native PyTorch operations on CUDA due to TorchScript not fusing all operations into a single CUDA kernel. Instead, it generates two CUDA kernels, leading to performance inefficiencies.","id":174,"text":"The performance of a TorchScript that handles the computation in the [Adam Optimizer](https://github.com/pytorch/pytorch/blob/master/torch/optim/adam.py) is worse than using the native PyTorch operations on CUDA. \r\n\r\n### Background\r\nI am trying to improve the GPU performance of the [Adam Optimizer](https://github.com/pytorch/pytorch/blob/master/torch/optim/adam.py) since it is the bottleneck in one of our workflows. \r\n\r\n### Results\r\nI have written a standalone script that compares PyTorch native tensor operations, TorchScript, and Numba (three runs each):\r\n```\r\nnative :  0.0506\r\nnative :  0.0322\r\nnative :  0.0322\r\nTScript:  0.1917\r\nTScript:  0.0442\r\nTScript:  0.0442\r\nnumba  :  0.1638\r\nnumba  :  0.0145\r\nnumba  :  0.0143\r\n```\r\nI suspect the problem is that TorchScript does not fuse all operations into a single CUDA. It generates two CUDA kernels (see the `PYTORCH_FUSION_DEBUG` output below)\r\n\r\n### Standalone Script\r\n```python\r\nimport math\r\nimport torch\r\nimport time\r\nimport random\r\nimport sys\r\n\r\ndef kernel_native(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n    exp_avg.mul_(beta1).add_(1 - beta1, grad)\r\n    exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\r\n    denom = exp_avg_sq.sqrt().add_(eps)\r\n    param.addcdiv_(-step_size, exp_avg, denom)   \r\n\r\n@torch.jit.script\r\ndef kernel_jit_script(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n    exp_avg.mul_(beta1)\r\n    exp_avg.add_((1 - beta1) + grad)\r\n    exp_avg_sq.mul_(beta2)\r\n    exp_avg_sq.add_((1 - beta2)*(grad*grad))\r\n    denom = exp_avg_sq.sqrt().add_(eps)\r\n    param += -step_size * (exp_avg / denom)\r\n\r\nfrom numba import cuda\r\n@cuda.jit\r\ndef kernel_numba_cuda(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    i = cuda.grid(1)\r\n    if i > grad.size:\r\n        return\r\n\r\n    exp_avg[i] = exp_avg[i] * beta1 + (1 - beta1) + grad[i]\r\n    exp_avg_sq[i] = exp_avg_sq[i] * beta2 + (1 - beta2) *grad[i]*grad[i]\r\n    denom = math.sqrt(exp_avg_sq[i]) + eps\r\n    param[i] += -step_size * (exp_avg[i] / denom)\r\n\r\ndef kernel_numba(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n\r\n    import ctypes\r\n    import numpy as np\r\n    def get_devicendarray(t):\r\n        assert t.type() == 'torch.cuda.FloatTensor'\r\n        numpy_dtype = np.float32\r\n        itemsize = 4\r\n\r\n        ctx = cuda.cudadrv.driver.driver.get_context()\r\n        mp = cuda.cudadrv.driver.MemoryPointer(ctx, ctypes.c_ulong(t.data_ptr()), t.numel()*itemsize)\r\n        return cuda.cudadrv.devicearray.DeviceNDArray(t.numel(), itemsize, numpy_dtype(), gpu_data=mp, stream=torch.cuda.current_stream().cuda_stream)\r\n\r\n    numba_param = get_devicendarray(param)\r\n    numba_grad = get_devicendarray(grad)\r\n    numba_exp_avg = get_devicendarray(exp_avg)\r\n    numba_exp_avg_sq = get_devicendarray(exp_avg_sq)\r\n    threadsperblock = 32\r\n    blockspergrid = (param.numel() + (threadsperblock - 1)) // threadsperblock\r\n    kernel_numba_cuda[blockspergrid, threadsperblock](numba_param, numba_grad, numba_exp_avg, numba_exp_avg_sq, beta1, beta2, step_size, eps)\r\n\r\nkernels = [(\"native \", kernel_native), (\"TScript\", kernel_jit_script), (\"numba  \", kernel_numba)]\r\n[Truncated]\n      size_t t1_dimIndex0 = t1_linearIndex ;\r\n      t1_offset += t1_dimIndex0 ;\r\n      IndexType t3_offset = 0;\r\n      IndexType t3_linearIndex = linearIndex;\r\n      \r\n      //printf(\"tensor t3 sizes[0] = %d, strides[0] = %d\\n\", t3.sizes[0],t3.strides[0]);\r\n      size_t t3_dimIndex0 = t3_linearIndex ;\r\n      t3_offset += t3_dimIndex0 ;\r\n      \r\n      // calculate the results\r\n      float n0 = __ldg(&t0.data[t0_offset]);\r\n      float n1 = __ldg(&t1.data[t1_offset]);\r\n      double n2 = s2;\r\n      float n3 = n0 / n1;\r\n      float n4 = n3 * ((float) n2);\r\n      t3.data[t3_offset] = n4;\r\n      \r\n    }\r\n}\r\n```","context":"<issue_start><issue_comment>Title: TorchScript Poor CUDA Performance \nusername_0: The performance of a TorchScript that handles the computation in the [Adam Optimizer](https://github.com/pytorch/pytorch/blob/master/torch/optim/adam.py) is worse than using the native PyTorch operations on CUDA. \r\n\r\n### Background\r\nI am trying to improve the GPU performance of the [Adam Optimizer](https://github.com/pytorch/pytorch/blob/master/torch/optim/adam.py) since it is the bottleneck in one of our workflows. \r\n\r\n### Results\r\nI have written a standalone script that compares PyTorch native tensor operations, TorchScript, and Numba (three runs each):\r\n```\r\nnative :  0.0506\r\nnative :  0.0322\r\nnative :  0.0322\r\nTScript:  0.1917\r\nTScript:  0.0442\r\nTScript:  0.0442\r\nnumba  :  0.1638\r\nnumba  :  0.0145\r\nnumba  :  0.0143\r\n```\r\nI suspect the problem is that TorchScript does not fuse all operations into a single CUDA. It generates two CUDA kernels (see the `PYTORCH_FUSION_DEBUG` output below)\r\n\r\n### Standalone Script\r\n```python\r\nimport math\r\nimport torch\r\nimport time\r\nimport random\r\nimport sys\r\n\r\ndef kernel_native(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n    exp_avg.mul_(beta1).add_(1 - beta1, grad)\r\n    exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\r\n    denom = exp_avg_sq.sqrt().add_(eps)\r\n    param.addcdiv_(-step_size, exp_avg, denom)   \r\n\r\n@torch.jit.script\r\ndef kernel_jit_script(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n    exp_avg.mul_(beta1)\r\n    exp_avg.add_((1 - beta1) + grad)\r\n    exp_avg_sq.mul_(beta2)\r\n    exp_avg_sq.add_((1 - beta2)*(grad*grad))\r\n    denom = exp_avg_sq.sqrt().add_(eps)\r\n    param += -step_size * (exp_avg / denom)\r\n\r\nfrom numba import cuda\r\n@cuda.jit\r\ndef kernel_numba_cuda(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    i = cuda.grid(1)\r\n    if i > grad.size:\r\n        return\r\n\r\n    exp_avg[i] = exp_avg[i] * beta1 + (1 - beta1) + grad[i]\r\n    exp_avg_sq[i] = exp_avg_sq[i] * beta2 + (1 - beta2) *grad[i]*grad[i]\r\n    denom = math.sqrt(exp_avg_sq[i]) + eps\r\n    param[i] += -step_size * (exp_avg[i] / denom)\r\n\r\ndef kernel_numba(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n\r\n    import ctypes\r\n    import numpy as np\r\n    def get_devicendarray(t):\r\n        assert t.type() == 'torch.cuda.FloatTensor'\r\n        numpy_dtype = np.float32\r\n        itemsize = 4\r\n\r\n        ctx = cuda.cudadrv.driver.driver.get_context()\r\n        mp = cuda.cudadrv.driver.MemoryPointer(ctx, ctypes.c_ulong(t.data_ptr()), t.numel()*itemsize)\r\n        return cuda.cudadrv.devicearray.DeviceNDArray(t.numel(), itemsize, numpy_dtype(), gpu_data=mp, stream=torch.cuda.current_stream().cuda_stream)\r\n\r\n    numba_param = get_devicendarray(param)\r\n    numba_grad = get_devicendarray(grad)\r\n    numba_exp_avg = get_devicendarray(exp_avg)\r\n    numba_exp_avg_sq = get_devicendarray(exp_avg_sq)\r\n    threadsperblock = 32\r\n    blockspergrid = (param.numel() + (threadsperblock - 1)) // threadsperblock\r\n    kernel_numba_cuda[blockspergrid, threadsperblock](numba_param, numba_grad, numba_exp_avg, numba_exp_avg_sq, beta1, beta2, step_size, eps)\r\n\r\nkernels = [(\"native \", kernel_native), (\"TScript\", kernel_jit_script), (\"numba  \", kernel_numba)]\r\n[Truncated]\n      size_t t1_dimIndex0 = t1_linearIndex ;\r\n      t1_offset += t1_dimIndex0 ;\r\n      IndexType t3_offset = 0;\r\n      IndexType t3_linearIndex = linearIndex;\r\n      \r\n      //printf(\"tensor t3 sizes[0] = %d, strides[0] = %d\\n\", t3.sizes[0],t3.strides[0]);\r\n      size_t t3_dimIndex0 = t3_linearIndex ;\r\n      t3_offset += t3_dimIndex0 ;\r\n      \r\n      // calculate the results\r\n      float n0 = __ldg(&t0.data[t0_offset]);\r\n      float n1 = __ldg(&t1.data[t1_offset]);\r\n      double n2 = s2;\r\n      float n3 = n0 / n1;\r\n      float n4 = n3 * ((float) n2);\r\n      t3.data[t3_offset] = n4;\r\n      \r\n    }\r\n}\r\n```\n<issue_comment>username_1: @username_0 can you try removing all the in-place operations in your scripted version?\r\nIIRC in-place ops block fusion.\r\n\r\nI was btw able to fuse most of sparse adam by removing those in-place ops, see https://github.com/pytorch/pytorch/pull/23522#issuecomment-516952462\n<issue_comment>username_2: cc: @username_3 and also relevant WIP PR: [JIT version of Adagrad](https://github.com/pytorch/pytorch/pull/23640)\n<issue_comment>username_3: Yeah the CUDA fusion does not support fusing in-place operations yet, Can you try the suggestion by @username_1 to see if you get a better perf?\n<issue_comment>username_3: @username_0 Also, it seems like you are only timing it in 3 iterations, and the perf numbers seems including the compilation time. The first time JIT/TorchScript sees an input it will specialize and do optimizations against that input, so when comparing performance please allow a compilation time and runtime optimization warmup for the input.\n<issue_comment>username_4: @username_3 That made a big difference to my traced module too. Any other tricks like this? It slows down the non-JIT version, however.\n<issue_comment>username_3: @username_4 That's pretty much all, let me summarize:\r\n\r\n1. JIT fusion does not support in-place operation fusion and reduction operation yet.\r\n2. we did the fusion code generation at the first time we see an input that the shape we haven't seen before (like if the number of dimensions are different), this will help us generate faster cuda kernels. \r\n\r\nSo the TorchScript compilation time will be gone after compile and the first run :)\n<issue_comment>username_0: Thanks for the feedback!\r\n\r\nI have removed all in-place operations and added three kernel implementations:\r\n```python\r\n\r\nimport math\r\nimport torch\r\nimport time\r\nimport random\r\nimport sys\r\n\r\ndef kernel_native(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n    exp_avg.mul_(beta1).add_(1 - beta1, grad)\r\n    exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\r\n    denom = exp_avg_sq.sqrt().add_(eps)\r\n    param.addcdiv_(-step_size, exp_avg, denom)   \r\n\r\n@torch.jit.script\r\ndef kernel_jit_script1(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n    exp_avg *= beta1\r\n    exp_avg += (1 - beta1) + grad\r\n    exp_avg_sq *= beta2\r\n    exp_avg_sq += (1 - beta2)*(grad*grad)\r\n    param += -step_size * (exp_avg / exp_avg_sq.sqrt() + eps)\r\n\r\n@torch.jit.script\r\ndef kernel_jit_script2(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n    exp_avg[:] = exp_avg * beta1 + (1 - beta1) + grad\r\n    exp_avg_sq[:] = exp_avg_sq * beta2 + (1 - beta2)*(grad*grad)\r\n    param[:] = param -step_size * (exp_avg / exp_avg_sq.sqrt() + eps)\r\n\r\n@torch.jit.script\r\ndef kernel_jit_script3(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> Tuple[Tensor, Tensor, Tensor, Tensor]\r\n    exp_avg = exp_avg * beta1 + (1 - beta1) + grad\r\n    exp_avg_sq = exp_avg_sq * beta2 + (1 - beta2)*(grad*grad)\r\n    param = param -step_size * (exp_avg / exp_avg_sq.sqrt() + eps)\r\n    return (param, grad, exp_avg, exp_avg_sq)\r\n\r\n\r\nfrom numba import cuda\r\n@cuda.jit\r\ndef kernel_numba_cuda(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    i = cuda.grid(1)\r\n    if i > grad.size:\r\n        return\r\n\r\n    exp_avg[i] = exp_avg[i] * beta1 + (1 - beta1) + grad[i]\r\n    exp_avg_sq[i] = exp_avg_sq[i] * beta2 + (1 - beta2) *grad[i]*grad[i]\r\n    denom = math.sqrt(exp_avg_sq[i]) + eps\r\n    param[i] += -step_size * (exp_avg[i] / denom)\r\n\r\ndef kernel_numba(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n\r\n    import ctypes\r\n    import numpy as np\r\n    def get_devicendarray(t):\r\n        assert t.type() == 'torch.cuda.FloatTensor'\r\n        numpy_dtype = np.float32\r\n        itemsize = 4\r\n\r\n        ctx = cuda.cudadrv.driver.driver.get_context()\r\n        mp = cuda.cudadrv.driver.MemoryPointer(ctx, ctypes.c_ulong(t.data_ptr()), t.numel()*itemsize)\r\n        return cuda.cudadrv.devicearray.DeviceNDArray(t.numel(), itemsize, numpy_dtype(), gpu_data=mp, stream=torch.cuda.current_stream().cuda_stream)\r\n\r\n    numba_param = get_devicendarray(param)\r\n    numba_grad = get_devicendarray(grad)\r\n    numba_exp_avg = get_devicendarray(exp_avg)\r\n    numba_exp_avg_sq = get_devicendarray(exp_avg_sq)\r\n    threadsperblock = 32\r\n    blockspergrid = (param.numel() + (threadsperblock - 1)) // threadsperblock\r\n    kernel_numba_cuda[blockspergrid, threadsperblock](numba_param, numba_grad, numba_exp_avg, numba_exp_avg_sq, beta1, beta2, step_size, eps)\r\n\r\nkernels = [(\"native \", kernel_native), (\"numba  \", kernel_numba)]\r\nshape = (4194304*64,)\r\nparam, grad, exp_avg, exp_avg_sq = [torch.rand(shape).to(\"cuda\") for x in range(4)]\r\n[Truncated]\n        res = func(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps)\r\n        if res is not None:\r\n            param[:], _, exp_avg[:], exp_avg_sq[:] = res\r\n        torch.cuda.synchronize()\r\n        t2 = time.time()\r\n        print(\"%s: \" % name, t2-t1)\r\n```\r\nThe results are:\r\n```\r\nnative :  0.0323\r\nnumba  :  0.0147\r\nTScript1:  0.0369\r\nTScript2:  0.0352\r\nTScript3:  0.0280\r\n```\r\nTScript3 is now better than native but still far from numba. TScript3 is now generating a single CUDA kernel but the back-copying of the result now dominates the timing. If I remove the line `param[:], _, exp_avg[:], exp_avg_sq[:] = res` the performance is almost on pair with numba:\r\n```\r\nTScript3 (no back-copy):  0.01664\r\n```\r\nThe problem is I really do need to update the values :/\n<issue_comment>username_5: cc @mruberry","repo":"pytorch/pytorch","issue_id":475728990,"issue_number":23655}
{"question":"How can in-place operations impact fusion in PyTorch?","answer":"In-place operations can block fusion in PyTorch.","id":175,"text":"@username_0 can you try removing all the in-place operations in your scripted version?\r\nIIRC in-place ops block fusion.\r\n\r\nI was btw able to fuse most of sparse adam by removing those in-place ops, see https://github.com/pytorch/pytorch/pull/23522#issuecomment-516952462","context":"<issue_start><issue_comment>Title: TorchScript Poor CUDA Performance \nusername_0: The performance of a TorchScript that handles the computation in the [Adam Optimizer](https://github.com/pytorch/pytorch/blob/master/torch/optim/adam.py) is worse than using the native PyTorch operations on CUDA. \r\n\r\n### Background\r\nI am trying to improve the GPU performance of the [Adam Optimizer](https://github.com/pytorch/pytorch/blob/master/torch/optim/adam.py) since it is the bottleneck in one of our workflows. \r\n\r\n### Results\r\nI have written a standalone script that compares PyTorch native tensor operations, TorchScript, and Numba (three runs each):\r\n```\r\nnative :  0.0506\r\nnative :  0.0322\r\nnative :  0.0322\r\nTScript:  0.1917\r\nTScript:  0.0442\r\nTScript:  0.0442\r\nnumba  :  0.1638\r\nnumba  :  0.0145\r\nnumba  :  0.0143\r\n```\r\nI suspect the problem is that TorchScript does not fuse all operations into a single CUDA. It generates two CUDA kernels (see the `PYTORCH_FUSION_DEBUG` output below)\r\n\r\n### Standalone Script\r\n```python\r\nimport math\r\nimport torch\r\nimport time\r\nimport random\r\nimport sys\r\n\r\ndef kernel_native(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n    exp_avg.mul_(beta1).add_(1 - beta1, grad)\r\n    exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\r\n    denom = exp_avg_sq.sqrt().add_(eps)\r\n    param.addcdiv_(-step_size, exp_avg, denom)   \r\n\r\n@torch.jit.script\r\ndef kernel_jit_script(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n    exp_avg.mul_(beta1)\r\n    exp_avg.add_((1 - beta1) + grad)\r\n    exp_avg_sq.mul_(beta2)\r\n    exp_avg_sq.add_((1 - beta2)*(grad*grad))\r\n    denom = exp_avg_sq.sqrt().add_(eps)\r\n    param += -step_size * (exp_avg / denom)\r\n\r\nfrom numba import cuda\r\n@cuda.jit\r\ndef kernel_numba_cuda(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    i = cuda.grid(1)\r\n    if i > grad.size:\r\n        return\r\n\r\n    exp_avg[i] = exp_avg[i] * beta1 + (1 - beta1) + grad[i]\r\n    exp_avg_sq[i] = exp_avg_sq[i] * beta2 + (1 - beta2) *grad[i]*grad[i]\r\n    denom = math.sqrt(exp_avg_sq[i]) + eps\r\n    param[i] += -step_size * (exp_avg[i] / denom)\r\n\r\ndef kernel_numba(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n\r\n    import ctypes\r\n    import numpy as np\r\n    def get_devicendarray(t):\r\n        assert t.type() == 'torch.cuda.FloatTensor'\r\n        numpy_dtype = np.float32\r\n        itemsize = 4\r\n\r\n        ctx = cuda.cudadrv.driver.driver.get_context()\r\n        mp = cuda.cudadrv.driver.MemoryPointer(ctx, ctypes.c_ulong(t.data_ptr()), t.numel()*itemsize)\r\n        return cuda.cudadrv.devicearray.DeviceNDArray(t.numel(), itemsize, numpy_dtype(), gpu_data=mp, stream=torch.cuda.current_stream().cuda_stream)\r\n\r\n    numba_param = get_devicendarray(param)\r\n    numba_grad = get_devicendarray(grad)\r\n    numba_exp_avg = get_devicendarray(exp_avg)\r\n    numba_exp_avg_sq = get_devicendarray(exp_avg_sq)\r\n    threadsperblock = 32\r\n    blockspergrid = (param.numel() + (threadsperblock - 1)) // threadsperblock\r\n    kernel_numba_cuda[blockspergrid, threadsperblock](numba_param, numba_grad, numba_exp_avg, numba_exp_avg_sq, beta1, beta2, step_size, eps)\r\n\r\nkernels = [(\"native \", kernel_native), (\"TScript\", kernel_jit_script), (\"numba  \", kernel_numba)]\r\n[Truncated]\n      size_t t1_dimIndex0 = t1_linearIndex ;\r\n      t1_offset += t1_dimIndex0 ;\r\n      IndexType t3_offset = 0;\r\n      IndexType t3_linearIndex = linearIndex;\r\n      \r\n      //printf(\"tensor t3 sizes[0] = %d, strides[0] = %d\\n\", t3.sizes[0],t3.strides[0]);\r\n      size_t t3_dimIndex0 = t3_linearIndex ;\r\n      t3_offset += t3_dimIndex0 ;\r\n      \r\n      // calculate the results\r\n      float n0 = __ldg(&t0.data[t0_offset]);\r\n      float n1 = __ldg(&t1.data[t1_offset]);\r\n      double n2 = s2;\r\n      float n3 = n0 / n1;\r\n      float n4 = n3 * ((float) n2);\r\n      t3.data[t3_offset] = n4;\r\n      \r\n    }\r\n}\r\n```\n<issue_comment>username_1: @username_0 can you try removing all the in-place operations in your scripted version?\r\nIIRC in-place ops block fusion.\r\n\r\nI was btw able to fuse most of sparse adam by removing those in-place ops, see https://github.com/pytorch/pytorch/pull/23522#issuecomment-516952462\n<issue_comment>username_2: cc: @username_3 and also relevant WIP PR: [JIT version of Adagrad](https://github.com/pytorch/pytorch/pull/23640)\n<issue_comment>username_3: Yeah the CUDA fusion does not support fusing in-place operations yet, Can you try the suggestion by @username_1 to see if you get a better perf?\n<issue_comment>username_3: @username_0 Also, it seems like you are only timing it in 3 iterations, and the perf numbers seems including the compilation time. The first time JIT/TorchScript sees an input it will specialize and do optimizations against that input, so when comparing performance please allow a compilation time and runtime optimization warmup for the input.\n<issue_comment>username_4: @username_3 That made a big difference to my traced module too. Any other tricks like this? It slows down the non-JIT version, however.\n<issue_comment>username_3: @username_4 That's pretty much all, let me summarize:\r\n\r\n1. JIT fusion does not support in-place operation fusion and reduction operation yet.\r\n2. we did the fusion code generation at the first time we see an input that the shape we haven't seen before (like if the number of dimensions are different), this will help us generate faster cuda kernels. \r\n\r\nSo the TorchScript compilation time will be gone after compile and the first run :)\n<issue_comment>username_0: Thanks for the feedback!\r\n\r\nI have removed all in-place operations and added three kernel implementations:\r\n```python\r\n\r\nimport math\r\nimport torch\r\nimport time\r\nimport random\r\nimport sys\r\n\r\ndef kernel_native(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n    exp_avg.mul_(beta1).add_(1 - beta1, grad)\r\n    exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\r\n    denom = exp_avg_sq.sqrt().add_(eps)\r\n    param.addcdiv_(-step_size, exp_avg, denom)   \r\n\r\n@torch.jit.script\r\ndef kernel_jit_script1(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n    exp_avg *= beta1\r\n    exp_avg += (1 - beta1) + grad\r\n    exp_avg_sq *= beta2\r\n    exp_avg_sq += (1 - beta2)*(grad*grad)\r\n    param += -step_size * (exp_avg / exp_avg_sq.sqrt() + eps)\r\n\r\n@torch.jit.script\r\ndef kernel_jit_script2(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n    exp_avg[:] = exp_avg * beta1 + (1 - beta1) + grad\r\n    exp_avg_sq[:] = exp_avg_sq * beta2 + (1 - beta2)*(grad*grad)\r\n    param[:] = param -step_size * (exp_avg / exp_avg_sq.sqrt() + eps)\r\n\r\n@torch.jit.script\r\ndef kernel_jit_script3(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> Tuple[Tensor, Tensor, Tensor, Tensor]\r\n    exp_avg = exp_avg * beta1 + (1 - beta1) + grad\r\n    exp_avg_sq = exp_avg_sq * beta2 + (1 - beta2)*(grad*grad)\r\n    param = param -step_size * (exp_avg / exp_avg_sq.sqrt() + eps)\r\n    return (param, grad, exp_avg, exp_avg_sq)\r\n\r\n\r\nfrom numba import cuda\r\n@cuda.jit\r\ndef kernel_numba_cuda(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    i = cuda.grid(1)\r\n    if i > grad.size:\r\n        return\r\n\r\n    exp_avg[i] = exp_avg[i] * beta1 + (1 - beta1) + grad[i]\r\n    exp_avg_sq[i] = exp_avg_sq[i] * beta2 + (1 - beta2) *grad[i]*grad[i]\r\n    denom = math.sqrt(exp_avg_sq[i]) + eps\r\n    param[i] += -step_size * (exp_avg[i] / denom)\r\n\r\ndef kernel_numba(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n\r\n    import ctypes\r\n    import numpy as np\r\n    def get_devicendarray(t):\r\n        assert t.type() == 'torch.cuda.FloatTensor'\r\n        numpy_dtype = np.float32\r\n        itemsize = 4\r\n\r\n        ctx = cuda.cudadrv.driver.driver.get_context()\r\n        mp = cuda.cudadrv.driver.MemoryPointer(ctx, ctypes.c_ulong(t.data_ptr()), t.numel()*itemsize)\r\n        return cuda.cudadrv.devicearray.DeviceNDArray(t.numel(), itemsize, numpy_dtype(), gpu_data=mp, stream=torch.cuda.current_stream().cuda_stream)\r\n\r\n    numba_param = get_devicendarray(param)\r\n    numba_grad = get_devicendarray(grad)\r\n    numba_exp_avg = get_devicendarray(exp_avg)\r\n    numba_exp_avg_sq = get_devicendarray(exp_avg_sq)\r\n    threadsperblock = 32\r\n    blockspergrid = (param.numel() + (threadsperblock - 1)) // threadsperblock\r\n    kernel_numba_cuda[blockspergrid, threadsperblock](numba_param, numba_grad, numba_exp_avg, numba_exp_avg_sq, beta1, beta2, step_size, eps)\r\n\r\nkernels = [(\"native \", kernel_native), (\"numba  \", kernel_numba)]\r\nshape = (4194304*64,)\r\nparam, grad, exp_avg, exp_avg_sq = [torch.rand(shape).to(\"cuda\") for x in range(4)]\r\n[Truncated]\n        res = func(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps)\r\n        if res is not None:\r\n            param[:], _, exp_avg[:], exp_avg_sq[:] = res\r\n        torch.cuda.synchronize()\r\n        t2 = time.time()\r\n        print(\"%s: \" % name, t2-t1)\r\n```\r\nThe results are:\r\n```\r\nnative :  0.0323\r\nnumba  :  0.0147\r\nTScript1:  0.0369\r\nTScript2:  0.0352\r\nTScript3:  0.0280\r\n```\r\nTScript3 is now better than native but still far from numba. TScript3 is now generating a single CUDA kernel but the back-copying of the result now dominates the timing. If I remove the line `param[:], _, exp_avg[:], exp_avg_sq[:] = res` the performance is almost on pair with numba:\r\n```\r\nTScript3 (no back-copy):  0.01664\r\n```\r\nThe problem is I really do need to update the values :/\n<issue_comment>username_5: cc @mruberry","repo":"pytorch/pytorch","issue_id":475728990,"issue_number":23655}
{"question":"What are the limitations of JIT fusion in the context described?","answer":"JIT fusion does not support in-place operation fusion and reduction operations yet.","id":180,"text":"@username_4 That's pretty much all, let me summarize:\r\n\r\n1. JIT fusion does not support in-place operation fusion and reduction operation yet.\r\n2. we did the fusion code generation at the first time we see an input that the shape we haven't seen before (like if the number of dimensions are different), this will help us generate faster cuda kernels. \r\n\r\nSo the TorchScript compilation time will be gone after compile and the first run :)","context":"<issue_start><issue_comment>Title: TorchScript Poor CUDA Performance \nusername_0: The performance of a TorchScript that handles the computation in the [Adam Optimizer](https://github.com/pytorch/pytorch/blob/master/torch/optim/adam.py) is worse than using the native PyTorch operations on CUDA. \r\n\r\n### Background\r\nI am trying to improve the GPU performance of the [Adam Optimizer](https://github.com/pytorch/pytorch/blob/master/torch/optim/adam.py) since it is the bottleneck in one of our workflows. \r\n\r\n### Results\r\nI have written a standalone script that compares PyTorch native tensor operations, TorchScript, and Numba (three runs each):\r\n```\r\nnative :  0.0506\r\nnative :  0.0322\r\nnative :  0.0322\r\nTScript:  0.1917\r\nTScript:  0.0442\r\nTScript:  0.0442\r\nnumba  :  0.1638\r\nnumba  :  0.0145\r\nnumba  :  0.0143\r\n```\r\nI suspect the problem is that TorchScript does not fuse all operations into a single CUDA. It generates two CUDA kernels (see the `PYTORCH_FUSION_DEBUG` output below)\r\n\r\n### Standalone Script\r\n```python\r\nimport math\r\nimport torch\r\nimport time\r\nimport random\r\nimport sys\r\n\r\ndef kernel_native(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n    exp_avg.mul_(beta1).add_(1 - beta1, grad)\r\n    exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\r\n    denom = exp_avg_sq.sqrt().add_(eps)\r\n    param.addcdiv_(-step_size, exp_avg, denom)   \r\n\r\n@torch.jit.script\r\ndef kernel_jit_script(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n    exp_avg.mul_(beta1)\r\n    exp_avg.add_((1 - beta1) + grad)\r\n    exp_avg_sq.mul_(beta2)\r\n    exp_avg_sq.add_((1 - beta2)*(grad*grad))\r\n    denom = exp_avg_sq.sqrt().add_(eps)\r\n    param += -step_size * (exp_avg / denom)\r\n\r\nfrom numba import cuda\r\n@cuda.jit\r\ndef kernel_numba_cuda(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    i = cuda.grid(1)\r\n    if i > grad.size:\r\n        return\r\n\r\n    exp_avg[i] = exp_avg[i] * beta1 + (1 - beta1) + grad[i]\r\n    exp_avg_sq[i] = exp_avg_sq[i] * beta2 + (1 - beta2) *grad[i]*grad[i]\r\n    denom = math.sqrt(exp_avg_sq[i]) + eps\r\n    param[i] += -step_size * (exp_avg[i] / denom)\r\n\r\ndef kernel_numba(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n\r\n    import ctypes\r\n    import numpy as np\r\n    def get_devicendarray(t):\r\n        assert t.type() == 'torch.cuda.FloatTensor'\r\n        numpy_dtype = np.float32\r\n        itemsize = 4\r\n\r\n        ctx = cuda.cudadrv.driver.driver.get_context()\r\n        mp = cuda.cudadrv.driver.MemoryPointer(ctx, ctypes.c_ulong(t.data_ptr()), t.numel()*itemsize)\r\n        return cuda.cudadrv.devicearray.DeviceNDArray(t.numel(), itemsize, numpy_dtype(), gpu_data=mp, stream=torch.cuda.current_stream().cuda_stream)\r\n\r\n    numba_param = get_devicendarray(param)\r\n    numba_grad = get_devicendarray(grad)\r\n    numba_exp_avg = get_devicendarray(exp_avg)\r\n    numba_exp_avg_sq = get_devicendarray(exp_avg_sq)\r\n    threadsperblock = 32\r\n    blockspergrid = (param.numel() + (threadsperblock - 1)) // threadsperblock\r\n    kernel_numba_cuda[blockspergrid, threadsperblock](numba_param, numba_grad, numba_exp_avg, numba_exp_avg_sq, beta1, beta2, step_size, eps)\r\n\r\nkernels = [(\"native \", kernel_native), (\"TScript\", kernel_jit_script), (\"numba  \", kernel_numba)]\r\n[Truncated]\n      size_t t1_dimIndex0 = t1_linearIndex ;\r\n      t1_offset += t1_dimIndex0 ;\r\n      IndexType t3_offset = 0;\r\n      IndexType t3_linearIndex = linearIndex;\r\n      \r\n      //printf(\"tensor t3 sizes[0] = %d, strides[0] = %d\\n\", t3.sizes[0],t3.strides[0]);\r\n      size_t t3_dimIndex0 = t3_linearIndex ;\r\n      t3_offset += t3_dimIndex0 ;\r\n      \r\n      // calculate the results\r\n      float n0 = __ldg(&t0.data[t0_offset]);\r\n      float n1 = __ldg(&t1.data[t1_offset]);\r\n      double n2 = s2;\r\n      float n3 = n0 / n1;\r\n      float n4 = n3 * ((float) n2);\r\n      t3.data[t3_offset] = n4;\r\n      \r\n    }\r\n}\r\n```\n<issue_comment>username_1: @username_0 can you try removing all the in-place operations in your scripted version?\r\nIIRC in-place ops block fusion.\r\n\r\nI was btw able to fuse most of sparse adam by removing those in-place ops, see https://github.com/pytorch/pytorch/pull/23522#issuecomment-516952462\n<issue_comment>username_2: cc: @username_3 and also relevant WIP PR: [JIT version of Adagrad](https://github.com/pytorch/pytorch/pull/23640)\n<issue_comment>username_3: Yeah the CUDA fusion does not support fusing in-place operations yet, Can you try the suggestion by @username_1 to see if you get a better perf?\n<issue_comment>username_3: @username_0 Also, it seems like you are only timing it in 3 iterations, and the perf numbers seems including the compilation time. The first time JIT/TorchScript sees an input it will specialize and do optimizations against that input, so when comparing performance please allow a compilation time and runtime optimization warmup for the input.\n<issue_comment>username_4: @username_3 That made a big difference to my traced module too. Any other tricks like this? It slows down the non-JIT version, however.\n<issue_comment>username_3: @username_4 That's pretty much all, let me summarize:\r\n\r\n1. JIT fusion does not support in-place operation fusion and reduction operation yet.\r\n2. we did the fusion code generation at the first time we see an input that the shape we haven't seen before (like if the number of dimensions are different), this will help us generate faster cuda kernels. \r\n\r\nSo the TorchScript compilation time will be gone after compile and the first run :)\n<issue_comment>username_0: Thanks for the feedback!\r\n\r\nI have removed all in-place operations and added three kernel implementations:\r\n```python\r\n\r\nimport math\r\nimport torch\r\nimport time\r\nimport random\r\nimport sys\r\n\r\ndef kernel_native(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n    exp_avg.mul_(beta1).add_(1 - beta1, grad)\r\n    exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\r\n    denom = exp_avg_sq.sqrt().add_(eps)\r\n    param.addcdiv_(-step_size, exp_avg, denom)   \r\n\r\n@torch.jit.script\r\ndef kernel_jit_script1(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n    exp_avg *= beta1\r\n    exp_avg += (1 - beta1) + grad\r\n    exp_avg_sq *= beta2\r\n    exp_avg_sq += (1 - beta2)*(grad*grad)\r\n    param += -step_size * (exp_avg / exp_avg_sq.sqrt() + eps)\r\n\r\n@torch.jit.script\r\ndef kernel_jit_script2(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n    exp_avg[:] = exp_avg * beta1 + (1 - beta1) + grad\r\n    exp_avg_sq[:] = exp_avg_sq * beta2 + (1 - beta2)*(grad*grad)\r\n    param[:] = param -step_size * (exp_avg / exp_avg_sq.sqrt() + eps)\r\n\r\n@torch.jit.script\r\ndef kernel_jit_script3(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> Tuple[Tensor, Tensor, Tensor, Tensor]\r\n    exp_avg = exp_avg * beta1 + (1 - beta1) + grad\r\n    exp_avg_sq = exp_avg_sq * beta2 + (1 - beta2)*(grad*grad)\r\n    param = param -step_size * (exp_avg / exp_avg_sq.sqrt() + eps)\r\n    return (param, grad, exp_avg, exp_avg_sq)\r\n\r\n\r\nfrom numba import cuda\r\n@cuda.jit\r\ndef kernel_numba_cuda(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    i = cuda.grid(1)\r\n    if i > grad.size:\r\n        return\r\n\r\n    exp_avg[i] = exp_avg[i] * beta1 + (1 - beta1) + grad[i]\r\n    exp_avg_sq[i] = exp_avg_sq[i] * beta2 + (1 - beta2) *grad[i]*grad[i]\r\n    denom = math.sqrt(exp_avg_sq[i]) + eps\r\n    param[i] += -step_size * (exp_avg[i] / denom)\r\n\r\ndef kernel_numba(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n\r\n    import ctypes\r\n    import numpy as np\r\n    def get_devicendarray(t):\r\n        assert t.type() == 'torch.cuda.FloatTensor'\r\n        numpy_dtype = np.float32\r\n        itemsize = 4\r\n\r\n        ctx = cuda.cudadrv.driver.driver.get_context()\r\n        mp = cuda.cudadrv.driver.MemoryPointer(ctx, ctypes.c_ulong(t.data_ptr()), t.numel()*itemsize)\r\n        return cuda.cudadrv.devicearray.DeviceNDArray(t.numel(), itemsize, numpy_dtype(), gpu_data=mp, stream=torch.cuda.current_stream().cuda_stream)\r\n\r\n    numba_param = get_devicendarray(param)\r\n    numba_grad = get_devicendarray(grad)\r\n    numba_exp_avg = get_devicendarray(exp_avg)\r\n    numba_exp_avg_sq = get_devicendarray(exp_avg_sq)\r\n    threadsperblock = 32\r\n    blockspergrid = (param.numel() + (threadsperblock - 1)) // threadsperblock\r\n    kernel_numba_cuda[blockspergrid, threadsperblock](numba_param, numba_grad, numba_exp_avg, numba_exp_avg_sq, beta1, beta2, step_size, eps)\r\n\r\nkernels = [(\"native \", kernel_native), (\"numba  \", kernel_numba)]\r\nshape = (4194304*64,)\r\nparam, grad, exp_avg, exp_avg_sq = [torch.rand(shape).to(\"cuda\") for x in range(4)]\r\n[Truncated]\n        res = func(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps)\r\n        if res is not None:\r\n            param[:], _, exp_avg[:], exp_avg_sq[:] = res\r\n        torch.cuda.synchronize()\r\n        t2 = time.time()\r\n        print(\"%s: \" % name, t2-t1)\r\n```\r\nThe results are:\r\n```\r\nnative :  0.0323\r\nnumba  :  0.0147\r\nTScript1:  0.0369\r\nTScript2:  0.0352\r\nTScript3:  0.0280\r\n```\r\nTScript3 is now better than native but still far from numba. TScript3 is now generating a single CUDA kernel but the back-copying of the result now dominates the timing. If I remove the line `param[:], _, exp_avg[:], exp_avg_sq[:] = res` the performance is almost on pair with numba:\r\n```\r\nTScript3 (no back-copy):  0.01664\r\n```\r\nThe problem is I really do need to update the values :/\n<issue_comment>username_5: cc @mruberry","repo":"pytorch/pytorch","issue_id":475728990,"issue_number":23655}
{"question":"What is the impact of removing the line 'param[:], _, exp_avg[:], exp_avg_sq[:] = res' in the TScript3 kernel implementation?","answer":"The performance of TScript3 without back-copying the results is almost on par with Numba, demonstrating improved performance when the back-copying process is eliminated for updating values.","id":181,"text":"Thanks for the feedback!\r\n\r\nI have removed all in-place operations and added three kernel implementations:\r\n```python\r\n\r\nimport math\r\nimport torch\r\nimport time\r\nimport random\r\nimport sys\r\n\r\ndef kernel_native(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n    exp_avg.mul_(beta1).add_(1 - beta1, grad)\r\n    exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\r\n    denom = exp_avg_sq.sqrt().add_(eps)\r\n    param.addcdiv_(-step_size, exp_avg, denom)   \r\n\r\n@torch.jit.script\r\ndef kernel_jit_script1(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n    exp_avg *= beta1\r\n    exp_avg += (1 - beta1) + grad\r\n    exp_avg_sq *= beta2\r\n    exp_avg_sq += (1 - beta2)*(grad*grad)\r\n    param += -step_size * (exp_avg / exp_avg_sq.sqrt() + eps)\r\n\r\n@torch.jit.script\r\ndef kernel_jit_script2(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n    exp_avg[:] = exp_avg * beta1 + (1 - beta1) + grad\r\n    exp_avg_sq[:] = exp_avg_sq * beta2 + (1 - beta2)*(grad*grad)\r\n    param[:] = param -step_size * (exp_avg / exp_avg_sq.sqrt() + eps)\r\n\r\n@torch.jit.script\r\ndef kernel_jit_script3(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> Tuple[Tensor, Tensor, Tensor, Tensor]\r\n    exp_avg = exp_avg * beta1 + (1 - beta1) + grad\r\n    exp_avg_sq = exp_avg_sq * beta2 + (1 - beta2)*(grad*grad)\r\n    param = param -step_size * (exp_avg / exp_avg_sq.sqrt() + eps)\r\n    return (param, grad, exp_avg, exp_avg_sq)\r\n\r\n\r\nfrom numba import cuda\r\n@cuda.jit\r\ndef kernel_numba_cuda(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    i = cuda.grid(1)\r\n    if i > grad.size:\r\n        return\r\n\r\n    exp_avg[i] = exp_avg[i] * beta1 + (1 - beta1) + grad[i]\r\n    exp_avg_sq[i] = exp_avg_sq[i] * beta2 + (1 - beta2) *grad[i]*grad[i]\r\n    denom = math.sqrt(exp_avg_sq[i]) + eps\r\n    param[i] += -step_size * (exp_avg[i] / denom)\r\n\r\ndef kernel_numba(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n\r\n    import ctypes\r\n    import numpy as np\r\n    def get_devicendarray(t):\r\n        assert t.type() == 'torch.cuda.FloatTensor'\r\n        numpy_dtype = np.float32\r\n        itemsize = 4\r\n\r\n        ctx = cuda.cudadrv.driver.driver.get_context()\r\n        mp = cuda.cudadrv.driver.MemoryPointer(ctx, ctypes.c_ulong(t.data_ptr()), t.numel()*itemsize)\r\n        return cuda.cudadrv.devicearray.DeviceNDArray(t.numel(), itemsize, numpy_dtype(), gpu_data=mp, stream=torch.cuda.current_stream().cuda_stream)\r\n\r\n    numba_param = get_devicendarray(param)\r\n    numba_grad = get_devicendarray(grad)\r\n    numba_exp_avg = get_devicendarray(exp_avg)\r\n    numba_exp_avg_sq = get_devicendarray(exp_avg_sq)\r\n    threadsperblock = 32\r\n    blockspergrid = (param.numel() + (threadsperblock - 1)) // threadsperblock\r\n    kernel_numba_cuda[blockspergrid, threadsperblock](numba_param, numba_grad, numba_exp_avg, numba_exp_avg_sq, beta1, beta2, step_size, eps)\r\n\r\nkernels = [(\"native \", kernel_native), (\"numba  \", kernel_numba)]\r\nshape = (4194304*64,)\r\nparam, grad, exp_avg, exp_avg_sq = [torch.rand(shape).to(\"cuda\") for x in range(4)]\r\n[Truncated]\n        res = func(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps)\r\n        if res is not None:\r\n            param[:], _, exp_avg[:], exp_avg_sq[:] = res\r\n        torch.cuda.synchronize()\r\n        t2 = time.time()\r\n        print(\"%s: \" % name, t2-t1)\r\n```\r\nThe results are:\r\n```\r\nnative :  0.0323\r\nnumba  :  0.0147\r\nTScript1:  0.0369\r\nTScript2:  0.0352\r\nTScript3:  0.0280\r\n```\r\nTScript3 is now better than native but still far from numba. TScript3 is now generating a single CUDA kernel but the back-copying of the result now dominates the timing. If I remove the line `param[:], _, exp_avg[:], exp_avg_sq[:] = res` the performance is almost on pair with numba:\r\n```\r\nTScript3 (no back-copy):  0.01664\r\n```\r\nThe problem is I really do need to update the values :/","context":"<issue_start><issue_comment>Title: TorchScript Poor CUDA Performance \nusername_0: The performance of a TorchScript that handles the computation in the [Adam Optimizer](https://github.com/pytorch/pytorch/blob/master/torch/optim/adam.py) is worse than using the native PyTorch operations on CUDA. \r\n\r\n### Background\r\nI am trying to improve the GPU performance of the [Adam Optimizer](https://github.com/pytorch/pytorch/blob/master/torch/optim/adam.py) since it is the bottleneck in one of our workflows. \r\n\r\n### Results\r\nI have written a standalone script that compares PyTorch native tensor operations, TorchScript, and Numba (three runs each):\r\n```\r\nnative :  0.0506\r\nnative :  0.0322\r\nnative :  0.0322\r\nTScript:  0.1917\r\nTScript:  0.0442\r\nTScript:  0.0442\r\nnumba  :  0.1638\r\nnumba  :  0.0145\r\nnumba  :  0.0143\r\n```\r\nI suspect the problem is that TorchScript does not fuse all operations into a single CUDA. It generates two CUDA kernels (see the `PYTORCH_FUSION_DEBUG` output below)\r\n\r\n### Standalone Script\r\n```python\r\nimport math\r\nimport torch\r\nimport time\r\nimport random\r\nimport sys\r\n\r\ndef kernel_native(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n    exp_avg.mul_(beta1).add_(1 - beta1, grad)\r\n    exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\r\n    denom = exp_avg_sq.sqrt().add_(eps)\r\n    param.addcdiv_(-step_size, exp_avg, denom)   \r\n\r\n@torch.jit.script\r\ndef kernel_jit_script(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n    exp_avg.mul_(beta1)\r\n    exp_avg.add_((1 - beta1) + grad)\r\n    exp_avg_sq.mul_(beta2)\r\n    exp_avg_sq.add_((1 - beta2)*(grad*grad))\r\n    denom = exp_avg_sq.sqrt().add_(eps)\r\n    param += -step_size * (exp_avg / denom)\r\n\r\nfrom numba import cuda\r\n@cuda.jit\r\ndef kernel_numba_cuda(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    i = cuda.grid(1)\r\n    if i > grad.size:\r\n        return\r\n\r\n    exp_avg[i] = exp_avg[i] * beta1 + (1 - beta1) + grad[i]\r\n    exp_avg_sq[i] = exp_avg_sq[i] * beta2 + (1 - beta2) *grad[i]*grad[i]\r\n    denom = math.sqrt(exp_avg_sq[i]) + eps\r\n    param[i] += -step_size * (exp_avg[i] / denom)\r\n\r\ndef kernel_numba(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n\r\n    import ctypes\r\n    import numpy as np\r\n    def get_devicendarray(t):\r\n        assert t.type() == 'torch.cuda.FloatTensor'\r\n        numpy_dtype = np.float32\r\n        itemsize = 4\r\n\r\n        ctx = cuda.cudadrv.driver.driver.get_context()\r\n        mp = cuda.cudadrv.driver.MemoryPointer(ctx, ctypes.c_ulong(t.data_ptr()), t.numel()*itemsize)\r\n        return cuda.cudadrv.devicearray.DeviceNDArray(t.numel(), itemsize, numpy_dtype(), gpu_data=mp, stream=torch.cuda.current_stream().cuda_stream)\r\n\r\n    numba_param = get_devicendarray(param)\r\n    numba_grad = get_devicendarray(grad)\r\n    numba_exp_avg = get_devicendarray(exp_avg)\r\n    numba_exp_avg_sq = get_devicendarray(exp_avg_sq)\r\n    threadsperblock = 32\r\n    blockspergrid = (param.numel() + (threadsperblock - 1)) // threadsperblock\r\n    kernel_numba_cuda[blockspergrid, threadsperblock](numba_param, numba_grad, numba_exp_avg, numba_exp_avg_sq, beta1, beta2, step_size, eps)\r\n\r\nkernels = [(\"native \", kernel_native), (\"TScript\", kernel_jit_script), (\"numba  \", kernel_numba)]\r\n[Truncated]\n      size_t t1_dimIndex0 = t1_linearIndex ;\r\n      t1_offset += t1_dimIndex0 ;\r\n      IndexType t3_offset = 0;\r\n      IndexType t3_linearIndex = linearIndex;\r\n      \r\n      //printf(\"tensor t3 sizes[0] = %d, strides[0] = %d\\n\", t3.sizes[0],t3.strides[0]);\r\n      size_t t3_dimIndex0 = t3_linearIndex ;\r\n      t3_offset += t3_dimIndex0 ;\r\n      \r\n      // calculate the results\r\n      float n0 = __ldg(&t0.data[t0_offset]);\r\n      float n1 = __ldg(&t1.data[t1_offset]);\r\n      double n2 = s2;\r\n      float n3 = n0 / n1;\r\n      float n4 = n3 * ((float) n2);\r\n      t3.data[t3_offset] = n4;\r\n      \r\n    }\r\n}\r\n```\n<issue_comment>username_1: @username_0 can you try removing all the in-place operations in your scripted version?\r\nIIRC in-place ops block fusion.\r\n\r\nI was btw able to fuse most of sparse adam by removing those in-place ops, see https://github.com/pytorch/pytorch/pull/23522#issuecomment-516952462\n<issue_comment>username_2: cc: @username_3 and also relevant WIP PR: [JIT version of Adagrad](https://github.com/pytorch/pytorch/pull/23640)\n<issue_comment>username_3: Yeah the CUDA fusion does not support fusing in-place operations yet, Can you try the suggestion by @username_1 to see if you get a better perf?\n<issue_comment>username_3: @username_0 Also, it seems like you are only timing it in 3 iterations, and the perf numbers seems including the compilation time. The first time JIT/TorchScript sees an input it will specialize and do optimizations against that input, so when comparing performance please allow a compilation time and runtime optimization warmup for the input.\n<issue_comment>username_4: @username_3 That made a big difference to my traced module too. Any other tricks like this? It slows down the non-JIT version, however.\n<issue_comment>username_3: @username_4 That's pretty much all, let me summarize:\r\n\r\n1. JIT fusion does not support in-place operation fusion and reduction operation yet.\r\n2. we did the fusion code generation at the first time we see an input that the shape we haven't seen before (like if the number of dimensions are different), this will help us generate faster cuda kernels. \r\n\r\nSo the TorchScript compilation time will be gone after compile and the first run :)\n<issue_comment>username_0: Thanks for the feedback!\r\n\r\nI have removed all in-place operations and added three kernel implementations:\r\n```python\r\n\r\nimport math\r\nimport torch\r\nimport time\r\nimport random\r\nimport sys\r\n\r\ndef kernel_native(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n    exp_avg.mul_(beta1).add_(1 - beta1, grad)\r\n    exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\r\n    denom = exp_avg_sq.sqrt().add_(eps)\r\n    param.addcdiv_(-step_size, exp_avg, denom)   \r\n\r\n@torch.jit.script\r\ndef kernel_jit_script1(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n    exp_avg *= beta1\r\n    exp_avg += (1 - beta1) + grad\r\n    exp_avg_sq *= beta2\r\n    exp_avg_sq += (1 - beta2)*(grad*grad)\r\n    param += -step_size * (exp_avg / exp_avg_sq.sqrt() + eps)\r\n\r\n@torch.jit.script\r\ndef kernel_jit_script2(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n    exp_avg[:] = exp_avg * beta1 + (1 - beta1) + grad\r\n    exp_avg_sq[:] = exp_avg_sq * beta2 + (1 - beta2)*(grad*grad)\r\n    param[:] = param -step_size * (exp_avg / exp_avg_sq.sqrt() + eps)\r\n\r\n@torch.jit.script\r\ndef kernel_jit_script3(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> Tuple[Tensor, Tensor, Tensor, Tensor]\r\n    exp_avg = exp_avg * beta1 + (1 - beta1) + grad\r\n    exp_avg_sq = exp_avg_sq * beta2 + (1 - beta2)*(grad*grad)\r\n    param = param -step_size * (exp_avg / exp_avg_sq.sqrt() + eps)\r\n    return (param, grad, exp_avg, exp_avg_sq)\r\n\r\n\r\nfrom numba import cuda\r\n@cuda.jit\r\ndef kernel_numba_cuda(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    i = cuda.grid(1)\r\n    if i > grad.size:\r\n        return\r\n\r\n    exp_avg[i] = exp_avg[i] * beta1 + (1 - beta1) + grad[i]\r\n    exp_avg_sq[i] = exp_avg_sq[i] * beta2 + (1 - beta2) *grad[i]*grad[i]\r\n    denom = math.sqrt(exp_avg_sq[i]) + eps\r\n    param[i] += -step_size * (exp_avg[i] / denom)\r\n\r\ndef kernel_numba(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n\r\n    import ctypes\r\n    import numpy as np\r\n    def get_devicendarray(t):\r\n        assert t.type() == 'torch.cuda.FloatTensor'\r\n        numpy_dtype = np.float32\r\n        itemsize = 4\r\n\r\n        ctx = cuda.cudadrv.driver.driver.get_context()\r\n        mp = cuda.cudadrv.driver.MemoryPointer(ctx, ctypes.c_ulong(t.data_ptr()), t.numel()*itemsize)\r\n        return cuda.cudadrv.devicearray.DeviceNDArray(t.numel(), itemsize, numpy_dtype(), gpu_data=mp, stream=torch.cuda.current_stream().cuda_stream)\r\n\r\n    numba_param = get_devicendarray(param)\r\n    numba_grad = get_devicendarray(grad)\r\n    numba_exp_avg = get_devicendarray(exp_avg)\r\n    numba_exp_avg_sq = get_devicendarray(exp_avg_sq)\r\n    threadsperblock = 32\r\n    blockspergrid = (param.numel() + (threadsperblock - 1)) // threadsperblock\r\n    kernel_numba_cuda[blockspergrid, threadsperblock](numba_param, numba_grad, numba_exp_avg, numba_exp_avg_sq, beta1, beta2, step_size, eps)\r\n\r\nkernels = [(\"native \", kernel_native), (\"numba  \", kernel_numba)]\r\nshape = (4194304*64,)\r\nparam, grad, exp_avg, exp_avg_sq = [torch.rand(shape).to(\"cuda\") for x in range(4)]\r\n[Truncated]\n        res = func(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps)\r\n        if res is not None:\r\n            param[:], _, exp_avg[:], exp_avg_sq[:] = res\r\n        torch.cuda.synchronize()\r\n        t2 = time.time()\r\n        print(\"%s: \" % name, t2-t1)\r\n```\r\nThe results are:\r\n```\r\nnative :  0.0323\r\nnumba  :  0.0147\r\nTScript1:  0.0369\r\nTScript2:  0.0352\r\nTScript3:  0.0280\r\n```\r\nTScript3 is now better than native but still far from numba. TScript3 is now generating a single CUDA kernel but the back-copying of the result now dominates the timing. If I remove the line `param[:], _, exp_avg[:], exp_avg_sq[:] = res` the performance is almost on pair with numba:\r\n```\r\nTScript3 (no back-copy):  0.01664\r\n```\r\nThe problem is I really do need to update the values :/\n<issue_comment>username_5: cc @mruberry","repo":"pytorch/pytorch","issue_id":475728990,"issue_number":23655}
{"question":"What change does the PR make to TensorIterator regarding name inference for binary ops?","answer":"The PR changes TensorIterator to compute names before shape checks and propagate them after binary ops. It adds a 'names_' field to TensorIterator to hold output names.","id":183,"text":"Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#25563 Fix binary op name inference to happen before shape checks**\n\nBefore, for binary ops, name inference occurred after shape checks. This\ndefeats the purposes for names because the names are supposed to tell\nthe user that i.e. their tensors are misaligned or that they are adding\nincompatible tensors.\n\nThis PR changes TensorIterator so that names are computed before shape checks and\npropagated after the binary ops are finished. In order to support this,\nthis PR makes the following changes:\n- adds a `names_` field to TensorIterator, similar to `shape_`. This is\nnecessary to hold the output names, that are computed in\n`compute_names`, until they are used in `propagate_names_to_outputs()`.\n\nDifferential Revision: [D17158869](https://our.internmc.facebook.com/intern/diff/D17158869)","context":"<issue_start><issue_comment>Title: Fix binary op name inference to happen before shape checks\nusername_0: Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#25563 Fix binary op name inference to happen before shape checks**\n\nBefore, for binary ops, name inference occurred after shape checks. This\ndefeats the purposes for names because the names are supposed to tell\nthe user that i.e. their tensors are misaligned or that they are adding\nincompatible tensors.\n\nThis PR changes TensorIterator so that names are computed before shape checks and\npropagated after the binary ops are finished. In order to support this,\nthis PR makes the following changes:\n- adds a `names_` field to TensorIterator, similar to `shape_`. This is\nnecessary to hold the output names, that are computed in\n`compute_names`, until they are used in `propagate_names_to_outputs()`.\n\nDifferential Revision: [D17158869](https://our.internmc.facebook.com/intern/diff/D17158869)","repo":"pytorch/pytorch","issue_id":488354709,"issue_number":25563}
{"question":"What issue did the author encounter when testing the changes with a quantized model?","answer":"The author encountered assertions in the code triggered by the presence of variables in specific code sections.","id":185,"text":"I got a chance to quickly test this out with my changes that run a quantized model. Seems like the flow now hits assertions in the code where it expects to not get a variable.\r\n\r\nFirst hit - \r\n`abort_message: assertion \"terminating with uncaught exception of type std::runtime_error: !self.is_variable() INTERNAL ASSERT FAILED at../aten/src/ATen/quantized/Quantizer.cpp`\r\n\r\nSecond hit\r\n`abort_message: assertion \"terminating with uncaught exception of type std::runtime_error: !options.is_variable() INTERNAL ASSERT FAILED at../aten/src/ATen/native/TensorFactories.cpp`\r\n\r\nI was able to run the model after commenting out both those assert statements. I'm guessing there may be other parts of the code where similar change needs to be made.","context":"<issue_start><issue_comment>Title: Switch static dispatch to use extractLegacyTypeId.\nusername_0: Stack from [ghstack](https://github.com/username_0/ghstack):\n* **#26813 Switch static dispatch to use extractLegacyTypeId.**\n\nAttempt to fix #26764\n<issue_comment>username_1: I got a chance to quickly test this out with my changes that run a quantized model. Seems like the flow now hits assertions in the code where it expects to not get a variable.\r\n\r\nFirst hit - \r\n`abort_message: assertion \"terminating with uncaught exception of type std::runtime_error: !self.is_variable() INTERNAL ASSERT FAILED at../aten/src/ATen/quantized/Quantizer.cpp`\r\n\r\nSecond hit\r\n`abort_message: assertion \"terminating with uncaught exception of type std::runtime_error: !options.is_variable() INTERNAL ASSERT FAILED at../aten/src/ATen/native/TensorFactories.cpp`\r\n\r\nI was able to run the model after commenting out both those assert statements. I'm guessing there may be other parts of the code where similar change needs to be made.\n<issue_comment>username_0: OK. So basically, this patch is a bandaid: the real problem is there is Tensor-Variable confusion somewhere in the code here, and that is triggering the asserts. While I don't feel too bad about unblocking by removing the asserts on mobile only, if this is reproducible server side it indicates a very real problem. My guess would be in the new quantization code :)\r\n\r\nIs it at all possible to run these mobile models, but on server? That would help diagnose further,.\n<issue_comment>username_2: fyi, it is possible with the \"scripts/build_mobile.sh\" script I recently created, for exactly this purpose.\r\nBut for this specific problem it involves some other not-yet-landed changes from others so it's more complicated debugging process. I updated the issue with some new findings.\n<issue_comment>username_0: I think this is subsumed by #26908.","repo":"pytorch/pytorch","issue_id":498366431,"issue_number":26813}
{"question":"What is the real problem mentioned in the text that is triggering asserts in the code?","answer":"The real problem is Tensor-Variable confusion somewhere in the code.","id":186,"text":"OK. So basically, this patch is a bandaid: the real problem is there is Tensor-Variable confusion somewhere in the code here, and that is triggering the asserts. While I don't feel too bad about unblocking by removing the asserts on mobile only, if this is reproducible server side it indicates a very real problem. My guess would be in the new quantization code :)\r\n\r\nIs it at all possible to run these mobile models, but on server? That would help diagnose further,.","context":"<issue_start><issue_comment>Title: Switch static dispatch to use extractLegacyTypeId.\nusername_0: Stack from [ghstack](https://github.com/username_0/ghstack):\n* **#26813 Switch static dispatch to use extractLegacyTypeId.**\n\nAttempt to fix #26764\n<issue_comment>username_1: I got a chance to quickly test this out with my changes that run a quantized model. Seems like the flow now hits assertions in the code where it expects to not get a variable.\r\n\r\nFirst hit - \r\n`abort_message: assertion \"terminating with uncaught exception of type std::runtime_error: !self.is_variable() INTERNAL ASSERT FAILED at../aten/src/ATen/quantized/Quantizer.cpp`\r\n\r\nSecond hit\r\n`abort_message: assertion \"terminating with uncaught exception of type std::runtime_error: !options.is_variable() INTERNAL ASSERT FAILED at../aten/src/ATen/native/TensorFactories.cpp`\r\n\r\nI was able to run the model after commenting out both those assert statements. I'm guessing there may be other parts of the code where similar change needs to be made.\n<issue_comment>username_0: OK. So basically, this patch is a bandaid: the real problem is there is Tensor-Variable confusion somewhere in the code here, and that is triggering the asserts. While I don't feel too bad about unblocking by removing the asserts on mobile only, if this is reproducible server side it indicates a very real problem. My guess would be in the new quantization code :)\r\n\r\nIs it at all possible to run these mobile models, but on server? That would help diagnose further,.\n<issue_comment>username_2: fyi, it is possible with the \"scripts/build_mobile.sh\" script I recently created, for exactly this purpose.\r\nBut for this specific problem it involves some other not-yet-landed changes from others so it's more complicated debugging process. I updated the issue with some new findings.\n<issue_comment>username_0: I think this is subsumed by #26908.","repo":"pytorch/pytorch","issue_id":498366431,"issue_number":26813}
{"question":"What script was recently created for a specific purpose according to the text chunk?","answer":"\"scripts/build_mobile.sh\"","id":187,"text":"fyi, it is possible with the \"scripts/build_mobile.sh\" script I recently created, for exactly this purpose.\r\nBut for this specific problem it involves some other not-yet-landed changes from others so it's more complicated debugging process. I updated the issue with some new findings.","context":"<issue_start><issue_comment>Title: Switch static dispatch to use extractLegacyTypeId.\nusername_0: Stack from [ghstack](https://github.com/username_0/ghstack):\n* **#26813 Switch static dispatch to use extractLegacyTypeId.**\n\nAttempt to fix #26764\n<issue_comment>username_1: I got a chance to quickly test this out with my changes that run a quantized model. Seems like the flow now hits assertions in the code where it expects to not get a variable.\r\n\r\nFirst hit - \r\n`abort_message: assertion \"terminating with uncaught exception of type std::runtime_error: !self.is_variable() INTERNAL ASSERT FAILED at../aten/src/ATen/quantized/Quantizer.cpp`\r\n\r\nSecond hit\r\n`abort_message: assertion \"terminating with uncaught exception of type std::runtime_error: !options.is_variable() INTERNAL ASSERT FAILED at../aten/src/ATen/native/TensorFactories.cpp`\r\n\r\nI was able to run the model after commenting out both those assert statements. I'm guessing there may be other parts of the code where similar change needs to be made.\n<issue_comment>username_0: OK. So basically, this patch is a bandaid: the real problem is there is Tensor-Variable confusion somewhere in the code here, and that is triggering the asserts. While I don't feel too bad about unblocking by removing the asserts on mobile only, if this is reproducible server side it indicates a very real problem. My guess would be in the new quantization code :)\r\n\r\nIs it at all possible to run these mobile models, but on server? That would help diagnose further,.\n<issue_comment>username_2: fyi, it is possible with the \"scripts/build_mobile.sh\" script I recently created, for exactly this purpose.\r\nBut for this specific problem it involves some other not-yet-landed changes from others so it's more complicated debugging process. I updated the issue with some new findings.\n<issue_comment>username_0: I think this is subsumed by #26908.","repo":"pytorch/pytorch","issue_id":498366431,"issue_number":26813}
{"question":"What build failure was recognized in the 'pytorch_xla_linux_xenial_py3_6_clang7_build'?","answer":"An error related to 'DispatchKey' in the namespace 'at' was recognized in the 'pytorch_xla_linux_xenial_py3_6_clang7_build'.","id":189,"text":"## :pill: CircleCI build failures summary and remediations\nAs of commit bf488282:\n\n\n* **1/1** failures introduced in this PR\n\n\n## Detailed failure analysis\n\nOne may explore the probable reasons each build failed interactively [on the Dr. CI website](https://dr.pytorch.org/commit-details.html?sha1=bf488282b9deb369e76de6cfffb0bdec2aed214f).\n\n### :detective: 1 new failure recognized by patterns\nThe following build failures do not appear to be due to upstream breakage:\n#### [![See CircleCI build](https://avatars0.githubusercontent.com/ml/7?s=12)](https://circleci.com/gh/pytorch/pytorch/4266171) pytorch_xla_linux_xenial_py3_6_clang7_build (1/1)\n**Step:** \"Build\" ([full log](https://dr.pytorch.org/api/view-log-full?build_id=86837350) | [pattern match details](https://dr.pytorch.org/build-details.html?build_id=86837350))\n<details>\n<summary>\n<code>Jan 16 05:27:23 torch_xla/csrc/aten_xla_type_default.cpp:9339:103: error: no member named 'DispatchKey' in namespace 'at'; did you mean 'Dispatcher'?</code>\n</summary>\n\n```\nJan 16 05:27:22 /opt/conda/lib/python3.6/site-packages/torch/include/ATen/core/dispatch/Dispatcher.h:35:18: note: 'Dispatcher' declared here \nJan 16 05:27:22 class CAFFE2_API Dispatcher final { \nJan 16 05:27:22                  ^ \nJan 16 05:27:23 torch_xla/csrc/aten_xla_type_default.cpp:9336:138: error: no member named 'DispatchKey' in namespace 'at'; did you mean 'Dispatcher'? \nJan 16 05:27:23       .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &, const at::Tensor &, bool), &AtenXlaTypeDefault::_cholesky_solve_helper>(at::DispatchKey::XLATensorId) \nJan 16 05:27:23                                                                                                                                      ~~~~^~~~~~~~~~~ \nJan 16 05:27:23                                                                                                                                          Dispatcher \nJan 16 05:27:23 /opt/conda/lib/python3.6/site-packages/torch/include/ATen/core/dispatch/Dispatcher.h:35:18: note: 'Dispatcher' declared here \nJan 16 05:27:23 class CAFFE2_API Dispatcher final { \nJan 16 05:27:23                  ^ \nJan 16 05:27:23 torch_xla/csrc/aten_xla_type_default.cpp:9339:103: error: no member named 'DispatchKey' in namespace 'at'; did you mean 'Dispatcher'? \nJan 16 05:27:23       .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &, bool), &AtenXlaTypeDefault::_coalesced_>(at::DispatchKey::XLATensorId) \nJan 16 05:27:23                                                                                                   ~~~~^~~~~~~~~~~ \nJan 16 05:27:23                                                                                                       Dispatcher \nJan 16 05:27:23 /opt/conda/lib/python3.6/site-packages/torch/include/ATen/core/dispatch/Dispatcher.h:35:18: note: 'Dispatcher' declared here \nJan 16 05:27:23 class CAFFE2_API Dispatcher final { \nJan 16 05:27:23                  ^ \nJan 16 05:27:23 fatal error: too many errors emitted, stopping now [-ferror-limit=] \nJan 16 05:27:23 20 errors generated. \npackages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.6/site-packages/torch/include/TH -I/opt/conda/lib/python3.6/site-packages/torch/include/THC -I/opt/conda/include/python3.6m -c torch_xla/csrc/layout_manager.cpp -o build/temp.linux-x86_64-3.6/torch_xla/csrc/layout_manager.o -std=c++14 -Wno-sign-compare -Wno-deprecated-declarations -Wno-return-type -Wno-macro-redefined -Wno-return-std-move -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=_XLAC -D_GLIBCXX_USE_CXX11_ABI=1 \n/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.6/site-packages/torch/include/TH -I/opt/conda/lib/python3.6/site-packages/torch/include/THC -I/opt/conda/include/python3.6m -c torch_xla/csrc/view.cpp -o build/temp.linux-x86_64-3.6/torch_xla/csrc/view.o -std=c++14 -Wno-sign-compare -Wno-deprecated-declarations -Wno-return-type -Wno-macro-redefined -Wno-return-std-move -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=_XLAC -D_GLIBCXX_USE_CXX11_ABI=1 \n```\n\n</details>\n\n\n\n---\n\n<details><summary>This comment was automatically generated by <a href=\"https://github.com/username_1/circleci-failure-tracker/tree/master/docs/from-pull-request-comment\">Dr. CI</a> (expand for details).</summary>Follow <a href=\"https://dr.pytorch.org/admin/comments-opt-out.html\">this link to opt-out</a> of these comments for your Pull Requests.\n\nPlease report bugs/suggestions on the <a href=\"https://github.com/username_1/circleci-failure-tracker/issues\">GitHub issue tracker</a>.\n</details>","context":"<issue_start><issue_comment>Title: [WIP] faster bailout tests\nusername_0: \n<issue_comment>username_1: ## :pill: CircleCI build failures summary and remediations\nAs of commit bf488282:\n\n\n* **1/1** failures introduced in this PR\n\n\n## Detailed failure analysis\n\nOne may explore the probable reasons each build failed interactively [on the Dr. CI website](https://dr.pytorch.org/commit-details.html?sha1=bf488282b9deb369e76de6cfffb0bdec2aed214f).\n\n### :detective: 1 new failure recognized by patterns\nThe following build failures do not appear to be due to upstream breakage:\n#### [![See CircleCI build](https://avatars0.githubusercontent.com/ml/7?s=12)](https://circleci.com/gh/pytorch/pytorch/4266171) pytorch_xla_linux_xenial_py3_6_clang7_build (1/1)\n**Step:** \"Build\" ([full log](https://dr.pytorch.org/api/view-log-full?build_id=86837350) | [pattern match details](https://dr.pytorch.org/build-details.html?build_id=86837350))\n<details>\n<summary>\n<code>Jan 16 05:27:23 torch_xla/csrc/aten_xla_type_default.cpp:9339:103: error: no member named 'DispatchKey' in namespace 'at'; did you mean 'Dispatcher'?</code>\n</summary>\n\n```\nJan 16 05:27:22 /opt/conda/lib/python3.6/site-packages/torch/include/ATen/core/dispatch/Dispatcher.h:35:18: note: 'Dispatcher' declared here \nJan 16 05:27:22 class CAFFE2_API Dispatcher final { \nJan 16 05:27:22                  ^ \nJan 16 05:27:23 torch_xla/csrc/aten_xla_type_default.cpp:9336:138: error: no member named 'DispatchKey' in namespace 'at'; did you mean 'Dispatcher'? \nJan 16 05:27:23       .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &, const at::Tensor &, bool), &AtenXlaTypeDefault::_cholesky_solve_helper>(at::DispatchKey::XLATensorId) \nJan 16 05:27:23                                                                                                                                      ~~~~^~~~~~~~~~~ \nJan 16 05:27:23                                                                                                                                          Dispatcher \nJan 16 05:27:23 /opt/conda/lib/python3.6/site-packages/torch/include/ATen/core/dispatch/Dispatcher.h:35:18: note: 'Dispatcher' declared here \nJan 16 05:27:23 class CAFFE2_API Dispatcher final { \nJan 16 05:27:23                  ^ \nJan 16 05:27:23 torch_xla/csrc/aten_xla_type_default.cpp:9339:103: error: no member named 'DispatchKey' in namespace 'at'; did you mean 'Dispatcher'? \nJan 16 05:27:23       .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &, bool), &AtenXlaTypeDefault::_coalesced_>(at::DispatchKey::XLATensorId) \nJan 16 05:27:23                                                                                                   ~~~~^~~~~~~~~~~ \nJan 16 05:27:23                                                                                                       Dispatcher \nJan 16 05:27:23 /opt/conda/lib/python3.6/site-packages/torch/include/ATen/core/dispatch/Dispatcher.h:35:18: note: 'Dispatcher' declared here \nJan 16 05:27:23 class CAFFE2_API Dispatcher final { \nJan 16 05:27:23                  ^ \nJan 16 05:27:23 fatal error: too many errors emitted, stopping now [-ferror-limit=] \nJan 16 05:27:23 20 errors generated. \npackages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.6/site-packages/torch/include/TH -I/opt/conda/lib/python3.6/site-packages/torch/include/THC -I/opt/conda/include/python3.6m -c torch_xla/csrc/layout_manager.cpp -o build/temp.linux-x86_64-3.6/torch_xla/csrc/layout_manager.o -std=c++14 -Wno-sign-compare -Wno-deprecated-declarations -Wno-return-type -Wno-macro-redefined -Wno-return-std-move -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=_XLAC -D_GLIBCXX_USE_CXX11_ABI=1 \n/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.6/site-packages/torch/include/TH -I/opt/conda/lib/python3.6/site-packages/torch/include/THC -I/opt/conda/include/python3.6m -c torch_xla/csrc/view.cpp -o build/temp.linux-x86_64-3.6/torch_xla/csrc/view.o -std=c++14 -Wno-sign-compare -Wno-deprecated-declarations -Wno-return-type -Wno-macro-redefined -Wno-return-std-move -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=_XLAC -D_GLIBCXX_USE_CXX11_ABI=1 \n```\n\n</details>\n\n\n\n---\n\n<details><summary>This comment was automatically generated by <a href=\"https://github.com/username_1/circleci-failure-tracker/tree/master/docs/from-pull-request-comment\">Dr. CI</a> (expand for details).</summary>Follow <a href=\"https://dr.pytorch.org/admin/comments-opt-out.html\">this link to opt-out</a> of these comments for your Pull Requests.\n\nPlease report bugs/suggestions on the <a href=\"https://github.com/username_1/circleci-failure-tracker/issues\">GitHub issue tracker</a>.\n</details>\n<issue_comment>username_0: I put together a small benchmark that runs 10,000 `prim::BailOut`s and no real ops. Even though, the fluctuations are huge on a devserver, `isCompatibleWith` is definitely much faster (4-8x).\r\n\r\n**baseline:**\r\n```\r\ntime = 0.224115446\r\ntime = 0.241094661\r\ntime = 0.206836688\r\ntime = 0.298701322\r\ntime = 0.236710434\r\ntime = 0.224483698\r\ntime = 0.224836235\r\ntime = 0.198836114\r\ntime = 0.187320452\r\ntime = 0.208056021\r\n```\r\n\r\n**isCompatibleWith:**\r\n\r\n```\r\ntime = 0.044660287\r\ntime = 0.045000739\r\ntime = 0.048079375\r\ntime = 0.085522459\r\ntime = 0.05170733\r\ntime = 0.047986804\r\ntime = 0.075046798\r\ntime = 0.075740901\r\ntime = 0.049676586\r\ntime = 0.046801684\r\n```","repo":"pytorch/pytorch","issue_id":550556777,"issue_number":32266}
