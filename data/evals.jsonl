{"question":"Is there any value in having a private package that will never be published?","answer":"No, there is no value in having a private package that will never be published.","id":2,"text":"That's a private package that will never be published so I don't think there's any value in having this. The real package that gets published to npm does have this set (https://github.com/facebook/react/blob/master/packages/react/package.json#L8). Thanks though!","context":"<issue_start><issue_comment>Title: Update package.json\nusername_0: Homepage URL for package.json\n<issue_comment>username_1: That's a private package that will never be published so I don't think there's any value in having this. The real package that gets published to npm does have this set (https://github.com/facebook/react/blob/master/packages/react/package.json#L8). Thanks though!","repo":"facebook/react","issue_id":145259386,"issue_number":6394}
{"question":"What value is assigned to the REACT_ELEMENT_TYPE variable in the provided link?","answer":"Symbol.for('react.element')","id":4,"text":"Thanks for the PR but I believe the original is clearer. REACT_ELEMENT_TYPE [is](https://github.com/username_0/react/blob/bedbc5f066b7a45520ab78ecb806ab557485535a/src/shared/utils/ReactElementSymbol.js#L18) Symbol.for('react.element'). Cheers!","context":"<issue_start><issue_comment>Title: Fix docs on ReactElement\nusername_0: We should test $$typeof field against REACT_ELEMENT_TYPE to check if something is a React Element.\n<issue_comment>username_1: Thanks for the PR but I believe the original is clearer. REACT_ELEMENT_TYPE [is](https://github.com/username_0/react/blob/bedbc5f066b7a45520ab78ecb806ab557485535a/src/shared/utils/ReactElementSymbol.js#L18) Symbol.for('react.element'). Cheers!\n<issue_comment>username_0: REACT_ELEMENT_TYPE is a plain number if there is no native Symbol nor polyfill.\n<issue_comment>username_1: Right, but this is a fallback case and doesn't seem as important to document here to me.\r\nMaybe could add \"or a magic number as a fallback\".\n<issue_comment>username_0: Got it. Then I'll just ignore it. Thanks.","repo":"facebook/react","issue_id":186222382,"issue_number":8160}
{"question":"Why are the 'ne' and 'fill' methods not implemented for the HalfTensor in the source code?","answer":"The 'ne' and 'fill' methods are not implemented for the HalfTensor in the source code due to specific reasons related to the design or limitations of the HalfTensor implementation, distinct from the absence of restrictions in the cwrap definitions.","id":10,"text":"@username_1 Hmm now I am bit curious about the reasons for the failure of the test cases. It says that ne and fill are not implemented for the HalfTensor. From from the source code I think we are actually generating those methods since there isn't any restriction in the cwrap definitions?","context":"<issue_start><issue_comment>Title: fix torch.is_tensor not recognizing HalfTensor\nusername_0: Fix #1922.\n<issue_comment>username_1: This needs some additional changes in the tests\n<issue_comment>username_0: @username_1 Hmm now I am bit curious about the reasons for the failure of the test cases. It says that ne and fill are not implemented for the HalfTensor. From from the source code I think we are actually generating those methods since there isn't any restriction in the cwrap definitions?\n<issue_comment>username_2: @username_0 CPU HalfTensor has no methods implemented except copy and serialization stuff. So almost all tests have to be filtered out for CPU HalfTensor\n<issue_comment>username_3: what we do in TH is basically define an \"is_tensor\" and a \"has_tensor_math\"; then you write the tests based on which of those you are actually testing.\n<issue_comment>username_0: @username_2 so I took a look at `test_torch.py`, it looks like the only reference to tensor_classes is in test_print. For other tests usually they are self-contained about which types they are testing against.\n<issue_comment>username_4: @pytorchbot add to whitelist\n<issue_comment>username_0: After fixing this issue. I wonder why it would be a good idea to have a CPU HalfTensor at all. Since HalfTensor is a subtype of Tensor, we would expect it to support the same interface as Tensor. However, the current implementation of HalfTensor does not allow such behaviour.\n<issue_comment>username_2: on the CPU, HalfTensor is just there for serialization purposes. For example saving a GPU HalfTensor and being able to load the weights back on a CPU-only machine.\r\n\r\nOther than that it has no purpose, as Half is not a native type on CPU and all operations will be very very very slow.","repo":"pytorch/pytorch","issue_id":239293180,"issue_number":1934}
{"question":"What is a challenge in testing CPU HalfTensor in machine learning frameworks according to the provided text?","answer":"Most tests have to be filtered out for CPU HalfTensor due to limited methods implemented.","id":11,"text":"@username_0 CPU HalfTensor has no methods implemented except copy and serialization stuff. So almost all tests have to be filtered out for CPU HalfTensor","context":"<issue_start><issue_comment>Title: fix torch.is_tensor not recognizing HalfTensor\nusername_0: Fix #1922.\n<issue_comment>username_1: This needs some additional changes in the tests\n<issue_comment>username_0: @username_1 Hmm now I am bit curious about the reasons for the failure of the test cases. It says that ne and fill are not implemented for the HalfTensor. From from the source code I think we are actually generating those methods since there isn't any restriction in the cwrap definitions?\n<issue_comment>username_2: @username_0 CPU HalfTensor has no methods implemented except copy and serialization stuff. So almost all tests have to be filtered out for CPU HalfTensor\n<issue_comment>username_3: what we do in TH is basically define an \"is_tensor\" and a \"has_tensor_math\"; then you write the tests based on which of those you are actually testing.\n<issue_comment>username_0: @username_2 so I took a look at `test_torch.py`, it looks like the only reference to tensor_classes is in test_print. For other tests usually they are self-contained about which types they are testing against.\n<issue_comment>username_4: @pytorchbot add to whitelist\n<issue_comment>username_0: After fixing this issue. I wonder why it would be a good idea to have a CPU HalfTensor at all. Since HalfTensor is a subtype of Tensor, we would expect it to support the same interface as Tensor. However, the current implementation of HalfTensor does not allow such behaviour.\n<issue_comment>username_2: on the CPU, HalfTensor is just there for serialization purposes. For example saving a GPU HalfTensor and being able to load the weights back on a CPU-only machine.\r\n\r\nOther than that it has no purpose, as Half is not a native type on CPU and all operations will be very very very slow.","repo":"pytorch/pytorch","issue_id":239293180,"issue_number":1934}
{"question":"What concepts are defined in TH for testing?","answer":"TH defines \"is_tensor\" and \"has_tensor_math\" for testing purposes.","id":12,"text":"what we do in TH is basically define an \"is_tensor\" and a \"has_tensor_math\"; then you write the tests based on which of those you are actually testing.","context":"<issue_start><issue_comment>Title: fix torch.is_tensor not recognizing HalfTensor\nusername_0: Fix #1922.\n<issue_comment>username_1: This needs some additional changes in the tests\n<issue_comment>username_0: @username_1 Hmm now I am bit curious about the reasons for the failure of the test cases. It says that ne and fill are not implemented for the HalfTensor. From from the source code I think we are actually generating those methods since there isn't any restriction in the cwrap definitions?\n<issue_comment>username_2: @username_0 CPU HalfTensor has no methods implemented except copy and serialization stuff. So almost all tests have to be filtered out for CPU HalfTensor\n<issue_comment>username_3: what we do in TH is basically define an \"is_tensor\" and a \"has_tensor_math\"; then you write the tests based on which of those you are actually testing.\n<issue_comment>username_0: @username_2 so I took a look at `test_torch.py`, it looks like the only reference to tensor_classes is in test_print. For other tests usually they are self-contained about which types they are testing against.\n<issue_comment>username_4: @pytorchbot add to whitelist\n<issue_comment>username_0: After fixing this issue. I wonder why it would be a good idea to have a CPU HalfTensor at all. Since HalfTensor is a subtype of Tensor, we would expect it to support the same interface as Tensor. However, the current implementation of HalfTensor does not allow such behaviour.\n<issue_comment>username_2: on the CPU, HalfTensor is just there for serialization purposes. For example saving a GPU HalfTensor and being able to load the weights back on a CPU-only machine.\r\n\r\nOther than that it has no purpose, as Half is not a native type on CPU and all operations will be very very very slow.","repo":"pytorch/pytorch","issue_id":239293180,"issue_number":1934}
{"question":"What is the specific runtime error mentioned in the text chunk with the link to the Stack Overflow question?","answer":"RuntimeError: DataLoader worker is killed by signal: Illegal instruction","id":131,"text":"Still can't get this to work.\r\n\r\nhttps://stackoverflow.com/questions/53449927/runtimeerror-dataloader-worker-is-killed-by-signal-illegal-instruction\r\n\r\nPlease help.","context":"<issue_start><issue_comment>Title: Problem in DataLoader (same problem with #4643, but not solved by it)\nusername_0: - OS: Ubuntu 16.04\r\n- PyTorch version: torch-0.3.1-cp36-cp36-linux_x86_64.whl\r\n- How you installed PyTorch (conda, pip, source): pip3 from the binary\r\n- Python version: 3.6\r\n- CUDA/cuDNN version: 9.1\r\n- GPU models and configuration: GTX 1070 Ti\r\n\r\nAfter install using the wheel file, I have changed the torch/utils/data/Dataloader.py according to the changes shown at https://github.com/pytorch/pytorch/pull/4643\r\n\r\nBut the same error still happen when I try the tutorial example to load image database:\r\n\r\nException ignored in: <bound method DataLoaderIter.__del__ of <torch.utils.data.dataloader.DataLoaderIter object at 0x56090c8c7348>>\r\nTraceback (most recent call last):\r\n  File \"/home/username_0/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 375, in __del__\r\n  File \"/home/username_0/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 361, in _shutdown_workers\r\n  File \"/home/username_0/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 39, in _get_from_queue\r\n  File \"/usr/pkg/lib/python3.6/multiprocessing/queues.py\", line 337, in get\r\nImportError: sys.meta_path is None, Python is likely shutting down\r\n\r\nSo could anyone help me on that? Or is there any other file I should modify to get it run?\n<issue_comment>username_0: And, I am not sure how can I use the changes in torch/csrc/DataLoader.cpp, should I clone all the source code on GitHub and build the newest binary to fully fix this my problem?\n<issue_comment>username_1: when does this error happen? is it happening when you exit python?\n<issue_comment>username_2: @username_1 and @username_3,\r\nI am running this:\r\n\r\npython train.py --dataroot ./datasets/horse2zebra --name horse2zebra_cyclegan --model cycle_gan --no_dropout --loadSize 64 --nThreads 1\r\n\r\nfrom \r\n\r\nhttps://github.com/junyanz/pytorch-CycleGAN-and-pix2pix\r\n\r\nThe model is succefully created:\r\nmodel [CycleGANModel] was created\r\n\r\nI am on ubuntu 16.04, pytorch 3.1, cuda 9.0 and cuDnn 7.1\r\n\r\nThe error I get is traced below:\r\ncreate web directory ./checkpoints/horse2zebra_cyclegan/web...\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 22, in <module>\r\n    for i, data in enumerate(dataset):\r\n  File \"/home/sam/Documents/ComputerVision/pytorch-CycleGAN-and-pix2pix/data/custom_dataset_data_loader.py\", line 44, in __iter__\r\n    for i, data in enumerate(self.dataloader):\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 275, in __next__\r\n    idx, batch = self._get_batch()\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 254, in _get_batch\r\n    return self.data_queue.get()\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/queues.py\", line 335, in get\r\n    res = self._reader.recv_bytes()\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\r\n    buf = self._recv_bytes(maxlength)\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\r\n    buf = self._recv(4)\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\r\n    chunk = read(handle, remaining)\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 175, in handler\r\n    _error_if_any_worker_fails()\r\nRuntimeError: DataLoader worker (pid 3792) is killed by signal: Illegal instruction.\n<issue_comment>username_3: @username_2 that error is because you are running it on a computer that is too old and does not have SSE4 instruction on CPU.\n<issue_comment>username_1: @username_2 Also, it is more likely due to that your CPU doesn't support the operations done in the dataset.__getitem__.\n<issue_comment>username_2: @username_1 & @username_3,\r\n\r\nSome months ago I made it work in windows10 (I think)\r\nThe my windows machine crashed. So here I am. Sorry\r\n\r\nHere is what I see in cat /proc/cpuinfo:\r\n\r\nprocessor\t: 0\r\nvendor_id\t: AuthenticAMD\r\ncpu family\t: 21\r\nmodel\t\t: 2\r\nmodel name\t: AMD FX(tm)-4300 Quad-Core Processor\r\nstepping\t: 0\r\nmicrocode\t: 0x600081c\r\ncpu MHz\t\t: 1400.000\r\ncache size\t: 2048 KB\r\nphysical id\t: 0\r\nsiblings\t: 4\r\ncore id\t\t: 0\r\ncpu cores\t: 2\r\napicid\t\t: 16\r\ninitial apicid\t: 0\r\nfpu\t\t: yes\r\nfpu_exception\t: yes\r\ncpuid level\t: 13\r\nwp\t\t: yes\r\nflags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf pni pclmulqdq monitor ssse3 fma cx16 **sse4_1 sse4_2** popcnt aes xsave avx f16c lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs xop skinit wdt lwp fma4 tce nodeid_msr tbm topoext perfctr_core perfctr_nb cpb hw_pstate retpoline retpoline_amd rsb_ctxsw vmmcall bmi1 arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold\r\nbugs\t\t: fxsave_leak sysret_ss_attrs null_seg spectre_v1 spectre_v2\r\nbogomips\t: 7634.15\r\nTLB size\t: 1536 4K pages\r\nclflush size\t: 64\r\ncache_alignment\t: 64\r\naddress sizes\t: 48 bits physical, 48 bits virtual\r\npower management: ts ttp tm 100mhzsteps hwpstate cpb eff_freq_ro\r\n\r\n@username_1 how do I check if CPU support dataset.getitem?\n<issue_comment>username_1: @username_2 run with `nThreads=0`\n<issue_comment>username_2: tried it with nThreads=0\r\ndifferent error...\r\nmodel [CycleGANModel] was created\r\ncreate web directory ./checkpoints/horse2zebra_cyclegan/web...\r\nIllegal instruction (core dumped)\n<issue_comment>username_2: I see something I am uneasy about:\r\nsee bold below\r\n\r\nCustomDatasetDataLoader\r\ndataset [UnalignedDataset] was created\r\n/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torchvision-0.2.0-py3.6.egg/torchvision/transforms/transforms.py:156: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\r\n#training images = 1334\r\ncycle_gan\r\n**initialization method [normal]\r\ninitialization method [normal]\r\ninitialization method [normal]\r\ninitialization method [normal]**\n<issue_comment>username_1: @username_2 No it's same error. We just wrap it in a nice way so you can see it even if it's in the workers. \r\n\r\nIsn't that expected? CycleGAN has four networks: D_X D_Y G F .\n<issue_comment>username_1: @username_2 anyways, it confirms my hypothesis that your CPU doesn't support the operation in `dataset.__getitem__`.\n<issue_comment>username_2: Oh... Can you suggest a workaround?\n<issue_comment>username_1: @username_2 You can try to put debug statements in CycleGAN code and see which one isn't supported by your CPU (probably some PIL operations), and search for replacements. Sorry I don't have much else to offer.\n<issue_comment>username_2: @username_1 \r\nThank you sir !<issue_closed>\n<issue_comment>username_1: closed due to inactivity. feel free to reopen if you can reproduce with latest stable (0.4 at the moment of writing).\n<issue_comment>username_4: Still can't get this to work.\r\n\r\nhttps://stackoverflow.com/questions/53449927/runtimeerror-dataloader-worker-is-killed-by-signal-illegal-instruction\r\n\r\nPlease help.","repo":"pytorch/pytorch","issue_id":304991946,"issue_number":5761}
{"question":"In `test_torch.py`, where is the only reference to `tensor_classes` found?","answer":"The only reference to `tensor_classes` is found in the `test_print` function.","id":13,"text":"@username_2 so I took a look at `test_torch.py`, it looks like the only reference to tensor_classes is in test_print. For other tests usually they are self-contained about which types they are testing against.","context":"<issue_start><issue_comment>Title: fix torch.is_tensor not recognizing HalfTensor\nusername_0: Fix #1922.\n<issue_comment>username_1: This needs some additional changes in the tests\n<issue_comment>username_0: @username_1 Hmm now I am bit curious about the reasons for the failure of the test cases. It says that ne and fill are not implemented for the HalfTensor. From from the source code I think we are actually generating those methods since there isn't any restriction in the cwrap definitions?\n<issue_comment>username_2: @username_0 CPU HalfTensor has no methods implemented except copy and serialization stuff. So almost all tests have to be filtered out for CPU HalfTensor\n<issue_comment>username_3: what we do in TH is basically define an \"is_tensor\" and a \"has_tensor_math\"; then you write the tests based on which of those you are actually testing.\n<issue_comment>username_0: @username_2 so I took a look at `test_torch.py`, it looks like the only reference to tensor_classes is in test_print. For other tests usually they are self-contained about which types they are testing against.\n<issue_comment>username_4: @pytorchbot add to whitelist\n<issue_comment>username_0: After fixing this issue. I wonder why it would be a good idea to have a CPU HalfTensor at all. Since HalfTensor is a subtype of Tensor, we would expect it to support the same interface as Tensor. However, the current implementation of HalfTensor does not allow such behaviour.\n<issue_comment>username_2: on the CPU, HalfTensor is just there for serialization purposes. For example saving a GPU HalfTensor and being able to load the weights back on a CPU-only machine.\r\n\r\nOther than that it has no purpose, as Half is not a native type on CPU and all operations will be very very very slow.","repo":"pytorch/pytorch","issue_id":239293180,"issue_number":1934}
{"question":"Why would it be a good idea to have a CPU HalfTensor with limitations compared to the expected behaviour of a Tensor?","answer":"The limitations of the CPU HalfTensor may be due to specific optimizations or trade-offs for efficiency and performance in operations that involve half-precision data, such as reducing memory usage or improving computational speed for certain tasks.","id":15,"text":"After fixing this issue. I wonder why it would be a good idea to have a CPU HalfTensor at all. Since HalfTensor is a subtype of Tensor, we would expect it to support the same interface as Tensor. However, the current implementation of HalfTensor does not allow such behaviour.","context":"<issue_start><issue_comment>Title: fix torch.is_tensor not recognizing HalfTensor\nusername_0: Fix #1922.\n<issue_comment>username_1: This needs some additional changes in the tests\n<issue_comment>username_0: @username_1 Hmm now I am bit curious about the reasons for the failure of the test cases. It says that ne and fill are not implemented for the HalfTensor. From from the source code I think we are actually generating those methods since there isn't any restriction in the cwrap definitions?\n<issue_comment>username_2: @username_0 CPU HalfTensor has no methods implemented except copy and serialization stuff. So almost all tests have to be filtered out for CPU HalfTensor\n<issue_comment>username_3: what we do in TH is basically define an \"is_tensor\" and a \"has_tensor_math\"; then you write the tests based on which of those you are actually testing.\n<issue_comment>username_0: @username_2 so I took a look at `test_torch.py`, it looks like the only reference to tensor_classes is in test_print. For other tests usually they are self-contained about which types they are testing against.\n<issue_comment>username_4: @pytorchbot add to whitelist\n<issue_comment>username_0: After fixing this issue. I wonder why it would be a good idea to have a CPU HalfTensor at all. Since HalfTensor is a subtype of Tensor, we would expect it to support the same interface as Tensor. However, the current implementation of HalfTensor does not allow such behaviour.\n<issue_comment>username_2: on the CPU, HalfTensor is just there for serialization purposes. For example saving a GPU HalfTensor and being able to load the weights back on a CPU-only machine.\r\n\r\nOther than that it has no purpose, as Half is not a native type on CPU and all operations will be very very very slow.","repo":"pytorch/pytorch","issue_id":239293180,"issue_number":1934}
{"question":"What is the purpose of HalfTensor on the CPU according to the text?","answer":"HalfTensor on the CPU is used for serialization purposes, such as saving a GPU HalfTensor and being able to load the weights back on a CPU-only machine. However, outside of serialization, HalfTensor serves no other purpose on the CPU as all operations with it will be very slow due to it not being a native type on the CPU.","id":16,"text":"on the CPU, HalfTensor is just there for serialization purposes. For example saving a GPU HalfTensor and being able to load the weights back on a CPU-only machine.\r\n\r\nOther than that it has no purpose, as Half is not a native type on CPU and all operations will be very very very slow.","context":"<issue_start><issue_comment>Title: fix torch.is_tensor not recognizing HalfTensor\nusername_0: Fix #1922.\n<issue_comment>username_1: This needs some additional changes in the tests\n<issue_comment>username_0: @username_1 Hmm now I am bit curious about the reasons for the failure of the test cases. It says that ne and fill are not implemented for the HalfTensor. From from the source code I think we are actually generating those methods since there isn't any restriction in the cwrap definitions?\n<issue_comment>username_2: @username_0 CPU HalfTensor has no methods implemented except copy and serialization stuff. So almost all tests have to be filtered out for CPU HalfTensor\n<issue_comment>username_3: what we do in TH is basically define an \"is_tensor\" and a \"has_tensor_math\"; then you write the tests based on which of those you are actually testing.\n<issue_comment>username_0: @username_2 so I took a look at `test_torch.py`, it looks like the only reference to tensor_classes is in test_print. For other tests usually they are self-contained about which types they are testing against.\n<issue_comment>username_4: @pytorchbot add to whitelist\n<issue_comment>username_0: After fixing this issue. I wonder why it would be a good idea to have a CPU HalfTensor at all. Since HalfTensor is a subtype of Tensor, we would expect it to support the same interface as Tensor. However, the current implementation of HalfTensor does not allow such behaviour.\n<issue_comment>username_2: on the CPU, HalfTensor is just there for serialization purposes. For example saving a GPU HalfTensor and being able to load the weights back on a CPU-only machine.\r\n\r\nOther than that it has no purpose, as Half is not a native type on CPU and all operations will be very very very slow.","repo":"pytorch/pytorch","issue_id":239293180,"issue_number":1934}
{"question":"What resource would you recommend to learn about integrating Facebook authentication in a React app and handling authentication and authorization for a Facebook app?","answer":"I would recommend the book 'Fullstack React: The Complete Guide to ReactJS and Friends' by Anthony Accomazzo, Nate Murray, and Ari Lerner. This book covers topics on building full-stack applications with React, including authentication strategies and integrating with APIs. It should provide valuable insights for integrating Facebook authentication in a React app and handling authentication and authorization.","id":17,"text":"In fact I would like to integrate Facebook authentication in my React app, and save user credentials in the database, along with additional data that will be used to authenticate with an API.\r\n\r\nI have no idea about how to handle authentication and authorization in a Facebook app. I went through some few books, but none of them touched that aspect. \r\n\r\nCan you point me to a book, a tutorial, any link ? whatever you think can point me to the right direction.\r\n\r\nThanks in advance.","context":"<issue_start><issue_comment>Title: (Non-issue) Need advise for adding authentication in a react app\nusername_0: In fact I would like to integrate Facebook authentication in my React app, and save user credentials in the database, along with additional data that will be used to authenticate with an API.\r\n\r\nI have no idea about how to handle authentication and authorization in a Facebook app. I went through some few books, but none of them touched that aspect. \r\n\r\nCan you point me to a book, a tutorial, any link ? whatever you think can point me to the right direction.\r\n\r\nThanks in advance.\n<issue_comment>username_1: @username_0  The best technique out there right now is to use Jason Web Token(JWT).\r\nI am sure you can find decent amount of information on that online\n<issue_comment>username_0: @username_1, thanks for your reply. I am fine with the API authentication, I opted for HAWK in HapiJS. So I can include Hawk credentials in my request header, and fetch data from my API.\r\n\r\nThis is what I want to achieve:\r\n\r\n1. User try to access a route in React Router that required Authentication (No idea how to set it)\r\n2. User is redirected to a Login component that allow Facebook login\r\n3. Upon successful login authentication, Facebook credentials are saved in a database, along with hawk credentials, and stored in a cookie\r\n4. User can now authenticate with the API using credentials in his cookie and fetch data.\r\n\r\nFrom what I know about JWT and HAWK, they are only concerned with step 4. My problem is steps 1 to 3.\r\n\r\nIs that possible ? Am I getting something wrong ?\r\n\r\nThanks<issue_closed>","repo":"facebook/react","issue_id":248247023,"issue_number":10395}
{"question":"What is the best technique mentioned in the text?","answer":"The best technique mentioned in the text is using Jason Web Token (JWT) for authentication and authorization purposes in web applications.","id":18,"text":"@username_0  The best technique out there right now is to use Jason Web Token(JWT).\r\nI am sure you can find decent amount of information on that online","context":"<issue_start><issue_comment>Title: (Non-issue) Need advise for adding authentication in a react app\nusername_0: In fact I would like to integrate Facebook authentication in my React app, and save user credentials in the database, along with additional data that will be used to authenticate with an API.\r\n\r\nI have no idea about how to handle authentication and authorization in a Facebook app. I went through some few books, but none of them touched that aspect. \r\n\r\nCan you point me to a book, a tutorial, any link ? whatever you think can point me to the right direction.\r\n\r\nThanks in advance.\n<issue_comment>username_1: @username_0  The best technique out there right now is to use Jason Web Token(JWT).\r\nI am sure you can find decent amount of information on that online\n<issue_comment>username_0: @username_1, thanks for your reply. I am fine with the API authentication, I opted for HAWK in HapiJS. So I can include Hawk credentials in my request header, and fetch data from my API.\r\n\r\nThis is what I want to achieve:\r\n\r\n1. User try to access a route in React Router that required Authentication (No idea how to set it)\r\n2. User is redirected to a Login component that allow Facebook login\r\n3. Upon successful login authentication, Facebook credentials are saved in a database, along with hawk credentials, and stored in a cookie\r\n4. User can now authenticate with the API using credentials in his cookie and fetch data.\r\n\r\nFrom what I know about JWT and HAWK, they are only concerned with step 4. My problem is steps 1 to 3.\r\n\r\nIs that possible ? Am I getting something wrong ?\r\n\r\nThanks<issue_closed>","repo":"facebook/react","issue_id":248247023,"issue_number":10395}
{"question":"What steps are involved in setting up authentication before accessing an API using HAWK credentials?","answer":"Setting up authentication before accessing an API using HAWK credentials involves the following steps:\n1. Redirecting the user to a Login component that allows Facebook login.\n2. Upon successful Facebook login, save both Facebook and HAWK credentials in a database and store them in a cookie.\n3. The stored credentials in the cookie are used for authenticating with the API to fetch data.\nJWT and HAWK primarily handle the authentication process at step 4, while steps 1 to 3 focus on setting up the initial authentication process.","id":19,"text":"@username_1, thanks for your reply. I am fine with the API authentication, I opted for HAWK in HapiJS. So I can include Hawk credentials in my request header, and fetch data from my API.\r\n\r\nThis is what I want to achieve:\r\n\r\n1. User try to access a route in React Router that required Authentication (No idea how to set it)\r\n2. User is redirected to a Login component that allow Facebook login\r\n3. Upon successful login authentication, Facebook credentials are saved in a database, along with hawk credentials, and stored in a cookie\r\n4. User can now authenticate with the API using credentials in his cookie and fetch data.\r\n\r\nFrom what I know about JWT and HAWK, they are only concerned with step 4. My problem is steps 1 to 3.\r\n\r\nIs that possible ? Am I getting something wrong ?\r\n\r\nThanks","context":"<issue_start><issue_comment>Title: (Non-issue) Need advise for adding authentication in a react app\nusername_0: In fact I would like to integrate Facebook authentication in my React app, and save user credentials in the database, along with additional data that will be used to authenticate with an API.\r\n\r\nI have no idea about how to handle authentication and authorization in a Facebook app. I went through some few books, but none of them touched that aspect. \r\n\r\nCan you point me to a book, a tutorial, any link ? whatever you think can point me to the right direction.\r\n\r\nThanks in advance.\n<issue_comment>username_1: @username_0  The best technique out there right now is to use Jason Web Token(JWT).\r\nI am sure you can find decent amount of information on that online\n<issue_comment>username_0: @username_1, thanks for your reply. I am fine with the API authentication, I opted for HAWK in HapiJS. So I can include Hawk credentials in my request header, and fetch data from my API.\r\n\r\nThis is what I want to achieve:\r\n\r\n1. User try to access a route in React Router that required Authentication (No idea how to set it)\r\n2. User is redirected to a Login component that allow Facebook login\r\n3. Upon successful login authentication, Facebook credentials are saved in a database, along with hawk credentials, and stored in a cookie\r\n4. User can now authenticate with the API using credentials in his cookie and fetch data.\r\n\r\nFrom what I know about JWT and HAWK, they are only concerned with step 4. My problem is steps 1 to 3.\r\n\r\nIs that possible ? Am I getting something wrong ?\r\n\r\nThanks<issue_closed>","repo":"facebook/react","issue_id":248247023,"issue_number":10395}
{"question":"Why is a wrapper added around each CUDA test in the provided code snippet?","answer":"A wrapper is added around each CUDA test to check if the CUDA memory usage before and after the test remains constant, in order to test if CUDA methods have memory leaks.","id":20,"text":"With `torch.cuda.memory_allocated` available, we can test if CUDA methods has memory leaks. This PR adds a wrapper around each CUDA tests that checks if the CUDA memory usage before and after stays constant.","context":"<issue_start><issue_comment>Title: Add memory leak check in CUDA tests\nusername_0: With `torch.cuda.memory_allocated` available, we can test if CUDA methods has memory leaks. This PR adds a wrapper around each CUDA tests that checks if the CUDA memory usage before and after stays constant.\n<issue_comment>username_1: this is kinda cool!\n<issue_comment>username_2: Excellent. Have you purposely introduced a memory leak and verified that this test catches it?\n<issue_comment>username_0: @username_2 I added a test for the new method in new commit.\r\n\r\ncc @username_3 for changes to `run_test.py`.\n<issue_comment>username_3: Cool, just make sure the tests actually run in the CI, even when green\n<issue_comment>username_4: Does it have any effect on the perf of the tests?\n<issue_comment>username_0: In local tests and CI test, this new mechanism detected `MaxUnpool3d` (`test_nn.TestNN.test_MaxUnpool3d_net_cuda`) and `MultiLabelMarginCriterion` leaking memory (`test_legacy_nn.TestNN.test_MultiLabelMarginCriterion_1d_cuda`).\n<issue_comment>username_0: @username_4 I don't have concrete numbers on full test suite. But for `test_cuda` on devgpu, the total time went from `20s` to `58s`. The overall test time increase on CUDA tests is likely less than 3 fold because `test_cuda` tests are small and thus the wrapper overhead area more obvious.\r\n\r\nStill, I think there are good reasons to get this in. The above detected leaks are good examples.\r\n\r\nOnce I fix those leaks, I will run the full suite and report the total increased time here.\n<issue_comment>username_2: Not for this PR, but we can do something similar for CPU leaks too; all we need to do is distinguish between tensor allocations (unlikely to stay live across tests) and metadata allocations (we won't track leaks for those.)\r\n\r\nAlthough, when @gchanan has his way all the manual refcounting in TH will go away and it should become a lot harder to make this kind of mistake.\n<issue_comment>username_0: @pytorchbot retest this please\n<issue_comment>username_2: Maybe, to avoid having to fix all of the leaking kernels before we merge this, we should just blacklist all of the currently failing tests and then subsequently fix them.\n<issue_comment>username_0: This PR also fixed the `test_cuda.py` running `TestTorch` problem, so the total test time increase on CUDA 9, CUDNN7, and py3 CI is 27s.\n<issue_comment>username_0: @username_4 @username_1 Require a review! \r\n\r\npr/caffe2-py2-gcc5-ubuntu16.04-test fail is unrelated and is being fixed.\n<issue_comment>username_2: @pytorchbot retest this please\n<issue_comment>username_0: @pytorchbot retest this please\n<issue_comment>username_0: @pytorchbot retest this please\n<issue_comment>username_0: @pytorchbot retest this please\n<issue_comment>username_2: @pytorchbot retest this please\n<issue_comment>username_0: @pytorchbot retest this please","repo":"pytorch/pytorch","issue_id":320115344,"issue_number":7270}
{"question":"What functions were detected to be leaking memory in the local and CI tests?","answer":"`MaxUnpool3d` and `MultiLabelMarginCriterion`","id":26,"text":"In local tests and CI test, this new mechanism detected `MaxUnpool3d` (`test_nn.TestNN.test_MaxUnpool3d_net_cuda`) and `MultiLabelMarginCriterion` leaking memory (`test_legacy_nn.TestNN.test_MultiLabelMarginCriterion_1d_cuda`).","context":"<issue_start><issue_comment>Title: Add memory leak check in CUDA tests\nusername_0: With `torch.cuda.memory_allocated` available, we can test if CUDA methods has memory leaks. This PR adds a wrapper around each CUDA tests that checks if the CUDA memory usage before and after stays constant.\n<issue_comment>username_1: this is kinda cool!\n<issue_comment>username_2: Excellent. Have you purposely introduced a memory leak and verified that this test catches it?\n<issue_comment>username_0: @username_2 I added a test for the new method in new commit.\r\n\r\ncc @username_3 for changes to `run_test.py`.\n<issue_comment>username_3: Cool, just make sure the tests actually run in the CI, even when green\n<issue_comment>username_4: Does it have any effect on the perf of the tests?\n<issue_comment>username_0: In local tests and CI test, this new mechanism detected `MaxUnpool3d` (`test_nn.TestNN.test_MaxUnpool3d_net_cuda`) and `MultiLabelMarginCriterion` leaking memory (`test_legacy_nn.TestNN.test_MultiLabelMarginCriterion_1d_cuda`).\n<issue_comment>username_0: @username_4 I don't have concrete numbers on full test suite. But for `test_cuda` on devgpu, the total time went from `20s` to `58s`. The overall test time increase on CUDA tests is likely less than 3 fold because `test_cuda` tests are small and thus the wrapper overhead area more obvious.\r\n\r\nStill, I think there are good reasons to get this in. The above detected leaks are good examples.\r\n\r\nOnce I fix those leaks, I will run the full suite and report the total increased time here.\n<issue_comment>username_2: Not for this PR, but we can do something similar for CPU leaks too; all we need to do is distinguish between tensor allocations (unlikely to stay live across tests) and metadata allocations (we won't track leaks for those.)\r\n\r\nAlthough, when @gchanan has his way all the manual refcounting in TH will go away and it should become a lot harder to make this kind of mistake.\n<issue_comment>username_0: @pytorchbot retest this please\n<issue_comment>username_2: Maybe, to avoid having to fix all of the leaking kernels before we merge this, we should just blacklist all of the currently failing tests and then subsequently fix them.\n<issue_comment>username_0: This PR also fixed the `test_cuda.py` running `TestTorch` problem, so the total test time increase on CUDA 9, CUDNN7, and py3 CI is 27s.\n<issue_comment>username_0: @username_4 @username_1 Require a review! \r\n\r\npr/caffe2-py2-gcc5-ubuntu16.04-test fail is unrelated and is being fixed.\n<issue_comment>username_2: @pytorchbot retest this please\n<issue_comment>username_0: @pytorchbot retest this please\n<issue_comment>username_0: @pytorchbot retest this please\n<issue_comment>username_0: @pytorchbot retest this please\n<issue_comment>username_2: @pytorchbot retest this please\n<issue_comment>username_0: @pytorchbot retest this please","repo":"pytorch/pytorch","issue_id":320115344,"issue_number":7270}
{"question":"What was the total time for the `test_cuda` on devgpu before and after the increase?","answer":"The total time for the `test_cuda` on devgpu went from 20s to 58s.","id":27,"text":"@username_4 I don't have concrete numbers on full test suite. But for `test_cuda` on devgpu, the total time went from `20s` to `58s`. The overall test time increase on CUDA tests is likely less than 3 fold because `test_cuda` tests are small and thus the wrapper overhead area more obvious.\r\n\r\nStill, I think there are good reasons to get this in. The above detected leaks are good examples.\r\n\r\nOnce I fix those leaks, I will run the full suite and report the total increased time here.","context":"<issue_start><issue_comment>Title: Add memory leak check in CUDA tests\nusername_0: With `torch.cuda.memory_allocated` available, we can test if CUDA methods has memory leaks. This PR adds a wrapper around each CUDA tests that checks if the CUDA memory usage before and after stays constant.\n<issue_comment>username_1: this is kinda cool!\n<issue_comment>username_2: Excellent. Have you purposely introduced a memory leak and verified that this test catches it?\n<issue_comment>username_0: @username_2 I added a test for the new method in new commit.\r\n\r\ncc @username_3 for changes to `run_test.py`.\n<issue_comment>username_3: Cool, just make sure the tests actually run in the CI, even when green\n<issue_comment>username_4: Does it have any effect on the perf of the tests?\n<issue_comment>username_0: In local tests and CI test, this new mechanism detected `MaxUnpool3d` (`test_nn.TestNN.test_MaxUnpool3d_net_cuda`) and `MultiLabelMarginCriterion` leaking memory (`test_legacy_nn.TestNN.test_MultiLabelMarginCriterion_1d_cuda`).\n<issue_comment>username_0: @username_4 I don't have concrete numbers on full test suite. But for `test_cuda` on devgpu, the total time went from `20s` to `58s`. The overall test time increase on CUDA tests is likely less than 3 fold because `test_cuda` tests are small and thus the wrapper overhead area more obvious.\r\n\r\nStill, I think there are good reasons to get this in. The above detected leaks are good examples.\r\n\r\nOnce I fix those leaks, I will run the full suite and report the total increased time here.\n<issue_comment>username_2: Not for this PR, but we can do something similar for CPU leaks too; all we need to do is distinguish between tensor allocations (unlikely to stay live across tests) and metadata allocations (we won't track leaks for those.)\r\n\r\nAlthough, when @gchanan has his way all the manual refcounting in TH will go away and it should become a lot harder to make this kind of mistake.\n<issue_comment>username_0: @pytorchbot retest this please\n<issue_comment>username_2: Maybe, to avoid having to fix all of the leaking kernels before we merge this, we should just blacklist all of the currently failing tests and then subsequently fix them.\n<issue_comment>username_0: This PR also fixed the `test_cuda.py` running `TestTorch` problem, so the total test time increase on CUDA 9, CUDNN7, and py3 CI is 27s.\n<issue_comment>username_0: @username_4 @username_1 Require a review! \r\n\r\npr/caffe2-py2-gcc5-ubuntu16.04-test fail is unrelated and is being fixed.\n<issue_comment>username_2: @pytorchbot retest this please\n<issue_comment>username_0: @pytorchbot retest this please\n<issue_comment>username_0: @pytorchbot retest this please\n<issue_comment>username_0: @pytorchbot retest this please\n<issue_comment>username_2: @pytorchbot retest this please\n<issue_comment>username_0: @pytorchbot retest this please","repo":"pytorch/pytorch","issue_id":320115344,"issue_number":7270}
{"question":"What distinction does the text mention for tracking CPU leaks in terms of tensor and metadata allocations?","answer":"The text mentions distinguishing between tensor allocations (unlikely to stay live across tests) and metadata allocations (for which leaks won't be tracked).","id":28,"text":"Not for this PR, but we can do something similar for CPU leaks too; all we need to do is distinguish between tensor allocations (unlikely to stay live across tests) and metadata allocations (we won't track leaks for those.)\r\n\r\nAlthough, when @gchanan has his way all the manual refcounting in TH will go away and it should become a lot harder to make this kind of mistake.","context":"<issue_start><issue_comment>Title: Add memory leak check in CUDA tests\nusername_0: With `torch.cuda.memory_allocated` available, we can test if CUDA methods has memory leaks. This PR adds a wrapper around each CUDA tests that checks if the CUDA memory usage before and after stays constant.\n<issue_comment>username_1: this is kinda cool!\n<issue_comment>username_2: Excellent. Have you purposely introduced a memory leak and verified that this test catches it?\n<issue_comment>username_0: @username_2 I added a test for the new method in new commit.\r\n\r\ncc @username_3 for changes to `run_test.py`.\n<issue_comment>username_3: Cool, just make sure the tests actually run in the CI, even when green\n<issue_comment>username_4: Does it have any effect on the perf of the tests?\n<issue_comment>username_0: In local tests and CI test, this new mechanism detected `MaxUnpool3d` (`test_nn.TestNN.test_MaxUnpool3d_net_cuda`) and `MultiLabelMarginCriterion` leaking memory (`test_legacy_nn.TestNN.test_MultiLabelMarginCriterion_1d_cuda`).\n<issue_comment>username_0: @username_4 I don't have concrete numbers on full test suite. But for `test_cuda` on devgpu, the total time went from `20s` to `58s`. The overall test time increase on CUDA tests is likely less than 3 fold because `test_cuda` tests are small and thus the wrapper overhead area more obvious.\r\n\r\nStill, I think there are good reasons to get this in. The above detected leaks are good examples.\r\n\r\nOnce I fix those leaks, I will run the full suite and report the total increased time here.\n<issue_comment>username_2: Not for this PR, but we can do something similar for CPU leaks too; all we need to do is distinguish between tensor allocations (unlikely to stay live across tests) and metadata allocations (we won't track leaks for those.)\r\n\r\nAlthough, when @gchanan has his way all the manual refcounting in TH will go away and it should become a lot harder to make this kind of mistake.\n<issue_comment>username_0: @pytorchbot retest this please\n<issue_comment>username_2: Maybe, to avoid having to fix all of the leaking kernels before we merge this, we should just blacklist all of the currently failing tests and then subsequently fix them.\n<issue_comment>username_0: This PR also fixed the `test_cuda.py` running `TestTorch` problem, so the total test time increase on CUDA 9, CUDNN7, and py3 CI is 27s.\n<issue_comment>username_0: @username_4 @username_1 Require a review! \r\n\r\npr/caffe2-py2-gcc5-ubuntu16.04-test fail is unrelated and is being fixed.\n<issue_comment>username_2: @pytorchbot retest this please\n<issue_comment>username_0: @pytorchbot retest this please\n<issue_comment>username_0: @pytorchbot retest this please\n<issue_comment>username_0: @pytorchbot retest this please\n<issue_comment>username_2: @pytorchbot retest this please\n<issue_comment>username_0: @pytorchbot retest this please","repo":"pytorch/pytorch","issue_id":320115344,"issue_number":7270}
{"question":"What strategy is proposed to address the leaking kernels issue in the text?","answer":"Blacklisting all currently failing tests and fixing them subsequently.","id":30,"text":"Maybe, to avoid having to fix all of the leaking kernels before we merge this, we should just blacklist all of the currently failing tests and then subsequently fix them.","context":"<issue_start><issue_comment>Title: Add memory leak check in CUDA tests\nusername_0: With `torch.cuda.memory_allocated` available, we can test if CUDA methods has memory leaks. This PR adds a wrapper around each CUDA tests that checks if the CUDA memory usage before and after stays constant.\n<issue_comment>username_1: this is kinda cool!\n<issue_comment>username_2: Excellent. Have you purposely introduced a memory leak and verified that this test catches it?\n<issue_comment>username_0: @username_2 I added a test for the new method in new commit.\r\n\r\ncc @username_3 for changes to `run_test.py`.\n<issue_comment>username_3: Cool, just make sure the tests actually run in the CI, even when green\n<issue_comment>username_4: Does it have any effect on the perf of the tests?\n<issue_comment>username_0: In local tests and CI test, this new mechanism detected `MaxUnpool3d` (`test_nn.TestNN.test_MaxUnpool3d_net_cuda`) and `MultiLabelMarginCriterion` leaking memory (`test_legacy_nn.TestNN.test_MultiLabelMarginCriterion_1d_cuda`).\n<issue_comment>username_0: @username_4 I don't have concrete numbers on full test suite. But for `test_cuda` on devgpu, the total time went from `20s` to `58s`. The overall test time increase on CUDA tests is likely less than 3 fold because `test_cuda` tests are small and thus the wrapper overhead area more obvious.\r\n\r\nStill, I think there are good reasons to get this in. The above detected leaks are good examples.\r\n\r\nOnce I fix those leaks, I will run the full suite and report the total increased time here.\n<issue_comment>username_2: Not for this PR, but we can do something similar for CPU leaks too; all we need to do is distinguish between tensor allocations (unlikely to stay live across tests) and metadata allocations (we won't track leaks for those.)\r\n\r\nAlthough, when @gchanan has his way all the manual refcounting in TH will go away and it should become a lot harder to make this kind of mistake.\n<issue_comment>username_0: @pytorchbot retest this please\n<issue_comment>username_2: Maybe, to avoid having to fix all of the leaking kernels before we merge this, we should just blacklist all of the currently failing tests and then subsequently fix them.\n<issue_comment>username_0: This PR also fixed the `test_cuda.py` running `TestTorch` problem, so the total test time increase on CUDA 9, CUDNN7, and py3 CI is 27s.\n<issue_comment>username_0: @username_4 @username_1 Require a review! \r\n\r\npr/caffe2-py2-gcc5-ubuntu16.04-test fail is unrelated and is being fixed.\n<issue_comment>username_2: @pytorchbot retest this please\n<issue_comment>username_0: @pytorchbot retest this please\n<issue_comment>username_0: @pytorchbot retest this please\n<issue_comment>username_0: @pytorchbot retest this please\n<issue_comment>username_2: @pytorchbot retest this please\n<issue_comment>username_0: @pytorchbot retest this please","repo":"pytorch/pytorch","issue_id":320115344,"issue_number":7270}
{"question":"Why is it mentioned that duplication is intentional in this context?","answer":"Duplication is intentional to avoid a dependency on a package from `react` and prevent accidental inlining of code during bundling.","id":55,"text":"Thanks, but we don't want a dependency on this package from `react`, and we also don't want to accidentally inline some code from it during bundling. The duplication is intentional.","context":"<issue_start><issue_comment>Title: removed duplicate code\nusername_0: use `isElement` from `react-is` inside `isValidElement`\n<issue_comment>username_1: Thanks, but we don't want a dependency on this package from `react`, and we also don't want to accidentally inline some code from it during bundling. The duplication is intentional.","repo":"facebook/react","issue_id":650765838,"issue_number":19253}
{"question":"What is the difference in console log order between Chrome and Safari when running the provided React code snippet?","answer":"The difference in console log order between Chrome and Safari is as follows: In Chrome, the order is `useEffect in child` followed by `timeout in render`, while in Safari, the order is `timeout in render` followed by `useEffect in child`.","id":56,"text":"In chrome and safari, the ```console.log``` was triggered in different order. [Example](https://codesandbox.io/s/amazing-payne-4lmxo) will show different result if opened in different browsers.\r\n\r\nReact version: 16.13.1\r\n\r\n## Steps To Reproduce\r\n\r\n1. Run following code in chrome and safari.\r\n2. Check console log\r\n\r\n```jsx\r\nimport React, { useState, useEffect } from \"react\";\r\n\r\nconst Test = props => {\r\n  useEffect(() => {\r\n    console.log(\"useEffect in child\");\r\n  }, []);\r\n\r\n  return \"Test useEffect\";\r\n};\r\n\r\nconst App = () => {\r\n  setTimeout(() => {\r\n    console.log(\"timeout in render\");\r\n  }, 0);\r\n\r\n  return <Test />;\r\n};\r\n\r\nexport default App\r\n```\r\n\r\nLink to code example:\r\n\r\n[Example](https://codesandbox.io/s/amazing-payne-4lmxo)\r\n\r\n## The current behavior\r\n\r\nThe message is not the same order;\r\n\r\n```jsx\r\n/**\r\n * chrome result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n\r\n/**\r\n * safari result\r\n *\r\n * timeout in render\r\n * useEffect in child\r\n */\r\n```\r\n\r\n## The expected behavior\r\n\r\nThe message should be same;\r\n\r\n```jsx\r\n/**\r\n * chrome result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n\r\n/**\r\n * safari result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n```","context":"<issue_start><issue_comment>Title: Bug: The trigger timing of \"useEffect\" is different in chrome and safari.\nusername_0: In chrome and safari, the ```console.log``` was triggered in different order. [Example](https://codesandbox.io/s/amazing-payne-4lmxo) will show different result if opened in different browsers.\r\n\r\nReact version: 16.13.1\r\n\r\n## Steps To Reproduce\r\n\r\n1. Run following code in chrome and safari.\r\n2. Check console log\r\n\r\n```jsx\r\nimport React, { useState, useEffect } from \"react\";\r\n\r\nconst Test = props => {\r\n  useEffect(() => {\r\n    console.log(\"useEffect in child\");\r\n  }, []);\r\n\r\n  return \"Test useEffect\";\r\n};\r\n\r\nconst App = () => {\r\n  setTimeout(() => {\r\n    console.log(\"timeout in render\");\r\n  }, 0);\r\n\r\n  return <Test />;\r\n};\r\n\r\nexport default App\r\n```\r\n\r\nLink to code example:\r\n\r\n[Example](https://codesandbox.io/s/amazing-payne-4lmxo)\r\n\r\n## The current behavior\r\n\r\nThe message is not the same order;\r\n\r\n```jsx\r\n/**\r\n * chrome result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n\r\n/**\r\n * safari result\r\n *\r\n * timeout in render\r\n * useEffect in child\r\n */\r\n```\r\n\r\n## The expected behavior\r\n\r\nThe message should be same;\r\n\r\n```jsx\r\n/**\r\n * chrome result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n\r\n/**\r\n * safari result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n```<issue_closed>\n<issue_comment>username_0: In chrome and safari, the ```console.log``` was triggered in different order. [Example](https://codesandbox.io/s/amazing-payne-4lmxo) will show different result if opened in different browsers.\r\n\r\nReact version: 16.13.1\r\n\r\n## Steps To Reproduce\r\n\r\n1. Run following code in chrome and safari.\r\n2. Check console log\r\n\r\n```jsx\r\nimport React, { useState, useEffect } from \"react\";\r\n\r\nconst Test = props => {\r\n  useEffect(() => {\r\n    console.log(\"useEffect in child\");\r\n  }, []);\r\n\r\n  return \"Test useEffect\";\r\n};\r\n\r\nconst App = () => {\r\n  setTimeout(() => {\r\n    console.log(\"timeout in render\");\r\n  }, 0);\r\n\r\n  return <Test />;\r\n};\r\n\r\nexport default App\r\n```\r\n\r\nLink to code example:\r\n\r\n[Example](https://codesandbox.io/s/amazing-payne-4lmxo)\r\n\r\n## The current behavior\r\n\r\nThe message is not the same order;\r\n\r\n```jsx\r\n/**\r\n * chrome result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n\r\n/**\r\n * safari result\r\n *\r\n * timeout in render\r\n * useEffect in child\r\n */\r\n```\r\n\r\n## The expected behavior\r\n\r\nThe message should be same;\r\n\r\n```jsx\r\n/**\r\n * chrome result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n\r\n/**\r\n * safari result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n```\n<issue_comment>username_1: You do the side effect in the render method:\r\n```typescript\r\nconst App = () => {\r\n  setTimeout(() => {\r\n    console.log(\"timeout in render\");\r\n  }, 0);\r\n\r\n  return <Test />;\r\n};\r\n```\r\nwhich is not allowed. React doesn't guarantee this kind of orders. you should put `setTimeout` inside an useEffect.\n<issue_comment>username_0: Yes, i agree this is not allowed. But different browsers should have same result.I think react should achieve lifecycle with more stable browser interfaces.\n<issue_comment>username_2: Many reasons why this is not a bug:\r\n\r\n - you are not allowed to make side effects in render. If you do that, the outcome is essentially \"undefined behaviour\", that is, nothing is guaranteed\r\n\r\n- React provides no guarantees about timing of `useEffect` with respect to render anyway.\r\n\r\nSo while this is an inconsistency and it could be fixed, there is not good reason to do so.\n<issue_comment>username_3: It is not supported to call `setTimeout` during rendering. React provides absolutely no guarantees about the order or the number of component render calls  so you can't rely on subtle timing aspects of them.\r\n\r\nIf I wrap it in `useEffect` I don't observe the difference.<issue_closed>","repo":"facebook/react","issue_id":664173077,"issue_number":19438}
{"question":"How can you check if a specific code passes Continuous Integration (CI) for Pybind11?","answer":"You can check if a specific code passes Continuous Integration (CI) for Pybind11 by running automated CI tests on the code.","id":39,"text":"See https://pybind11.readthedocs.io/en/stable/faq.html#someclass-declared-with-greater-visibility-than-the-type-of-its-field-someclass-member-wattributes\r\n\r\nLet's see whether it passes CI","context":"<issue_start><issue_comment>Title: Add hidden visibility to _C.so\nusername_0: See https://pybind11.readthedocs.io/en/stable/faq.html#someclass-declared-with-greater-visibility-than-the-type-of-its-field-someclass-member-wattributes\r\n\r\nLet's see whether it passes CI\n<issue_comment>username_1: wouldn't C++ extensions fail because of this? they require the symbols coming out of libtorch_python and stuff to be global visibility in the current process, or else they fail to load at runtime (but the C++ extensions will compile fine)\n<issue_comment>username_2: UBSAN failure looks real:\r\n\r\n```\r\nJun 21 16:45:39 /var/lib/jenkins/workspace/build/aten/src/ATen/Functions.h:2779:12: runtime error: call to function at::TypeDefault::randn(c10::ArrayRef<long>, c10::TensorOptions const&) through pointer to incorrect function type 'at::Tensor (*)(c10::ArrayRef<long>, const c10::TensorOptions &)'\r\nJun 21 16:45:39 (/opt/conda/lib/python3.6/site-packages/torch/lib/libtorch.so+0xa6b7860): note: at::TypeDefault::randn(c10::ArrayRef<long>, c10::TensorOptions const&) defined here\r\nJun 21 16:45:40     #0 0x7f25f2696882 in torch::randn(c10::ArrayRef<long>, c10::TensorOptions const&) (/opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_python.so+0x11aa882)\r\nJun 21 16:45:40     #1 0x7f25f2694ccc in torch::autograd::dispatch_randn(c10::ArrayRef<long>, c10::TensorOptions const&) (/opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_python.so+0x11a8ccc)\r\nJun 21 16:45:40     #2 0x7f25f258d08d in torch::autograd::THPVariable_randn(_object*, _object*, _object*) (/opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_python.so+0x10a108d)\r\nJun 21 16:45:40     #3 0x558025dc7743 in _PyCFunction_FastCallDict /tmp/build/80754af9/python_1546130271559/work/Objects/methodobject.c:231\r\nJun 21 16:45:40     #4 0x558025e4e42b in call_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4851\r\nJun 21 16:45:40     #5 0x558025e73389 in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3335\r\nJun 21 16:45:40     #6 0x558025e47bfd in _PyEval_EvalCodeWithName /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4166\r\nJun 21 16:45:40     #7 0x558025e48770 in fast_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4992\r\nJun 21 16:45:40     #8 0x558025e4e504 in call_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4872\r\nJun 21 16:45:40     #9 0x558025e73389 in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3335\r\nJun 21 16:45:40     #10 0x558025e49288 in _PyEval_EvalCodeWithName /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4166\r\nJun 21 16:45:40     #11 0x558025e49288 in PyEval_EvalCodeEx /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4187\r\nJun 21 16:45:40     #12 0x558025e4a01b in PyEval_EvalCode /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:731\r\nJun 21 16:45:40     #13 0x558025e70c8a in builtin_exec_impl.isra.11 /tmp/build/80754af9/python_1546130271559/work/Python/bltinmodule.c:983\r\nJun 21 16:45:40     #14 0x558025e70c8a in builtin_exec /tmp/build/80754af9/python_1546130271559/work/Python/clinic/bltinmodule.c.h:283\r\nJun 21 16:45:40     #15 0x558025dca710 in PyCFunction_Call /tmp/build/80754af9/python_1546130271559/work/Objects/methodobject.c:114\r\nJun 21 16:45:40     #16 0x558025e784ac in do_call_core /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:5116\r\nJun 21 16:45:40     #17 0x558025e784ac in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3404\r\nJun 21 16:45:40     #18 0x558025e478e3 in _PyEval_EvalCodeWithName /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4166\r\nJun 21 16:45:40     #19 0x558025e48770 in fast_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4992\r\nJun 21 16:45:40     #20 0x558025e4e504 in call_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4872\r\nJun 21 16:45:40     #21 0x558025e73389 in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3335\r\nJun 21 16:45:40     #22 0x558025e4853a in _PyFunction_FastCall /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4933\r\nJun 21 16:45:40     #23 0x558025e4853a in fast_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4968\r\nJun 21 16:45:40     #24 0x558025e4e504 in call_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4872\r\nJun 21 16:45:40     #25 0x558025e73389 in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3335\r\nJun 21 16:45:40     #26 0x558025e4853a in _PyFunction_FastCall /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4933\r\nJun 21 16:45:40     #27 0x558025e4853a in fast_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4968\r\nJun 21 16:45:40     #28 0x558025e4e504 in call_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4872\r\nJun 21 16:45:40     #29 0x558025e73389 in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3335\r\nJun 21 16:45:40     #30 0x558025e4853a in _PyFunction_FastCall /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4933\r\nJun 21 16:45:40     #31 0x558025e4853a in fast_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4968\r\nJun 21 16:45:40     #32 0x558025e4e504 in call_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4872\r\nJun 21 16:45:40     #33 0x558025e73389 in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3335\r\nJun 21 16:45:40     #34 0x558025e48baa in _PyFunction_FastCall /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4933\r\nJun 21 16:45:40     #35 0x558025e48baa in _PyFunction_FastCallDict /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:5035\r\nJun 21 16:45:40     #36 0x558025dc7b0e in _PyObject_FastCallDict /tmp/build/80754af9/python_1546130271559/work/Objects/abstract.c:2310\r\nJun 21 16:45:40     #37 0x558025e0980f in _PyObject_CallMethodIdObjArgs /tmp/build/80754af9/python_1546130271559/work/Objects/abstract.c:2796\r\nJun 21 16:45:40     #38 0x558025dbeb0f in PyImport_ImportModuleLevelObject /tmp/build/80754af9/python_1546130271559/work/Python/import.c:1578\r\nJun 21 16:45:40     #39 0x558025e75a8a in import_name /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:5245\r\nJun 21 16:45:40     #40 0x558025e75a8a in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:2899\r\nJun 21 16:45:40     #41 0x558025e49288 in _PyEval_EvalCodeWithName /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4166\r\nJun 21 16:45:40     #42 0x558025e49288 in PyEval_EvalCodeEx /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4187\r\nJun 21 16:45:40     #43 0x558025e4a01b in PyEval_EvalCode /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:731\r\nJun 21 16:45:40     #44 0x558025ecc3c3 in run_mod /tmp/build/80754af9/python_1546130271559/work/Python/pythonrun.c:1025\r\nJun 21 16:45:40     #45 0x558025ecc7c0 in PyRun_FileExFlags /tmp/build/80754af9/python_1546130271559/work/Python/pythonrun.c:978\r\nJun 21 16:45:40     #46 0x558025ecc9c2 in PyRun_SimpleFileExFlags /tmp/build/80754af9/python_1546130271559/work/Python/pythonrun.c:419\r\nJun 21 16:45:40     #47 0x558025ed04b2 in run_file /tmp/build/80754af9/python_1546130271559/work/Modules/main.c:340\r\nJun 21 16:45:40     #48 0x558025ed04b2 in Py_Main /tmp/build/80754af9/python_1546130271559/work/Modules/main.c:811\r\nJun 21 16:45:40     #49 0x558025d9902d in main /tmp/build/80754af9/python_1546130271559/work/Programs/python.c:69\r\nJun 21 16:45:40     #50 0x7f26053c082f in __libc_start_main /build/glibc-LK5gWL/glibc-2.23/csu/../csu/libc-start.c:291\r\nJun 21 16:45:40     #51 0x558025e79e0d in _start /home/rdonnelly/mc/conda-bld/compilers_linux-64_1534865402226/work/.build/src/glibc-2.12.2/csu/../sysdeps/x86_64/elf/start.S:103\r\nJun 21 16:45:40 \r\nJun 21 16:45:40 SUMMARY: UndefinedBehaviorSanitizer: undefined-behavior /var/lib/jenkins/workspace/build/aten/src/ATen/Functions.h:2779:12 in \r\nJun 21 16:45:40 Traceback (most recent call last):\r\n```\n<issue_comment>username_2: I guess, ideally, we'd expose symbols that our ours, but not expose anything from pybind11. I don't know how easy that is to do. (Maybe this is a reason not to use pybind11, but that ship has kind of sailed...)\n<issue_comment>username_3: If everything works on MSVC with its default option -- that is, no exposure unless explicitly asked to -- could replacing all occurrences of `__declspec(dllexport)` with something also compatible with `gcc` work? Like something similar to [this](https://stackoverflow.com/a/8258786/1150462)?\n<issue_comment>username_0: Yes, I realized that it needs adjusting annotations.\r\n\r\nAs @username_3 pointed out, we should have already annotated all the right things to make Windows work. But we use windows-only macro in THP_API/THP_CLASS (https://github.com/pytorch/pytorch/blob/master/torch/csrc/THP_export.h). We should do the same thing as for other places - https://github.com/pytorch/pytorch/blob/master/c10/macros/Export.h\r\n\r\nI'll try to get back to it at some point.","repo":"pytorch/pytorch","issue_id":459270796,"issue_number":22076}
{"question":"Why would C++ extensions fail if symbols from libtorch_python are not globally visible in the current process?","answer":"C++ extensions fail to load at runtime if symbols from libtorch_python are not globally visible in the current process because the extensions require these symbols to be accessible for proper execution.","id":40,"text":"wouldn't C++ extensions fail because of this? they require the symbols coming out of libtorch_python and stuff to be global visibility in the current process, or else they fail to load at runtime (but the C++ extensions will compile fine)","context":"<issue_start><issue_comment>Title: Add hidden visibility to _C.so\nusername_0: See https://pybind11.readthedocs.io/en/stable/faq.html#someclass-declared-with-greater-visibility-than-the-type-of-its-field-someclass-member-wattributes\r\n\r\nLet's see whether it passes CI\n<issue_comment>username_1: wouldn't C++ extensions fail because of this? they require the symbols coming out of libtorch_python and stuff to be global visibility in the current process, or else they fail to load at runtime (but the C++ extensions will compile fine)\n<issue_comment>username_2: UBSAN failure looks real:\r\n\r\n```\r\nJun 21 16:45:39 /var/lib/jenkins/workspace/build/aten/src/ATen/Functions.h:2779:12: runtime error: call to function at::TypeDefault::randn(c10::ArrayRef<long>, c10::TensorOptions const&) through pointer to incorrect function type 'at::Tensor (*)(c10::ArrayRef<long>, const c10::TensorOptions &)'\r\nJun 21 16:45:39 (/opt/conda/lib/python3.6/site-packages/torch/lib/libtorch.so+0xa6b7860): note: at::TypeDefault::randn(c10::ArrayRef<long>, c10::TensorOptions const&) defined here\r\nJun 21 16:45:40     #0 0x7f25f2696882 in torch::randn(c10::ArrayRef<long>, c10::TensorOptions const&) (/opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_python.so+0x11aa882)\r\nJun 21 16:45:40     #1 0x7f25f2694ccc in torch::autograd::dispatch_randn(c10::ArrayRef<long>, c10::TensorOptions const&) (/opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_python.so+0x11a8ccc)\r\nJun 21 16:45:40     #2 0x7f25f258d08d in torch::autograd::THPVariable_randn(_object*, _object*, _object*) (/opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_python.so+0x10a108d)\r\nJun 21 16:45:40     #3 0x558025dc7743 in _PyCFunction_FastCallDict /tmp/build/80754af9/python_1546130271559/work/Objects/methodobject.c:231\r\nJun 21 16:45:40     #4 0x558025e4e42b in call_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4851\r\nJun 21 16:45:40     #5 0x558025e73389 in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3335\r\nJun 21 16:45:40     #6 0x558025e47bfd in _PyEval_EvalCodeWithName /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4166\r\nJun 21 16:45:40     #7 0x558025e48770 in fast_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4992\r\nJun 21 16:45:40     #8 0x558025e4e504 in call_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4872\r\nJun 21 16:45:40     #9 0x558025e73389 in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3335\r\nJun 21 16:45:40     #10 0x558025e49288 in _PyEval_EvalCodeWithName /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4166\r\nJun 21 16:45:40     #11 0x558025e49288 in PyEval_EvalCodeEx /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4187\r\nJun 21 16:45:40     #12 0x558025e4a01b in PyEval_EvalCode /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:731\r\nJun 21 16:45:40     #13 0x558025e70c8a in builtin_exec_impl.isra.11 /tmp/build/80754af9/python_1546130271559/work/Python/bltinmodule.c:983\r\nJun 21 16:45:40     #14 0x558025e70c8a in builtin_exec /tmp/build/80754af9/python_1546130271559/work/Python/clinic/bltinmodule.c.h:283\r\nJun 21 16:45:40     #15 0x558025dca710 in PyCFunction_Call /tmp/build/80754af9/python_1546130271559/work/Objects/methodobject.c:114\r\nJun 21 16:45:40     #16 0x558025e784ac in do_call_core /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:5116\r\nJun 21 16:45:40     #17 0x558025e784ac in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3404\r\nJun 21 16:45:40     #18 0x558025e478e3 in _PyEval_EvalCodeWithName /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4166\r\nJun 21 16:45:40     #19 0x558025e48770 in fast_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4992\r\nJun 21 16:45:40     #20 0x558025e4e504 in call_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4872\r\nJun 21 16:45:40     #21 0x558025e73389 in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3335\r\nJun 21 16:45:40     #22 0x558025e4853a in _PyFunction_FastCall /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4933\r\nJun 21 16:45:40     #23 0x558025e4853a in fast_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4968\r\nJun 21 16:45:40     #24 0x558025e4e504 in call_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4872\r\nJun 21 16:45:40     #25 0x558025e73389 in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3335\r\nJun 21 16:45:40     #26 0x558025e4853a in _PyFunction_FastCall /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4933\r\nJun 21 16:45:40     #27 0x558025e4853a in fast_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4968\r\nJun 21 16:45:40     #28 0x558025e4e504 in call_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4872\r\nJun 21 16:45:40     #29 0x558025e73389 in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3335\r\nJun 21 16:45:40     #30 0x558025e4853a in _PyFunction_FastCall /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4933\r\nJun 21 16:45:40     #31 0x558025e4853a in fast_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4968\r\nJun 21 16:45:40     #32 0x558025e4e504 in call_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4872\r\nJun 21 16:45:40     #33 0x558025e73389 in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3335\r\nJun 21 16:45:40     #34 0x558025e48baa in _PyFunction_FastCall /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4933\r\nJun 21 16:45:40     #35 0x558025e48baa in _PyFunction_FastCallDict /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:5035\r\nJun 21 16:45:40     #36 0x558025dc7b0e in _PyObject_FastCallDict /tmp/build/80754af9/python_1546130271559/work/Objects/abstract.c:2310\r\nJun 21 16:45:40     #37 0x558025e0980f in _PyObject_CallMethodIdObjArgs /tmp/build/80754af9/python_1546130271559/work/Objects/abstract.c:2796\r\nJun 21 16:45:40     #38 0x558025dbeb0f in PyImport_ImportModuleLevelObject /tmp/build/80754af9/python_1546130271559/work/Python/import.c:1578\r\nJun 21 16:45:40     #39 0x558025e75a8a in import_name /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:5245\r\nJun 21 16:45:40     #40 0x558025e75a8a in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:2899\r\nJun 21 16:45:40     #41 0x558025e49288 in _PyEval_EvalCodeWithName /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4166\r\nJun 21 16:45:40     #42 0x558025e49288 in PyEval_EvalCodeEx /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4187\r\nJun 21 16:45:40     #43 0x558025e4a01b in PyEval_EvalCode /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:731\r\nJun 21 16:45:40     #44 0x558025ecc3c3 in run_mod /tmp/build/80754af9/python_1546130271559/work/Python/pythonrun.c:1025\r\nJun 21 16:45:40     #45 0x558025ecc7c0 in PyRun_FileExFlags /tmp/build/80754af9/python_1546130271559/work/Python/pythonrun.c:978\r\nJun 21 16:45:40     #46 0x558025ecc9c2 in PyRun_SimpleFileExFlags /tmp/build/80754af9/python_1546130271559/work/Python/pythonrun.c:419\r\nJun 21 16:45:40     #47 0x558025ed04b2 in run_file /tmp/build/80754af9/python_1546130271559/work/Modules/main.c:340\r\nJun 21 16:45:40     #48 0x558025ed04b2 in Py_Main /tmp/build/80754af9/python_1546130271559/work/Modules/main.c:811\r\nJun 21 16:45:40     #49 0x558025d9902d in main /tmp/build/80754af9/python_1546130271559/work/Programs/python.c:69\r\nJun 21 16:45:40     #50 0x7f26053c082f in __libc_start_main /build/glibc-LK5gWL/glibc-2.23/csu/../csu/libc-start.c:291\r\nJun 21 16:45:40     #51 0x558025e79e0d in _start /home/rdonnelly/mc/conda-bld/compilers_linux-64_1534865402226/work/.build/src/glibc-2.12.2/csu/../sysdeps/x86_64/elf/start.S:103\r\nJun 21 16:45:40 \r\nJun 21 16:45:40 SUMMARY: UndefinedBehaviorSanitizer: undefined-behavior /var/lib/jenkins/workspace/build/aten/src/ATen/Functions.h:2779:12 in \r\nJun 21 16:45:40 Traceback (most recent call last):\r\n```\n<issue_comment>username_2: I guess, ideally, we'd expose symbols that our ours, but not expose anything from pybind11. I don't know how easy that is to do. (Maybe this is a reason not to use pybind11, but that ship has kind of sailed...)\n<issue_comment>username_3: If everything works on MSVC with its default option -- that is, no exposure unless explicitly asked to -- could replacing all occurrences of `__declspec(dllexport)` with something also compatible with `gcc` work? Like something similar to [this](https://stackoverflow.com/a/8258786/1150462)?\n<issue_comment>username_0: Yes, I realized that it needs adjusting annotations.\r\n\r\nAs @username_3 pointed out, we should have already annotated all the right things to make Windows work. But we use windows-only macro in THP_API/THP_CLASS (https://github.com/pytorch/pytorch/blob/master/torch/csrc/THP_export.h). We should do the same thing as for other places - https://github.com/pytorch/pytorch/blob/master/c10/macros/Export.h\r\n\r\nI'll try to get back to it at some point.","repo":"pytorch/pytorch","issue_id":459270796,"issue_number":22076}
{"question":"What should be considered when evaluating performance in JIT/TorchScript according to the provided text?","answer":"Compilation time and runtime optimization warmup should be allowed for the input to account for initial specialization and optimization based on the input, leading to potential performance improvements.","id":178,"text":"@username_0 Also, it seems like you are only timing it in 3 iterations, and the perf numbers seems including the compilation time. The first time JIT/TorchScript sees an input it will specialize and do optimizations against that input, so when comparing performance please allow a compilation time and runtime optimization warmup for the input.","context":"<issue_start><issue_comment>Title: TorchScript Poor CUDA Performance \nusername_0: The performance of a TorchScript that handles the computation in the [Adam Optimizer](https://github.com/pytorch/pytorch/blob/master/torch/optim/adam.py) is worse than using the native PyTorch operations on CUDA. \r\n\r\n### Background\r\nI am trying to improve the GPU performance of the [Adam Optimizer](https://github.com/pytorch/pytorch/blob/master/torch/optim/adam.py) since it is the bottleneck in one of our workflows. \r\n\r\n### Results\r\nI have written a standalone script that compares PyTorch native tensor operations, TorchScript, and Numba (three runs each):\r\n```\r\nnative :  0.0506\r\nnative :  0.0322\r\nnative :  0.0322\r\nTScript:  0.1917\r\nTScript:  0.0442\r\nTScript:  0.0442\r\nnumba  :  0.1638\r\nnumba  :  0.0145\r\nnumba  :  0.0143\r\n```\r\nI suspect the problem is that TorchScript does not fuse all operations into a single CUDA. It generates two CUDA kernels (see the `PYTORCH_FUSION_DEBUG` output below)\r\n\r\n### Standalone Script\r\n```python\r\nimport math\r\nimport torch\r\nimport time\r\nimport random\r\nimport sys\r\n\r\ndef kernel_native(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n    exp_avg.mul_(beta1).add_(1 - beta1, grad)\r\n    exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\r\n    denom = exp_avg_sq.sqrt().add_(eps)\r\n    param.addcdiv_(-step_size, exp_avg, denom)   \r\n\r\n@torch.jit.script\r\ndef kernel_jit_script(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n    exp_avg.mul_(beta1)\r\n    exp_avg.add_((1 - beta1) + grad)\r\n    exp_avg_sq.mul_(beta2)\r\n    exp_avg_sq.add_((1 - beta2)*(grad*grad))\r\n    denom = exp_avg_sq.sqrt().add_(eps)\r\n    param += -step_size * (exp_avg / denom)\r\n\r\nfrom numba import cuda\r\n@cuda.jit\r\ndef kernel_numba_cuda(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    i = cuda.grid(1)\r\n    if i > grad.size:\r\n        return\r\n\r\n    exp_avg[i] = exp_avg[i] * beta1 + (1 - beta1) + grad[i]\r\n    exp_avg_sq[i] = exp_avg_sq[i] * beta2 + (1 - beta2) *grad[i]*grad[i]\r\n    denom = math.sqrt(exp_avg_sq[i]) + eps\r\n    param[i] += -step_size * (exp_avg[i] / denom)\r\n\r\ndef kernel_numba(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n\r\n    import ctypes\r\n    import numpy as np\r\n    def get_devicendarray(t):\r\n        assert t.type() == 'torch.cuda.FloatTensor'\r\n        numpy_dtype = np.float32\r\n        itemsize = 4\r\n\r\n        ctx = cuda.cudadrv.driver.driver.get_context()\r\n        mp = cuda.cudadrv.driver.MemoryPointer(ctx, ctypes.c_ulong(t.data_ptr()), t.numel()*itemsize)\r\n        return cuda.cudadrv.devicearray.DeviceNDArray(t.numel(), itemsize, numpy_dtype(), gpu_data=mp, stream=torch.cuda.current_stream().cuda_stream)\r\n\r\n    numba_param = get_devicendarray(param)\r\n    numba_grad = get_devicendarray(grad)\r\n    numba_exp_avg = get_devicendarray(exp_avg)\r\n    numba_exp_avg_sq = get_devicendarray(exp_avg_sq)\r\n    threadsperblock = 32\r\n    blockspergrid = (param.numel() + (threadsperblock - 1)) // threadsperblock\r\n    kernel_numba_cuda[blockspergrid, threadsperblock](numba_param, numba_grad, numba_exp_avg, numba_exp_avg_sq, beta1, beta2, step_size, eps)\r\n\r\nkernels = [(\"native \", kernel_native), (\"TScript\", kernel_jit_script), (\"numba  \", kernel_numba)]\r\n[Truncated]\n      size_t t1_dimIndex0 = t1_linearIndex ;\r\n      t1_offset += t1_dimIndex0 ;\r\n      IndexType t3_offset = 0;\r\n      IndexType t3_linearIndex = linearIndex;\r\n      \r\n      //printf(\"tensor t3 sizes[0] = %d, strides[0] = %d\\n\", t3.sizes[0],t3.strides[0]);\r\n      size_t t3_dimIndex0 = t3_linearIndex ;\r\n      t3_offset += t3_dimIndex0 ;\r\n      \r\n      // calculate the results\r\n      float n0 = __ldg(&t0.data[t0_offset]);\r\n      float n1 = __ldg(&t1.data[t1_offset]);\r\n      double n2 = s2;\r\n      float n3 = n0 / n1;\r\n      float n4 = n3 * ((float) n2);\r\n      t3.data[t3_offset] = n4;\r\n      \r\n    }\r\n}\r\n```\n<issue_comment>username_1: @username_0 can you try removing all the in-place operations in your scripted version?\r\nIIRC in-place ops block fusion.\r\n\r\nI was btw able to fuse most of sparse adam by removing those in-place ops, see https://github.com/pytorch/pytorch/pull/23522#issuecomment-516952462\n<issue_comment>username_2: cc: @username_3 and also relevant WIP PR: [JIT version of Adagrad](https://github.com/pytorch/pytorch/pull/23640)\n<issue_comment>username_3: Yeah the CUDA fusion does not support fusing in-place operations yet, Can you try the suggestion by @username_1 to see if you get a better perf?\n<issue_comment>username_3: @username_0 Also, it seems like you are only timing it in 3 iterations, and the perf numbers seems including the compilation time. The first time JIT/TorchScript sees an input it will specialize and do optimizations against that input, so when comparing performance please allow a compilation time and runtime optimization warmup for the input.\n<issue_comment>username_4: @username_3 That made a big difference to my traced module too. Any other tricks like this? It slows down the non-JIT version, however.\n<issue_comment>username_3: @username_4 That's pretty much all, let me summarize:\r\n\r\n1. JIT fusion does not support in-place operation fusion and reduction operation yet.\r\n2. we did the fusion code generation at the first time we see an input that the shape we haven't seen before (like if the number of dimensions are different), this will help us generate faster cuda kernels. \r\n\r\nSo the TorchScript compilation time will be gone after compile and the first run :)\n<issue_comment>username_0: Thanks for the feedback!\r\n\r\nI have removed all in-place operations and added three kernel implementations:\r\n```python\r\n\r\nimport math\r\nimport torch\r\nimport time\r\nimport random\r\nimport sys\r\n\r\ndef kernel_native(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n    exp_avg.mul_(beta1).add_(1 - beta1, grad)\r\n    exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\r\n    denom = exp_avg_sq.sqrt().add_(eps)\r\n    param.addcdiv_(-step_size, exp_avg, denom)   \r\n\r\n@torch.jit.script\r\ndef kernel_jit_script1(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n    exp_avg *= beta1\r\n    exp_avg += (1 - beta1) + grad\r\n    exp_avg_sq *= beta2\r\n    exp_avg_sq += (1 - beta2)*(grad*grad)\r\n    param += -step_size * (exp_avg / exp_avg_sq.sqrt() + eps)\r\n\r\n@torch.jit.script\r\ndef kernel_jit_script2(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n    exp_avg[:] = exp_avg * beta1 + (1 - beta1) + grad\r\n    exp_avg_sq[:] = exp_avg_sq * beta2 + (1 - beta2)*(grad*grad)\r\n    param[:] = param -step_size * (exp_avg / exp_avg_sq.sqrt() + eps)\r\n\r\n@torch.jit.script\r\ndef kernel_jit_script3(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> Tuple[Tensor, Tensor, Tensor, Tensor]\r\n    exp_avg = exp_avg * beta1 + (1 - beta1) + grad\r\n    exp_avg_sq = exp_avg_sq * beta2 + (1 - beta2)*(grad*grad)\r\n    param = param -step_size * (exp_avg / exp_avg_sq.sqrt() + eps)\r\n    return (param, grad, exp_avg, exp_avg_sq)\r\n\r\n\r\nfrom numba import cuda\r\n@cuda.jit\r\ndef kernel_numba_cuda(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    i = cuda.grid(1)\r\n    if i > grad.size:\r\n        return\r\n\r\n    exp_avg[i] = exp_avg[i] * beta1 + (1 - beta1) + grad[i]\r\n    exp_avg_sq[i] = exp_avg_sq[i] * beta2 + (1 - beta2) *grad[i]*grad[i]\r\n    denom = math.sqrt(exp_avg_sq[i]) + eps\r\n    param[i] += -step_size * (exp_avg[i] / denom)\r\n\r\ndef kernel_numba(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n\r\n    import ctypes\r\n    import numpy as np\r\n    def get_devicendarray(t):\r\n        assert t.type() == 'torch.cuda.FloatTensor'\r\n        numpy_dtype = np.float32\r\n        itemsize = 4\r\n\r\n        ctx = cuda.cudadrv.driver.driver.get_context()\r\n        mp = cuda.cudadrv.driver.MemoryPointer(ctx, ctypes.c_ulong(t.data_ptr()), t.numel()*itemsize)\r\n        return cuda.cudadrv.devicearray.DeviceNDArray(t.numel(), itemsize, numpy_dtype(), gpu_data=mp, stream=torch.cuda.current_stream().cuda_stream)\r\n\r\n    numba_param = get_devicendarray(param)\r\n    numba_grad = get_devicendarray(grad)\r\n    numba_exp_avg = get_devicendarray(exp_avg)\r\n    numba_exp_avg_sq = get_devicendarray(exp_avg_sq)\r\n    threadsperblock = 32\r\n    blockspergrid = (param.numel() + (threadsperblock - 1)) // threadsperblock\r\n    kernel_numba_cuda[blockspergrid, threadsperblock](numba_param, numba_grad, numba_exp_avg, numba_exp_avg_sq, beta1, beta2, step_size, eps)\r\n\r\nkernels = [(\"native \", kernel_native), (\"numba  \", kernel_numba)]\r\nshape = (4194304*64,)\r\nparam, grad, exp_avg, exp_avg_sq = [torch.rand(shape).to(\"cuda\") for x in range(4)]\r\n[Truncated]\n        res = func(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps)\r\n        if res is not None:\r\n            param[:], _, exp_avg[:], exp_avg_sq[:] = res\r\n        torch.cuda.synchronize()\r\n        t2 = time.time()\r\n        print(\"%s: \" % name, t2-t1)\r\n```\r\nThe results are:\r\n```\r\nnative :  0.0323\r\nnumba  :  0.0147\r\nTScript1:  0.0369\r\nTScript2:  0.0352\r\nTScript3:  0.0280\r\n```\r\nTScript3 is now better than native but still far from numba. TScript3 is now generating a single CUDA kernel but the back-copying of the result now dominates the timing. If I remove the line `param[:], _, exp_avg[:], exp_avg_sq[:] = res` the performance is almost on pair with numba:\r\n```\r\nTScript3 (no back-copy):  0.01664\r\n```\r\nThe problem is I really do need to update the values :/\n<issue_comment>username_5: cc @mruberry","repo":"pytorch/pytorch","issue_id":475728990,"issue_number":23655}
{"question":"What was the specific error detected by the UndefinedBehaviorSanitizer in the ATen library?","answer":"UndefinedBehaviorSanitizer: undefined-behavior /var/lib/jenkins/workspace/build/aten/src/ATen/Functions.h:2779:12","id":41,"text":"UBSAN failure looks real:\r\n\r\n```\r\nJun 21 16:45:39 /var/lib/jenkins/workspace/build/aten/src/ATen/Functions.h:2779:12: runtime error: call to function at::TypeDefault::randn(c10::ArrayRef<long>, c10::TensorOptions const&) through pointer to incorrect function type 'at::Tensor (*)(c10::ArrayRef<long>, const c10::TensorOptions &)'\r\nJun 21 16:45:39 (/opt/conda/lib/python3.6/site-packages/torch/lib/libtorch.so+0xa6b7860): note: at::TypeDefault::randn(c10::ArrayRef<long>, c10::TensorOptions const&) defined here\r\nJun 21 16:45:40     #0 0x7f25f2696882 in torch::randn(c10::ArrayRef<long>, c10::TensorOptions const&) (/opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_python.so+0x11aa882)\r\nJun 21 16:45:40     #1 0x7f25f2694ccc in torch::autograd::dispatch_randn(c10::ArrayRef<long>, c10::TensorOptions const&) (/opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_python.so+0x11a8ccc)\r\nJun 21 16:45:40     #2 0x7f25f258d08d in torch::autograd::THPVariable_randn(_object*, _object*, _object*) (/opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_python.so+0x10a108d)\r\nJun 21 16:45:40     #3 0x558025dc7743 in _PyCFunction_FastCallDict /tmp/build/80754af9/python_1546130271559/work/Objects/methodobject.c:231\r\nJun 21 16:45:40     #4 0x558025e4e42b in call_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4851\r\nJun 21 16:45:40     #5 0x558025e73389 in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3335\r\nJun 21 16:45:40     #6 0x558025e47bfd in _PyEval_EvalCodeWithName /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4166\r\nJun 21 16:45:40     #7 0x558025e48770 in fast_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4992\r\nJun 21 16:45:40     #8 0x558025e4e504 in call_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4872\r\nJun 21 16:45:40     #9 0x558025e73389 in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3335\r\nJun 21 16:45:40     #10 0x558025e49288 in _PyEval_EvalCodeWithName /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4166\r\nJun 21 16:45:40     #11 0x558025e49288 in PyEval_EvalCodeEx /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4187\r\nJun 21 16:45:40     #12 0x558025e4a01b in PyEval_EvalCode /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:731\r\nJun 21 16:45:40     #13 0x558025e70c8a in builtin_exec_impl.isra.11 /tmp/build/80754af9/python_1546130271559/work/Python/bltinmodule.c:983\r\nJun 21 16:45:40     #14 0x558025e70c8a in builtin_exec /tmp/build/80754af9/python_1546130271559/work/Python/clinic/bltinmodule.c.h:283\r\nJun 21 16:45:40     #15 0x558025dca710 in PyCFunction_Call /tmp/build/80754af9/python_1546130271559/work/Objects/methodobject.c:114\r\nJun 21 16:45:40     #16 0x558025e784ac in do_call_core /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:5116\r\nJun 21 16:45:40     #17 0x558025e784ac in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3404\r\nJun 21 16:45:40     #18 0x558025e478e3 in _PyEval_EvalCodeWithName /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4166\r\nJun 21 16:45:40     #19 0x558025e48770 in fast_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4992\r\nJun 21 16:45:40     #20 0x558025e4e504 in call_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4872\r\nJun 21 16:45:40     #21 0x558025e73389 in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3335\r\nJun 21 16:45:40     #22 0x558025e4853a in _PyFunction_FastCall /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4933\r\nJun 21 16:45:40     #23 0x558025e4853a in fast_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4968\r\nJun 21 16:45:40     #24 0x558025e4e504 in call_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4872\r\nJun 21 16:45:40     #25 0x558025e73389 in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3335\r\nJun 21 16:45:40     #26 0x558025e4853a in _PyFunction_FastCall /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4933\r\nJun 21 16:45:40     #27 0x558025e4853a in fast_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4968\r\nJun 21 16:45:40     #28 0x558025e4e504 in call_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4872\r\nJun 21 16:45:40     #29 0x558025e73389 in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3335\r\nJun 21 16:45:40     #30 0x558025e4853a in _PyFunction_FastCall /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4933\r\nJun 21 16:45:40     #31 0x558025e4853a in fast_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4968\r\nJun 21 16:45:40     #32 0x558025e4e504 in call_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4872\r\nJun 21 16:45:40     #33 0x558025e73389 in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3335\r\nJun 21 16:45:40     #34 0x558025e48baa in _PyFunction_FastCall /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4933\r\nJun 21 16:45:40     #35 0x558025e48baa in _PyFunction_FastCallDict /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:5035\r\nJun 21 16:45:40     #36 0x558025dc7b0e in _PyObject_FastCallDict /tmp/build/80754af9/python_1546130271559/work/Objects/abstract.c:2310\r\nJun 21 16:45:40     #37 0x558025e0980f in _PyObject_CallMethodIdObjArgs /tmp/build/80754af9/python_1546130271559/work/Objects/abstract.c:2796\r\nJun 21 16:45:40     #38 0x558025dbeb0f in PyImport_ImportModuleLevelObject /tmp/build/80754af9/python_1546130271559/work/Python/import.c:1578\r\nJun 21 16:45:40     #39 0x558025e75a8a in import_name /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:5245\r\nJun 21 16:45:40     #40 0x558025e75a8a in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:2899\r\nJun 21 16:45:40     #41 0x558025e49288 in _PyEval_EvalCodeWithName /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4166\r\nJun 21 16:45:40     #42 0x558025e49288 in PyEval_EvalCodeEx /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4187\r\nJun 21 16:45:40     #43 0x558025e4a01b in PyEval_EvalCode /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:731\r\nJun 21 16:45:40     #44 0x558025ecc3c3 in run_mod /tmp/build/80754af9/python_1546130271559/work/Python/pythonrun.c:1025\r\nJun 21 16:45:40     #45 0x558025ecc7c0 in PyRun_FileExFlags /tmp/build/80754af9/python_1546130271559/work/Python/pythonrun.c:978\r\nJun 21 16:45:40     #46 0x558025ecc9c2 in PyRun_SimpleFileExFlags /tmp/build/80754af9/python_1546130271559/work/Python/pythonrun.c:419\r\nJun 21 16:45:40     #47 0x558025ed04b2 in run_file /tmp/build/80754af9/python_1546130271559/work/Modules/main.c:340\r\nJun 21 16:45:40     #48 0x558025ed04b2 in Py_Main /tmp/build/80754af9/python_1546130271559/work/Modules/main.c:811\r\nJun 21 16:45:40     #49 0x558025d9902d in main /tmp/build/80754af9/python_1546130271559/work/Programs/python.c:69\r\nJun 21 16:45:40     #50 0x7f26053c082f in __libc_start_main /build/glibc-LK5gWL/glibc-2.23/csu/../csu/libc-start.c:291\r\nJun 21 16:45:40     #51 0x558025e79e0d in _start /home/rdonnelly/mc/conda-bld/compilers_linux-64_1534865402226/work/.build/src/glibc-2.12.2/csu/../sysdeps/x86_64/elf/start.S:103\r\nJun 21 16:45:40 \r\nJun 21 16:45:40 SUMMARY: UndefinedBehaviorSanitizer: undefined-behavior /var/lib/jenkins/workspace/build/aten/src/ATen/Functions.h:2779:12 in \r\nJun 21 16:45:40 Traceback (most recent call last):\r\n```","context":"<issue_start><issue_comment>Title: Add hidden visibility to _C.so\nusername_0: See https://pybind11.readthedocs.io/en/stable/faq.html#someclass-declared-with-greater-visibility-than-the-type-of-its-field-someclass-member-wattributes\r\n\r\nLet's see whether it passes CI\n<issue_comment>username_1: wouldn't C++ extensions fail because of this? they require the symbols coming out of libtorch_python and stuff to be global visibility in the current process, or else they fail to load at runtime (but the C++ extensions will compile fine)\n<issue_comment>username_2: UBSAN failure looks real:\r\n\r\n```\r\nJun 21 16:45:39 /var/lib/jenkins/workspace/build/aten/src/ATen/Functions.h:2779:12: runtime error: call to function at::TypeDefault::randn(c10::ArrayRef<long>, c10::TensorOptions const&) through pointer to incorrect function type 'at::Tensor (*)(c10::ArrayRef<long>, const c10::TensorOptions &)'\r\nJun 21 16:45:39 (/opt/conda/lib/python3.6/site-packages/torch/lib/libtorch.so+0xa6b7860): note: at::TypeDefault::randn(c10::ArrayRef<long>, c10::TensorOptions const&) defined here\r\nJun 21 16:45:40     #0 0x7f25f2696882 in torch::randn(c10::ArrayRef<long>, c10::TensorOptions const&) (/opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_python.so+0x11aa882)\r\nJun 21 16:45:40     #1 0x7f25f2694ccc in torch::autograd::dispatch_randn(c10::ArrayRef<long>, c10::TensorOptions const&) (/opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_python.so+0x11a8ccc)\r\nJun 21 16:45:40     #2 0x7f25f258d08d in torch::autograd::THPVariable_randn(_object*, _object*, _object*) (/opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_python.so+0x10a108d)\r\nJun 21 16:45:40     #3 0x558025dc7743 in _PyCFunction_FastCallDict /tmp/build/80754af9/python_1546130271559/work/Objects/methodobject.c:231\r\nJun 21 16:45:40     #4 0x558025e4e42b in call_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4851\r\nJun 21 16:45:40     #5 0x558025e73389 in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3335\r\nJun 21 16:45:40     #6 0x558025e47bfd in _PyEval_EvalCodeWithName /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4166\r\nJun 21 16:45:40     #7 0x558025e48770 in fast_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4992\r\nJun 21 16:45:40     #8 0x558025e4e504 in call_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4872\r\nJun 21 16:45:40     #9 0x558025e73389 in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3335\r\nJun 21 16:45:40     #10 0x558025e49288 in _PyEval_EvalCodeWithName /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4166\r\nJun 21 16:45:40     #11 0x558025e49288 in PyEval_EvalCodeEx /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4187\r\nJun 21 16:45:40     #12 0x558025e4a01b in PyEval_EvalCode /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:731\r\nJun 21 16:45:40     #13 0x558025e70c8a in builtin_exec_impl.isra.11 /tmp/build/80754af9/python_1546130271559/work/Python/bltinmodule.c:983\r\nJun 21 16:45:40     #14 0x558025e70c8a in builtin_exec /tmp/build/80754af9/python_1546130271559/work/Python/clinic/bltinmodule.c.h:283\r\nJun 21 16:45:40     #15 0x558025dca710 in PyCFunction_Call /tmp/build/80754af9/python_1546130271559/work/Objects/methodobject.c:114\r\nJun 21 16:45:40     #16 0x558025e784ac in do_call_core /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:5116\r\nJun 21 16:45:40     #17 0x558025e784ac in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3404\r\nJun 21 16:45:40     #18 0x558025e478e3 in _PyEval_EvalCodeWithName /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4166\r\nJun 21 16:45:40     #19 0x558025e48770 in fast_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4992\r\nJun 21 16:45:40     #20 0x558025e4e504 in call_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4872\r\nJun 21 16:45:40     #21 0x558025e73389 in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3335\r\nJun 21 16:45:40     #22 0x558025e4853a in _PyFunction_FastCall /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4933\r\nJun 21 16:45:40     #23 0x558025e4853a in fast_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4968\r\nJun 21 16:45:40     #24 0x558025e4e504 in call_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4872\r\nJun 21 16:45:40     #25 0x558025e73389 in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3335\r\nJun 21 16:45:40     #26 0x558025e4853a in _PyFunction_FastCall /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4933\r\nJun 21 16:45:40     #27 0x558025e4853a in fast_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4968\r\nJun 21 16:45:40     #28 0x558025e4e504 in call_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4872\r\nJun 21 16:45:40     #29 0x558025e73389 in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3335\r\nJun 21 16:45:40     #30 0x558025e4853a in _PyFunction_FastCall /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4933\r\nJun 21 16:45:40     #31 0x558025e4853a in fast_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4968\r\nJun 21 16:45:40     #32 0x558025e4e504 in call_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4872\r\nJun 21 16:45:40     #33 0x558025e73389 in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3335\r\nJun 21 16:45:40     #34 0x558025e48baa in _PyFunction_FastCall /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4933\r\nJun 21 16:45:40     #35 0x558025e48baa in _PyFunction_FastCallDict /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:5035\r\nJun 21 16:45:40     #36 0x558025dc7b0e in _PyObject_FastCallDict /tmp/build/80754af9/python_1546130271559/work/Objects/abstract.c:2310\r\nJun 21 16:45:40     #37 0x558025e0980f in _PyObject_CallMethodIdObjArgs /tmp/build/80754af9/python_1546130271559/work/Objects/abstract.c:2796\r\nJun 21 16:45:40     #38 0x558025dbeb0f in PyImport_ImportModuleLevelObject /tmp/build/80754af9/python_1546130271559/work/Python/import.c:1578\r\nJun 21 16:45:40     #39 0x558025e75a8a in import_name /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:5245\r\nJun 21 16:45:40     #40 0x558025e75a8a in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:2899\r\nJun 21 16:45:40     #41 0x558025e49288 in _PyEval_EvalCodeWithName /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4166\r\nJun 21 16:45:40     #42 0x558025e49288 in PyEval_EvalCodeEx /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4187\r\nJun 21 16:45:40     #43 0x558025e4a01b in PyEval_EvalCode /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:731\r\nJun 21 16:45:40     #44 0x558025ecc3c3 in run_mod /tmp/build/80754af9/python_1546130271559/work/Python/pythonrun.c:1025\r\nJun 21 16:45:40     #45 0x558025ecc7c0 in PyRun_FileExFlags /tmp/build/80754af9/python_1546130271559/work/Python/pythonrun.c:978\r\nJun 21 16:45:40     #46 0x558025ecc9c2 in PyRun_SimpleFileExFlags /tmp/build/80754af9/python_1546130271559/work/Python/pythonrun.c:419\r\nJun 21 16:45:40     #47 0x558025ed04b2 in run_file /tmp/build/80754af9/python_1546130271559/work/Modules/main.c:340\r\nJun 21 16:45:40     #48 0x558025ed04b2 in Py_Main /tmp/build/80754af9/python_1546130271559/work/Modules/main.c:811\r\nJun 21 16:45:40     #49 0x558025d9902d in main /tmp/build/80754af9/python_1546130271559/work/Programs/python.c:69\r\nJun 21 16:45:40     #50 0x7f26053c082f in __libc_start_main /build/glibc-LK5gWL/glibc-2.23/csu/../csu/libc-start.c:291\r\nJun 21 16:45:40     #51 0x558025e79e0d in _start /home/rdonnelly/mc/conda-bld/compilers_linux-64_1534865402226/work/.build/src/glibc-2.12.2/csu/../sysdeps/x86_64/elf/start.S:103\r\nJun 21 16:45:40 \r\nJun 21 16:45:40 SUMMARY: UndefinedBehaviorSanitizer: undefined-behavior /var/lib/jenkins/workspace/build/aten/src/ATen/Functions.h:2779:12 in \r\nJun 21 16:45:40 Traceback (most recent call last):\r\n```\n<issue_comment>username_2: I guess, ideally, we'd expose symbols that our ours, but not expose anything from pybind11. I don't know how easy that is to do. (Maybe this is a reason not to use pybind11, but that ship has kind of sailed...)\n<issue_comment>username_3: If everything works on MSVC with its default option -- that is, no exposure unless explicitly asked to -- could replacing all occurrences of `__declspec(dllexport)` with something also compatible with `gcc` work? Like something similar to [this](https://stackoverflow.com/a/8258786/1150462)?\n<issue_comment>username_0: Yes, I realized that it needs adjusting annotations.\r\n\r\nAs @username_3 pointed out, we should have already annotated all the right things to make Windows work. But we use windows-only macro in THP_API/THP_CLASS (https://github.com/pytorch/pytorch/blob/master/torch/csrc/THP_export.h). We should do the same thing as for other places - https://github.com/pytorch/pytorch/blob/master/c10/macros/Export.h\r\n\r\nI'll try to get back to it at some point.","repo":"pytorch/pytorch","issue_id":459270796,"issue_number":22076}
{"question":"What concerns are raised about exposing symbols specific to the project but not from pybind11?","answer":"The concern is regarding the difficulty in exposing symbols specific to the project without exposing anything from pybind11, suggesting a potential limitation in using pybind11 for this purpose.","id":42,"text":"I guess, ideally, we'd expose symbols that our ours, but not expose anything from pybind11. I don't know how easy that is to do. (Maybe this is a reason not to use pybind11, but that ship has kind of sailed...)","context":"<issue_start><issue_comment>Title: Add hidden visibility to _C.so\nusername_0: See https://pybind11.readthedocs.io/en/stable/faq.html#someclass-declared-with-greater-visibility-than-the-type-of-its-field-someclass-member-wattributes\r\n\r\nLet's see whether it passes CI\n<issue_comment>username_1: wouldn't C++ extensions fail because of this? they require the symbols coming out of libtorch_python and stuff to be global visibility in the current process, or else they fail to load at runtime (but the C++ extensions will compile fine)\n<issue_comment>username_2: UBSAN failure looks real:\r\n\r\n```\r\nJun 21 16:45:39 /var/lib/jenkins/workspace/build/aten/src/ATen/Functions.h:2779:12: runtime error: call to function at::TypeDefault::randn(c10::ArrayRef<long>, c10::TensorOptions const&) through pointer to incorrect function type 'at::Tensor (*)(c10::ArrayRef<long>, const c10::TensorOptions &)'\r\nJun 21 16:45:39 (/opt/conda/lib/python3.6/site-packages/torch/lib/libtorch.so+0xa6b7860): note: at::TypeDefault::randn(c10::ArrayRef<long>, c10::TensorOptions const&) defined here\r\nJun 21 16:45:40     #0 0x7f25f2696882 in torch::randn(c10::ArrayRef<long>, c10::TensorOptions const&) (/opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_python.so+0x11aa882)\r\nJun 21 16:45:40     #1 0x7f25f2694ccc in torch::autograd::dispatch_randn(c10::ArrayRef<long>, c10::TensorOptions const&) (/opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_python.so+0x11a8ccc)\r\nJun 21 16:45:40     #2 0x7f25f258d08d in torch::autograd::THPVariable_randn(_object*, _object*, _object*) (/opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_python.so+0x10a108d)\r\nJun 21 16:45:40     #3 0x558025dc7743 in _PyCFunction_FastCallDict /tmp/build/80754af9/python_1546130271559/work/Objects/methodobject.c:231\r\nJun 21 16:45:40     #4 0x558025e4e42b in call_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4851\r\nJun 21 16:45:40     #5 0x558025e73389 in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3335\r\nJun 21 16:45:40     #6 0x558025e47bfd in _PyEval_EvalCodeWithName /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4166\r\nJun 21 16:45:40     #7 0x558025e48770 in fast_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4992\r\nJun 21 16:45:40     #8 0x558025e4e504 in call_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4872\r\nJun 21 16:45:40     #9 0x558025e73389 in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3335\r\nJun 21 16:45:40     #10 0x558025e49288 in _PyEval_EvalCodeWithName /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4166\r\nJun 21 16:45:40     #11 0x558025e49288 in PyEval_EvalCodeEx /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4187\r\nJun 21 16:45:40     #12 0x558025e4a01b in PyEval_EvalCode /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:731\r\nJun 21 16:45:40     #13 0x558025e70c8a in builtin_exec_impl.isra.11 /tmp/build/80754af9/python_1546130271559/work/Python/bltinmodule.c:983\r\nJun 21 16:45:40     #14 0x558025e70c8a in builtin_exec /tmp/build/80754af9/python_1546130271559/work/Python/clinic/bltinmodule.c.h:283\r\nJun 21 16:45:40     #15 0x558025dca710 in PyCFunction_Call /tmp/build/80754af9/python_1546130271559/work/Objects/methodobject.c:114\r\nJun 21 16:45:40     #16 0x558025e784ac in do_call_core /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:5116\r\nJun 21 16:45:40     #17 0x558025e784ac in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3404\r\nJun 21 16:45:40     #18 0x558025e478e3 in _PyEval_EvalCodeWithName /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4166\r\nJun 21 16:45:40     #19 0x558025e48770 in fast_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4992\r\nJun 21 16:45:40     #20 0x558025e4e504 in call_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4872\r\nJun 21 16:45:40     #21 0x558025e73389 in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3335\r\nJun 21 16:45:40     #22 0x558025e4853a in _PyFunction_FastCall /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4933\r\nJun 21 16:45:40     #23 0x558025e4853a in fast_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4968\r\nJun 21 16:45:40     #24 0x558025e4e504 in call_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4872\r\nJun 21 16:45:40     #25 0x558025e73389 in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3335\r\nJun 21 16:45:40     #26 0x558025e4853a in _PyFunction_FastCall /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4933\r\nJun 21 16:45:40     #27 0x558025e4853a in fast_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4968\r\nJun 21 16:45:40     #28 0x558025e4e504 in call_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4872\r\nJun 21 16:45:40     #29 0x558025e73389 in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3335\r\nJun 21 16:45:40     #30 0x558025e4853a in _PyFunction_FastCall /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4933\r\nJun 21 16:45:40     #31 0x558025e4853a in fast_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4968\r\nJun 21 16:45:40     #32 0x558025e4e504 in call_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4872\r\nJun 21 16:45:40     #33 0x558025e73389 in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3335\r\nJun 21 16:45:40     #34 0x558025e48baa in _PyFunction_FastCall /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4933\r\nJun 21 16:45:40     #35 0x558025e48baa in _PyFunction_FastCallDict /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:5035\r\nJun 21 16:45:40     #36 0x558025dc7b0e in _PyObject_FastCallDict /tmp/build/80754af9/python_1546130271559/work/Objects/abstract.c:2310\r\nJun 21 16:45:40     #37 0x558025e0980f in _PyObject_CallMethodIdObjArgs /tmp/build/80754af9/python_1546130271559/work/Objects/abstract.c:2796\r\nJun 21 16:45:40     #38 0x558025dbeb0f in PyImport_ImportModuleLevelObject /tmp/build/80754af9/python_1546130271559/work/Python/import.c:1578\r\nJun 21 16:45:40     #39 0x558025e75a8a in import_name /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:5245\r\nJun 21 16:45:40     #40 0x558025e75a8a in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:2899\r\nJun 21 16:45:40     #41 0x558025e49288 in _PyEval_EvalCodeWithName /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4166\r\nJun 21 16:45:40     #42 0x558025e49288 in PyEval_EvalCodeEx /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4187\r\nJun 21 16:45:40     #43 0x558025e4a01b in PyEval_EvalCode /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:731\r\nJun 21 16:45:40     #44 0x558025ecc3c3 in run_mod /tmp/build/80754af9/python_1546130271559/work/Python/pythonrun.c:1025\r\nJun 21 16:45:40     #45 0x558025ecc7c0 in PyRun_FileExFlags /tmp/build/80754af9/python_1546130271559/work/Python/pythonrun.c:978\r\nJun 21 16:45:40     #46 0x558025ecc9c2 in PyRun_SimpleFileExFlags /tmp/build/80754af9/python_1546130271559/work/Python/pythonrun.c:419\r\nJun 21 16:45:40     #47 0x558025ed04b2 in run_file /tmp/build/80754af9/python_1546130271559/work/Modules/main.c:340\r\nJun 21 16:45:40     #48 0x558025ed04b2 in Py_Main /tmp/build/80754af9/python_1546130271559/work/Modules/main.c:811\r\nJun 21 16:45:40     #49 0x558025d9902d in main /tmp/build/80754af9/python_1546130271559/work/Programs/python.c:69\r\nJun 21 16:45:40     #50 0x7f26053c082f in __libc_start_main /build/glibc-LK5gWL/glibc-2.23/csu/../csu/libc-start.c:291\r\nJun 21 16:45:40     #51 0x558025e79e0d in _start /home/rdonnelly/mc/conda-bld/compilers_linux-64_1534865402226/work/.build/src/glibc-2.12.2/csu/../sysdeps/x86_64/elf/start.S:103\r\nJun 21 16:45:40 \r\nJun 21 16:45:40 SUMMARY: UndefinedBehaviorSanitizer: undefined-behavior /var/lib/jenkins/workspace/build/aten/src/ATen/Functions.h:2779:12 in \r\nJun 21 16:45:40 Traceback (most recent call last):\r\n```\n<issue_comment>username_2: I guess, ideally, we'd expose symbols that our ours, but not expose anything from pybind11. I don't know how easy that is to do. (Maybe this is a reason not to use pybind11, but that ship has kind of sailed...)\n<issue_comment>username_3: If everything works on MSVC with its default option -- that is, no exposure unless explicitly asked to -- could replacing all occurrences of `__declspec(dllexport)` with something also compatible with `gcc` work? Like something similar to [this](https://stackoverflow.com/a/8258786/1150462)?\n<issue_comment>username_0: Yes, I realized that it needs adjusting annotations.\r\n\r\nAs @username_3 pointed out, we should have already annotated all the right things to make Windows work. But we use windows-only macro in THP_API/THP_CLASS (https://github.com/pytorch/pytorch/blob/master/torch/csrc/THP_export.h). We should do the same thing as for other places - https://github.com/pytorch/pytorch/blob/master/c10/macros/Export.h\r\n\r\nI'll try to get back to it at some point.","repo":"pytorch/pytorch","issue_id":459270796,"issue_number":22076}
{"question":"Can replacing all occurrences of `__declspec(dllexport)` with something compatible with `gcc` work in the context of MSVC and GCC compatibility?","answer":"The answer to this question depends on the specific context and requirements of the codebase. While the solution provided on Stack Overflow may offer a possible approach, it is important to thoroughly test and validate any modifications to ensure compatibility with both MSVC and GCC compilers.","id":43,"text":"If everything works on MSVC with its default option -- that is, no exposure unless explicitly asked to -- could replacing all occurrences of `__declspec(dllexport)` with something also compatible with `gcc` work? Like something similar to [this](https://stackoverflow.com/a/8258786/1150462)?","context":"<issue_start><issue_comment>Title: Add hidden visibility to _C.so\nusername_0: See https://pybind11.readthedocs.io/en/stable/faq.html#someclass-declared-with-greater-visibility-than-the-type-of-its-field-someclass-member-wattributes\r\n\r\nLet's see whether it passes CI\n<issue_comment>username_1: wouldn't C++ extensions fail because of this? they require the symbols coming out of libtorch_python and stuff to be global visibility in the current process, or else they fail to load at runtime (but the C++ extensions will compile fine)\n<issue_comment>username_2: UBSAN failure looks real:\r\n\r\n```\r\nJun 21 16:45:39 /var/lib/jenkins/workspace/build/aten/src/ATen/Functions.h:2779:12: runtime error: call to function at::TypeDefault::randn(c10::ArrayRef<long>, c10::TensorOptions const&) through pointer to incorrect function type 'at::Tensor (*)(c10::ArrayRef<long>, const c10::TensorOptions &)'\r\nJun 21 16:45:39 (/opt/conda/lib/python3.6/site-packages/torch/lib/libtorch.so+0xa6b7860): note: at::TypeDefault::randn(c10::ArrayRef<long>, c10::TensorOptions const&) defined here\r\nJun 21 16:45:40     #0 0x7f25f2696882 in torch::randn(c10::ArrayRef<long>, c10::TensorOptions const&) (/opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_python.so+0x11aa882)\r\nJun 21 16:45:40     #1 0x7f25f2694ccc in torch::autograd::dispatch_randn(c10::ArrayRef<long>, c10::TensorOptions const&) (/opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_python.so+0x11a8ccc)\r\nJun 21 16:45:40     #2 0x7f25f258d08d in torch::autograd::THPVariable_randn(_object*, _object*, _object*) (/opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_python.so+0x10a108d)\r\nJun 21 16:45:40     #3 0x558025dc7743 in _PyCFunction_FastCallDict /tmp/build/80754af9/python_1546130271559/work/Objects/methodobject.c:231\r\nJun 21 16:45:40     #4 0x558025e4e42b in call_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4851\r\nJun 21 16:45:40     #5 0x558025e73389 in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3335\r\nJun 21 16:45:40     #6 0x558025e47bfd in _PyEval_EvalCodeWithName /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4166\r\nJun 21 16:45:40     #7 0x558025e48770 in fast_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4992\r\nJun 21 16:45:40     #8 0x558025e4e504 in call_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4872\r\nJun 21 16:45:40     #9 0x558025e73389 in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3335\r\nJun 21 16:45:40     #10 0x558025e49288 in _PyEval_EvalCodeWithName /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4166\r\nJun 21 16:45:40     #11 0x558025e49288 in PyEval_EvalCodeEx /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4187\r\nJun 21 16:45:40     #12 0x558025e4a01b in PyEval_EvalCode /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:731\r\nJun 21 16:45:40     #13 0x558025e70c8a in builtin_exec_impl.isra.11 /tmp/build/80754af9/python_1546130271559/work/Python/bltinmodule.c:983\r\nJun 21 16:45:40     #14 0x558025e70c8a in builtin_exec /tmp/build/80754af9/python_1546130271559/work/Python/clinic/bltinmodule.c.h:283\r\nJun 21 16:45:40     #15 0x558025dca710 in PyCFunction_Call /tmp/build/80754af9/python_1546130271559/work/Objects/methodobject.c:114\r\nJun 21 16:45:40     #16 0x558025e784ac in do_call_core /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:5116\r\nJun 21 16:45:40     #17 0x558025e784ac in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3404\r\nJun 21 16:45:40     #18 0x558025e478e3 in _PyEval_EvalCodeWithName /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4166\r\nJun 21 16:45:40     #19 0x558025e48770 in fast_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4992\r\nJun 21 16:45:40     #20 0x558025e4e504 in call_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4872\r\nJun 21 16:45:40     #21 0x558025e73389 in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3335\r\nJun 21 16:45:40     #22 0x558025e4853a in _PyFunction_FastCall /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4933\r\nJun 21 16:45:40     #23 0x558025e4853a in fast_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4968\r\nJun 21 16:45:40     #24 0x558025e4e504 in call_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4872\r\nJun 21 16:45:40     #25 0x558025e73389 in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3335\r\nJun 21 16:45:40     #26 0x558025e4853a in _PyFunction_FastCall /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4933\r\nJun 21 16:45:40     #27 0x558025e4853a in fast_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4968\r\nJun 21 16:45:40     #28 0x558025e4e504 in call_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4872\r\nJun 21 16:45:40     #29 0x558025e73389 in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3335\r\nJun 21 16:45:40     #30 0x558025e4853a in _PyFunction_FastCall /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4933\r\nJun 21 16:45:40     #31 0x558025e4853a in fast_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4968\r\nJun 21 16:45:40     #32 0x558025e4e504 in call_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4872\r\nJun 21 16:45:40     #33 0x558025e73389 in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3335\r\nJun 21 16:45:40     #34 0x558025e48baa in _PyFunction_FastCall /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4933\r\nJun 21 16:45:40     #35 0x558025e48baa in _PyFunction_FastCallDict /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:5035\r\nJun 21 16:45:40     #36 0x558025dc7b0e in _PyObject_FastCallDict /tmp/build/80754af9/python_1546130271559/work/Objects/abstract.c:2310\r\nJun 21 16:45:40     #37 0x558025e0980f in _PyObject_CallMethodIdObjArgs /tmp/build/80754af9/python_1546130271559/work/Objects/abstract.c:2796\r\nJun 21 16:45:40     #38 0x558025dbeb0f in PyImport_ImportModuleLevelObject /tmp/build/80754af9/python_1546130271559/work/Python/import.c:1578\r\nJun 21 16:45:40     #39 0x558025e75a8a in import_name /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:5245\r\nJun 21 16:45:40     #40 0x558025e75a8a in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:2899\r\nJun 21 16:45:40     #41 0x558025e49288 in _PyEval_EvalCodeWithName /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4166\r\nJun 21 16:45:40     #42 0x558025e49288 in PyEval_EvalCodeEx /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4187\r\nJun 21 16:45:40     #43 0x558025e4a01b in PyEval_EvalCode /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:731\r\nJun 21 16:45:40     #44 0x558025ecc3c3 in run_mod /tmp/build/80754af9/python_1546130271559/work/Python/pythonrun.c:1025\r\nJun 21 16:45:40     #45 0x558025ecc7c0 in PyRun_FileExFlags /tmp/build/80754af9/python_1546130271559/work/Python/pythonrun.c:978\r\nJun 21 16:45:40     #46 0x558025ecc9c2 in PyRun_SimpleFileExFlags /tmp/build/80754af9/python_1546130271559/work/Python/pythonrun.c:419\r\nJun 21 16:45:40     #47 0x558025ed04b2 in run_file /tmp/build/80754af9/python_1546130271559/work/Modules/main.c:340\r\nJun 21 16:45:40     #48 0x558025ed04b2 in Py_Main /tmp/build/80754af9/python_1546130271559/work/Modules/main.c:811\r\nJun 21 16:45:40     #49 0x558025d9902d in main /tmp/build/80754af9/python_1546130271559/work/Programs/python.c:69\r\nJun 21 16:45:40     #50 0x7f26053c082f in __libc_start_main /build/glibc-LK5gWL/glibc-2.23/csu/../csu/libc-start.c:291\r\nJun 21 16:45:40     #51 0x558025e79e0d in _start /home/rdonnelly/mc/conda-bld/compilers_linux-64_1534865402226/work/.build/src/glibc-2.12.2/csu/../sysdeps/x86_64/elf/start.S:103\r\nJun 21 16:45:40 \r\nJun 21 16:45:40 SUMMARY: UndefinedBehaviorSanitizer: undefined-behavior /var/lib/jenkins/workspace/build/aten/src/ATen/Functions.h:2779:12 in \r\nJun 21 16:45:40 Traceback (most recent call last):\r\n```\n<issue_comment>username_2: I guess, ideally, we'd expose symbols that our ours, but not expose anything from pybind11. I don't know how easy that is to do. (Maybe this is a reason not to use pybind11, but that ship has kind of sailed...)\n<issue_comment>username_3: If everything works on MSVC with its default option -- that is, no exposure unless explicitly asked to -- could replacing all occurrences of `__declspec(dllexport)` with something also compatible with `gcc` work? Like something similar to [this](https://stackoverflow.com/a/8258786/1150462)?\n<issue_comment>username_0: Yes, I realized that it needs adjusting annotations.\r\n\r\nAs @username_3 pointed out, we should have already annotated all the right things to make Windows work. But we use windows-only macro in THP_API/THP_CLASS (https://github.com/pytorch/pytorch/blob/master/torch/csrc/THP_export.h). We should do the same thing as for other places - https://github.com/pytorch/pytorch/blob/master/c10/macros/Export.h\r\n\r\nI'll try to get back to it at some point.","repo":"pytorch/pytorch","issue_id":459270796,"issue_number":22076}
{"question":"What specific adjustments were noted for annotations in the codebase?","answer":"The need to adjust annotations for Windows functionality, particularly related to using a windows-only macro in THP_API/THP_CLASS file.","id":44,"text":"Yes, I realized that it needs adjusting annotations.\r\n\r\nAs @username_3 pointed out, we should have already annotated all the right things to make Windows work. But we use windows-only macro in THP_API/THP_CLASS (https://github.com/pytorch/pytorch/blob/master/torch/csrc/THP_export.h). We should do the same thing as for other places - https://github.com/pytorch/pytorch/blob/master/c10/macros/Export.h\r\n\r\nI'll try to get back to it at some point.","context":"<issue_start><issue_comment>Title: Add hidden visibility to _C.so\nusername_0: See https://pybind11.readthedocs.io/en/stable/faq.html#someclass-declared-with-greater-visibility-than-the-type-of-its-field-someclass-member-wattributes\r\n\r\nLet's see whether it passes CI\n<issue_comment>username_1: wouldn't C++ extensions fail because of this? they require the symbols coming out of libtorch_python and stuff to be global visibility in the current process, or else they fail to load at runtime (but the C++ extensions will compile fine)\n<issue_comment>username_2: UBSAN failure looks real:\r\n\r\n```\r\nJun 21 16:45:39 /var/lib/jenkins/workspace/build/aten/src/ATen/Functions.h:2779:12: runtime error: call to function at::TypeDefault::randn(c10::ArrayRef<long>, c10::TensorOptions const&) through pointer to incorrect function type 'at::Tensor (*)(c10::ArrayRef<long>, const c10::TensorOptions &)'\r\nJun 21 16:45:39 (/opt/conda/lib/python3.6/site-packages/torch/lib/libtorch.so+0xa6b7860): note: at::TypeDefault::randn(c10::ArrayRef<long>, c10::TensorOptions const&) defined here\r\nJun 21 16:45:40     #0 0x7f25f2696882 in torch::randn(c10::ArrayRef<long>, c10::TensorOptions const&) (/opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_python.so+0x11aa882)\r\nJun 21 16:45:40     #1 0x7f25f2694ccc in torch::autograd::dispatch_randn(c10::ArrayRef<long>, c10::TensorOptions const&) (/opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_python.so+0x11a8ccc)\r\nJun 21 16:45:40     #2 0x7f25f258d08d in torch::autograd::THPVariable_randn(_object*, _object*, _object*) (/opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_python.so+0x10a108d)\r\nJun 21 16:45:40     #3 0x558025dc7743 in _PyCFunction_FastCallDict /tmp/build/80754af9/python_1546130271559/work/Objects/methodobject.c:231\r\nJun 21 16:45:40     #4 0x558025e4e42b in call_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4851\r\nJun 21 16:45:40     #5 0x558025e73389 in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3335\r\nJun 21 16:45:40     #6 0x558025e47bfd in _PyEval_EvalCodeWithName /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4166\r\nJun 21 16:45:40     #7 0x558025e48770 in fast_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4992\r\nJun 21 16:45:40     #8 0x558025e4e504 in call_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4872\r\nJun 21 16:45:40     #9 0x558025e73389 in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3335\r\nJun 21 16:45:40     #10 0x558025e49288 in _PyEval_EvalCodeWithName /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4166\r\nJun 21 16:45:40     #11 0x558025e49288 in PyEval_EvalCodeEx /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4187\r\nJun 21 16:45:40     #12 0x558025e4a01b in PyEval_EvalCode /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:731\r\nJun 21 16:45:40     #13 0x558025e70c8a in builtin_exec_impl.isra.11 /tmp/build/80754af9/python_1546130271559/work/Python/bltinmodule.c:983\r\nJun 21 16:45:40     #14 0x558025e70c8a in builtin_exec /tmp/build/80754af9/python_1546130271559/work/Python/clinic/bltinmodule.c.h:283\r\nJun 21 16:45:40     #15 0x558025dca710 in PyCFunction_Call /tmp/build/80754af9/python_1546130271559/work/Objects/methodobject.c:114\r\nJun 21 16:45:40     #16 0x558025e784ac in do_call_core /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:5116\r\nJun 21 16:45:40     #17 0x558025e784ac in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3404\r\nJun 21 16:45:40     #18 0x558025e478e3 in _PyEval_EvalCodeWithName /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4166\r\nJun 21 16:45:40     #19 0x558025e48770 in fast_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4992\r\nJun 21 16:45:40     #20 0x558025e4e504 in call_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4872\r\nJun 21 16:45:40     #21 0x558025e73389 in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3335\r\nJun 21 16:45:40     #22 0x558025e4853a in _PyFunction_FastCall /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4933\r\nJun 21 16:45:40     #23 0x558025e4853a in fast_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4968\r\nJun 21 16:45:40     #24 0x558025e4e504 in call_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4872\r\nJun 21 16:45:40     #25 0x558025e73389 in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3335\r\nJun 21 16:45:40     #26 0x558025e4853a in _PyFunction_FastCall /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4933\r\nJun 21 16:45:40     #27 0x558025e4853a in fast_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4968\r\nJun 21 16:45:40     #28 0x558025e4e504 in call_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4872\r\nJun 21 16:45:40     #29 0x558025e73389 in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3335\r\nJun 21 16:45:40     #30 0x558025e4853a in _PyFunction_FastCall /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4933\r\nJun 21 16:45:40     #31 0x558025e4853a in fast_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4968\r\nJun 21 16:45:40     #32 0x558025e4e504 in call_function /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4872\r\nJun 21 16:45:40     #33 0x558025e73389 in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3335\r\nJun 21 16:45:40     #34 0x558025e48baa in _PyFunction_FastCall /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4933\r\nJun 21 16:45:40     #35 0x558025e48baa in _PyFunction_FastCallDict /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:5035\r\nJun 21 16:45:40     #36 0x558025dc7b0e in _PyObject_FastCallDict /tmp/build/80754af9/python_1546130271559/work/Objects/abstract.c:2310\r\nJun 21 16:45:40     #37 0x558025e0980f in _PyObject_CallMethodIdObjArgs /tmp/build/80754af9/python_1546130271559/work/Objects/abstract.c:2796\r\nJun 21 16:45:40     #38 0x558025dbeb0f in PyImport_ImportModuleLevelObject /tmp/build/80754af9/python_1546130271559/work/Python/import.c:1578\r\nJun 21 16:45:40     #39 0x558025e75a8a in import_name /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:5245\r\nJun 21 16:45:40     #40 0x558025e75a8a in _PyEval_EvalFrameDefault /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:2899\r\nJun 21 16:45:40     #41 0x558025e49288 in _PyEval_EvalCodeWithName /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4166\r\nJun 21 16:45:40     #42 0x558025e49288 in PyEval_EvalCodeEx /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:4187\r\nJun 21 16:45:40     #43 0x558025e4a01b in PyEval_EvalCode /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:731\r\nJun 21 16:45:40     #44 0x558025ecc3c3 in run_mod /tmp/build/80754af9/python_1546130271559/work/Python/pythonrun.c:1025\r\nJun 21 16:45:40     #45 0x558025ecc7c0 in PyRun_FileExFlags /tmp/build/80754af9/python_1546130271559/work/Python/pythonrun.c:978\r\nJun 21 16:45:40     #46 0x558025ecc9c2 in PyRun_SimpleFileExFlags /tmp/build/80754af9/python_1546130271559/work/Python/pythonrun.c:419\r\nJun 21 16:45:40     #47 0x558025ed04b2 in run_file /tmp/build/80754af9/python_1546130271559/work/Modules/main.c:340\r\nJun 21 16:45:40     #48 0x558025ed04b2 in Py_Main /tmp/build/80754af9/python_1546130271559/work/Modules/main.c:811\r\nJun 21 16:45:40     #49 0x558025d9902d in main /tmp/build/80754af9/python_1546130271559/work/Programs/python.c:69\r\nJun 21 16:45:40     #50 0x7f26053c082f in __libc_start_main /build/glibc-LK5gWL/glibc-2.23/csu/../csu/libc-start.c:291\r\nJun 21 16:45:40     #51 0x558025e79e0d in _start /home/rdonnelly/mc/conda-bld/compilers_linux-64_1534865402226/work/.build/src/glibc-2.12.2/csu/../sysdeps/x86_64/elf/start.S:103\r\nJun 21 16:45:40 \r\nJun 21 16:45:40 SUMMARY: UndefinedBehaviorSanitizer: undefined-behavior /var/lib/jenkins/workspace/build/aten/src/ATen/Functions.h:2779:12 in \r\nJun 21 16:45:40 Traceback (most recent call last):\r\n```\n<issue_comment>username_2: I guess, ideally, we'd expose symbols that our ours, but not expose anything from pybind11. I don't know how easy that is to do. (Maybe this is a reason not to use pybind11, but that ship has kind of sailed...)\n<issue_comment>username_3: If everything works on MSVC with its default option -- that is, no exposure unless explicitly asked to -- could replacing all occurrences of `__declspec(dllexport)` with something also compatible with `gcc` work? Like something similar to [this](https://stackoverflow.com/a/8258786/1150462)?\n<issue_comment>username_0: Yes, I realized that it needs adjusting annotations.\r\n\r\nAs @username_3 pointed out, we should have already annotated all the right things to make Windows work. But we use windows-only macro in THP_API/THP_CLASS (https://github.com/pytorch/pytorch/blob/master/torch/csrc/THP_export.h). We should do the same thing as for other places - https://github.com/pytorch/pytorch/blob/master/c10/macros/Export.h\r\n\r\nI'll try to get back to it at some point.","repo":"pytorch/pytorch","issue_id":459270796,"issue_number":22076}
{"question":"What error occurs when attempting to create a sparse tensor from a tensor of indices and values placed on different GPUs?","answer":"The error 'device of indices (0) must match device of values (1)' is encountered.","id":45,"text":"##  Bug\r\n\r\nAttempting to create a sparse tensor from a tensor of indices and values placed on another GPU than `cuda:0` (for instance `cuda:1`) fails with error `device of indices (0) must match device of values (1)`.\r\n\r\nPlacing the values on `cuda:0` while leaving the indices on `cuda:1` works and returns a sparse tensor on `cuda:0`.\r\n\r\nThe following demo should show the bug:\r\n```\r\nIn [1]: import torch                                                            \r\n\r\nIn [2]: ind = torch.arange(10).unsqueeze(0).to('cuda:1')                        \r\n\r\nIn [3]: sp_ind = torch.cat((ind, ind), 0)                                       \r\n\r\nIn [4]: sp_val = torch.rand(10).to('cuda:1')                                    \r\n\r\nIn [5]: sp_shape = (10, 10)                                                     \r\n\r\nIn [6]: x = torch.sparse_coo_tensor(sp_ind, sp_val, sp_shape)                   \r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-6-776f23c64083> in <module>\r\n----> 1 x = torch.sparse_coo_tensor(sp_ind, sp_val, sp_shape)\r\n\r\nRuntimeError: device of indices (0) must match device of values (1)\r\n\r\nIn [7]: # Now this                                                              \r\n\r\nIn [8]: x = torch.sparse_coo_tensor(sp_ind, sp_val.cuda(0), sp_shape)           \r\n\r\nIn [9]: x                                                                       \r\nOut[9]: \r\ntensor(indices=tensor([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\r\n                       [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]]),\r\n       values=tensor([0.5324, 0.1738, 0.4186, 0.2776, 0.4665, 0.9525, 0.6555,\r\n                      0.2805, 0.9244, 0.4847]),\r\n       device='cuda:0', size=(10, 10), nnz=10, layout=torch.sparse_coo)\r\n\r\nIn [10]: sp_ind                                                                 \r\nOut[10]: \r\ntensor([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\r\n        [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]], device='cuda:1')\r\n\r\nIn [11]: sp_val                                                                 \r\nOut[11]: \r\ntensor([0.5324, 0.1738, 0.4186, 0.2776, 0.4665, 0.9525, 0.6555, 0.2805, 0.9244,\r\n        0.4847], device='cuda:1')\r\n\r\n```\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n**See code snipped above.**\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n## Expected behavior\r\n\r\nProvided both indices and values are placed on the same device, `torch.sparse_coo_tensor` should return a sparse tensor on that device.\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n## Environment\r\n\r\nPlease copy and paste the output from our\r\n[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)\r\n(or fill out the checklist below manually).\r\n\r\nYou can get the script and run it with:\r\n```\r\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\r\n# For security purposes, please check the contents of collect_env.py before running it.\r\npython collect_env.py\r\n```\r\n\r\n - PyTorch Version (e.g., 1.0): 1.3.0\r\n - OS (e.g., Linux): Ubuntu 18.04 LTS\r\n - How you installed PyTorch (`conda`, `pip`, source): conda\r\n - Build command you used (if compiling from source): N/A\r\n - Python version: 3.7.4\r\n - CUDA/cuDNN version: 10.2/7.5\r\n - GPU models and configuration: 2x2080 Ti with Nvlink\r\n - Any other relevant information:\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context about the problem here. -->","context":"<issue_start><issue_comment>Title: Sparse tensor creation ignores indices placement\nusername_0: ##  Bug\r\n\r\nAttempting to create a sparse tensor from a tensor of indices and values placed on another GPU than `cuda:0` (for instance `cuda:1`) fails with error `device of indices (0) must match device of values (1)`.\r\n\r\nPlacing the values on `cuda:0` while leaving the indices on `cuda:1` works and returns a sparse tensor on `cuda:0`.\r\n\r\nThe following demo should show the bug:\r\n```\r\nIn [1]: import torch                                                            \r\n\r\nIn [2]: ind = torch.arange(10).unsqueeze(0).to('cuda:1')                        \r\n\r\nIn [3]: sp_ind = torch.cat((ind, ind), 0)                                       \r\n\r\nIn [4]: sp_val = torch.rand(10).to('cuda:1')                                    \r\n\r\nIn [5]: sp_shape = (10, 10)                                                     \r\n\r\nIn [6]: x = torch.sparse_coo_tensor(sp_ind, sp_val, sp_shape)                   \r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-6-776f23c64083> in <module>\r\n----> 1 x = torch.sparse_coo_tensor(sp_ind, sp_val, sp_shape)\r\n\r\nRuntimeError: device of indices (0) must match device of values (1)\r\n\r\nIn [7]: # Now this                                                              \r\n\r\nIn [8]: x = torch.sparse_coo_tensor(sp_ind, sp_val.cuda(0), sp_shape)           \r\n\r\nIn [9]: x                                                                       \r\nOut[9]: \r\ntensor(indices=tensor([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\r\n                       [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]]),\r\n       values=tensor([0.5324, 0.1738, 0.4186, 0.2776, 0.4665, 0.9525, 0.6555,\r\n                      0.2805, 0.9244, 0.4847]),\r\n       device='cuda:0', size=(10, 10), nnz=10, layout=torch.sparse_coo)\r\n\r\nIn [10]: sp_ind                                                                 \r\nOut[10]: \r\ntensor([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\r\n        [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]], device='cuda:1')\r\n\r\nIn [11]: sp_val                                                                 \r\nOut[11]: \r\ntensor([0.5324, 0.1738, 0.4186, 0.2776, 0.4665, 0.9525, 0.6555, 0.2805, 0.9244,\r\n        0.4847], device='cuda:1')\r\n\r\n```\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n**See code snipped above.**\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n## Expected behavior\r\n\r\nProvided both indices and values are placed on the same device, `torch.sparse_coo_tensor` should return a sparse tensor on that device.\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n## Environment\r\n\r\nPlease copy and paste the output from our\r\n[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)\r\n(or fill out the checklist below manually).\r\n\r\nYou can get the script and run it with:\r\n```\r\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\r\n# For security purposes, please check the contents of collect_env.py before running it.\r\npython collect_env.py\r\n```\r\n\r\n - PyTorch Version (e.g., 1.0): 1.3.0\r\n - OS (e.g., Linux): Ubuntu 18.04 LTS\r\n - How you installed PyTorch (`conda`, `pip`, source): conda\r\n - Build command you used (if compiling from source): N/A\r\n - Python version: 3.7.4\r\n - CUDA/cuDNN version: 10.2/7.5\r\n - GPU models and configuration: 2x2080 Ti with Nvlink\r\n - Any other relevant information:\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\n<issue_comment>username_1: Oh, I meet the same thing!\n<issue_comment>username_2: I find `torch.sparse.FloatTensor' to be a good walk around.\n<issue_comment>username_3: @username_4 , can we close this issue via https://github.com/pytorch/pytorch/pull/41984 ?\n<issue_comment>username_4: Yes, we can close. It's solved already\n<issue_comment>username_5: Closing issue, since last comment indicates it as resolved.<issue_closed>","repo":"pytorch/pytorch","issue_id":511249220,"issue_number":28500}
{"question":"What change is expected in the code regarding operators added in the mentioned PyTorch PR 33294?","answer":"The \"_\" prefix registration will be removed when the operators are all migrated to mobile.","id":51,"text":"cc @iseeyuan \r\n\r\nThey were added in https://github.com/pytorch/pytorch/pull/33294 and the PR says \"The \"_\" prefix registration will be removed when the operators are all migrated to mobile.\" This should be reflected in the code somewhere.","context":"<issue_start><issue_comment>Title: Meaning of _quantized namespace isn't documented\nusername_0: cc @iseeyuan \r\n\r\nThey were added in https://github.com/pytorch/pytorch/pull/33294 and the PR says \"The \"_\" prefix registration will be removed when the operators are all migrated to mobile.\" This should be reflected in the code somewhere.\n<issue_comment>username_0: I guess I'll fix this when I reorg the operator registrations\n<issue_comment>username_1: Hey @username_0  assigning this to you based on your comment. We can take this up if you can't get to it. Thanks!","repo":"pytorch/pytorch","issue_id":599121342,"issue_number":36510}
{"question":"What is the difference in behavior of console.log triggering order between Chrome and Safari when running the provided React code?","answer":"In Chrome, the console.log triggers in the order: useEffect in child, timeout in render. In Safari, the console.log triggers in the order: timeout in render, useEffect in child.","id":57,"text":"In chrome and safari, the ```console.log``` was triggered in different order. [Example](https://codesandbox.io/s/amazing-payne-4lmxo) will show different result if opened in different browsers.\r\n\r\nReact version: 16.13.1\r\n\r\n## Steps To Reproduce\r\n\r\n1. Run following code in chrome and safari.\r\n2. Check console log\r\n\r\n```jsx\r\nimport React, { useState, useEffect } from \"react\";\r\n\r\nconst Test = props => {\r\n  useEffect(() => {\r\n    console.log(\"useEffect in child\");\r\n  }, []);\r\n\r\n  return \"Test useEffect\";\r\n};\r\n\r\nconst App = () => {\r\n  setTimeout(() => {\r\n    console.log(\"timeout in render\");\r\n  }, 0);\r\n\r\n  return <Test />;\r\n};\r\n\r\nexport default App\r\n```\r\n\r\nLink to code example:\r\n\r\n[Example](https://codesandbox.io/s/amazing-payne-4lmxo)\r\n\r\n## The current behavior\r\n\r\nThe message is not the same order;\r\n\r\n```jsx\r\n/**\r\n * chrome result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n\r\n/**\r\n * safari result\r\n *\r\n * timeout in render\r\n * useEffect in child\r\n */\r\n```\r\n\r\n## The expected behavior\r\n\r\nThe message should be same;\r\n\r\n```jsx\r\n/**\r\n * chrome result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n\r\n/**\r\n * safari result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n```","context":"<issue_start><issue_comment>Title: Bug: The trigger timing of \"useEffect\" is different in chrome and safari.\nusername_0: In chrome and safari, the ```console.log``` was triggered in different order. [Example](https://codesandbox.io/s/amazing-payne-4lmxo) will show different result if opened in different browsers.\r\n\r\nReact version: 16.13.1\r\n\r\n## Steps To Reproduce\r\n\r\n1. Run following code in chrome and safari.\r\n2. Check console log\r\n\r\n```jsx\r\nimport React, { useState, useEffect } from \"react\";\r\n\r\nconst Test = props => {\r\n  useEffect(() => {\r\n    console.log(\"useEffect in child\");\r\n  }, []);\r\n\r\n  return \"Test useEffect\";\r\n};\r\n\r\nconst App = () => {\r\n  setTimeout(() => {\r\n    console.log(\"timeout in render\");\r\n  }, 0);\r\n\r\n  return <Test />;\r\n};\r\n\r\nexport default App\r\n```\r\n\r\nLink to code example:\r\n\r\n[Example](https://codesandbox.io/s/amazing-payne-4lmxo)\r\n\r\n## The current behavior\r\n\r\nThe message is not the same order;\r\n\r\n```jsx\r\n/**\r\n * chrome result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n\r\n/**\r\n * safari result\r\n *\r\n * timeout in render\r\n * useEffect in child\r\n */\r\n```\r\n\r\n## The expected behavior\r\n\r\nThe message should be same;\r\n\r\n```jsx\r\n/**\r\n * chrome result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n\r\n/**\r\n * safari result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n```<issue_closed>\n<issue_comment>username_0: In chrome and safari, the ```console.log``` was triggered in different order. [Example](https://codesandbox.io/s/amazing-payne-4lmxo) will show different result if opened in different browsers.\r\n\r\nReact version: 16.13.1\r\n\r\n## Steps To Reproduce\r\n\r\n1. Run following code in chrome and safari.\r\n2. Check console log\r\n\r\n```jsx\r\nimport React, { useState, useEffect } from \"react\";\r\n\r\nconst Test = props => {\r\n  useEffect(() => {\r\n    console.log(\"useEffect in child\");\r\n  }, []);\r\n\r\n  return \"Test useEffect\";\r\n};\r\n\r\nconst App = () => {\r\n  setTimeout(() => {\r\n    console.log(\"timeout in render\");\r\n  }, 0);\r\n\r\n  return <Test />;\r\n};\r\n\r\nexport default App\r\n```\r\n\r\nLink to code example:\r\n\r\n[Example](https://codesandbox.io/s/amazing-payne-4lmxo)\r\n\r\n## The current behavior\r\n\r\nThe message is not the same order;\r\n\r\n```jsx\r\n/**\r\n * chrome result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n\r\n/**\r\n * safari result\r\n *\r\n * timeout in render\r\n * useEffect in child\r\n */\r\n```\r\n\r\n## The expected behavior\r\n\r\nThe message should be same;\r\n\r\n```jsx\r\n/**\r\n * chrome result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n\r\n/**\r\n * safari result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n```\n<issue_comment>username_1: You do the side effect in the render method:\r\n```typescript\r\nconst App = () => {\r\n  setTimeout(() => {\r\n    console.log(\"timeout in render\");\r\n  }, 0);\r\n\r\n  return <Test />;\r\n};\r\n```\r\nwhich is not allowed. React doesn't guarantee this kind of orders. you should put `setTimeout` inside an useEffect.\n<issue_comment>username_0: Yes, i agree this is not allowed. But different browsers should have same result.I think react should achieve lifecycle with more stable browser interfaces.\n<issue_comment>username_2: Many reasons why this is not a bug:\r\n\r\n - you are not allowed to make side effects in render. If you do that, the outcome is essentially \"undefined behaviour\", that is, nothing is guaranteed\r\n\r\n- React provides no guarantees about timing of `useEffect` with respect to render anyway.\r\n\r\nSo while this is an inconsistency and it could be fixed, there is not good reason to do so.\n<issue_comment>username_3: It is not supported to call `setTimeout` during rendering. React provides absolutely no guarantees about the order or the number of component render calls  so you can't rely on subtle timing aspects of them.\r\n\r\nIf I wrap it in `useEffect` I don't observe the difference.<issue_closed>","repo":"facebook/react","issue_id":664173077,"issue_number":19438}
{"question":"Why is it not allowed to perform side effects directly in the render method of a React component?","answer":"Performing side effects directly in the render method of a React component is not allowed because React does not guarantee the order of execution for such side effects. For example, using `setTimeout` directly in the render method may lead to unexpected behavior. It is recommended to place side effects like `setTimeout` inside a `useEffect` hook in React components.","id":58,"text":"You do the side effect in the render method:\r\n```typescript\r\nconst App = () => {\r\n  setTimeout(() => {\r\n    console.log(\"timeout in render\");\r\n  }, 0);\r\n\r\n  return <Test />;\r\n};\r\n```\r\nwhich is not allowed. React doesn't guarantee this kind of orders. you should put `setTimeout` inside an useEffect.","context":"<issue_start><issue_comment>Title: Bug: The trigger timing of \"useEffect\" is different in chrome and safari.\nusername_0: In chrome and safari, the ```console.log``` was triggered in different order. [Example](https://codesandbox.io/s/amazing-payne-4lmxo) will show different result if opened in different browsers.\r\n\r\nReact version: 16.13.1\r\n\r\n## Steps To Reproduce\r\n\r\n1. Run following code in chrome and safari.\r\n2. Check console log\r\n\r\n```jsx\r\nimport React, { useState, useEffect } from \"react\";\r\n\r\nconst Test = props => {\r\n  useEffect(() => {\r\n    console.log(\"useEffect in child\");\r\n  }, []);\r\n\r\n  return \"Test useEffect\";\r\n};\r\n\r\nconst App = () => {\r\n  setTimeout(() => {\r\n    console.log(\"timeout in render\");\r\n  }, 0);\r\n\r\n  return <Test />;\r\n};\r\n\r\nexport default App\r\n```\r\n\r\nLink to code example:\r\n\r\n[Example](https://codesandbox.io/s/amazing-payne-4lmxo)\r\n\r\n## The current behavior\r\n\r\nThe message is not the same order;\r\n\r\n```jsx\r\n/**\r\n * chrome result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n\r\n/**\r\n * safari result\r\n *\r\n * timeout in render\r\n * useEffect in child\r\n */\r\n```\r\n\r\n## The expected behavior\r\n\r\nThe message should be same;\r\n\r\n```jsx\r\n/**\r\n * chrome result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n\r\n/**\r\n * safari result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n```<issue_closed>\n<issue_comment>username_0: In chrome and safari, the ```console.log``` was triggered in different order. [Example](https://codesandbox.io/s/amazing-payne-4lmxo) will show different result if opened in different browsers.\r\n\r\nReact version: 16.13.1\r\n\r\n## Steps To Reproduce\r\n\r\n1. Run following code in chrome and safari.\r\n2. Check console log\r\n\r\n```jsx\r\nimport React, { useState, useEffect } from \"react\";\r\n\r\nconst Test = props => {\r\n  useEffect(() => {\r\n    console.log(\"useEffect in child\");\r\n  }, []);\r\n\r\n  return \"Test useEffect\";\r\n};\r\n\r\nconst App = () => {\r\n  setTimeout(() => {\r\n    console.log(\"timeout in render\");\r\n  }, 0);\r\n\r\n  return <Test />;\r\n};\r\n\r\nexport default App\r\n```\r\n\r\nLink to code example:\r\n\r\n[Example](https://codesandbox.io/s/amazing-payne-4lmxo)\r\n\r\n## The current behavior\r\n\r\nThe message is not the same order;\r\n\r\n```jsx\r\n/**\r\n * chrome result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n\r\n/**\r\n * safari result\r\n *\r\n * timeout in render\r\n * useEffect in child\r\n */\r\n```\r\n\r\n## The expected behavior\r\n\r\nThe message should be same;\r\n\r\n```jsx\r\n/**\r\n * chrome result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n\r\n/**\r\n * safari result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n```\n<issue_comment>username_1: You do the side effect in the render method:\r\n```typescript\r\nconst App = () => {\r\n  setTimeout(() => {\r\n    console.log(\"timeout in render\");\r\n  }, 0);\r\n\r\n  return <Test />;\r\n};\r\n```\r\nwhich is not allowed. React doesn't guarantee this kind of orders. you should put `setTimeout` inside an useEffect.\n<issue_comment>username_0: Yes, i agree this is not allowed. But different browsers should have same result.I think react should achieve lifecycle with more stable browser interfaces.\n<issue_comment>username_2: Many reasons why this is not a bug:\r\n\r\n - you are not allowed to make side effects in render. If you do that, the outcome is essentially \"undefined behaviour\", that is, nothing is guaranteed\r\n\r\n- React provides no guarantees about timing of `useEffect` with respect to render anyway.\r\n\r\nSo while this is an inconsistency and it could be fixed, there is not good reason to do so.\n<issue_comment>username_3: It is not supported to call `setTimeout` during rendering. React provides absolutely no guarantees about the order or the number of component render calls  so you can't rely on subtle timing aspects of them.\r\n\r\nIf I wrap it in `useEffect` I don't observe the difference.<issue_closed>","repo":"facebook/react","issue_id":664173077,"issue_number":19438}
{"question":"How does the text suggest React should handle lifecycle to ensure consistent results across different browsers?","answer":"React should achieve lifecycle with more stable browser interfaces to ensure consistency across different browsers.","id":59,"text":"Yes, i agree this is not allowed. But different browsers should have same result.I think react should achieve lifecycle with more stable browser interfaces.","context":"<issue_start><issue_comment>Title: Bug: The trigger timing of \"useEffect\" is different in chrome and safari.\nusername_0: In chrome and safari, the ```console.log``` was triggered in different order. [Example](https://codesandbox.io/s/amazing-payne-4lmxo) will show different result if opened in different browsers.\r\n\r\nReact version: 16.13.1\r\n\r\n## Steps To Reproduce\r\n\r\n1. Run following code in chrome and safari.\r\n2. Check console log\r\n\r\n```jsx\r\nimport React, { useState, useEffect } from \"react\";\r\n\r\nconst Test = props => {\r\n  useEffect(() => {\r\n    console.log(\"useEffect in child\");\r\n  }, []);\r\n\r\n  return \"Test useEffect\";\r\n};\r\n\r\nconst App = () => {\r\n  setTimeout(() => {\r\n    console.log(\"timeout in render\");\r\n  }, 0);\r\n\r\n  return <Test />;\r\n};\r\n\r\nexport default App\r\n```\r\n\r\nLink to code example:\r\n\r\n[Example](https://codesandbox.io/s/amazing-payne-4lmxo)\r\n\r\n## The current behavior\r\n\r\nThe message is not the same order;\r\n\r\n```jsx\r\n/**\r\n * chrome result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n\r\n/**\r\n * safari result\r\n *\r\n * timeout in render\r\n * useEffect in child\r\n */\r\n```\r\n\r\n## The expected behavior\r\n\r\nThe message should be same;\r\n\r\n```jsx\r\n/**\r\n * chrome result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n\r\n/**\r\n * safari result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n```<issue_closed>\n<issue_comment>username_0: In chrome and safari, the ```console.log``` was triggered in different order. [Example](https://codesandbox.io/s/amazing-payne-4lmxo) will show different result if opened in different browsers.\r\n\r\nReact version: 16.13.1\r\n\r\n## Steps To Reproduce\r\n\r\n1. Run following code in chrome and safari.\r\n2. Check console log\r\n\r\n```jsx\r\nimport React, { useState, useEffect } from \"react\";\r\n\r\nconst Test = props => {\r\n  useEffect(() => {\r\n    console.log(\"useEffect in child\");\r\n  }, []);\r\n\r\n  return \"Test useEffect\";\r\n};\r\n\r\nconst App = () => {\r\n  setTimeout(() => {\r\n    console.log(\"timeout in render\");\r\n  }, 0);\r\n\r\n  return <Test />;\r\n};\r\n\r\nexport default App\r\n```\r\n\r\nLink to code example:\r\n\r\n[Example](https://codesandbox.io/s/amazing-payne-4lmxo)\r\n\r\n## The current behavior\r\n\r\nThe message is not the same order;\r\n\r\n```jsx\r\n/**\r\n * chrome result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n\r\n/**\r\n * safari result\r\n *\r\n * timeout in render\r\n * useEffect in child\r\n */\r\n```\r\n\r\n## The expected behavior\r\n\r\nThe message should be same;\r\n\r\n```jsx\r\n/**\r\n * chrome result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n\r\n/**\r\n * safari result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n```\n<issue_comment>username_1: You do the side effect in the render method:\r\n```typescript\r\nconst App = () => {\r\n  setTimeout(() => {\r\n    console.log(\"timeout in render\");\r\n  }, 0);\r\n\r\n  return <Test />;\r\n};\r\n```\r\nwhich is not allowed. React doesn't guarantee this kind of orders. you should put `setTimeout` inside an useEffect.\n<issue_comment>username_0: Yes, i agree this is not allowed. But different browsers should have same result.I think react should achieve lifecycle with more stable browser interfaces.\n<issue_comment>username_2: Many reasons why this is not a bug:\r\n\r\n - you are not allowed to make side effects in render. If you do that, the outcome is essentially \"undefined behaviour\", that is, nothing is guaranteed\r\n\r\n- React provides no guarantees about timing of `useEffect` with respect to render anyway.\r\n\r\nSo while this is an inconsistency and it could be fixed, there is not good reason to do so.\n<issue_comment>username_3: It is not supported to call `setTimeout` during rendering. React provides absolutely no guarantees about the order or the number of component render calls  so you can't rely on subtle timing aspects of them.\r\n\r\nIf I wrap it in `useEffect` I don't observe the difference.<issue_closed>","repo":"facebook/react","issue_id":664173077,"issue_number":19438}
{"question":"Why might a behavior in React not be considered a bug according to the text?","answer":"The behavior may not be considered a bug because making side effects in render is not allowed in React, leading to undefined behavior. Additionally, React does not provide timing guarantees for useEffect relative to rendering, so inconsistencies may not necessarily be seen as bugs.","id":60,"text":"Many reasons why this is not a bug:\r\n\r\n - you are not allowed to make side effects in render. If you do that, the outcome is essentially \"undefined behaviour\", that is, nothing is guaranteed\r\n\r\n- React provides no guarantees about timing of `useEffect` with respect to render anyway.\r\n\r\nSo while this is an inconsistency and it could be fixed, there is not good reason to do so.","context":"<issue_start><issue_comment>Title: Bug: The trigger timing of \"useEffect\" is different in chrome and safari.\nusername_0: In chrome and safari, the ```console.log``` was triggered in different order. [Example](https://codesandbox.io/s/amazing-payne-4lmxo) will show different result if opened in different browsers.\r\n\r\nReact version: 16.13.1\r\n\r\n## Steps To Reproduce\r\n\r\n1. Run following code in chrome and safari.\r\n2. Check console log\r\n\r\n```jsx\r\nimport React, { useState, useEffect } from \"react\";\r\n\r\nconst Test = props => {\r\n  useEffect(() => {\r\n    console.log(\"useEffect in child\");\r\n  }, []);\r\n\r\n  return \"Test useEffect\";\r\n};\r\n\r\nconst App = () => {\r\n  setTimeout(() => {\r\n    console.log(\"timeout in render\");\r\n  }, 0);\r\n\r\n  return <Test />;\r\n};\r\n\r\nexport default App\r\n```\r\n\r\nLink to code example:\r\n\r\n[Example](https://codesandbox.io/s/amazing-payne-4lmxo)\r\n\r\n## The current behavior\r\n\r\nThe message is not the same order;\r\n\r\n```jsx\r\n/**\r\n * chrome result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n\r\n/**\r\n * safari result\r\n *\r\n * timeout in render\r\n * useEffect in child\r\n */\r\n```\r\n\r\n## The expected behavior\r\n\r\nThe message should be same;\r\n\r\n```jsx\r\n/**\r\n * chrome result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n\r\n/**\r\n * safari result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n```<issue_closed>\n<issue_comment>username_0: In chrome and safari, the ```console.log``` was triggered in different order. [Example](https://codesandbox.io/s/amazing-payne-4lmxo) will show different result if opened in different browsers.\r\n\r\nReact version: 16.13.1\r\n\r\n## Steps To Reproduce\r\n\r\n1. Run following code in chrome and safari.\r\n2. Check console log\r\n\r\n```jsx\r\nimport React, { useState, useEffect } from \"react\";\r\n\r\nconst Test = props => {\r\n  useEffect(() => {\r\n    console.log(\"useEffect in child\");\r\n  }, []);\r\n\r\n  return \"Test useEffect\";\r\n};\r\n\r\nconst App = () => {\r\n  setTimeout(() => {\r\n    console.log(\"timeout in render\");\r\n  }, 0);\r\n\r\n  return <Test />;\r\n};\r\n\r\nexport default App\r\n```\r\n\r\nLink to code example:\r\n\r\n[Example](https://codesandbox.io/s/amazing-payne-4lmxo)\r\n\r\n## The current behavior\r\n\r\nThe message is not the same order;\r\n\r\n```jsx\r\n/**\r\n * chrome result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n\r\n/**\r\n * safari result\r\n *\r\n * timeout in render\r\n * useEffect in child\r\n */\r\n```\r\n\r\n## The expected behavior\r\n\r\nThe message should be same;\r\n\r\n```jsx\r\n/**\r\n * chrome result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n\r\n/**\r\n * safari result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n```\n<issue_comment>username_1: You do the side effect in the render method:\r\n```typescript\r\nconst App = () => {\r\n  setTimeout(() => {\r\n    console.log(\"timeout in render\");\r\n  }, 0);\r\n\r\n  return <Test />;\r\n};\r\n```\r\nwhich is not allowed. React doesn't guarantee this kind of orders. you should put `setTimeout` inside an useEffect.\n<issue_comment>username_0: Yes, i agree this is not allowed. But different browsers should have same result.I think react should achieve lifecycle with more stable browser interfaces.\n<issue_comment>username_2: Many reasons why this is not a bug:\r\n\r\n - you are not allowed to make side effects in render. If you do that, the outcome is essentially \"undefined behaviour\", that is, nothing is guaranteed\r\n\r\n- React provides no guarantees about timing of `useEffect` with respect to render anyway.\r\n\r\nSo while this is an inconsistency and it could be fixed, there is not good reason to do so.\n<issue_comment>username_3: It is not supported to call `setTimeout` during rendering. React provides absolutely no guarantees about the order or the number of component render calls  so you can't rely on subtle timing aspects of them.\r\n\r\nIf I wrap it in `useEffect` I don't observe the difference.<issue_closed>","repo":"facebook/react","issue_id":664173077,"issue_number":19438}
{"question":"Why is it not supported to call setTimeout during rendering in React?","answer":"Calling setTimeout during rendering in React is not supported because React provides no guarantees about the order or number of component render calls, making it unreliable to rely on subtle timing aspects.","id":61,"text":"It is not supported to call `setTimeout` during rendering. React provides absolutely no guarantees about the order or the number of component render calls  so you can't rely on subtle timing aspects of them.\r\n\r\nIf I wrap it in `useEffect` I don't observe the difference.","context":"<issue_start><issue_comment>Title: Bug: The trigger timing of \"useEffect\" is different in chrome and safari.\nusername_0: In chrome and safari, the ```console.log``` was triggered in different order. [Example](https://codesandbox.io/s/amazing-payne-4lmxo) will show different result if opened in different browsers.\r\n\r\nReact version: 16.13.1\r\n\r\n## Steps To Reproduce\r\n\r\n1. Run following code in chrome and safari.\r\n2. Check console log\r\n\r\n```jsx\r\nimport React, { useState, useEffect } from \"react\";\r\n\r\nconst Test = props => {\r\n  useEffect(() => {\r\n    console.log(\"useEffect in child\");\r\n  }, []);\r\n\r\n  return \"Test useEffect\";\r\n};\r\n\r\nconst App = () => {\r\n  setTimeout(() => {\r\n    console.log(\"timeout in render\");\r\n  }, 0);\r\n\r\n  return <Test />;\r\n};\r\n\r\nexport default App\r\n```\r\n\r\nLink to code example:\r\n\r\n[Example](https://codesandbox.io/s/amazing-payne-4lmxo)\r\n\r\n## The current behavior\r\n\r\nThe message is not the same order;\r\n\r\n```jsx\r\n/**\r\n * chrome result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n\r\n/**\r\n * safari result\r\n *\r\n * timeout in render\r\n * useEffect in child\r\n */\r\n```\r\n\r\n## The expected behavior\r\n\r\nThe message should be same;\r\n\r\n```jsx\r\n/**\r\n * chrome result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n\r\n/**\r\n * safari result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n```<issue_closed>\n<issue_comment>username_0: In chrome and safari, the ```console.log``` was triggered in different order. [Example](https://codesandbox.io/s/amazing-payne-4lmxo) will show different result if opened in different browsers.\r\n\r\nReact version: 16.13.1\r\n\r\n## Steps To Reproduce\r\n\r\n1. Run following code in chrome and safari.\r\n2. Check console log\r\n\r\n```jsx\r\nimport React, { useState, useEffect } from \"react\";\r\n\r\nconst Test = props => {\r\n  useEffect(() => {\r\n    console.log(\"useEffect in child\");\r\n  }, []);\r\n\r\n  return \"Test useEffect\";\r\n};\r\n\r\nconst App = () => {\r\n  setTimeout(() => {\r\n    console.log(\"timeout in render\");\r\n  }, 0);\r\n\r\n  return <Test />;\r\n};\r\n\r\nexport default App\r\n```\r\n\r\nLink to code example:\r\n\r\n[Example](https://codesandbox.io/s/amazing-payne-4lmxo)\r\n\r\n## The current behavior\r\n\r\nThe message is not the same order;\r\n\r\n```jsx\r\n/**\r\n * chrome result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n\r\n/**\r\n * safari result\r\n *\r\n * timeout in render\r\n * useEffect in child\r\n */\r\n```\r\n\r\n## The expected behavior\r\n\r\nThe message should be same;\r\n\r\n```jsx\r\n/**\r\n * chrome result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n\r\n/**\r\n * safari result\r\n *\r\n * useEffect in child\r\n * timeout in render\r\n */\r\n```\n<issue_comment>username_1: You do the side effect in the render method:\r\n```typescript\r\nconst App = () => {\r\n  setTimeout(() => {\r\n    console.log(\"timeout in render\");\r\n  }, 0);\r\n\r\n  return <Test />;\r\n};\r\n```\r\nwhich is not allowed. React doesn't guarantee this kind of orders. you should put `setTimeout` inside an useEffect.\n<issue_comment>username_0: Yes, i agree this is not allowed. But different browsers should have same result.I think react should achieve lifecycle with more stable browser interfaces.\n<issue_comment>username_2: Many reasons why this is not a bug:\r\n\r\n - you are not allowed to make side effects in render. If you do that, the outcome is essentially \"undefined behaviour\", that is, nothing is guaranteed\r\n\r\n- React provides no guarantees about timing of `useEffect` with respect to render anyway.\r\n\r\nSo while this is an inconsistency and it could be fixed, there is not good reason to do so.\n<issue_comment>username_3: It is not supported to call `setTimeout` during rendering. React provides absolutely no guarantees about the order or the number of component render calls  so you can't rely on subtle timing aspects of them.\r\n\r\nIf I wrap it in `useEffect` I don't observe the difference.<issue_closed>","repo":"facebook/react","issue_id":664173077,"issue_number":19438}
{"question":"What changes were made to support vmap in the accumulate_grad code?","answer":"Changes made to support vmap in the accumulate_grad code include: adding vmap support for Tensor::strides(), changing an instance of empty_strided to new_empty_strided, and replacing an in-place operation in accumulate_grad.h.","id":62,"text":"Stack from [ghstack](https://github.com/username_1/ghstack):\n* #49120 Add batched grad testing to gradcheck, turn it on in test_autograd\n* **#49119 Update accumulate_grad to support vmap**\n* #49118 Move inplace_is_vmap_compatible to BatchedTensorImpl.h\n\nI don't know how the accumulate_grad code gets hit, so I went through\nall places in accumulate_grad that are definitely impossible to vmap\nthrough and changed them.\n\nTo support this:\n- I added vmap support for Tensor::strides(). It returns the strides\nthat correspond to the public dimensions of the tensor (not the ones\nbeing vmapped over).\n- Changed an instance of empty_strided to new_empty_strided.\n- Replaced an in-place operation in accumulate_grad.h\n\nTest Plan:\n- added a test for calling strides() inside of vmap\n- added a test that exercise some of the accumulate grad code path. I\ndon't know if it exercises all of the code changes in this PR.\nSuggestions for more test cases are very welcome.","context":"<issue_start><issue_comment>Title: Update accumulate_grad to support vmap\nusername_0: Stack from [ghstack](https://github.com/username_1/ghstack):\n* #49120 Add batched grad testing to gradcheck, turn it on in test_autograd\n* **#49119 Update accumulate_grad to support vmap**\n* #49118 Move inplace_is_vmap_compatible to BatchedTensorImpl.h\n\nI don't know how the accumulate_grad code gets hit, so I went through\nall places in accumulate_grad that are definitely impossible to vmap\nthrough and changed them.\n\nTo support this:\n- I added vmap support for Tensor::strides(). It returns the strides\nthat correspond to the public dimensions of the tensor (not the ones\nbeing vmapped over).\n- Changed an instance of empty_strided to new_empty_strided.\n- Replaced an in-place operation in accumulate_grad.h\n\nTest Plan:\n- added a test for calling strides() inside of vmap\n- added a test that exercise some of the accumulate grad code path. I\ndon't know if it exercises all of the code changes in this PR.\nSuggestions for more test cases are very welcome.\n<issue_comment>username_1: So, now that vmap supports strides, there was something I wasn't too clear about. Suppose that I have a contiguous tensor with strides (8, 4, 2, 1) and let's say I vmap with batch dim = 2 (e.g., not the first dimension). Is the BatchedTensor contiguous? (I don't think it should be!) I couldn't tell from this PR what would happen in this case.\n<issue_comment>username_0: It's not contiguous. The reason behind this is asking \"is a BatchedTensor contiguous\" is equivalent to asking \"are the per-examples of the BatchedTensor contiguous\". The per-examples in your example would have strides (8, 4, 1) which make them not contiguous","repo":"pytorch/pytorch","issue_id":760718102,"issue_number":49119}
{"question":"If a contiguous tensor with strides (8, 4, 2, 1) is vmap-ed with batch dim = 2 (not the first dimension), would the BatchedTensor be contiguous?","answer":"No, the BatchedTensor would not be contiguous in this case.","id":63,"text":"So, now that vmap supports strides, there was something I wasn't too clear about. Suppose that I have a contiguous tensor with strides (8, 4, 2, 1) and let's say I vmap with batch dim = 2 (e.g., not the first dimension). Is the BatchedTensor contiguous? (I don't think it should be!) I couldn't tell from this PR what would happen in this case.","context":"<issue_start><issue_comment>Title: Update accumulate_grad to support vmap\nusername_0: Stack from [ghstack](https://github.com/username_1/ghstack):\n* #49120 Add batched grad testing to gradcheck, turn it on in test_autograd\n* **#49119 Update accumulate_grad to support vmap**\n* #49118 Move inplace_is_vmap_compatible to BatchedTensorImpl.h\n\nI don't know how the accumulate_grad code gets hit, so I went through\nall places in accumulate_grad that are definitely impossible to vmap\nthrough and changed them.\n\nTo support this:\n- I added vmap support for Tensor::strides(). It returns the strides\nthat correspond to the public dimensions of the tensor (not the ones\nbeing vmapped over).\n- Changed an instance of empty_strided to new_empty_strided.\n- Replaced an in-place operation in accumulate_grad.h\n\nTest Plan:\n- added a test for calling strides() inside of vmap\n- added a test that exercise some of the accumulate grad code path. I\ndon't know if it exercises all of the code changes in this PR.\nSuggestions for more test cases are very welcome.\n<issue_comment>username_1: So, now that vmap supports strides, there was something I wasn't too clear about. Suppose that I have a contiguous tensor with strides (8, 4, 2, 1) and let's say I vmap with batch dim = 2 (e.g., not the first dimension). Is the BatchedTensor contiguous? (I don't think it should be!) I couldn't tell from this PR what would happen in this case.\n<issue_comment>username_0: It's not contiguous. The reason behind this is asking \"is a BatchedTensor contiguous\" is equivalent to asking \"are the per-examples of the BatchedTensor contiguous\". The per-examples in your example would have strides (8, 4, 1) which make them not contiguous","repo":"pytorch/pytorch","issue_id":760718102,"issue_number":49119}
{"question":"Are the per-examples of a BatchedTensor contiguous?","answer":"No, the per-examples of a BatchedTensor are not contiguous, as they have strides of (8, 4, 1) which make them non-contiguous.","id":64,"text":"It's not contiguous. The reason behind this is asking \"is a BatchedTensor contiguous\" is equivalent to asking \"are the per-examples of the BatchedTensor contiguous\". The per-examples in your example would have strides (8, 4, 1) which make them not contiguous","context":"<issue_start><issue_comment>Title: Update accumulate_grad to support vmap\nusername_0: Stack from [ghstack](https://github.com/username_1/ghstack):\n* #49120 Add batched grad testing to gradcheck, turn it on in test_autograd\n* **#49119 Update accumulate_grad to support vmap**\n* #49118 Move inplace_is_vmap_compatible to BatchedTensorImpl.h\n\nI don't know how the accumulate_grad code gets hit, so I went through\nall places in accumulate_grad that are definitely impossible to vmap\nthrough and changed them.\n\nTo support this:\n- I added vmap support for Tensor::strides(). It returns the strides\nthat correspond to the public dimensions of the tensor (not the ones\nbeing vmapped over).\n- Changed an instance of empty_strided to new_empty_strided.\n- Replaced an in-place operation in accumulate_grad.h\n\nTest Plan:\n- added a test for calling strides() inside of vmap\n- added a test that exercise some of the accumulate grad code path. I\ndon't know if it exercises all of the code changes in this PR.\nSuggestions for more test cases are very welcome.\n<issue_comment>username_1: So, now that vmap supports strides, there was something I wasn't too clear about. Suppose that I have a contiguous tensor with strides (8, 4, 2, 1) and let's say I vmap with batch dim = 2 (e.g., not the first dimension). Is the BatchedTensor contiguous? (I don't think it should be!) I couldn't tell from this PR what would happen in this case.\n<issue_comment>username_0: It's not contiguous. The reason behind this is asking \"is a BatchedTensor contiguous\" is equivalent to asking \"are the per-examples of the BatchedTensor contiguous\". The per-examples in your example would have strides (8, 4, 1) which make them not contiguous","repo":"pytorch/pytorch","issue_id":760718102,"issue_number":49119}
{"question":"What content would be beneficial to add to the React documentation based on the provided URLs and the mentioned addon example?","answer":"The React documentation could benefit from including information about the addons mentioned in the provided URLs and examples of utilizing the ReactRAFBatchingStrategy.js addon or a similar one.","id":65,"text":"I didn't realise this was in the addons bundle, would be good to get it added to the docs.\r\n\r\nhttp://facebook.github.io/react/docs/addons.html\r\nhttps://github.com/facebook/react/blob/master/src/browser/ReactWithAddons.js#L40\r\n\r\nWould be good to also include an example of using https://github.com/facebook/react/blob/master/src/addons/ReactRAFBatchingStrategy.js or something similar.","context":"<issue_start><issue_comment>Title: Document React.addons.batchedUpdates\nusername_0: I didn't realise this was in the addons bundle, would be good to get it added to the docs.\r\n\r\nhttp://facebook.github.io/react/docs/addons.html\r\nhttps://github.com/facebook/react/blob/master/src/browser/ReactWithAddons.js#L40\r\n\r\nWould be good to also include an example of using https://github.com/facebook/react/blob/master/src/addons/ReactRAFBatchingStrategy.js or something similar.\n<issue_comment>username_1: rAF batching isn't supported and won't go in the docs, but batchedUpdates should.\n<issue_comment>username_2: There are also known issues with rAF batching (mostly around controlled components).\n<issue_comment>username_3: Should we deprecate and eventually remove the RAF batching?\n<issue_comment>username_1: It's never been supported in any fashion.\n<issue_comment>username_1: (i.e., it's already deprecated?)\n<issue_comment>username_4: shouldn't this file be deleted in that case?\r\nhttps://github.com/facebook/react/blob/master/src/addons/ReactRAFBatchingStrategy.js\n<issue_comment>username_1: That's not exposed as a public API in any of our builds, but sure, we can delete it  PR incoming.\n<issue_comment>username_5: Killed in https://github.com/facebook/react/pull/6016<issue_closed>\n<issue_comment>username_1: We should document ReactDOM.unstable_batchedUpdates.\n<issue_comment>username_1: I didn't realise this was in the addons bundle, would be good to get it added to the docs.\r\n\r\nhttp://facebook.github.io/react/docs/addons.html\r\nhttps://github.com/facebook/react/blob/master/src/browser/ReactWithAddons.js#L40\r\n\r\nWould be good to also include an example of using https://github.com/facebook/react/blob/master/src/addons/ReactRAFBatchingStrategy.js or something similar.\n<issue_comment>username_5: @username_1 Since when did we start documenting `unstable_` features?  Haven't we always left them intentionally undocumented?  We also have unstable_renderSubtree and unstable_handleError and others, but I thought the decision was to not document them until they become stable.  We document them as part of the release process.\n<issue_comment>username_1: Sure.<issue_closed>","repo":"facebook/react","issue_id":65878895,"issue_number":3570}
{"question":"What functions are being used in UpSampleBicubic2d according to the provided GitHub links?","answer":"UpSampleBicubic2d is using upsample_get_value_bounded and upsample_increment_value_bounded based on the GitHub links provided.","id":156,"text":"it seems just UpSampleBicubic2d is using upsample_get_value_bounded (https://github.com/pytorch/pytorch/pull/19630/files#diff-5092da792c30694ee4adf0d0ae2a37c6R171) and upsample_increment_value_bounded (https://github.com/pytorch/pytorch/pull/19630/files#diff-5092da792c30694ee4adf0d0ae2a37c6R191)\r\n\r\nmaybe the problem could be inside one of these functions ... maybe related to the order of indexes (x, y) ...\r\n\r\nbut the problem seems to be related to cuda block/threads ... so not sure if it is really related to these functions.","context":"<issue_start><issue_comment>Title: [WIP] UpSample GPU Porting \nusername_0: resolves #16158\n<issue_comment>username_1: Fewer errors than before, 4 instead of 8 with the same message:\r\n```\r\nRuntimeError: CUDA error: too many resources requested for launch\r\n```\r\nThis seems to be an existing problem: gh-8103.\r\n\r\n@username_2 could you have a look at this and tell us what you think?\n<issue_comment>username_2: Looking at it very briefly, the `Jetson TX` referenced in the issue only has 256 cores.\r\n\r\nSo while the issue looks the same, I'd probably not expect it on the CI platforms.  FWIW, some recent CI tests in other issues are green.\n<issue_comment>username_2: @pytorchbot retest this please.\n<issue_comment>username_1: same failures.\n<issue_comment>username_2: Is this rebased on the latest master (you can also ask the bot to rebase)?\n<issue_comment>username_0: I rebased that yesterday ... also needed to resolve conflicts because 7 days ago upsample files were changed.\n<issue_comment>username_0: @username_2 do you have any idea about what could be this problem? or a way to debug that?\r\nalso it seems some jobs is taking a lot of time to build, for example, for some job the estimate time is 19h .. not sure if it the regular estimate ..\n<issue_comment>username_2: @username_0 If you can't reproduce it at home it seems hard to debug other than reading at the diffs again.  I can take a look on Monday, it's getting a bit late here.\n<issue_comment>username_2: Also, has anyone found a way to show the actual hardware used on the CI in detail?\n<issue_comment>username_0: @username_2 I am working in parallel on a paperspace environment .. it tooks a lot of time it is running with 8 cpu cores.\n<issue_comment>username_2: But have you ever been able to reproduce this issue on paperspace?\n<issue_comment>username_1: I can reproduce it locally: Arch Linux, CUDA 10.0, RTX2070 GPU. Given the CI failures, I think it should be reproducible for multiple CUDA and GPU versions. I just haven't worked on any CUDA code before, and am short on time, so I'd rather not dig too deep.\n<issue_comment>username_0: not yet .. my last building was with a my previous commit ... I will work again in this task in some minutes :)\n<issue_comment>username_1: @username_0 your previous commit had the same failure though (except for the less clear exception message), and it seems quite reproducible. So I think you'll see it now.\n<issue_comment>username_2: Ah, thanks.  @username_0, then I guess just compiling with \"DEBUG=1\" and stepping through the offending code with gdb may be the fastest way.\n<issue_comment>username_0: @username_2 thanks! I will try that!\n<issue_comment>username_0: it seems just UpSampleBicubic2d is using upsample_get_value_bounded (https://github.com/pytorch/pytorch/pull/19630/files#diff-5092da792c30694ee4adf0d0ae2a37c6R171) and upsample_increment_value_bounded (https://github.com/pytorch/pytorch/pull/19630/files#diff-5092da792c30694ee4adf0d0ae2a37c6R191)\r\n\r\nmaybe the problem could be inside one of these functions ... maybe related to the order of indexes (x, y) ...\r\n\r\nbut the problem seems to be related to cuda block/threads ... so not sure if it is really related to these functions.\n<issue_comment>username_2: The new code uses far more registers than the existing one. I verified that the both versions actually call the offending test case with `1024` in `blockDim`.  So it's very likely a register issue.\r\n\r\n`Existing` uses `64` registers, which seems to be optimal or my card:\r\n\r\n```\r\nptxas info    : 77696 bytes gmem, 72 bytes cmem[3]\r\nptxas info    : Compiling entry function '_Z23bicubic_interp2d_kerneliddb15THCDeviceTensorIdLi4Ei16DefaultPtrTraitsES1_' for 'sm_61'\r\nptxas info    : Function properties for _Z23bicubic_interp2d_kerneliddb15THCDeviceTensorIdLi4Ei16DefaultPtrTraitsES1_\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 64 registers, 432 bytes cmem[0], 24 bytes cmem[2]\r\n```\r\n\r\n`New` uses `124` registers, which is too much for my card:\r\n\r\n```\r\nptxas info    : 77696 bytes gmem, 72 bytes cmem[3]\r\nptxas info    : Compiling entry function '_ZN2at6native76_GLOBAL__N__52_tmpxft_0000626d_00000000_6_UpSampleBicubic2d_cpp1_ii_b4c1e1f328upsample_bicubic2d_out_frameElddbNS_20PackedTensorAccessorIdLm4ENS_16DefaultPtrTraitsElEES4_' for 'sm_61'\r\nptxas info    : Function properties for _ZN2at6native76_GLOBAL__N__52_tmpxft_0000626d_00000000_6_UpSampleBicubic2d_cpp1_ii_b4c1e1f328upsample_bicubic2d_out_frameElddbNS_20PackedTensorAccessorIdLm4ENS_16DefaultPtrTraitsElEES4_\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 124 registers, 496 bytes cmem[0], 24 bytes cmem[2]\r\n```\r\n\r\nWith `__launch_bounds__(1024)`, the code again uses `64` registers.\r\n\r\nIf you use `C10_LAUNCH_BOUNDS_1(1024)` **for both kernels**, the tests pass here.\r\n\r\n\r\nNow *why* is the regcount higher in the new code? It could be `PackedTensorAccessor`, it could be the fact that many instances of `int` have been changed to `int64_t`. :)\r\n\r\nYou could experiment or just use the launch bounds.  Other code in `native/cuda` seems to use lower bounds for `blockDim`, too.  1024 seems to be an outlier.\n<issue_comment>username_0: @username_2 \r\n\r\nit seems it worked locally! thank you so much!  I really appreciate that!\n<issue_comment>username_0: thanks @username_1 and @username_2 for all the help!\r\n\r\n@username_3 it is done for review!\n<issue_comment>username_3: CircleCI runs on AWS, and the I'm fairly sure we've got one of the g3 types, either g3.4xlarge or g3.8xlarge\n<issue_comment>username_3: cc @ngimel, if you want to take a look at this (I'm planning to do a review too)\n<issue_comment>username_0: @username_4 could it be addressed in another PR?\n<issue_comment>username_4: @username_0 Sure, it's definitely better to rename both cpu & gpu impls together in a separate PR. Just want to make sure we have the correct one after porting.\n<issue_comment>username_3: Just as a clarification: I'm OK with having changes which are improvements over the old kernels come in latter patches.\n<issue_comment>username_3: I finished reviewing one of the kernels, and did a cursory scan of the others. In general the patch looks like it's in good shape. I'd suggest that we fix up the major correctness issues (PackedTensorAccessor ref, and int32 versus int64 indexing) and anything small that is easy to fix (for example, double zeroing the array), and leave other improvements for follow ups. Should be ready to land soon!\n<issue_comment>username_0: thanks so much @username_3!\r\nI will let you know when it is ready again for a new review! thanks!\n<issue_comment>username_0: @username_2 @username_3 \r\nnot sure but the errors on CI seems to be related to jenkins ... it seems all these jobs that failed (for building) ran by 01h 01min ... not sure if it is a coincidence ...\n<issue_comment>username_1: @username_0 indeed it looks like all jobs were aborted at the same time. no obvious issues in the build log related to your code. Comparing e.g. the `cuda9-cudnn7` build with a successful one from another PR, it takes 1hr 14min there and your build is about at the place where the other one is after an hour.\r\n\r\nI suggest to just push a new commit to rebuild. Probably a temporary CI hiccup.\n<issue_comment>username_3: I accidentally rebooted Jenkins yesterday which is the likely cause, my apologies.\r\n\r\n@pytorchbot retest this please\n<issue_comment>username_0: @username_3 @username_2 @username_1 \r\n\r\nall tests passed except `pr/caffe2-py2-cuda9.0-cudnn7-windows-build`:\r\n \r\n```\r\n14:43:49 Build timed out (after 180 minutes). Marking the build as failed.\r\n14:43:49 Build was aborted\r\n14:43:49 [BFA] Scanning build for known causes...\r\n14:43:49 [BFA] No failure causes found\r\n14:43:49 [BFA] Done. 0s\r\n14:43:49 Finished: FAILURE\r\n```\r\n\r\nnot sure if this timeout means that the code now is slower.\r\n\r\nwhat do you think? do you have any suggestion?\n<issue_comment>username_3: Sometimes the Windows build flakes out like that. It didn't timeout while running a relevant test, so I judge it to be not your problem.\n<issue_comment>username_3: The launch bounds logic is wrong, but I acknowledge that this is a big patch already; just fix it in a follow up. I am going to go ahead and land this.\n<issue_comment>username_0: sounds good @username_3 thanks!","repo":"pytorch/pytorch","issue_id":436372857,"issue_number":19630}
{"question":"What content should be added to the docs related to the addons bundle mentioned in the text?","answer":"The specific content that needs to be added to the docs is provided in the link: http://facebook.github.io/react/docs/addons.html","id":75,"text":"I didn't realise this was in the addons bundle, would be good to get it added to the docs.\r\n\r\nhttp://facebook.github.io/react/docs/addons.html\r\nhttps://github.com/facebook/react/blob/master/src/browser/ReactWithAddons.js#L40\r\n\r\nWould be good to also include an example of using https://github.com/facebook/react/blob/master/src/addons/ReactRAFBatchingStrategy.js or something similar.","context":"<issue_start><issue_comment>Title: Document React.addons.batchedUpdates\nusername_0: I didn't realise this was in the addons bundle, would be good to get it added to the docs.\r\n\r\nhttp://facebook.github.io/react/docs/addons.html\r\nhttps://github.com/facebook/react/blob/master/src/browser/ReactWithAddons.js#L40\r\n\r\nWould be good to also include an example of using https://github.com/facebook/react/blob/master/src/addons/ReactRAFBatchingStrategy.js or something similar.\n<issue_comment>username_1: rAF batching isn't supported and won't go in the docs, but batchedUpdates should.\n<issue_comment>username_2: There are also known issues with rAF batching (mostly around controlled components).\n<issue_comment>username_3: Should we deprecate and eventually remove the RAF batching?\n<issue_comment>username_1: It's never been supported in any fashion.\n<issue_comment>username_1: (i.e., it's already deprecated?)\n<issue_comment>username_4: shouldn't this file be deleted in that case?\r\nhttps://github.com/facebook/react/blob/master/src/addons/ReactRAFBatchingStrategy.js\n<issue_comment>username_1: That's not exposed as a public API in any of our builds, but sure, we can delete it  PR incoming.\n<issue_comment>username_5: Killed in https://github.com/facebook/react/pull/6016<issue_closed>\n<issue_comment>username_1: We should document ReactDOM.unstable_batchedUpdates.\n<issue_comment>username_1: I didn't realise this was in the addons bundle, would be good to get it added to the docs.\r\n\r\nhttp://facebook.github.io/react/docs/addons.html\r\nhttps://github.com/facebook/react/blob/master/src/browser/ReactWithAddons.js#L40\r\n\r\nWould be good to also include an example of using https://github.com/facebook/react/blob/master/src/addons/ReactRAFBatchingStrategy.js or something similar.\n<issue_comment>username_5: @username_1 Since when did we start documenting `unstable_` features?  Haven't we always left them intentionally undocumented?  We also have unstable_renderSubtree and unstable_handleError and others, but I thought the decision was to not document them until they become stable.  We document them as part of the release process.\n<issue_comment>username_1: Sure.<issue_closed>","repo":"facebook/react","issue_id":65878895,"issue_number":3570}
{"question":"Why were `unstable_` features like `unstable_renderSubtree` and `unstable_handleError` not documented until they become stable?","answer":"The decision was made to not document `unstable_` features like `unstable_renderSubtree` and `unstable_handleError` until they become stable. These features are documented as part of the release process to ensure that only stable features are officially documented and released to users.","id":76,"text":"@username_1 Since when did we start documenting `unstable_` features?  Haven't we always left them intentionally undocumented?  We also have unstable_renderSubtree and unstable_handleError and others, but I thought the decision was to not document them until they become stable.  We document them as part of the release process.","context":"<issue_start><issue_comment>Title: Document React.addons.batchedUpdates\nusername_0: I didn't realise this was in the addons bundle, would be good to get it added to the docs.\r\n\r\nhttp://facebook.github.io/react/docs/addons.html\r\nhttps://github.com/facebook/react/blob/master/src/browser/ReactWithAddons.js#L40\r\n\r\nWould be good to also include an example of using https://github.com/facebook/react/blob/master/src/addons/ReactRAFBatchingStrategy.js or something similar.\n<issue_comment>username_1: rAF batching isn't supported and won't go in the docs, but batchedUpdates should.\n<issue_comment>username_2: There are also known issues with rAF batching (mostly around controlled components).\n<issue_comment>username_3: Should we deprecate and eventually remove the RAF batching?\n<issue_comment>username_1: It's never been supported in any fashion.\n<issue_comment>username_1: (i.e., it's already deprecated?)\n<issue_comment>username_4: shouldn't this file be deleted in that case?\r\nhttps://github.com/facebook/react/blob/master/src/addons/ReactRAFBatchingStrategy.js\n<issue_comment>username_1: That's not exposed as a public API in any of our builds, but sure, we can delete it  PR incoming.\n<issue_comment>username_5: Killed in https://github.com/facebook/react/pull/6016<issue_closed>\n<issue_comment>username_1: We should document ReactDOM.unstable_batchedUpdates.\n<issue_comment>username_1: I didn't realise this was in the addons bundle, would be good to get it added to the docs.\r\n\r\nhttp://facebook.github.io/react/docs/addons.html\r\nhttps://github.com/facebook/react/blob/master/src/browser/ReactWithAddons.js#L40\r\n\r\nWould be good to also include an example of using https://github.com/facebook/react/blob/master/src/addons/ReactRAFBatchingStrategy.js or something similar.\n<issue_comment>username_5: @username_1 Since when did we start documenting `unstable_` features?  Haven't we always left them intentionally undocumented?  We also have unstable_renderSubtree and unstable_handleError and others, but I thought the decision was to not document them until they become stable.  We document them as part of the release process.\n<issue_comment>username_1: Sure.<issue_closed>","repo":"facebook/react","issue_id":65878895,"issue_number":3570}
{"question":"What bug is reported by the user regarding the React package versions 16.6.2 and 16.6.3?","answer":"The user reported a bug where the comment at the top of the React package files for versions 16.6.2 and 16.6.3 incorrectly states the version as React 16.6.1 instead of the actual version.","id":78,"text":"<!--\r\n  Note: if the issue is about documentation or the website, please file it at:\r\n  https://github.com/reactjs/reactjs.org/issues/new\r\n-->\r\n\r\n**Do you want to request a *feature* or report a *bug*?** bug\r\n\r\n**What is the current behavior?** The react@16.6.3 package has a comment at the top claiming to be React 16.6.1.\r\n\r\n**If the current behavior is a bug, please provide the steps to reproduce and if possible a minimal demo of the problem. Your bug will get fixed much faster if we can run your code and it doesn't have dependencies other than React. Paste the link to your JSFiddle (https://jsfiddle.net/Luktwrdm/) or CodeSandbox (https://codesandbox.io/s/new) example below:**\r\n```shell\r\ncd /tmp\r\nnpm pack react@16.6.3\r\ntar xf react-16.6.3.tgz\r\nhead -1 package/cjs/react.development.js\r\n# output: /** @license React v16.6.1\r\n```\r\n\r\n**What is the expected behavior?**\r\nThe comment at the top of the build indicates the correct version.\r\n\r\n**Which versions of React, and which browser / OS are affected by this issue? Did this work in previous versions of React?**\r\n16.6.2 and 16.6.3 both have the wrong comment. 16.6.1 and 16.6.0 are correct.","context":"<issue_start><issue_comment>Title: Comments in NPM package have wrong version\nusername_0: <!--\r\n  Note: if the issue is about documentation or the website, please file it at:\r\n  https://github.com/reactjs/reactjs.org/issues/new\r\n-->\r\n\r\n**Do you want to request a *feature* or report a *bug*?** bug\r\n\r\n**What is the current behavior?** The react@16.6.3 package has a comment at the top claiming to be React 16.6.1.\r\n\r\n**If the current behavior is a bug, please provide the steps to reproduce and if possible a minimal demo of the problem. Your bug will get fixed much faster if we can run your code and it doesn't have dependencies other than React. Paste the link to your JSFiddle (https://jsfiddle.net/Luktwrdm/) or CodeSandbox (https://codesandbox.io/s/new) example below:**\r\n```shell\r\ncd /tmp\r\nnpm pack react@16.6.3\r\ntar xf react-16.6.3.tgz\r\nhead -1 package/cjs/react.development.js\r\n# output: /** @license React v16.6.1\r\n```\r\n\r\n**What is the expected behavior?**\r\nThe comment at the top of the build indicates the correct version.\r\n\r\n**Which versions of React, and which browser / OS are affected by this issue? Did this work in previous versions of React?**\r\n16.6.2 and 16.6.3 both have the wrong comment. 16.6.1 and 16.6.0 are correct.\n<issue_comment>username_0: Also, https://github.com/facebook/react/blob/v16.6.3/packages/shared/ReactVersion.js claims to be `16.6.1`, but in the actual NPM package ReactVersion is set correctly.\n<issue_comment>username_1: @username_2 should this be handled as part of the release flow you've been working on?<issue_closed>\n<issue_comment>username_2: This wasa transient issue, caused by a somewhat manual release that Andrew ran. (There's no bug in the release script.) This issue should only affect 16.6.2 and 16.6.3. Fortunately it shouldn't cause any observable behavior unless someone reads the comments. ","repo":"facebook/react","issue_id":389079792,"issue_number":14410}
{"question":"What version of React is claimed to be in https://github.com/facebook/react/blob/v16.6.3/packages/shared/ReactVersion.js?","answer":"16.6.1","id":79,"text":"Also, https://github.com/facebook/react/blob/v16.6.3/packages/shared/ReactVersion.js claims to be `16.6.1`, but in the actual NPM package ReactVersion is set correctly.","context":"<issue_start><issue_comment>Title: Comments in NPM package have wrong version\nusername_0: <!--\r\n  Note: if the issue is about documentation or the website, please file it at:\r\n  https://github.com/reactjs/reactjs.org/issues/new\r\n-->\r\n\r\n**Do you want to request a *feature* or report a *bug*?** bug\r\n\r\n**What is the current behavior?** The react@16.6.3 package has a comment at the top claiming to be React 16.6.1.\r\n\r\n**If the current behavior is a bug, please provide the steps to reproduce and if possible a minimal demo of the problem. Your bug will get fixed much faster if we can run your code and it doesn't have dependencies other than React. Paste the link to your JSFiddle (https://jsfiddle.net/Luktwrdm/) or CodeSandbox (https://codesandbox.io/s/new) example below:**\r\n```shell\r\ncd /tmp\r\nnpm pack react@16.6.3\r\ntar xf react-16.6.3.tgz\r\nhead -1 package/cjs/react.development.js\r\n# output: /** @license React v16.6.1\r\n```\r\n\r\n**What is the expected behavior?**\r\nThe comment at the top of the build indicates the correct version.\r\n\r\n**Which versions of React, and which browser / OS are affected by this issue? Did this work in previous versions of React?**\r\n16.6.2 and 16.6.3 both have the wrong comment. 16.6.1 and 16.6.0 are correct.\n<issue_comment>username_0: Also, https://github.com/facebook/react/blob/v16.6.3/packages/shared/ReactVersion.js claims to be `16.6.1`, but in the actual NPM package ReactVersion is set correctly.\n<issue_comment>username_1: @username_2 should this be handled as part of the release flow you've been working on?<issue_closed>\n<issue_comment>username_2: This wasa transient issue, caused by a somewhat manual release that Andrew ran. (There's no bug in the release script.) This issue should only affect 16.6.2 and 16.6.3. Fortunately it shouldn't cause any observable behavior unless someone reads the comments. ","repo":"facebook/react","issue_id":389079792,"issue_number":14410}
{"question":"What caused the transient issue in versions 16.6.2 and 16.6.3?","answer":"The transient issue was caused by a manual release run by Andrew, not a bug in the release script.","id":81,"text":"This wasa transient issue, caused by a somewhat manual release that Andrew ran. (There's no bug in the release script.) This issue should only affect 16.6.2 and 16.6.3. Fortunately it shouldn't cause any observable behavior unless someone reads the comments. ","context":"<issue_start><issue_comment>Title: Comments in NPM package have wrong version\nusername_0: <!--\r\n  Note: if the issue is about documentation or the website, please file it at:\r\n  https://github.com/reactjs/reactjs.org/issues/new\r\n-->\r\n\r\n**Do you want to request a *feature* or report a *bug*?** bug\r\n\r\n**What is the current behavior?** The react@16.6.3 package has a comment at the top claiming to be React 16.6.1.\r\n\r\n**If the current behavior is a bug, please provide the steps to reproduce and if possible a minimal demo of the problem. Your bug will get fixed much faster if we can run your code and it doesn't have dependencies other than React. Paste the link to your JSFiddle (https://jsfiddle.net/Luktwrdm/) or CodeSandbox (https://codesandbox.io/s/new) example below:**\r\n```shell\r\ncd /tmp\r\nnpm pack react@16.6.3\r\ntar xf react-16.6.3.tgz\r\nhead -1 package/cjs/react.development.js\r\n# output: /** @license React v16.6.1\r\n```\r\n\r\n**What is the expected behavior?**\r\nThe comment at the top of the build indicates the correct version.\r\n\r\n**Which versions of React, and which browser / OS are affected by this issue? Did this work in previous versions of React?**\r\n16.6.2 and 16.6.3 both have the wrong comment. 16.6.1 and 16.6.0 are correct.\n<issue_comment>username_0: Also, https://github.com/facebook/react/blob/v16.6.3/packages/shared/ReactVersion.js claims to be `16.6.1`, but in the actual NPM package ReactVersion is set correctly.\n<issue_comment>username_1: @username_2 should this be handled as part of the release flow you've been working on?<issue_closed>\n<issue_comment>username_2: This wasa transient issue, caused by a somewhat manual release that Andrew ran. (There's no bug in the release script.) This issue should only affect 16.6.2 and 16.6.3. Fortunately it shouldn't cause any observable behavior unless someone reads the comments. ","repo":"facebook/react","issue_id":389079792,"issue_number":14410}
{"question":"Why was the PyTorch execution blocked when using 2 GPUs on the same machine, whereas it ran successfully with 1 GPU?","answer":"The blockage during the PyTorch execution with 2 GPUs on the same machine could be due to issues related to parallel processing and GPU utilization. PyTorch may face challenges in coordinating and distributing computations efficiently across multiple GPUs, leading to synchronization problems or resource conflicts. In contrast, running with 1 GPU simplifies the execution process, reducing the likelihood of such issues and enabling smoother operation.","id":82,"text":"The pytorch version is 1.0.0, when use 2 gpus on one machine it was blocked,but it run successfully when use 1 gpu\r\n\r\nThe call stack in python as follow:\r\n```\r\nFile \"/home/admin/code/PROPAGATE/tools/0_train_base.py\", line 319, in <module>\r\n    process(sys.argv[1], sys.argv[2], int(sys.argv[3]), sys.argv[4])\r\n  File \"/home/admin/code/PROPAGATE/tools/0_train_base.py\", line 241, in process\r\n    solver.solve()\r\n  File \"/home/admin/code/PROPAGATE/tools/0_train_base.py\", line 190, in solve\r\n    loss, accB, accN, lossC = self.iterate(XB, YB, XN, YN, True)\r\n  File \"/home/admin/code/PROPAGATE/tools/0_train_base.py\", line 140, in iterate\r\n    self.optimizer.step()\r\n  File \"/home/admin/code/PROPAGATE/rpm/anaconda/lib/python2.7/site-packages/torch/optim/sgd.py\", line 107, in step\r\n    p.data.add_(-group['lr'], d_p)\r\n  File \"<string>\", line 1, in <module>\r\n  File \"<string>\", line 1, in <module>\r\n```","context":"<issue_start><issue_comment>Title: pytorch was blocked at optimizer.step\nusername_0: The pytorch version is 1.0.0, when use 2 gpus on one machine it was blocked,but it run successfully when use 1 gpu\r\n\r\nThe call stack in python as follow:\r\n```\r\nFile \"/home/admin/code/PROPAGATE/tools/0_train_base.py\", line 319, in <module>\r\n    process(sys.argv[1], sys.argv[2], int(sys.argv[3]), sys.argv[4])\r\n  File \"/home/admin/code/PROPAGATE/tools/0_train_base.py\", line 241, in process\r\n    solver.solve()\r\n  File \"/home/admin/code/PROPAGATE/tools/0_train_base.py\", line 190, in solve\r\n    loss, accB, accN, lossC = self.iterate(XB, YB, XN, YN, True)\r\n  File \"/home/admin/code/PROPAGATE/tools/0_train_base.py\", line 140, in iterate\r\n    self.optimizer.step()\r\n  File \"/home/admin/code/PROPAGATE/rpm/anaconda/lib/python2.7/site-packages/torch/optim/sgd.py\", line 107, in step\r\n    p.data.add_(-group['lr'], d_p)\r\n  File \"<string>\", line 1, in <module>\r\n  File \"<string>\", line 1, in <module>\r\n```\n<issue_comment>username_1: Please provide a minimal repro script. It's hard to debug without an repro here. Thanks!\n<issue_comment>username_2: Closing due to age and lack of repro script<issue_closed>","repo":"pytorch/pytorch","issue_id":408518494,"issue_number":16934}
{"question":"What changes were made to the functional options in the ghstack project according to the text?","answer":"Passing F::*FuncOptions instead of torch::nn::*Options to functionals and making all non-input arguments to functionals part of its options.","id":85,"text":"Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#29319 Make all non-input arguments to functionals part of its options**\n* #29318 Pass F::*FuncOptions instead of torch::nn::*Options to functionals, and make F::*FuncOptions a different class when necessary","context":"<issue_start><issue_comment>Title: Make all non-input arguments to functionals part of its options\nusername_0: Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#29319 Make all non-input arguments to functionals part of its options**\n* #29318 Pass F::*FuncOptions instead of torch::nn::*Options to functionals, and make F::*FuncOptions a different class when necessary\n<issue_comment>username_1: ## CircleCI build failures summary\nAs of commit 2e954dd:\n* 0/19 recognized as flaky\n* 19/19 failures introduced in this PR\n\n\n\nHere are the [reasons each build failed](https://dr.pytorch.org/commit-details.html?sha1=2e954dd7243d98f23b016bc55bfaff8aff11dba1).\n\n---\nThis comment was automatically generated by [Dr. CI](https://github.com/username_1/circleci-failure-tracker/tree/master/docs/from-pull-request-comment).\nFollow [this link to opt-out](https://dr.pytorch.org/admin/comments-opt-out.html) of these comments for your Pull Requests.\n\n\nPlease report bugs/suggestions on the [GitHub issue tracker](https://github.com/username_1/circleci-failure-tracker/issues).","repo":"pytorch/pytorch","issue_id":518718409,"issue_number":29319}
{"question":"How many failures were introduced in the PR according to the CircleCI build failures summary?","answer":"19 failures were introduced in the PR according to the CircleCI build failures summary.","id":86,"text":"## CircleCI build failures summary\nAs of commit 2e954dd:\n* 0/19 recognized as flaky\n* 19/19 failures introduced in this PR\n\n\n\nHere are the [reasons each build failed](https://dr.pytorch.org/commit-details.html?sha1=2e954dd7243d98f23b016bc55bfaff8aff11dba1).\n\n---\nThis comment was automatically generated by [Dr. CI](https://github.com/username_1/circleci-failure-tracker/tree/master/docs/from-pull-request-comment).\nFollow [this link to opt-out](https://dr.pytorch.org/admin/comments-opt-out.html) of these comments for your Pull Requests.\n\n\nPlease report bugs/suggestions on the [GitHub issue tracker](https://github.com/username_1/circleci-failure-tracker/issues).","context":"<issue_start><issue_comment>Title: Make all non-input arguments to functionals part of its options\nusername_0: Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#29319 Make all non-input arguments to functionals part of its options**\n* #29318 Pass F::*FuncOptions instead of torch::nn::*Options to functionals, and make F::*FuncOptions a different class when necessary\n<issue_comment>username_1: ## CircleCI build failures summary\nAs of commit 2e954dd:\n* 0/19 recognized as flaky\n* 19/19 failures introduced in this PR\n\n\n\nHere are the [reasons each build failed](https://dr.pytorch.org/commit-details.html?sha1=2e954dd7243d98f23b016bc55bfaff8aff11dba1).\n\n---\nThis comment was automatically generated by [Dr. CI](https://github.com/username_1/circleci-failure-tracker/tree/master/docs/from-pull-request-comment).\nFollow [this link to opt-out](https://dr.pytorch.org/admin/comments-opt-out.html) of these comments for your Pull Requests.\n\n\nPlease report bugs/suggestions on the [GitHub issue tracker](https://github.com/username_1/circleci-failure-tracker/issues).","repo":"pytorch/pytorch","issue_id":518718409,"issue_number":29319}
{"question":"What action is recommended regarding the master Persons of Interest in the GitHub repository in version 1.2.0?","answer":"Updating the master Persons of Interest to match that from version 1.2.0 of the GitHub repository.","id":87,"text":"Update master Persons of Interest to match that from v1.2.0\r\nhttps://github.com/pytorch/pytorch/blob/v1.2.0/docs/source/community/persons_of_interest.rst","context":"<issue_start><issue_comment>Title: Update master Persons of Interest to match v1.2.0\nusername_0: Update master Persons of Interest to match that from v1.2.0\r\nhttps://github.com/pytorch/pytorch/blob/v1.2.0/docs/source/community/persons_of_interest.rst\n<issue_comment>username_1: yeah, there is definitely a lot going on here. I think it might be better to can this PR and start with a new PR on top what we currently have. It would certainly be much more incremental.","repo":"pytorch/pytorch","issue_id":530629662,"issue_number":30582}
{"question":"What approach does the text recommend for handling the current pull request (PR)?","answer":"The text recommends cancelling the current PR and starting a new PR on top of what is currently available for a more incremental approach.","id":88,"text":"yeah, there is definitely a lot going on here. I think it might be better to can this PR and start with a new PR on top what we currently have. It would certainly be much more incremental.","context":"<issue_start><issue_comment>Title: Update master Persons of Interest to match v1.2.0\nusername_0: Update master Persons of Interest to match that from v1.2.0\r\nhttps://github.com/pytorch/pytorch/blob/v1.2.0/docs/source/community/persons_of_interest.rst\n<issue_comment>username_1: yeah, there is definitely a lot going on here. I think it might be better to can this PR and start with a new PR on top what we currently have. It would certainly be much more incremental.","repo":"pytorch/pytorch","issue_id":530629662,"issue_number":30582}
{"question":"What is the comparison being made in the assertion error message 'AssertionError: 11 not less than or equal to 1e-05'?","answer":"The comparison being made is whether the absolute difference between the values 11 and 1e-05 is less than or equal to a precision of 1e-05.","id":89,"text":"https://app.circleci.com/jobs/github/pytorch/pytorch/4155093\r\n\r\n```\r\nJan 07 20:36:55 ======================================================================\r\nJan 07 20:36:55 FAIL [3.205s]: test_nccl_errors_blocking_clean_exit (__main__.NcclErrorHandlingTest)\r\nJan 07 20:36:55 ----------------------------------------------------------------------\r\nJan 07 20:36:55 Traceback (most recent call last):\r\nJan 07 20:36:55   File \"/var/lib/jenkins/workspace/test/common_distributed.py\", line 130, in wrapper\r\nJan 07 20:36:55     self._join_processes(fn)\r\nJan 07 20:36:55   File \"/var/lib/jenkins/workspace/test/common_distributed.py\", line 211, in _join_processes\r\nJan 07 20:36:55     self._check_return_codes(elapsed_time)\r\nJan 07 20:36:55   File \"/var/lib/jenkins/workspace/test/common_distributed.py\", line 231, in _check_return_codes\r\nJan 07 20:36:55     self.assertEqual(p.exitcode, first_process.exitcode)\r\nJan 07 20:36:55   File \"/var/lib/jenkins/workspace/test/common_utils.py\", line 888, in assertEqual\r\nJan 07 20:36:55     super(TestCase, self).assertLessEqual(abs(x - y), prec, message)\r\nJan 07 20:36:55 AssertionError: 11 not less than or equal to 1e-05 : \r\n```","context":"<issue_start><issue_comment>Title: test_nccl_errors_blocking_clean_exit is flaky\nusername_0: https://app.circleci.com/jobs/github/pytorch/pytorch/4155093\r\n\r\n```\r\nJan 07 20:36:55 ======================================================================\r\nJan 07 20:36:55 FAIL [3.205s]: test_nccl_errors_blocking_clean_exit (__main__.NcclErrorHandlingTest)\r\nJan 07 20:36:55 ----------------------------------------------------------------------\r\nJan 07 20:36:55 Traceback (most recent call last):\r\nJan 07 20:36:55   File \"/var/lib/jenkins/workspace/test/common_distributed.py\", line 130, in wrapper\r\nJan 07 20:36:55     self._join_processes(fn)\r\nJan 07 20:36:55   File \"/var/lib/jenkins/workspace/test/common_distributed.py\", line 211, in _join_processes\r\nJan 07 20:36:55     self._check_return_codes(elapsed_time)\r\nJan 07 20:36:55   File \"/var/lib/jenkins/workspace/test/common_distributed.py\", line 231, in _check_return_codes\r\nJan 07 20:36:55     self.assertEqual(p.exitcode, first_process.exitcode)\r\nJan 07 20:36:55   File \"/var/lib/jenkins/workspace/test/common_utils.py\", line 888, in assertEqual\r\nJan 07 20:36:55     super(TestCase, self).assertLessEqual(abs(x - y), prec, message)\r\nJan 07 20:36:55 AssertionError: 11 not less than or equal to 1e-05 : \r\n```\n<issue_comment>username_1: Cannot reproduce this failure after running the same test 100 times on 8 GPUs.<issue_closed>","repo":"pytorch/pytorch","issue_id":546524878,"issue_number":31924}
{"question":"What is the significance of the 'iOS 1.4.0 binary push' task in the context of the ghstack workflow?","answer":"The 'iOS 1.4.0 binary push' task in the ghstack workflow involves pushing a binary release of version 1.4.0 for the iOS platform. This step is crucial for deploying the latest version of the iOS app to users. It ensures that the compiled binary package is distributed and made available for installation on iOS devices, marking a milestone in the development and release process of the iOS project.","id":91,"text":"Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#32207 [DO NOT MERGE] iOS 1.4.0 binary push**\n* #32147 [iOS] Update Gemfile\n* #31912 [CI]Suppress pip logs","context":"<issue_start><issue_comment>Title: [DO NOT MERGE] iOS 1.4.0 binary push\nusername_0: Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#32207 [DO NOT MERGE] iOS 1.4.0 binary push**\n* #32147 [iOS] Update Gemfile\n* #31912 [CI]Suppress pip logs\n<issue_comment>username_1: ## :pill: CircleCI build failures summary and remediations\nAs of commit 2f788305:\n\n\n* **1/1** failures introduced in this PR\n\n\n## Detailed failure analysis\n\nOne may explore the probable reasons each build failed interactively [on the Dr. CI website](https://dr.pytorch.org/commit-details.html?sha1=2f7883058b67b349112aba3524e2774237bd2e6c).\n\n### :detective: 1 new failure recognized by patterns\nThe following build failures do not appear to be due to upstream breakage:\n#### [![See CircleCI build](https://avatars0.githubusercontent.com/ml/7?s=12)](https://circleci.com/gh/pytorch/pytorch/4249792) binary_ios_upload (1/1)\n**Step:** \"Upload\" ([full log](https://dr.pytorch.org/api/view-log-full?build_id=86265225) | [pattern match details](https://dr.pytorch.org/build-details.html?build_id=86265225))\n<details>\n<summary>\n<code>fatal error: /Applications/Xcode-11.2.1.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/lipo: can't open input file: /Users/distiller/workspace/ios/x86_64/lib/libtorch_cpu.a (No such file or directory)</code>\n</summary>\n\n```\n++ lipo -create /Users/distiller/workspace/ios/x86_64/lib/libcpuinfo.a /Users/distiller/workspace/ios/arm64/lib/libcpuinfo.a -o /Users/distiller/workspace/zip/install/lib/libcpuinfo.a \n++ for lib in '${target_libs[*]}' \n++ libs=(${ARTIFACTS_DIR}/x86_64/lib/${lib} ${ARTIFACTS_DIR}/arm64/lib/${lib}) \n++ lipo -create /Users/distiller/workspace/ios/x86_64/lib/libeigen_blas.a /Users/distiller/workspace/ios/arm64/lib/libeigen_blas.a -o /Users/distiller/workspace/zip/install/lib/libeigen_blas.a \n++ for lib in '${target_libs[*]}' \n++ libs=(${ARTIFACTS_DIR}/x86_64/lib/${lib} ${ARTIFACTS_DIR}/arm64/lib/${lib}) \n++ lipo -create /Users/distiller/workspace/ios/x86_64/lib/libpytorch_qnnpack.a /Users/distiller/workspace/ios/arm64/lib/libpytorch_qnnpack.a -o /Users/distiller/workspace/zip/install/lib/libpytorch_qnnpack.a \n++ for lib in '${target_libs[*]}' \n++ libs=(${ARTIFACTS_DIR}/x86_64/lib/${lib} ${ARTIFACTS_DIR}/arm64/lib/${lib}) \n++ lipo -create /Users/distiller/workspace/ios/x86_64/lib/libtorch_cpu.a /Users/distiller/workspace/ios/arm64/lib/libtorch_cpu.a -o /Users/distiller/workspace/zip/install/lib/libtorch_cpu.a \nfatal error: /Applications/Xcode-11.2.1.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/lipo: can't open input file: /Users/distiller/workspace/ios/x86_64/lib/libtorch_cpu.a (No such file or directory) \n```\n\n</details>\n\n\n\n---\n\n<details><summary>This comment was automatically generated by <a href=\"https://github.com/username_1/circleci-failure-tracker/tree/master/docs/from-pull-request-comment\">Dr. CI</a> (expand for details).</summary>Follow <a href=\"https://dr.pytorch.org/admin/comments-opt-out.html\">this link to opt-out</a> of these comments for your Pull Requests.\n\nPlease report bugs/suggestions on the <a href=\"https://github.com/username_1/circleci-failure-tracker/issues\">GitHub issue tracker</a>.\n</details>","repo":"pytorch/pytorch","issue_id":549943186,"issue_number":32207}
{"question":"What is the specific step that caused the build failure in the CircleCI build related to commit 2f788305?","answer":"The specific step that caused the build failure is the \"Upload\" step.","id":92,"text":"## :pill: CircleCI build failures summary and remediations\nAs of commit 2f788305:\n\n\n* **1/1** failures introduced in this PR\n\n\n## Detailed failure analysis\n\nOne may explore the probable reasons each build failed interactively [on the Dr. CI website](https://dr.pytorch.org/commit-details.html?sha1=2f7883058b67b349112aba3524e2774237bd2e6c).\n\n### :detective: 1 new failure recognized by patterns\nThe following build failures do not appear to be due to upstream breakage:\n#### [![See CircleCI build](https://avatars0.githubusercontent.com/ml/7?s=12)](https://circleci.com/gh/pytorch/pytorch/4249792) binary_ios_upload (1/1)\n**Step:** \"Upload\" ([full log](https://dr.pytorch.org/api/view-log-full?build_id=86265225) | [pattern match details](https://dr.pytorch.org/build-details.html?build_id=86265225))\n<details>\n<summary>\n<code>fatal error: /Applications/Xcode-11.2.1.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/lipo: can't open input file: /Users/distiller/workspace/ios/x86_64/lib/libtorch_cpu.a (No such file or directory)</code>\n</summary>\n\n```\n++ lipo -create /Users/distiller/workspace/ios/x86_64/lib/libcpuinfo.a /Users/distiller/workspace/ios/arm64/lib/libcpuinfo.a -o /Users/distiller/workspace/zip/install/lib/libcpuinfo.a \n++ for lib in '${target_libs[*]}' \n++ libs=(${ARTIFACTS_DIR}/x86_64/lib/${lib} ${ARTIFACTS_DIR}/arm64/lib/${lib}) \n++ lipo -create /Users/distiller/workspace/ios/x86_64/lib/libeigen_blas.a /Users/distiller/workspace/ios/arm64/lib/libeigen_blas.a -o /Users/distiller/workspace/zip/install/lib/libeigen_blas.a \n++ for lib in '${target_libs[*]}' \n++ libs=(${ARTIFACTS_DIR}/x86_64/lib/${lib} ${ARTIFACTS_DIR}/arm64/lib/${lib}) \n++ lipo -create /Users/distiller/workspace/ios/x86_64/lib/libpytorch_qnnpack.a /Users/distiller/workspace/ios/arm64/lib/libpytorch_qnnpack.a -o /Users/distiller/workspace/zip/install/lib/libpytorch_qnnpack.a \n++ for lib in '${target_libs[*]}' \n++ libs=(${ARTIFACTS_DIR}/x86_64/lib/${lib} ${ARTIFACTS_DIR}/arm64/lib/${lib}) \n++ lipo -create /Users/distiller/workspace/ios/x86_64/lib/libtorch_cpu.a /Users/distiller/workspace/ios/arm64/lib/libtorch_cpu.a -o /Users/distiller/workspace/zip/install/lib/libtorch_cpu.a \nfatal error: /Applications/Xcode-11.2.1.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/lipo: can't open input file: /Users/distiller/workspace/ios/x86_64/lib/libtorch_cpu.a (No such file or directory) \n```\n\n</details>\n\n\n\n---\n\n<details><summary>This comment was automatically generated by <a href=\"https://github.com/username_1/circleci-failure-tracker/tree/master/docs/from-pull-request-comment\">Dr. CI</a> (expand for details).</summary>Follow <a href=\"https://dr.pytorch.org/admin/comments-opt-out.html\">this link to opt-out</a> of these comments for your Pull Requests.\n\nPlease report bugs/suggestions on the <a href=\"https://github.com/username_1/circleci-failure-tracker/issues\">GitHub issue tracker</a>.\n</details>","context":"<issue_start><issue_comment>Title: [DO NOT MERGE] iOS 1.4.0 binary push\nusername_0: Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#32207 [DO NOT MERGE] iOS 1.4.0 binary push**\n* #32147 [iOS] Update Gemfile\n* #31912 [CI]Suppress pip logs\n<issue_comment>username_1: ## :pill: CircleCI build failures summary and remediations\nAs of commit 2f788305:\n\n\n* **1/1** failures introduced in this PR\n\n\n## Detailed failure analysis\n\nOne may explore the probable reasons each build failed interactively [on the Dr. CI website](https://dr.pytorch.org/commit-details.html?sha1=2f7883058b67b349112aba3524e2774237bd2e6c).\n\n### :detective: 1 new failure recognized by patterns\nThe following build failures do not appear to be due to upstream breakage:\n#### [![See CircleCI build](https://avatars0.githubusercontent.com/ml/7?s=12)](https://circleci.com/gh/pytorch/pytorch/4249792) binary_ios_upload (1/1)\n**Step:** \"Upload\" ([full log](https://dr.pytorch.org/api/view-log-full?build_id=86265225) | [pattern match details](https://dr.pytorch.org/build-details.html?build_id=86265225))\n<details>\n<summary>\n<code>fatal error: /Applications/Xcode-11.2.1.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/lipo: can't open input file: /Users/distiller/workspace/ios/x86_64/lib/libtorch_cpu.a (No such file or directory)</code>\n</summary>\n\n```\n++ lipo -create /Users/distiller/workspace/ios/x86_64/lib/libcpuinfo.a /Users/distiller/workspace/ios/arm64/lib/libcpuinfo.a -o /Users/distiller/workspace/zip/install/lib/libcpuinfo.a \n++ for lib in '${target_libs[*]}' \n++ libs=(${ARTIFACTS_DIR}/x86_64/lib/${lib} ${ARTIFACTS_DIR}/arm64/lib/${lib}) \n++ lipo -create /Users/distiller/workspace/ios/x86_64/lib/libeigen_blas.a /Users/distiller/workspace/ios/arm64/lib/libeigen_blas.a -o /Users/distiller/workspace/zip/install/lib/libeigen_blas.a \n++ for lib in '${target_libs[*]}' \n++ libs=(${ARTIFACTS_DIR}/x86_64/lib/${lib} ${ARTIFACTS_DIR}/arm64/lib/${lib}) \n++ lipo -create /Users/distiller/workspace/ios/x86_64/lib/libpytorch_qnnpack.a /Users/distiller/workspace/ios/arm64/lib/libpytorch_qnnpack.a -o /Users/distiller/workspace/zip/install/lib/libpytorch_qnnpack.a \n++ for lib in '${target_libs[*]}' \n++ libs=(${ARTIFACTS_DIR}/x86_64/lib/${lib} ${ARTIFACTS_DIR}/arm64/lib/${lib}) \n++ lipo -create /Users/distiller/workspace/ios/x86_64/lib/libtorch_cpu.a /Users/distiller/workspace/ios/arm64/lib/libtorch_cpu.a -o /Users/distiller/workspace/zip/install/lib/libtorch_cpu.a \nfatal error: /Applications/Xcode-11.2.1.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/lipo: can't open input file: /Users/distiller/workspace/ios/x86_64/lib/libtorch_cpu.a (No such file or directory) \n```\n\n</details>\n\n\n\n---\n\n<details><summary>This comment was automatically generated by <a href=\"https://github.com/username_1/circleci-failure-tracker/tree/master/docs/from-pull-request-comment\">Dr. CI</a> (expand for details).</summary>Follow <a href=\"https://dr.pytorch.org/admin/comments-opt-out.html\">this link to opt-out</a> of these comments for your Pull Requests.\n\nPlease report bugs/suggestions on the <a href=\"https://github.com/username_1/circleci-failure-tracker/issues\">GitHub issue tracker</a>.\n</details>","repo":"pytorch/pytorch","issue_id":549943186,"issue_number":32207}
{"question":"What does the following code snippet achieve: \n\nt = torch.empty(... , dtype=torch.int64)\nt.random_(torch.iinfo(torch.int64).min, None)","answer":"The code snippet creates a PyTorch tensor `t` filled with random 64-bit integers, including `torch.iinfo(torch.int64).min` and `torch.iinfo(torch.int64).max` values.","id":93,"text":"```\r\nt = torch.empty(... , dtype=torch.int64)\r\nt.random_(torch.iinfo(torch.int64).min, None)\r\n```\r\nshould be able to fill tensor with all 64 bit number including `torch.iinfo(torch.int64).min` and `torch.iinfo(torch.int64).max`","context":"<issue_start><issue_comment>Title: Tensor.random_ should be able to generate all 64 bit numbers including min and max value\nusername_0: ```\r\nt = torch.empty(... , dtype=torch.int64)\r\nt.random_(torch.iinfo(torch.int64).min, None)\r\n```\r\nshould be able to fill tensor with all 64 bit number including `torch.iinfo(torch.int64).min` and `torch.iinfo(torch.int64).max`\n<issue_comment>username_1: This is a dupe of #16944\n<issue_comment>username_0: Fixed in https://github.com/pytorch/pytorch/commit/095de1e872671c4da2cc0965976058cf752c3be0\r\nhttps://github.com/pytorch/pytorch/blob/master/aten/src/ATen/test/cpu_rng_test.cpp#L93 demonstrates how to generate uint64_t max value, (2^64 - 1)<issue_closed>\n<issue_comment>username_0: ```\r\nIn [1]: import torch\r\n\r\nIn [2]: \"{0:b}\".format(torch.empty(1, dtype=torch.int64, device='cpu').random_(torch.iinfo(torch.int64).max, None)[0])\r\nOut[2]: '111111111111111111111111111111111111111111111111111111111111111'\r\n\r\nIn [3]: \"{0:b}\".format(torch.empty(1, dtype=torch.int64, device='cuda').random_(torch.iinfo(torch.int64).max, None)[0])\r\nOut[3]: '111111111111111111111111111111111111111111111111111111111111111'\r\n```","repo":"pytorch/pytorch","issue_id":564830266,"issue_number":33299}
{"question":"How is the maximum unsigned 64-bit integer value (2^64 - 1) generated in the PyTorch codebase?","answer":"The maximum unsigned 64-bit integer value (2^64 - 1) is generated in the PyTorch codebase using a specific method demonstrated in the cpu_rng_test.cpp file at line 93 in the PyTorch repository. The method can be found in the mentioned GitHub commit.","id":95,"text":"Fixed in https://github.com/pytorch/pytorch/commit/095de1e872671c4da2cc0965976058cf752c3be0\r\nhttps://github.com/pytorch/pytorch/blob/master/aten/src/ATen/test/cpu_rng_test.cpp#L93 demonstrates how to generate uint64_t max value, (2^64 - 1)","context":"<issue_start><issue_comment>Title: Tensor.random_ should be able to generate all 64 bit numbers including min and max value\nusername_0: ```\r\nt = torch.empty(... , dtype=torch.int64)\r\nt.random_(torch.iinfo(torch.int64).min, None)\r\n```\r\nshould be able to fill tensor with all 64 bit number including `torch.iinfo(torch.int64).min` and `torch.iinfo(torch.int64).max`\n<issue_comment>username_1: This is a dupe of #16944\n<issue_comment>username_0: Fixed in https://github.com/pytorch/pytorch/commit/095de1e872671c4da2cc0965976058cf752c3be0\r\nhttps://github.com/pytorch/pytorch/blob/master/aten/src/ATen/test/cpu_rng_test.cpp#L93 demonstrates how to generate uint64_t max value, (2^64 - 1)<issue_closed>\n<issue_comment>username_0: ```\r\nIn [1]: import torch\r\n\r\nIn [2]: \"{0:b}\".format(torch.empty(1, dtype=torch.int64, device='cpu').random_(torch.iinfo(torch.int64).max, None)[0])\r\nOut[2]: '111111111111111111111111111111111111111111111111111111111111111'\r\n\r\nIn [3]: \"{0:b}\".format(torch.empty(1, dtype=torch.int64, device='cuda').random_(torch.iinfo(torch.int64).max, None)[0])\r\nOut[3]: '111111111111111111111111111111111111111111111111111111111111111'\r\n```","repo":"pytorch/pytorch","issue_id":564830266,"issue_number":33299}
{"question":"What library is used to generate random integer values in the code snippet?","answer":"torch","id":96,"text":"```\r\nIn [1]: import torch\r\n\r\nIn [2]: \"{0:b}\".format(torch.empty(1, dtype=torch.int64, device='cpu').random_(torch.iinfo(torch.int64).max, None)[0])\r\nOut[2]: '111111111111111111111111111111111111111111111111111111111111111'\r\n\r\nIn [3]: \"{0:b}\".format(torch.empty(1, dtype=torch.int64, device='cuda').random_(torch.iinfo(torch.int64).max, None)[0])\r\nOut[3]: '111111111111111111111111111111111111111111111111111111111111111'\r\n```","context":"<issue_start><issue_comment>Title: Tensor.random_ should be able to generate all 64 bit numbers including min and max value\nusername_0: ```\r\nt = torch.empty(... , dtype=torch.int64)\r\nt.random_(torch.iinfo(torch.int64).min, None)\r\n```\r\nshould be able to fill tensor with all 64 bit number including `torch.iinfo(torch.int64).min` and `torch.iinfo(torch.int64).max`\n<issue_comment>username_1: This is a dupe of #16944\n<issue_comment>username_0: Fixed in https://github.com/pytorch/pytorch/commit/095de1e872671c4da2cc0965976058cf752c3be0\r\nhttps://github.com/pytorch/pytorch/blob/master/aten/src/ATen/test/cpu_rng_test.cpp#L93 demonstrates how to generate uint64_t max value, (2^64 - 1)<issue_closed>\n<issue_comment>username_0: ```\r\nIn [1]: import torch\r\n\r\nIn [2]: \"{0:b}\".format(torch.empty(1, dtype=torch.int64, device='cpu').random_(torch.iinfo(torch.int64).max, None)[0])\r\nOut[2]: '111111111111111111111111111111111111111111111111111111111111111'\r\n\r\nIn [3]: \"{0:b}\".format(torch.empty(1, dtype=torch.int64, device='cuda').random_(torch.iinfo(torch.int64).max, None)[0])\r\nOut[3]: '111111111111111111111111111111111111111111111111111111111111111'\r\n```","repo":"pytorch/pytorch","issue_id":564830266,"issue_number":33299}
{"question":"What is the specific suggestion provided in the text chunk regarding test_jit.py test setup and teardown operations?","answer":"Don't change global executor state in test_jit.py - rather do that in setUp and tearDown.","id":97,"text":"Stack from [ghstack](https://github.com/ezyang/ghstack):\n* #34983 Refactor test_jit_fuser legacy tests.\n* **#34982 Don't change global executor state in test_jit.py - rather do that in setUp and tearDown.**\n* #34981 Query graph executor mode in test fixture rather than relying on a global variable.\n* #34980 Fix warnings in test/test_jit_fuser.py\n\n\n\nDifferential Revision: [D20520370](https://our.internmc.facebook.com/intern/diff/D20520370)","context":"<issue_start><issue_comment>Title: Don't change global executor state in test_jit.py - rather do that in setUp and tearDown.\nusername_0: Stack from [ghstack](https://github.com/ezyang/ghstack):\n* #34983 Refactor test_jit_fuser legacy tests.\n* **#34982 Don't change global executor state in test_jit.py - rather do that in setUp and tearDown.**\n* #34981 Query graph executor mode in test fixture rather than relying on a global variable.\n* #34980 Fix warnings in test/test_jit_fuser.py\n\n\n\nDifferential Revision: [D20520370](https://our.internmc.facebook.com/intern/diff/D20520370)","repo":"pytorch/pytorch","issue_id":583995217,"issue_number":34982}
{"question":"What problem was fixed in the GitHub pull request https://github.com/pytorch/pytorch/pull/58382?","answer":"The problem fixed in the GitHub pull request https://github.com/pytorch/pytorch/pull/58382 is not specified in the text.","id":98,"text":"Stack from [ghstack](https://github.com/ezyang/ghstack):\n* (to be filled)\n\nAn additional (and hopefully more robust) way of fixing the same problem https://github.com/pytorch/pytorch/pull/58382 fixed.\n\nDifferential Revision: [D28474154](https://our.internmc.facebook.com/intern/diff/D28474154/)","context":"<issue_start><issue_comment>Title: Prevent lock inversions with GIL in Future\nusername_0: Stack from [ghstack](https://github.com/ezyang/ghstack):\n* (to be filled)\n\nAn additional (and hopefully more robust) way of fixing the same problem https://github.com/pytorch/pytorch/pull/58382 fixed.\n\nDifferential Revision: [D28474154](https://our.internmc.facebook.com/intern/diff/D28474154/)","repo":"pytorch/pytorch","issue_id":893273577,"issue_number":58391}
{"question":"What error is encountered when calling `torch.linalg.svd` on a large float64 tensor on the CPU?","answer":"The error encountered is `Intel MKL ERROR: Parameter 12 was incorrect on entry to DGESDD`.","id":99,"text":"##  Bug\r\n\r\nCalling `torch.linalg.svd` on a large float64 tensor on the CPU (size 90k x ~34k) leads to this error. `torch.linalg.eigh` on `tensor.T@tensor` seg faults.\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. I don't know. I have a large tensor of type float64. Calling `torch.linalg.svd`on it produces this error.\r\n\r\n```\r\nIntel MKL ERROR: Parameter 12 was incorrect on entry to DGESDD.\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n/tmp/ipykernel_3300/2679862595.py in <module>\r\n----> 1 torch.linalg.svd(tensor)\r\n\r\nRuntimeError: falseINTERNAL ASSERT FAILED at \"/opt/conda/conda-bld/pytorch_1634272068185/work/aten/src/ATen/native/LinearAlgebraUtils.h\":244, please report a bug to PyTorch. svd_cpu: Argument 12 has illegal value. Most certainly there is a bug in the implementation calling the backend library.\r\n```\r\n## Expected behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n## Environment\r\n\r\nPlease copy and paste the output from our\r\n[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)\r\n(or fill out the checklist below manually).\r\n\r\nYou can get the script and run it with:\r\n```\r\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\r\n# For security purposes, please check the contents of collect_env.py before running it.\r\npython collect_env.py\r\n```\r\n\r\n - PyTorch Version (e.g., 1.0): '1.10.0'\r\n - OS (e.g., Linux): Linux\r\n - How you installed PyTorch (`conda`, `pip`, source): conda\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.9.5\r\n - CUDA/cuDNN version:\r\n - GPU models and configuration:On the CPU\r\n - Any other relevant information:\r\n\r\n## Additional context\r\n`torch.linalg.eigh` on `tensor.T@tensor` seg faults.","context":"<issue_start><issue_comment>Title: INTERNAL ASSERT FAILED at \"/opt/conda/conda-bld/pytorch_1634272068185/work/aten/src/ATen/native/LinearAlgebraUtils.h\":244\nusername_0: ##  Bug\r\n\r\nCalling `torch.linalg.svd` on a large float64 tensor on the CPU (size 90k x ~34k) leads to this error. `torch.linalg.eigh` on `tensor.T@tensor` seg faults.\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. I don't know. I have a large tensor of type float64. Calling `torch.linalg.svd`on it produces this error.\r\n\r\n```\r\nIntel MKL ERROR: Parameter 12 was incorrect on entry to DGESDD.\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n/tmp/ipykernel_3300/2679862595.py in <module>\r\n----> 1 torch.linalg.svd(tensor)\r\n\r\nRuntimeError: falseINTERNAL ASSERT FAILED at \"/opt/conda/conda-bld/pytorch_1634272068185/work/aten/src/ATen/native/LinearAlgebraUtils.h\":244, please report a bug to PyTorch. svd_cpu: Argument 12 has illegal value. Most certainly there is a bug in the implementation calling the backend library.\r\n```\r\n## Expected behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n## Environment\r\n\r\nPlease copy and paste the output from our\r\n[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)\r\n(or fill out the checklist below manually).\r\n\r\nYou can get the script and run it with:\r\n```\r\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\r\n# For security purposes, please check the contents of collect_env.py before running it.\r\npython collect_env.py\r\n```\r\n\r\n - PyTorch Version (e.g., 1.0): '1.10.0'\r\n - OS (e.g., Linux): Linux\r\n - How you installed PyTorch (`conda`, `pip`, source): conda\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.9.5\r\n - CUDA/cuDNN version:\r\n - GPU models and configuration:On the CPU\r\n - Any other relevant information:\r\n\r\n## Additional context\r\n`torch.linalg.eigh` on `tensor.T@tensor` seg faults.\n<issue_comment>username_0: I have been trying to figure out what is special about this tensor that is causing the algorithm to fail.\r\n\r\nI note that `torch.linalg.svdvals` is successful.\n<issue_comment>username_0: Ok, I am now able to replicate this problem using just `torch.rand` and `torch.linalg.svd`.\r\n\r\nThe following fails:\r\n```\r\nx = torch.rand(30000,30000)\r\nu,s,v = torch.linalg.svd(x)\r\n```\n<issue_comment>username_1: Thank you for the tiny repro! I can reproduce in master. cc @username_2\n<issue_comment>username_2: The problem is related to https://github.com/pytorch/pytorch/issues/51720.\r\n\r\nThe optimal workspace size for the input of this size is `2.70021e+09` which doesn't fit into `int` type. In PyTorch, we use a 32-bit interface to backend linear algebra libraries.\r\nWe can \"fix\" the internal assert with this patch\r\n```\r\ndiff --git a/aten/src/ATen/native/BatchLinearAlgebra.cpp b/aten/src/ATen/native/BatchLinearAlgebra.cpp\r\nindex 853f5c2cbe..8902958d98 100644\r\n--- a/aten/src/ATen/native/BatchLinearAlgebra.cpp\r\n+++ b/aten/src/ATen/native/BatchLinearAlgebra.cpp\r\n@@ -2984,7 +2984,9 @@ static void apply_svd(Tensor& self, Tensor& U, Tensor& S, Tensor& VT,\r\n   int lwork = -1;\r\n   scalar_t wkopt;\r\n   lapackSvd<scalar_t, value_t>(jobz, m, n, self_data, lda, S_data, U_data, lda, VT_data, ldvt, &wkopt, lwork, rwork_data, iwork_data, &info);\r\n-  lwork = std::max<int>(1, real_impl<scalar_t, value_t>(wkopt));\r\n+  TORCH_CHECK(\r\n+      (real_impl<scalar_t, value_t>(wkopt)) < INT_MAX,\r\n+      \"svd: The optimal workspace size is larger than allowed by 32-bit interface to backend math library.\");\r\n   Tensor work = at::empty({lwork}, self.options());\r\n   auto work_data = work.data_ptr<scalar_t>();\r\n```\r\nThe issue is likely there for other functions as well and we could include checks of the values before casting them to int.\n<issue_comment>username_0: @username_2 will there be any way around this in the future for large SVDs? Or is there an upper size limit due to the 32bit interface?\n<issue_comment>username_2: Unfortunately, there is no workaround available.\r\nFrom the MKL documentation the workspace size (`lwork`) can be computed using these guidelines:\r\n```\r\nFor real flavors:\r\n\r\nIf jobz = 'N', lwork  3*min(m, n) + max (max(m,n), 6*min(m, n));\r\n\r\nIf jobz = 'O', lwork  3*(min(m, n))2 + max (max(m, n), 5*(min(m, n))2 + 4*min(m, n));\r\n\r\nIf jobz = 'S' or 'A', lwork  3*(min(m, n))2 + max (max(m, n), 4*(min(m, n))2 + 4*min(m, n))\r\n\r\nFor complex flavors:\r\n\r\nIf jobz = 'N', lwork  2*min(m, n) + max(m, n);\r\n\r\nIf jobz = 'O', lwork  2*(min(m, n))2 + max(m, n) + 2*min(m, n);\r\n\r\nIf jobz = 'S' or 'A', lwork  (min(m, n))2 + max(m, n) + 2*min(m, n);\r\n```\r\n`lwork` must be smaller than `2147483647` (maximum `int`) in order to not hit the limitation of the 32-bit interface.\n<issue_comment>username_0: @username_2 Any clue as to why `svdvals` and `eigvalsh` work?\n<issue_comment>username_1: For what is worth, `svdvals` and `eigvalsh` may work in master when the input does not require gradients, as we do not compute the `U` and the `Vh` in those cases, so the working space needed is less than for the full SVD / eigendecomposition.","repo":"pytorch/pytorch","issue_id":1052456616,"issue_number":68291}
{"question":"What special property of the tensor is causing the algorithm to fail, as indicated by the success of 'torch.linalg.svdvals'?","answer":"The algorithm is failing due to the singular values or singular value decomposition of the tensor.","id":100,"text":"I have been trying to figure out what is special about this tensor that is causing the algorithm to fail.\r\n\r\nI note that `torch.linalg.svdvals` is successful.","context":"<issue_start><issue_comment>Title: INTERNAL ASSERT FAILED at \"/opt/conda/conda-bld/pytorch_1634272068185/work/aten/src/ATen/native/LinearAlgebraUtils.h\":244\nusername_0: ##  Bug\r\n\r\nCalling `torch.linalg.svd` on a large float64 tensor on the CPU (size 90k x ~34k) leads to this error. `torch.linalg.eigh` on `tensor.T@tensor` seg faults.\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. I don't know. I have a large tensor of type float64. Calling `torch.linalg.svd`on it produces this error.\r\n\r\n```\r\nIntel MKL ERROR: Parameter 12 was incorrect on entry to DGESDD.\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n/tmp/ipykernel_3300/2679862595.py in <module>\r\n----> 1 torch.linalg.svd(tensor)\r\n\r\nRuntimeError: falseINTERNAL ASSERT FAILED at \"/opt/conda/conda-bld/pytorch_1634272068185/work/aten/src/ATen/native/LinearAlgebraUtils.h\":244, please report a bug to PyTorch. svd_cpu: Argument 12 has illegal value. Most certainly there is a bug in the implementation calling the backend library.\r\n```\r\n## Expected behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n## Environment\r\n\r\nPlease copy and paste the output from our\r\n[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)\r\n(or fill out the checklist below manually).\r\n\r\nYou can get the script and run it with:\r\n```\r\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\r\n# For security purposes, please check the contents of collect_env.py before running it.\r\npython collect_env.py\r\n```\r\n\r\n - PyTorch Version (e.g., 1.0): '1.10.0'\r\n - OS (e.g., Linux): Linux\r\n - How you installed PyTorch (`conda`, `pip`, source): conda\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.9.5\r\n - CUDA/cuDNN version:\r\n - GPU models and configuration:On the CPU\r\n - Any other relevant information:\r\n\r\n## Additional context\r\n`torch.linalg.eigh` on `tensor.T@tensor` seg faults.\n<issue_comment>username_0: I have been trying to figure out what is special about this tensor that is causing the algorithm to fail.\r\n\r\nI note that `torch.linalg.svdvals` is successful.\n<issue_comment>username_0: Ok, I am now able to replicate this problem using just `torch.rand` and `torch.linalg.svd`.\r\n\r\nThe following fails:\r\n```\r\nx = torch.rand(30000,30000)\r\nu,s,v = torch.linalg.svd(x)\r\n```\n<issue_comment>username_1: Thank you for the tiny repro! I can reproduce in master. cc @username_2\n<issue_comment>username_2: The problem is related to https://github.com/pytorch/pytorch/issues/51720.\r\n\r\nThe optimal workspace size for the input of this size is `2.70021e+09` which doesn't fit into `int` type. In PyTorch, we use a 32-bit interface to backend linear algebra libraries.\r\nWe can \"fix\" the internal assert with this patch\r\n```\r\ndiff --git a/aten/src/ATen/native/BatchLinearAlgebra.cpp b/aten/src/ATen/native/BatchLinearAlgebra.cpp\r\nindex 853f5c2cbe..8902958d98 100644\r\n--- a/aten/src/ATen/native/BatchLinearAlgebra.cpp\r\n+++ b/aten/src/ATen/native/BatchLinearAlgebra.cpp\r\n@@ -2984,7 +2984,9 @@ static void apply_svd(Tensor& self, Tensor& U, Tensor& S, Tensor& VT,\r\n   int lwork = -1;\r\n   scalar_t wkopt;\r\n   lapackSvd<scalar_t, value_t>(jobz, m, n, self_data, lda, S_data, U_data, lda, VT_data, ldvt, &wkopt, lwork, rwork_data, iwork_data, &info);\r\n-  lwork = std::max<int>(1, real_impl<scalar_t, value_t>(wkopt));\r\n+  TORCH_CHECK(\r\n+      (real_impl<scalar_t, value_t>(wkopt)) < INT_MAX,\r\n+      \"svd: The optimal workspace size is larger than allowed by 32-bit interface to backend math library.\");\r\n   Tensor work = at::empty({lwork}, self.options());\r\n   auto work_data = work.data_ptr<scalar_t>();\r\n```\r\nThe issue is likely there for other functions as well and we could include checks of the values before casting them to int.\n<issue_comment>username_0: @username_2 will there be any way around this in the future for large SVDs? Or is there an upper size limit due to the 32bit interface?\n<issue_comment>username_2: Unfortunately, there is no workaround available.\r\nFrom the MKL documentation the workspace size (`lwork`) can be computed using these guidelines:\r\n```\r\nFor real flavors:\r\n\r\nIf jobz = 'N', lwork  3*min(m, n) + max (max(m,n), 6*min(m, n));\r\n\r\nIf jobz = 'O', lwork  3*(min(m, n))2 + max (max(m, n), 5*(min(m, n))2 + 4*min(m, n));\r\n\r\nIf jobz = 'S' or 'A', lwork  3*(min(m, n))2 + max (max(m, n), 4*(min(m, n))2 + 4*min(m, n))\r\n\r\nFor complex flavors:\r\n\r\nIf jobz = 'N', lwork  2*min(m, n) + max(m, n);\r\n\r\nIf jobz = 'O', lwork  2*(min(m, n))2 + max(m, n) + 2*min(m, n);\r\n\r\nIf jobz = 'S' or 'A', lwork  (min(m, n))2 + max(m, n) + 2*min(m, n);\r\n```\r\n`lwork` must be smaller than `2147483647` (maximum `int`) in order to not hit the limitation of the 32-bit interface.\n<issue_comment>username_0: @username_2 Any clue as to why `svdvals` and `eigvalsh` work?\n<issue_comment>username_1: For what is worth, `svdvals` and `eigvalsh` may work in master when the input does not require gradients, as we do not compute the `U` and the `Vh` in those cases, so the working space needed is less than for the full SVD / eigendecomposition.","repo":"pytorch/pytorch","issue_id":1052456616,"issue_number":68291}
{"question":"What functions are used in PyTorch to replicate a specific problem with a randomly generated matrix and SVD decomposition?","answer":"The functions used are `torch.rand` and `torch.linalg.svd` in PyTorch.","id":101,"text":"Ok, I am now able to replicate this problem using just `torch.rand` and `torch.linalg.svd`.\r\n\r\nThe following fails:\r\n```\r\nx = torch.rand(30000,30000)\r\nu,s,v = torch.linalg.svd(x)\r\n```","context":"<issue_start><issue_comment>Title: INTERNAL ASSERT FAILED at \"/opt/conda/conda-bld/pytorch_1634272068185/work/aten/src/ATen/native/LinearAlgebraUtils.h\":244\nusername_0: ##  Bug\r\n\r\nCalling `torch.linalg.svd` on a large float64 tensor on the CPU (size 90k x ~34k) leads to this error. `torch.linalg.eigh` on `tensor.T@tensor` seg faults.\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. I don't know. I have a large tensor of type float64. Calling `torch.linalg.svd`on it produces this error.\r\n\r\n```\r\nIntel MKL ERROR: Parameter 12 was incorrect on entry to DGESDD.\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n/tmp/ipykernel_3300/2679862595.py in <module>\r\n----> 1 torch.linalg.svd(tensor)\r\n\r\nRuntimeError: falseINTERNAL ASSERT FAILED at \"/opt/conda/conda-bld/pytorch_1634272068185/work/aten/src/ATen/native/LinearAlgebraUtils.h\":244, please report a bug to PyTorch. svd_cpu: Argument 12 has illegal value. Most certainly there is a bug in the implementation calling the backend library.\r\n```\r\n## Expected behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n## Environment\r\n\r\nPlease copy and paste the output from our\r\n[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)\r\n(or fill out the checklist below manually).\r\n\r\nYou can get the script and run it with:\r\n```\r\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\r\n# For security purposes, please check the contents of collect_env.py before running it.\r\npython collect_env.py\r\n```\r\n\r\n - PyTorch Version (e.g., 1.0): '1.10.0'\r\n - OS (e.g., Linux): Linux\r\n - How you installed PyTorch (`conda`, `pip`, source): conda\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.9.5\r\n - CUDA/cuDNN version:\r\n - GPU models and configuration:On the CPU\r\n - Any other relevant information:\r\n\r\n## Additional context\r\n`torch.linalg.eigh` on `tensor.T@tensor` seg faults.\n<issue_comment>username_0: I have been trying to figure out what is special about this tensor that is causing the algorithm to fail.\r\n\r\nI note that `torch.linalg.svdvals` is successful.\n<issue_comment>username_0: Ok, I am now able to replicate this problem using just `torch.rand` and `torch.linalg.svd`.\r\n\r\nThe following fails:\r\n```\r\nx = torch.rand(30000,30000)\r\nu,s,v = torch.linalg.svd(x)\r\n```\n<issue_comment>username_1: Thank you for the tiny repro! I can reproduce in master. cc @username_2\n<issue_comment>username_2: The problem is related to https://github.com/pytorch/pytorch/issues/51720.\r\n\r\nThe optimal workspace size for the input of this size is `2.70021e+09` which doesn't fit into `int` type. In PyTorch, we use a 32-bit interface to backend linear algebra libraries.\r\nWe can \"fix\" the internal assert with this patch\r\n```\r\ndiff --git a/aten/src/ATen/native/BatchLinearAlgebra.cpp b/aten/src/ATen/native/BatchLinearAlgebra.cpp\r\nindex 853f5c2cbe..8902958d98 100644\r\n--- a/aten/src/ATen/native/BatchLinearAlgebra.cpp\r\n+++ b/aten/src/ATen/native/BatchLinearAlgebra.cpp\r\n@@ -2984,7 +2984,9 @@ static void apply_svd(Tensor& self, Tensor& U, Tensor& S, Tensor& VT,\r\n   int lwork = -1;\r\n   scalar_t wkopt;\r\n   lapackSvd<scalar_t, value_t>(jobz, m, n, self_data, lda, S_data, U_data, lda, VT_data, ldvt, &wkopt, lwork, rwork_data, iwork_data, &info);\r\n-  lwork = std::max<int>(1, real_impl<scalar_t, value_t>(wkopt));\r\n+  TORCH_CHECK(\r\n+      (real_impl<scalar_t, value_t>(wkopt)) < INT_MAX,\r\n+      \"svd: The optimal workspace size is larger than allowed by 32-bit interface to backend math library.\");\r\n   Tensor work = at::empty({lwork}, self.options());\r\n   auto work_data = work.data_ptr<scalar_t>();\r\n```\r\nThe issue is likely there for other functions as well and we could include checks of the values before casting them to int.\n<issue_comment>username_0: @username_2 will there be any way around this in the future for large SVDs? Or is there an upper size limit due to the 32bit interface?\n<issue_comment>username_2: Unfortunately, there is no workaround available.\r\nFrom the MKL documentation the workspace size (`lwork`) can be computed using these guidelines:\r\n```\r\nFor real flavors:\r\n\r\nIf jobz = 'N', lwork  3*min(m, n) + max (max(m,n), 6*min(m, n));\r\n\r\nIf jobz = 'O', lwork  3*(min(m, n))2 + max (max(m, n), 5*(min(m, n))2 + 4*min(m, n));\r\n\r\nIf jobz = 'S' or 'A', lwork  3*(min(m, n))2 + max (max(m, n), 4*(min(m, n))2 + 4*min(m, n))\r\n\r\nFor complex flavors:\r\n\r\nIf jobz = 'N', lwork  2*min(m, n) + max(m, n);\r\n\r\nIf jobz = 'O', lwork  2*(min(m, n))2 + max(m, n) + 2*min(m, n);\r\n\r\nIf jobz = 'S' or 'A', lwork  (min(m, n))2 + max(m, n) + 2*min(m, n);\r\n```\r\n`lwork` must be smaller than `2147483647` (maximum `int`) in order to not hit the limitation of the 32-bit interface.\n<issue_comment>username_0: @username_2 Any clue as to why `svdvals` and `eigvalsh` work?\n<issue_comment>username_1: For what is worth, `svdvals` and `eigvalsh` may work in master when the input does not require gradients, as we do not compute the `U` and the `Vh` in those cases, so the working space needed is less than for the full SVD / eigendecomposition.","repo":"pytorch/pytorch","issue_id":1052456616,"issue_number":68291}
{"question":"What should the user do to identify unsupported operations in CycleGAN code and find replacements for them?","answer":"The user should put debug statements in the CycleGAN code to identify unsupported operations by their CPU and search for replacements for those operations.","id":128,"text":"@username_2 You can try to put debug statements in CycleGAN code and see which one isn't supported by your CPU (probably some PIL operations), and search for replacements. Sorry I don't have much else to offer.","context":"<issue_start><issue_comment>Title: Problem in DataLoader (same problem with #4643, but not solved by it)\nusername_0: - OS: Ubuntu 16.04\r\n- PyTorch version: torch-0.3.1-cp36-cp36-linux_x86_64.whl\r\n- How you installed PyTorch (conda, pip, source): pip3 from the binary\r\n- Python version: 3.6\r\n- CUDA/cuDNN version: 9.1\r\n- GPU models and configuration: GTX 1070 Ti\r\n\r\nAfter install using the wheel file, I have changed the torch/utils/data/Dataloader.py according to the changes shown at https://github.com/pytorch/pytorch/pull/4643\r\n\r\nBut the same error still happen when I try the tutorial example to load image database:\r\n\r\nException ignored in: <bound method DataLoaderIter.__del__ of <torch.utils.data.dataloader.DataLoaderIter object at 0x56090c8c7348>>\r\nTraceback (most recent call last):\r\n  File \"/home/username_0/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 375, in __del__\r\n  File \"/home/username_0/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 361, in _shutdown_workers\r\n  File \"/home/username_0/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 39, in _get_from_queue\r\n  File \"/usr/pkg/lib/python3.6/multiprocessing/queues.py\", line 337, in get\r\nImportError: sys.meta_path is None, Python is likely shutting down\r\n\r\nSo could anyone help me on that? Or is there any other file I should modify to get it run?\n<issue_comment>username_0: And, I am not sure how can I use the changes in torch/csrc/DataLoader.cpp, should I clone all the source code on GitHub and build the newest binary to fully fix this my problem?\n<issue_comment>username_1: when does this error happen? is it happening when you exit python?\n<issue_comment>username_2: @username_1 and @username_3,\r\nI am running this:\r\n\r\npython train.py --dataroot ./datasets/horse2zebra --name horse2zebra_cyclegan --model cycle_gan --no_dropout --loadSize 64 --nThreads 1\r\n\r\nfrom \r\n\r\nhttps://github.com/junyanz/pytorch-CycleGAN-and-pix2pix\r\n\r\nThe model is succefully created:\r\nmodel [CycleGANModel] was created\r\n\r\nI am on ubuntu 16.04, pytorch 3.1, cuda 9.0 and cuDnn 7.1\r\n\r\nThe error I get is traced below:\r\ncreate web directory ./checkpoints/horse2zebra_cyclegan/web...\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 22, in <module>\r\n    for i, data in enumerate(dataset):\r\n  File \"/home/sam/Documents/ComputerVision/pytorch-CycleGAN-and-pix2pix/data/custom_dataset_data_loader.py\", line 44, in __iter__\r\n    for i, data in enumerate(self.dataloader):\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 275, in __next__\r\n    idx, batch = self._get_batch()\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 254, in _get_batch\r\n    return self.data_queue.get()\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/queues.py\", line 335, in get\r\n    res = self._reader.recv_bytes()\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\r\n    buf = self._recv_bytes(maxlength)\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\r\n    buf = self._recv(4)\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\r\n    chunk = read(handle, remaining)\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 175, in handler\r\n    _error_if_any_worker_fails()\r\nRuntimeError: DataLoader worker (pid 3792) is killed by signal: Illegal instruction.\n<issue_comment>username_3: @username_2 that error is because you are running it on a computer that is too old and does not have SSE4 instruction on CPU.\n<issue_comment>username_1: @username_2 Also, it is more likely due to that your CPU doesn't support the operations done in the dataset.__getitem__.\n<issue_comment>username_2: @username_1 & @username_3,\r\n\r\nSome months ago I made it work in windows10 (I think)\r\nThe my windows machine crashed. So here I am. Sorry\r\n\r\nHere is what I see in cat /proc/cpuinfo:\r\n\r\nprocessor\t: 0\r\nvendor_id\t: AuthenticAMD\r\ncpu family\t: 21\r\nmodel\t\t: 2\r\nmodel name\t: AMD FX(tm)-4300 Quad-Core Processor\r\nstepping\t: 0\r\nmicrocode\t: 0x600081c\r\ncpu MHz\t\t: 1400.000\r\ncache size\t: 2048 KB\r\nphysical id\t: 0\r\nsiblings\t: 4\r\ncore id\t\t: 0\r\ncpu cores\t: 2\r\napicid\t\t: 16\r\ninitial apicid\t: 0\r\nfpu\t\t: yes\r\nfpu_exception\t: yes\r\ncpuid level\t: 13\r\nwp\t\t: yes\r\nflags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf pni pclmulqdq monitor ssse3 fma cx16 **sse4_1 sse4_2** popcnt aes xsave avx f16c lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs xop skinit wdt lwp fma4 tce nodeid_msr tbm topoext perfctr_core perfctr_nb cpb hw_pstate retpoline retpoline_amd rsb_ctxsw vmmcall bmi1 arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold\r\nbugs\t\t: fxsave_leak sysret_ss_attrs null_seg spectre_v1 spectre_v2\r\nbogomips\t: 7634.15\r\nTLB size\t: 1536 4K pages\r\nclflush size\t: 64\r\ncache_alignment\t: 64\r\naddress sizes\t: 48 bits physical, 48 bits virtual\r\npower management: ts ttp tm 100mhzsteps hwpstate cpb eff_freq_ro\r\n\r\n@username_1 how do I check if CPU support dataset.getitem?\n<issue_comment>username_1: @username_2 run with `nThreads=0`\n<issue_comment>username_2: tried it with nThreads=0\r\ndifferent error...\r\nmodel [CycleGANModel] was created\r\ncreate web directory ./checkpoints/horse2zebra_cyclegan/web...\r\nIllegal instruction (core dumped)\n<issue_comment>username_2: I see something I am uneasy about:\r\nsee bold below\r\n\r\nCustomDatasetDataLoader\r\ndataset [UnalignedDataset] was created\r\n/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torchvision-0.2.0-py3.6.egg/torchvision/transforms/transforms.py:156: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\r\n#training images = 1334\r\ncycle_gan\r\n**initialization method [normal]\r\ninitialization method [normal]\r\ninitialization method [normal]\r\ninitialization method [normal]**\n<issue_comment>username_1: @username_2 No it's same error. We just wrap it in a nice way so you can see it even if it's in the workers. \r\n\r\nIsn't that expected? CycleGAN has four networks: D_X D_Y G F .\n<issue_comment>username_1: @username_2 anyways, it confirms my hypothesis that your CPU doesn't support the operation in `dataset.__getitem__`.\n<issue_comment>username_2: Oh... Can you suggest a workaround?\n<issue_comment>username_1: @username_2 You can try to put debug statements in CycleGAN code and see which one isn't supported by your CPU (probably some PIL operations), and search for replacements. Sorry I don't have much else to offer.\n<issue_comment>username_2: @username_1 \r\nThank you sir !<issue_closed>\n<issue_comment>username_1: closed due to inactivity. feel free to reopen if you can reproduce with latest stable (0.4 at the moment of writing).\n<issue_comment>username_4: Still can't get this to work.\r\n\r\nhttps://stackoverflow.com/questions/53449927/runtimeerror-dataloader-worker-is-killed-by-signal-illegal-instruction\r\n\r\nPlease help.","repo":"pytorch/pytorch","issue_id":304991946,"issue_number":5761}
{"question":"What problem is the text chunk addressing and how does the proposed patch in the BatchLinearAlgebra.cpp file aim to solve it?","answer":"The text addresses a problem related to a large workspace size not fitting into the `int` type in PyTorch. The proposed patch in the BatchLinearAlgebra.cpp file includes a check for the optimal workspace size before casting it to `int`. This check ensures that the workspace size remains within the limits allowed by the 32-bit interface to backend linear algebra libraries in PyTorch.","id":103,"text":"The problem is related to https://github.com/pytorch/pytorch/issues/51720.\r\n\r\nThe optimal workspace size for the input of this size is `2.70021e+09` which doesn't fit into `int` type. In PyTorch, we use a 32-bit interface to backend linear algebra libraries.\r\nWe can \"fix\" the internal assert with this patch\r\n```\r\ndiff --git a/aten/src/ATen/native/BatchLinearAlgebra.cpp b/aten/src/ATen/native/BatchLinearAlgebra.cpp\r\nindex 853f5c2cbe..8902958d98 100644\r\n--- a/aten/src/ATen/native/BatchLinearAlgebra.cpp\r\n+++ b/aten/src/ATen/native/BatchLinearAlgebra.cpp\r\n@@ -2984,7 +2984,9 @@ static void apply_svd(Tensor& self, Tensor& U, Tensor& S, Tensor& VT,\r\n   int lwork = -1;\r\n   scalar_t wkopt;\r\n   lapackSvd<scalar_t, value_t>(jobz, m, n, self_data, lda, S_data, U_data, lda, VT_data, ldvt, &wkopt, lwork, rwork_data, iwork_data, &info);\r\n-  lwork = std::max<int>(1, real_impl<scalar_t, value_t>(wkopt));\r\n+  TORCH_CHECK(\r\n+      (real_impl<scalar_t, value_t>(wkopt)) < INT_MAX,\r\n+      \"svd: The optimal workspace size is larger than allowed by 32-bit interface to backend math library.\");\r\n   Tensor work = at::empty({lwork}, self.options());\r\n   auto work_data = work.data_ptr<scalar_t>();\r\n```\r\nThe issue is likely there for other functions as well and we could include checks of the values before casting them to int.","context":"<issue_start><issue_comment>Title: INTERNAL ASSERT FAILED at \"/opt/conda/conda-bld/pytorch_1634272068185/work/aten/src/ATen/native/LinearAlgebraUtils.h\":244\nusername_0: ##  Bug\r\n\r\nCalling `torch.linalg.svd` on a large float64 tensor on the CPU (size 90k x ~34k) leads to this error. `torch.linalg.eigh` on `tensor.T@tensor` seg faults.\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. I don't know. I have a large tensor of type float64. Calling `torch.linalg.svd`on it produces this error.\r\n\r\n```\r\nIntel MKL ERROR: Parameter 12 was incorrect on entry to DGESDD.\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n/tmp/ipykernel_3300/2679862595.py in <module>\r\n----> 1 torch.linalg.svd(tensor)\r\n\r\nRuntimeError: falseINTERNAL ASSERT FAILED at \"/opt/conda/conda-bld/pytorch_1634272068185/work/aten/src/ATen/native/LinearAlgebraUtils.h\":244, please report a bug to PyTorch. svd_cpu: Argument 12 has illegal value. Most certainly there is a bug in the implementation calling the backend library.\r\n```\r\n## Expected behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n## Environment\r\n\r\nPlease copy and paste the output from our\r\n[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)\r\n(or fill out the checklist below manually).\r\n\r\nYou can get the script and run it with:\r\n```\r\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\r\n# For security purposes, please check the contents of collect_env.py before running it.\r\npython collect_env.py\r\n```\r\n\r\n - PyTorch Version (e.g., 1.0): '1.10.0'\r\n - OS (e.g., Linux): Linux\r\n - How you installed PyTorch (`conda`, `pip`, source): conda\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.9.5\r\n - CUDA/cuDNN version:\r\n - GPU models and configuration:On the CPU\r\n - Any other relevant information:\r\n\r\n## Additional context\r\n`torch.linalg.eigh` on `tensor.T@tensor` seg faults.\n<issue_comment>username_0: I have been trying to figure out what is special about this tensor that is causing the algorithm to fail.\r\n\r\nI note that `torch.linalg.svdvals` is successful.\n<issue_comment>username_0: Ok, I am now able to replicate this problem using just `torch.rand` and `torch.linalg.svd`.\r\n\r\nThe following fails:\r\n```\r\nx = torch.rand(30000,30000)\r\nu,s,v = torch.linalg.svd(x)\r\n```\n<issue_comment>username_1: Thank you for the tiny repro! I can reproduce in master. cc @username_2\n<issue_comment>username_2: The problem is related to https://github.com/pytorch/pytorch/issues/51720.\r\n\r\nThe optimal workspace size for the input of this size is `2.70021e+09` which doesn't fit into `int` type. In PyTorch, we use a 32-bit interface to backend linear algebra libraries.\r\nWe can \"fix\" the internal assert with this patch\r\n```\r\ndiff --git a/aten/src/ATen/native/BatchLinearAlgebra.cpp b/aten/src/ATen/native/BatchLinearAlgebra.cpp\r\nindex 853f5c2cbe..8902958d98 100644\r\n--- a/aten/src/ATen/native/BatchLinearAlgebra.cpp\r\n+++ b/aten/src/ATen/native/BatchLinearAlgebra.cpp\r\n@@ -2984,7 +2984,9 @@ static void apply_svd(Tensor& self, Tensor& U, Tensor& S, Tensor& VT,\r\n   int lwork = -1;\r\n   scalar_t wkopt;\r\n   lapackSvd<scalar_t, value_t>(jobz, m, n, self_data, lda, S_data, U_data, lda, VT_data, ldvt, &wkopt, lwork, rwork_data, iwork_data, &info);\r\n-  lwork = std::max<int>(1, real_impl<scalar_t, value_t>(wkopt));\r\n+  TORCH_CHECK(\r\n+      (real_impl<scalar_t, value_t>(wkopt)) < INT_MAX,\r\n+      \"svd: The optimal workspace size is larger than allowed by 32-bit interface to backend math library.\");\r\n   Tensor work = at::empty({lwork}, self.options());\r\n   auto work_data = work.data_ptr<scalar_t>();\r\n```\r\nThe issue is likely there for other functions as well and we could include checks of the values before casting them to int.\n<issue_comment>username_0: @username_2 will there be any way around this in the future for large SVDs? Or is there an upper size limit due to the 32bit interface?\n<issue_comment>username_2: Unfortunately, there is no workaround available.\r\nFrom the MKL documentation the workspace size (`lwork`) can be computed using these guidelines:\r\n```\r\nFor real flavors:\r\n\r\nIf jobz = 'N', lwork  3*min(m, n) + max (max(m,n), 6*min(m, n));\r\n\r\nIf jobz = 'O', lwork  3*(min(m, n))2 + max (max(m, n), 5*(min(m, n))2 + 4*min(m, n));\r\n\r\nIf jobz = 'S' or 'A', lwork  3*(min(m, n))2 + max (max(m, n), 4*(min(m, n))2 + 4*min(m, n))\r\n\r\nFor complex flavors:\r\n\r\nIf jobz = 'N', lwork  2*min(m, n) + max(m, n);\r\n\r\nIf jobz = 'O', lwork  2*(min(m, n))2 + max(m, n) + 2*min(m, n);\r\n\r\nIf jobz = 'S' or 'A', lwork  (min(m, n))2 + max(m, n) + 2*min(m, n);\r\n```\r\n`lwork` must be smaller than `2147483647` (maximum `int`) in order to not hit the limitation of the 32-bit interface.\n<issue_comment>username_0: @username_2 Any clue as to why `svdvals` and `eigvalsh` work?\n<issue_comment>username_1: For what is worth, `svdvals` and `eigvalsh` may work in master when the input does not require gradients, as we do not compute the `U` and the `Vh` in those cases, so the working space needed is less than for the full SVD / eigendecomposition.","repo":"pytorch/pytorch","issue_id":1052456616,"issue_number":68291}
{"question":"How can the workspace size (`lwork`) be computed according to the MKL documentation guidelines?","answer":"The workspace size (`lwork`) can be computed based on the guidelines provided in the MKL documentation, which vary for real and complex flavors and depend on the value of `jobz`. Different formulas are specified for calculating the minimum required `lwork` based on the dimensions of the matrices `m` and `n`, with an additional constraint that `lwork` must be smaller than `2147483647` to avoid exceeding the 32-bit interface limitation.","id":105,"text":"Unfortunately, there is no workaround available.\r\nFrom the MKL documentation the workspace size (`lwork`) can be computed using these guidelines:\r\n```\r\nFor real flavors:\r\n\r\nIf jobz = 'N', lwork  3*min(m, n) + max (max(m,n), 6*min(m, n));\r\n\r\nIf jobz = 'O', lwork  3*(min(m, n))2 + max (max(m, n), 5*(min(m, n))2 + 4*min(m, n));\r\n\r\nIf jobz = 'S' or 'A', lwork  3*(min(m, n))2 + max (max(m, n), 4*(min(m, n))2 + 4*min(m, n))\r\n\r\nFor complex flavors:\r\n\r\nIf jobz = 'N', lwork  2*min(m, n) + max(m, n);\r\n\r\nIf jobz = 'O', lwork  2*(min(m, n))2 + max(m, n) + 2*min(m, n);\r\n\r\nIf jobz = 'S' or 'A', lwork  (min(m, n))2 + max(m, n) + 2*min(m, n);\r\n```\r\n`lwork` must be smaller than `2147483647` (maximum `int`) in order to not hit the limitation of the 32-bit interface.","context":"<issue_start><issue_comment>Title: INTERNAL ASSERT FAILED at \"/opt/conda/conda-bld/pytorch_1634272068185/work/aten/src/ATen/native/LinearAlgebraUtils.h\":244\nusername_0: ##  Bug\r\n\r\nCalling `torch.linalg.svd` on a large float64 tensor on the CPU (size 90k x ~34k) leads to this error. `torch.linalg.eigh` on `tensor.T@tensor` seg faults.\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. I don't know. I have a large tensor of type float64. Calling `torch.linalg.svd`on it produces this error.\r\n\r\n```\r\nIntel MKL ERROR: Parameter 12 was incorrect on entry to DGESDD.\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n/tmp/ipykernel_3300/2679862595.py in <module>\r\n----> 1 torch.linalg.svd(tensor)\r\n\r\nRuntimeError: falseINTERNAL ASSERT FAILED at \"/opt/conda/conda-bld/pytorch_1634272068185/work/aten/src/ATen/native/LinearAlgebraUtils.h\":244, please report a bug to PyTorch. svd_cpu: Argument 12 has illegal value. Most certainly there is a bug in the implementation calling the backend library.\r\n```\r\n## Expected behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n## Environment\r\n\r\nPlease copy and paste the output from our\r\n[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)\r\n(or fill out the checklist below manually).\r\n\r\nYou can get the script and run it with:\r\n```\r\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\r\n# For security purposes, please check the contents of collect_env.py before running it.\r\npython collect_env.py\r\n```\r\n\r\n - PyTorch Version (e.g., 1.0): '1.10.0'\r\n - OS (e.g., Linux): Linux\r\n - How you installed PyTorch (`conda`, `pip`, source): conda\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.9.5\r\n - CUDA/cuDNN version:\r\n - GPU models and configuration:On the CPU\r\n - Any other relevant information:\r\n\r\n## Additional context\r\n`torch.linalg.eigh` on `tensor.T@tensor` seg faults.\n<issue_comment>username_0: I have been trying to figure out what is special about this tensor that is causing the algorithm to fail.\r\n\r\nI note that `torch.linalg.svdvals` is successful.\n<issue_comment>username_0: Ok, I am now able to replicate this problem using just `torch.rand` and `torch.linalg.svd`.\r\n\r\nThe following fails:\r\n```\r\nx = torch.rand(30000,30000)\r\nu,s,v = torch.linalg.svd(x)\r\n```\n<issue_comment>username_1: Thank you for the tiny repro! I can reproduce in master. cc @username_2\n<issue_comment>username_2: The problem is related to https://github.com/pytorch/pytorch/issues/51720.\r\n\r\nThe optimal workspace size for the input of this size is `2.70021e+09` which doesn't fit into `int` type. In PyTorch, we use a 32-bit interface to backend linear algebra libraries.\r\nWe can \"fix\" the internal assert with this patch\r\n```\r\ndiff --git a/aten/src/ATen/native/BatchLinearAlgebra.cpp b/aten/src/ATen/native/BatchLinearAlgebra.cpp\r\nindex 853f5c2cbe..8902958d98 100644\r\n--- a/aten/src/ATen/native/BatchLinearAlgebra.cpp\r\n+++ b/aten/src/ATen/native/BatchLinearAlgebra.cpp\r\n@@ -2984,7 +2984,9 @@ static void apply_svd(Tensor& self, Tensor& U, Tensor& S, Tensor& VT,\r\n   int lwork = -1;\r\n   scalar_t wkopt;\r\n   lapackSvd<scalar_t, value_t>(jobz, m, n, self_data, lda, S_data, U_data, lda, VT_data, ldvt, &wkopt, lwork, rwork_data, iwork_data, &info);\r\n-  lwork = std::max<int>(1, real_impl<scalar_t, value_t>(wkopt));\r\n+  TORCH_CHECK(\r\n+      (real_impl<scalar_t, value_t>(wkopt)) < INT_MAX,\r\n+      \"svd: The optimal workspace size is larger than allowed by 32-bit interface to backend math library.\");\r\n   Tensor work = at::empty({lwork}, self.options());\r\n   auto work_data = work.data_ptr<scalar_t>();\r\n```\r\nThe issue is likely there for other functions as well and we could include checks of the values before casting them to int.\n<issue_comment>username_0: @username_2 will there be any way around this in the future for large SVDs? Or is there an upper size limit due to the 32bit interface?\n<issue_comment>username_2: Unfortunately, there is no workaround available.\r\nFrom the MKL documentation the workspace size (`lwork`) can be computed using these guidelines:\r\n```\r\nFor real flavors:\r\n\r\nIf jobz = 'N', lwork  3*min(m, n) + max (max(m,n), 6*min(m, n));\r\n\r\nIf jobz = 'O', lwork  3*(min(m, n))2 + max (max(m, n), 5*(min(m, n))2 + 4*min(m, n));\r\n\r\nIf jobz = 'S' or 'A', lwork  3*(min(m, n))2 + max (max(m, n), 4*(min(m, n))2 + 4*min(m, n))\r\n\r\nFor complex flavors:\r\n\r\nIf jobz = 'N', lwork  2*min(m, n) + max(m, n);\r\n\r\nIf jobz = 'O', lwork  2*(min(m, n))2 + max(m, n) + 2*min(m, n);\r\n\r\nIf jobz = 'S' or 'A', lwork  (min(m, n))2 + max(m, n) + 2*min(m, n);\r\n```\r\n`lwork` must be smaller than `2147483647` (maximum `int`) in order to not hit the limitation of the 32-bit interface.\n<issue_comment>username_0: @username_2 Any clue as to why `svdvals` and `eigvalsh` work?\n<issue_comment>username_1: For what is worth, `svdvals` and `eigvalsh` may work in master when the input does not require gradients, as we do not compute the `U` and the `Vh` in those cases, so the working space needed is less than for the full SVD / eigendecomposition.","repo":"pytorch/pytorch","issue_id":1052456616,"issue_number":68291}
{"question":"In which cases do `svdvals` and `eigvalsh` work in master without computing `U` and `Vh` and requiring less working space?","answer":"`svdvals` and `eigvalsh` may work in master when the input does not require gradients, as the `U` and the `Vh` are not computed in those cases, leading to less working space needed compared to the full SVD / eigendecomposition.","id":107,"text":"For what is worth, `svdvals` and `eigvalsh` may work in master when the input does not require gradients, as we do not compute the `U` and the `Vh` in those cases, so the working space needed is less than for the full SVD / eigendecomposition.","context":"<issue_start><issue_comment>Title: INTERNAL ASSERT FAILED at \"/opt/conda/conda-bld/pytorch_1634272068185/work/aten/src/ATen/native/LinearAlgebraUtils.h\":244\nusername_0: ##  Bug\r\n\r\nCalling `torch.linalg.svd` on a large float64 tensor on the CPU (size 90k x ~34k) leads to this error. `torch.linalg.eigh` on `tensor.T@tensor` seg faults.\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. I don't know. I have a large tensor of type float64. Calling `torch.linalg.svd`on it produces this error.\r\n\r\n```\r\nIntel MKL ERROR: Parameter 12 was incorrect on entry to DGESDD.\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n/tmp/ipykernel_3300/2679862595.py in <module>\r\n----> 1 torch.linalg.svd(tensor)\r\n\r\nRuntimeError: falseINTERNAL ASSERT FAILED at \"/opt/conda/conda-bld/pytorch_1634272068185/work/aten/src/ATen/native/LinearAlgebraUtils.h\":244, please report a bug to PyTorch. svd_cpu: Argument 12 has illegal value. Most certainly there is a bug in the implementation calling the backend library.\r\n```\r\n## Expected behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n## Environment\r\n\r\nPlease copy and paste the output from our\r\n[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)\r\n(or fill out the checklist below manually).\r\n\r\nYou can get the script and run it with:\r\n```\r\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\r\n# For security purposes, please check the contents of collect_env.py before running it.\r\npython collect_env.py\r\n```\r\n\r\n - PyTorch Version (e.g., 1.0): '1.10.0'\r\n - OS (e.g., Linux): Linux\r\n - How you installed PyTorch (`conda`, `pip`, source): conda\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.9.5\r\n - CUDA/cuDNN version:\r\n - GPU models and configuration:On the CPU\r\n - Any other relevant information:\r\n\r\n## Additional context\r\n`torch.linalg.eigh` on `tensor.T@tensor` seg faults.\n<issue_comment>username_0: I have been trying to figure out what is special about this tensor that is causing the algorithm to fail.\r\n\r\nI note that `torch.linalg.svdvals` is successful.\n<issue_comment>username_0: Ok, I am now able to replicate this problem using just `torch.rand` and `torch.linalg.svd`.\r\n\r\nThe following fails:\r\n```\r\nx = torch.rand(30000,30000)\r\nu,s,v = torch.linalg.svd(x)\r\n```\n<issue_comment>username_1: Thank you for the tiny repro! I can reproduce in master. cc @username_2\n<issue_comment>username_2: The problem is related to https://github.com/pytorch/pytorch/issues/51720.\r\n\r\nThe optimal workspace size for the input of this size is `2.70021e+09` which doesn't fit into `int` type. In PyTorch, we use a 32-bit interface to backend linear algebra libraries.\r\nWe can \"fix\" the internal assert with this patch\r\n```\r\ndiff --git a/aten/src/ATen/native/BatchLinearAlgebra.cpp b/aten/src/ATen/native/BatchLinearAlgebra.cpp\r\nindex 853f5c2cbe..8902958d98 100644\r\n--- a/aten/src/ATen/native/BatchLinearAlgebra.cpp\r\n+++ b/aten/src/ATen/native/BatchLinearAlgebra.cpp\r\n@@ -2984,7 +2984,9 @@ static void apply_svd(Tensor& self, Tensor& U, Tensor& S, Tensor& VT,\r\n   int lwork = -1;\r\n   scalar_t wkopt;\r\n   lapackSvd<scalar_t, value_t>(jobz, m, n, self_data, lda, S_data, U_data, lda, VT_data, ldvt, &wkopt, lwork, rwork_data, iwork_data, &info);\r\n-  lwork = std::max<int>(1, real_impl<scalar_t, value_t>(wkopt));\r\n+  TORCH_CHECK(\r\n+      (real_impl<scalar_t, value_t>(wkopt)) < INT_MAX,\r\n+      \"svd: The optimal workspace size is larger than allowed by 32-bit interface to backend math library.\");\r\n   Tensor work = at::empty({lwork}, self.options());\r\n   auto work_data = work.data_ptr<scalar_t>();\r\n```\r\nThe issue is likely there for other functions as well and we could include checks of the values before casting them to int.\n<issue_comment>username_0: @username_2 will there be any way around this in the future for large SVDs? Or is there an upper size limit due to the 32bit interface?\n<issue_comment>username_2: Unfortunately, there is no workaround available.\r\nFrom the MKL documentation the workspace size (`lwork`) can be computed using these guidelines:\r\n```\r\nFor real flavors:\r\n\r\nIf jobz = 'N', lwork  3*min(m, n) + max (max(m,n), 6*min(m, n));\r\n\r\nIf jobz = 'O', lwork  3*(min(m, n))2 + max (max(m, n), 5*(min(m, n))2 + 4*min(m, n));\r\n\r\nIf jobz = 'S' or 'A', lwork  3*(min(m, n))2 + max (max(m, n), 4*(min(m, n))2 + 4*min(m, n))\r\n\r\nFor complex flavors:\r\n\r\nIf jobz = 'N', lwork  2*min(m, n) + max(m, n);\r\n\r\nIf jobz = 'O', lwork  2*(min(m, n))2 + max(m, n) + 2*min(m, n);\r\n\r\nIf jobz = 'S' or 'A', lwork  (min(m, n))2 + max(m, n) + 2*min(m, n);\r\n```\r\n`lwork` must be smaller than `2147483647` (maximum `int`) in order to not hit the limitation of the 32-bit interface.\n<issue_comment>username_0: @username_2 Any clue as to why `svdvals` and `eigvalsh` work?\n<issue_comment>username_1: For what is worth, `svdvals` and `eigvalsh` may work in master when the input does not require gradients, as we do not compute the `U` and the `Vh` in those cases, so the working space needed is less than for the full SVD / eigendecomposition.","repo":"pytorch/pytorch","issue_id":1052456616,"issue_number":68291}
{"question":"What is the purpose of the refactoring change discussed in the text?","answer":"The purpose of the refactoring change is to ensure that methods with large bodies are moved from `.h` (header) files to `.cpp` (source) files in order to prevent slowing down compilation speed.","id":108,"text":"Stack from [ghstack](https://github.com/username_1/ghstack):\n* (to be filled)\n\nSimple refactoring change to ensure that methods with large bodies don't show up in `.h` (header) files. Instead they should be in `.cpp` (source) files to prevent slowing down compilation speed.\n\n#accept2ship\n\nDifferential Revision: [D34432507](https://our.internmc.facebook.com/intern/diff/D34432507/)","context":"<issue_start><issue_comment>Title: [PyTorch] [Model Tracer] Refactor method bodies in various record function handlers into source (.cpp files)\nusername_0: Stack from [ghstack](https://github.com/username_1/ghstack):\n* (to be filled)\n\nSimple refactoring change to ensure that methods with large bodies don't show up in `.h` (header) files. Instead they should be in `.cpp` (source) files to prevent slowing down compilation speed.\n\n#accept2ship\n\nDifferential Revision: [D34432507](https://our.internmc.facebook.com/intern/diff/D34432507/)\n<issue_comment>username_1: @username_1 has imported this pull request.  If you are a Facebook employee, you can view this diff [on Phabricator](https://www.internalfb.com/diff/D34432507).","repo":"pytorch/pytorch","issue_id":1148648561,"issue_number":73321}
{"question":"What action did @username_1 take with the pull request mentioned in the text?","answer":"@username_1 has imported the pull request.","id":109,"text":"@username_1 has imported this pull request.  If you are a Facebook employee, you can view this diff [on Phabricator](https://www.internalfb.com/diff/D34432507).","context":"<issue_start><issue_comment>Title: [PyTorch] [Model Tracer] Refactor method bodies in various record function handlers into source (.cpp files)\nusername_0: Stack from [ghstack](https://github.com/username_1/ghstack):\n* (to be filled)\n\nSimple refactoring change to ensure that methods with large bodies don't show up in `.h` (header) files. Instead they should be in `.cpp` (source) files to prevent slowing down compilation speed.\n\n#accept2ship\n\nDifferential Revision: [D34432507](https://our.internmc.facebook.com/intern/diff/D34432507/)\n<issue_comment>username_1: @username_1 has imported this pull request.  If you are a Facebook employee, you can view this diff [on Phabricator](https://www.internalfb.com/diff/D34432507).","repo":"pytorch/pytorch","issue_id":1148648561,"issue_number":73321}
{"question":"In the context of versioning schemes like Semantic Versioning (semver), what does the term 'bumping back' imply?","answer":"The term 'bumping back' in the context of versioning schemes like Semantic Versioning (semver) refers to revising the version of a software update to fit the 'semver-minor' category, even if the update itself does not technically qualify as 'semver-minor'. This adjustment is made to ensure compatibility with the versioning system's requirements for a release.","id":111,"text":"(Not technically semver-minor on its own but bumping back to that since it otherwise thwarts my attempt to run a release with only semver-patch & semver-exempt changes)","context":"<issue_start><issue_comment>Title: Fix tests from #6158 to use Jasmine 2\nusername_0: When merging, I didnt notice it uses old test format.\r\nFixing these to unbreak master.\n<issue_comment>username_1: (Not technically semver-minor on its own but bumping back to that since it otherwise thwarts my attempt to run a release with only semver-patch & semver-exempt changes)","repo":"facebook/react","issue_id":162356676,"issue_number":7126}
{"question":"What are the steps to be completed before submitting a pull request to the repository?","answer":"1. Fork the repository and create a branch from `master`. 2. Add tests for new code. 3. Update documentation for API changes. 4. Ensure the test suite passes (`npm test`). 5. Format code with Prettier (`npm run prettier`). 6. Run code linting (`npm run lint`). 7. Check Flow typechecks (`npm run flow`). 8. Complete the CLA (Contributor License Agreement).","id":112,"text":"**Before submitting a pull request,** please make sure the following is done:\r\n\r\n1. Fork [the repository](https://github.com/facebook/react) and create your branch from `master`.\r\n2. If you've added code that should be tested, add tests!\r\n3. If you've changed APIs, update the documentation.\r\n4. Ensure the test suite passes (`npm test`).\r\n5. Format your code with [prettier](https://github.com/prettier/prettier) (`npm run prettier`).\r\n6. Make sure your code lints (`npm run lint`).\r\n7. Run the [Flow](https://flowtype.org/) typechecks (`npm run flow`).\r\n8. If you haven't already, complete the CLA.","context":"<issue_start><issue_comment>Title: Update conferences\nusername_0: **Before submitting a pull request,** please make sure the following is done:\r\n\r\n1. Fork [the repository](https://github.com/facebook/react) and create your branch from `master`.\r\n2. If you've added code that should be tested, add tests!\r\n3. If you've changed APIs, update the documentation.\r\n4. Ensure the test suite passes (`npm test`).\r\n5. Format your code with [prettier](https://github.com/prettier/prettier) (`npm run prettier`).\r\n6. Make sure your code lints (`npm run lint`).\r\n7. Run the [Flow](https://flowtype.org/) typechecks (`npm run flow`).\r\n8. If you haven't already, complete the CLA.\n<issue_comment>username_1: React Boston and React Alicante also ended.\n<issue_comment>username_0: Thanks!","repo":"facebook/react","issue_id":259881446,"issue_number":10781}
{"question":"What modifications were made to the DataLoader.py file after installing PyTorch using the wheel file, and what could be done to address the ImportError related to sys.meta_path when loading an image database?","answer":"After installing PyTorch from the binary wheel file, modifications were made to DataLoader.py as shown in https://github.com/pytorch/pytorch/pull/4643. To address the ImportError regarding sys.meta_path when loading an image database, additional modifications or checks may be required in the Python environment to ensure proper initialization and handling of modules during runtime.","id":115,"text":"- OS: Ubuntu 16.04\r\n- PyTorch version: torch-0.3.1-cp36-cp36-linux_x86_64.whl\r\n- How you installed PyTorch (conda, pip, source): pip3 from the binary\r\n- Python version: 3.6\r\n- CUDA/cuDNN version: 9.1\r\n- GPU models and configuration: GTX 1070 Ti\r\n\r\nAfter install using the wheel file, I have changed the torch/utils/data/Dataloader.py according to the changes shown at https://github.com/pytorch/pytorch/pull/4643\r\n\r\nBut the same error still happen when I try the tutorial example to load image database:\r\n\r\nException ignored in: <bound method DataLoaderIter.__del__ of <torch.utils.data.dataloader.DataLoaderIter object at 0x56090c8c7348>>\r\nTraceback (most recent call last):\r\n  File \"/home/username_0/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 375, in __del__\r\n  File \"/home/username_0/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 361, in _shutdown_workers\r\n  File \"/home/username_0/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 39, in _get_from_queue\r\n  File \"/usr/pkg/lib/python3.6/multiprocessing/queues.py\", line 337, in get\r\nImportError: sys.meta_path is None, Python is likely shutting down\r\n\r\nSo could anyone help me on that? Or is there any other file I should modify to get it run?","context":"<issue_start><issue_comment>Title: Problem in DataLoader (same problem with #4643, but not solved by it)\nusername_0: - OS: Ubuntu 16.04\r\n- PyTorch version: torch-0.3.1-cp36-cp36-linux_x86_64.whl\r\n- How you installed PyTorch (conda, pip, source): pip3 from the binary\r\n- Python version: 3.6\r\n- CUDA/cuDNN version: 9.1\r\n- GPU models and configuration: GTX 1070 Ti\r\n\r\nAfter install using the wheel file, I have changed the torch/utils/data/Dataloader.py according to the changes shown at https://github.com/pytorch/pytorch/pull/4643\r\n\r\nBut the same error still happen when I try the tutorial example to load image database:\r\n\r\nException ignored in: <bound method DataLoaderIter.__del__ of <torch.utils.data.dataloader.DataLoaderIter object at 0x56090c8c7348>>\r\nTraceback (most recent call last):\r\n  File \"/home/username_0/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 375, in __del__\r\n  File \"/home/username_0/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 361, in _shutdown_workers\r\n  File \"/home/username_0/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 39, in _get_from_queue\r\n  File \"/usr/pkg/lib/python3.6/multiprocessing/queues.py\", line 337, in get\r\nImportError: sys.meta_path is None, Python is likely shutting down\r\n\r\nSo could anyone help me on that? Or is there any other file I should modify to get it run?\n<issue_comment>username_0: And, I am not sure how can I use the changes in torch/csrc/DataLoader.cpp, should I clone all the source code on GitHub and build the newest binary to fully fix this my problem?\n<issue_comment>username_1: when does this error happen? is it happening when you exit python?\n<issue_comment>username_2: @username_1 and @username_3,\r\nI am running this:\r\n\r\npython train.py --dataroot ./datasets/horse2zebra --name horse2zebra_cyclegan --model cycle_gan --no_dropout --loadSize 64 --nThreads 1\r\n\r\nfrom \r\n\r\nhttps://github.com/junyanz/pytorch-CycleGAN-and-pix2pix\r\n\r\nThe model is succefully created:\r\nmodel [CycleGANModel] was created\r\n\r\nI am on ubuntu 16.04, pytorch 3.1, cuda 9.0 and cuDnn 7.1\r\n\r\nThe error I get is traced below:\r\ncreate web directory ./checkpoints/horse2zebra_cyclegan/web...\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 22, in <module>\r\n    for i, data in enumerate(dataset):\r\n  File \"/home/sam/Documents/ComputerVision/pytorch-CycleGAN-and-pix2pix/data/custom_dataset_data_loader.py\", line 44, in __iter__\r\n    for i, data in enumerate(self.dataloader):\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 275, in __next__\r\n    idx, batch = self._get_batch()\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 254, in _get_batch\r\n    return self.data_queue.get()\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/queues.py\", line 335, in get\r\n    res = self._reader.recv_bytes()\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\r\n    buf = self._recv_bytes(maxlength)\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\r\n    buf = self._recv(4)\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\r\n    chunk = read(handle, remaining)\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 175, in handler\r\n    _error_if_any_worker_fails()\r\nRuntimeError: DataLoader worker (pid 3792) is killed by signal: Illegal instruction.\n<issue_comment>username_3: @username_2 that error is because you are running it on a computer that is too old and does not have SSE4 instruction on CPU.\n<issue_comment>username_1: @username_2 Also, it is more likely due to that your CPU doesn't support the operations done in the dataset.__getitem__.\n<issue_comment>username_2: @username_1 & @username_3,\r\n\r\nSome months ago I made it work in windows10 (I think)\r\nThe my windows machine crashed. So here I am. Sorry\r\n\r\nHere is what I see in cat /proc/cpuinfo:\r\n\r\nprocessor\t: 0\r\nvendor_id\t: AuthenticAMD\r\ncpu family\t: 21\r\nmodel\t\t: 2\r\nmodel name\t: AMD FX(tm)-4300 Quad-Core Processor\r\nstepping\t: 0\r\nmicrocode\t: 0x600081c\r\ncpu MHz\t\t: 1400.000\r\ncache size\t: 2048 KB\r\nphysical id\t: 0\r\nsiblings\t: 4\r\ncore id\t\t: 0\r\ncpu cores\t: 2\r\napicid\t\t: 16\r\ninitial apicid\t: 0\r\nfpu\t\t: yes\r\nfpu_exception\t: yes\r\ncpuid level\t: 13\r\nwp\t\t: yes\r\nflags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf pni pclmulqdq monitor ssse3 fma cx16 **sse4_1 sse4_2** popcnt aes xsave avx f16c lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs xop skinit wdt lwp fma4 tce nodeid_msr tbm topoext perfctr_core perfctr_nb cpb hw_pstate retpoline retpoline_amd rsb_ctxsw vmmcall bmi1 arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold\r\nbugs\t\t: fxsave_leak sysret_ss_attrs null_seg spectre_v1 spectre_v2\r\nbogomips\t: 7634.15\r\nTLB size\t: 1536 4K pages\r\nclflush size\t: 64\r\ncache_alignment\t: 64\r\naddress sizes\t: 48 bits physical, 48 bits virtual\r\npower management: ts ttp tm 100mhzsteps hwpstate cpb eff_freq_ro\r\n\r\n@username_1 how do I check if CPU support dataset.getitem?\n<issue_comment>username_1: @username_2 run with `nThreads=0`\n<issue_comment>username_2: tried it with nThreads=0\r\ndifferent error...\r\nmodel [CycleGANModel] was created\r\ncreate web directory ./checkpoints/horse2zebra_cyclegan/web...\r\nIllegal instruction (core dumped)\n<issue_comment>username_2: I see something I am uneasy about:\r\nsee bold below\r\n\r\nCustomDatasetDataLoader\r\ndataset [UnalignedDataset] was created\r\n/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torchvision-0.2.0-py3.6.egg/torchvision/transforms/transforms.py:156: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\r\n#training images = 1334\r\ncycle_gan\r\n**initialization method [normal]\r\ninitialization method [normal]\r\ninitialization method [normal]\r\ninitialization method [normal]**\n<issue_comment>username_1: @username_2 No it's same error. We just wrap it in a nice way so you can see it even if it's in the workers. \r\n\r\nIsn't that expected? CycleGAN has four networks: D_X D_Y G F .\n<issue_comment>username_1: @username_2 anyways, it confirms my hypothesis that your CPU doesn't support the operation in `dataset.__getitem__`.\n<issue_comment>username_2: Oh... Can you suggest a workaround?\n<issue_comment>username_1: @username_2 You can try to put debug statements in CycleGAN code and see which one isn't supported by your CPU (probably some PIL operations), and search for replacements. Sorry I don't have much else to offer.\n<issue_comment>username_2: @username_1 \r\nThank you sir !<issue_closed>\n<issue_comment>username_1: closed due to inactivity. feel free to reopen if you can reproduce with latest stable (0.4 at the moment of writing).\n<issue_comment>username_4: Still can't get this to work.\r\n\r\nhttps://stackoverflow.com/questions/53449927/runtimeerror-dataloader-worker-is-killed-by-signal-illegal-instruction\r\n\r\nPlease help.","repo":"pytorch/pytorch","issue_id":304991946,"issue_number":5761}
{"question":"How can I utilize the changes in torch/csrc/DataLoader.cpp? Should I clone all the source code on GitHub and build the newest binary to fully fix my problem?","answer":"To utilize the changes in DataLoader.cpp, you can follow these steps:\n1. Clone the source code repository from GitHub.\n2. Build the newest binary based on the changes in DataLoader.cpp.\n3. Test the new binary to see if it resolves your problem.","id":116,"text":"And, I am not sure how can I use the changes in torch/csrc/DataLoader.cpp, should I clone all the source code on GitHub and build the newest binary to fully fix this my problem?","context":"<issue_start><issue_comment>Title: Problem in DataLoader (same problem with #4643, but not solved by it)\nusername_0: - OS: Ubuntu 16.04\r\n- PyTorch version: torch-0.3.1-cp36-cp36-linux_x86_64.whl\r\n- How you installed PyTorch (conda, pip, source): pip3 from the binary\r\n- Python version: 3.6\r\n- CUDA/cuDNN version: 9.1\r\n- GPU models and configuration: GTX 1070 Ti\r\n\r\nAfter install using the wheel file, I have changed the torch/utils/data/Dataloader.py according to the changes shown at https://github.com/pytorch/pytorch/pull/4643\r\n\r\nBut the same error still happen when I try the tutorial example to load image database:\r\n\r\nException ignored in: <bound method DataLoaderIter.__del__ of <torch.utils.data.dataloader.DataLoaderIter object at 0x56090c8c7348>>\r\nTraceback (most recent call last):\r\n  File \"/home/username_0/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 375, in __del__\r\n  File \"/home/username_0/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 361, in _shutdown_workers\r\n  File \"/home/username_0/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 39, in _get_from_queue\r\n  File \"/usr/pkg/lib/python3.6/multiprocessing/queues.py\", line 337, in get\r\nImportError: sys.meta_path is None, Python is likely shutting down\r\n\r\nSo could anyone help me on that? Or is there any other file I should modify to get it run?\n<issue_comment>username_0: And, I am not sure how can I use the changes in torch/csrc/DataLoader.cpp, should I clone all the source code on GitHub and build the newest binary to fully fix this my problem?\n<issue_comment>username_1: when does this error happen? is it happening when you exit python?\n<issue_comment>username_2: @username_1 and @username_3,\r\nI am running this:\r\n\r\npython train.py --dataroot ./datasets/horse2zebra --name horse2zebra_cyclegan --model cycle_gan --no_dropout --loadSize 64 --nThreads 1\r\n\r\nfrom \r\n\r\nhttps://github.com/junyanz/pytorch-CycleGAN-and-pix2pix\r\n\r\nThe model is succefully created:\r\nmodel [CycleGANModel] was created\r\n\r\nI am on ubuntu 16.04, pytorch 3.1, cuda 9.0 and cuDnn 7.1\r\n\r\nThe error I get is traced below:\r\ncreate web directory ./checkpoints/horse2zebra_cyclegan/web...\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 22, in <module>\r\n    for i, data in enumerate(dataset):\r\n  File \"/home/sam/Documents/ComputerVision/pytorch-CycleGAN-and-pix2pix/data/custom_dataset_data_loader.py\", line 44, in __iter__\r\n    for i, data in enumerate(self.dataloader):\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 275, in __next__\r\n    idx, batch = self._get_batch()\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 254, in _get_batch\r\n    return self.data_queue.get()\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/queues.py\", line 335, in get\r\n    res = self._reader.recv_bytes()\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\r\n    buf = self._recv_bytes(maxlength)\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\r\n    buf = self._recv(4)\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\r\n    chunk = read(handle, remaining)\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 175, in handler\r\n    _error_if_any_worker_fails()\r\nRuntimeError: DataLoader worker (pid 3792) is killed by signal: Illegal instruction.\n<issue_comment>username_3: @username_2 that error is because you are running it on a computer that is too old and does not have SSE4 instruction on CPU.\n<issue_comment>username_1: @username_2 Also, it is more likely due to that your CPU doesn't support the operations done in the dataset.__getitem__.\n<issue_comment>username_2: @username_1 & @username_3,\r\n\r\nSome months ago I made it work in windows10 (I think)\r\nThe my windows machine crashed. So here I am. Sorry\r\n\r\nHere is what I see in cat /proc/cpuinfo:\r\n\r\nprocessor\t: 0\r\nvendor_id\t: AuthenticAMD\r\ncpu family\t: 21\r\nmodel\t\t: 2\r\nmodel name\t: AMD FX(tm)-4300 Quad-Core Processor\r\nstepping\t: 0\r\nmicrocode\t: 0x600081c\r\ncpu MHz\t\t: 1400.000\r\ncache size\t: 2048 KB\r\nphysical id\t: 0\r\nsiblings\t: 4\r\ncore id\t\t: 0\r\ncpu cores\t: 2\r\napicid\t\t: 16\r\ninitial apicid\t: 0\r\nfpu\t\t: yes\r\nfpu_exception\t: yes\r\ncpuid level\t: 13\r\nwp\t\t: yes\r\nflags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf pni pclmulqdq monitor ssse3 fma cx16 **sse4_1 sse4_2** popcnt aes xsave avx f16c lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs xop skinit wdt lwp fma4 tce nodeid_msr tbm topoext perfctr_core perfctr_nb cpb hw_pstate retpoline retpoline_amd rsb_ctxsw vmmcall bmi1 arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold\r\nbugs\t\t: fxsave_leak sysret_ss_attrs null_seg spectre_v1 spectre_v2\r\nbogomips\t: 7634.15\r\nTLB size\t: 1536 4K pages\r\nclflush size\t: 64\r\ncache_alignment\t: 64\r\naddress sizes\t: 48 bits physical, 48 bits virtual\r\npower management: ts ttp tm 100mhzsteps hwpstate cpb eff_freq_ro\r\n\r\n@username_1 how do I check if CPU support dataset.getitem?\n<issue_comment>username_1: @username_2 run with `nThreads=0`\n<issue_comment>username_2: tried it with nThreads=0\r\ndifferent error...\r\nmodel [CycleGANModel] was created\r\ncreate web directory ./checkpoints/horse2zebra_cyclegan/web...\r\nIllegal instruction (core dumped)\n<issue_comment>username_2: I see something I am uneasy about:\r\nsee bold below\r\n\r\nCustomDatasetDataLoader\r\ndataset [UnalignedDataset] was created\r\n/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torchvision-0.2.0-py3.6.egg/torchvision/transforms/transforms.py:156: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\r\n#training images = 1334\r\ncycle_gan\r\n**initialization method [normal]\r\ninitialization method [normal]\r\ninitialization method [normal]\r\ninitialization method [normal]**\n<issue_comment>username_1: @username_2 No it's same error. We just wrap it in a nice way so you can see it even if it's in the workers. \r\n\r\nIsn't that expected? CycleGAN has four networks: D_X D_Y G F .\n<issue_comment>username_1: @username_2 anyways, it confirms my hypothesis that your CPU doesn't support the operation in `dataset.__getitem__`.\n<issue_comment>username_2: Oh... Can you suggest a workaround?\n<issue_comment>username_1: @username_2 You can try to put debug statements in CycleGAN code and see which one isn't supported by your CPU (probably some PIL operations), and search for replacements. Sorry I don't have much else to offer.\n<issue_comment>username_2: @username_1 \r\nThank you sir !<issue_closed>\n<issue_comment>username_1: closed due to inactivity. feel free to reopen if you can reproduce with latest stable (0.4 at the moment of writing).\n<issue_comment>username_4: Still can't get this to work.\r\n\r\nhttps://stackoverflow.com/questions/53449927/runtimeerror-dataloader-worker-is-killed-by-signal-illegal-instruction\r\n\r\nPlease help.","repo":"pytorch/pytorch","issue_id":304991946,"issue_number":5761}
{"question":"What error is indicated by the provided traceback message related to the DataLoader worker?","answer":"The error indicated by the provided traceback message related to the DataLoader worker is 'Illegal instruction' which is causing the DataLoader worker to be killed by signal.","id":118,"text":"@username_1 and @username_3,\r\nI am running this:\r\n\r\npython train.py --dataroot ./datasets/horse2zebra --name horse2zebra_cyclegan --model cycle_gan --no_dropout --loadSize 64 --nThreads 1\r\n\r\nfrom \r\n\r\nhttps://github.com/junyanz/pytorch-CycleGAN-and-pix2pix\r\n\r\nThe model is succefully created:\r\nmodel [CycleGANModel] was created\r\n\r\nI am on ubuntu 16.04, pytorch 3.1, cuda 9.0 and cuDnn 7.1\r\n\r\nThe error I get is traced below:\r\ncreate web directory ./checkpoints/horse2zebra_cyclegan/web...\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 22, in <module>\r\n    for i, data in enumerate(dataset):\r\n  File \"/home/sam/Documents/ComputerVision/pytorch-CycleGAN-and-pix2pix/data/custom_dataset_data_loader.py\", line 44, in __iter__\r\n    for i, data in enumerate(self.dataloader):\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 275, in __next__\r\n    idx, batch = self._get_batch()\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 254, in _get_batch\r\n    return self.data_queue.get()\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/queues.py\", line 335, in get\r\n    res = self._reader.recv_bytes()\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\r\n    buf = self._recv_bytes(maxlength)\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\r\n    buf = self._recv(4)\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\r\n    chunk = read(handle, remaining)\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 175, in handler\r\n    _error_if_any_worker_fails()\r\nRuntimeError: DataLoader worker (pid 3792) is killed by signal: Illegal instruction.","context":"<issue_start><issue_comment>Title: Problem in DataLoader (same problem with #4643, but not solved by it)\nusername_0: - OS: Ubuntu 16.04\r\n- PyTorch version: torch-0.3.1-cp36-cp36-linux_x86_64.whl\r\n- How you installed PyTorch (conda, pip, source): pip3 from the binary\r\n- Python version: 3.6\r\n- CUDA/cuDNN version: 9.1\r\n- GPU models and configuration: GTX 1070 Ti\r\n\r\nAfter install using the wheel file, I have changed the torch/utils/data/Dataloader.py according to the changes shown at https://github.com/pytorch/pytorch/pull/4643\r\n\r\nBut the same error still happen when I try the tutorial example to load image database:\r\n\r\nException ignored in: <bound method DataLoaderIter.__del__ of <torch.utils.data.dataloader.DataLoaderIter object at 0x56090c8c7348>>\r\nTraceback (most recent call last):\r\n  File \"/home/username_0/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 375, in __del__\r\n  File \"/home/username_0/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 361, in _shutdown_workers\r\n  File \"/home/username_0/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 39, in _get_from_queue\r\n  File \"/usr/pkg/lib/python3.6/multiprocessing/queues.py\", line 337, in get\r\nImportError: sys.meta_path is None, Python is likely shutting down\r\n\r\nSo could anyone help me on that? Or is there any other file I should modify to get it run?\n<issue_comment>username_0: And, I am not sure how can I use the changes in torch/csrc/DataLoader.cpp, should I clone all the source code on GitHub and build the newest binary to fully fix this my problem?\n<issue_comment>username_1: when does this error happen? is it happening when you exit python?\n<issue_comment>username_2: @username_1 and @username_3,\r\nI am running this:\r\n\r\npython train.py --dataroot ./datasets/horse2zebra --name horse2zebra_cyclegan --model cycle_gan --no_dropout --loadSize 64 --nThreads 1\r\n\r\nfrom \r\n\r\nhttps://github.com/junyanz/pytorch-CycleGAN-and-pix2pix\r\n\r\nThe model is succefully created:\r\nmodel [CycleGANModel] was created\r\n\r\nI am on ubuntu 16.04, pytorch 3.1, cuda 9.0 and cuDnn 7.1\r\n\r\nThe error I get is traced below:\r\ncreate web directory ./checkpoints/horse2zebra_cyclegan/web...\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 22, in <module>\r\n    for i, data in enumerate(dataset):\r\n  File \"/home/sam/Documents/ComputerVision/pytorch-CycleGAN-and-pix2pix/data/custom_dataset_data_loader.py\", line 44, in __iter__\r\n    for i, data in enumerate(self.dataloader):\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 275, in __next__\r\n    idx, batch = self._get_batch()\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 254, in _get_batch\r\n    return self.data_queue.get()\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/queues.py\", line 335, in get\r\n    res = self._reader.recv_bytes()\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\r\n    buf = self._recv_bytes(maxlength)\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\r\n    buf = self._recv(4)\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\r\n    chunk = read(handle, remaining)\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 175, in handler\r\n    _error_if_any_worker_fails()\r\nRuntimeError: DataLoader worker (pid 3792) is killed by signal: Illegal instruction.\n<issue_comment>username_3: @username_2 that error is because you are running it on a computer that is too old and does not have SSE4 instruction on CPU.\n<issue_comment>username_1: @username_2 Also, it is more likely due to that your CPU doesn't support the operations done in the dataset.__getitem__.\n<issue_comment>username_2: @username_1 & @username_3,\r\n\r\nSome months ago I made it work in windows10 (I think)\r\nThe my windows machine crashed. So here I am. Sorry\r\n\r\nHere is what I see in cat /proc/cpuinfo:\r\n\r\nprocessor\t: 0\r\nvendor_id\t: AuthenticAMD\r\ncpu family\t: 21\r\nmodel\t\t: 2\r\nmodel name\t: AMD FX(tm)-4300 Quad-Core Processor\r\nstepping\t: 0\r\nmicrocode\t: 0x600081c\r\ncpu MHz\t\t: 1400.000\r\ncache size\t: 2048 KB\r\nphysical id\t: 0\r\nsiblings\t: 4\r\ncore id\t\t: 0\r\ncpu cores\t: 2\r\napicid\t\t: 16\r\ninitial apicid\t: 0\r\nfpu\t\t: yes\r\nfpu_exception\t: yes\r\ncpuid level\t: 13\r\nwp\t\t: yes\r\nflags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf pni pclmulqdq monitor ssse3 fma cx16 **sse4_1 sse4_2** popcnt aes xsave avx f16c lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs xop skinit wdt lwp fma4 tce nodeid_msr tbm topoext perfctr_core perfctr_nb cpb hw_pstate retpoline retpoline_amd rsb_ctxsw vmmcall bmi1 arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold\r\nbugs\t\t: fxsave_leak sysret_ss_attrs null_seg spectre_v1 spectre_v2\r\nbogomips\t: 7634.15\r\nTLB size\t: 1536 4K pages\r\nclflush size\t: 64\r\ncache_alignment\t: 64\r\naddress sizes\t: 48 bits physical, 48 bits virtual\r\npower management: ts ttp tm 100mhzsteps hwpstate cpb eff_freq_ro\r\n\r\n@username_1 how do I check if CPU support dataset.getitem?\n<issue_comment>username_1: @username_2 run with `nThreads=0`\n<issue_comment>username_2: tried it with nThreads=0\r\ndifferent error...\r\nmodel [CycleGANModel] was created\r\ncreate web directory ./checkpoints/horse2zebra_cyclegan/web...\r\nIllegal instruction (core dumped)\n<issue_comment>username_2: I see something I am uneasy about:\r\nsee bold below\r\n\r\nCustomDatasetDataLoader\r\ndataset [UnalignedDataset] was created\r\n/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torchvision-0.2.0-py3.6.egg/torchvision/transforms/transforms.py:156: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\r\n#training images = 1334\r\ncycle_gan\r\n**initialization method [normal]\r\ninitialization method [normal]\r\ninitialization method [normal]\r\ninitialization method [normal]**\n<issue_comment>username_1: @username_2 No it's same error. We just wrap it in a nice way so you can see it even if it's in the workers. \r\n\r\nIsn't that expected? CycleGAN has four networks: D_X D_Y G F .\n<issue_comment>username_1: @username_2 anyways, it confirms my hypothesis that your CPU doesn't support the operation in `dataset.__getitem__`.\n<issue_comment>username_2: Oh... Can you suggest a workaround?\n<issue_comment>username_1: @username_2 You can try to put debug statements in CycleGAN code and see which one isn't supported by your CPU (probably some PIL operations), and search for replacements. Sorry I don't have much else to offer.\n<issue_comment>username_2: @username_1 \r\nThank you sir !<issue_closed>\n<issue_comment>username_1: closed due to inactivity. feel free to reopen if you can reproduce with latest stable (0.4 at the moment of writing).\n<issue_comment>username_4: Still can't get this to work.\r\n\r\nhttps://stackoverflow.com/questions/53449927/runtimeerror-dataloader-worker-is-killed-by-signal-illegal-instruction\r\n\r\nPlease help.","repo":"pytorch/pytorch","issue_id":304991946,"issue_number":5761}
{"question":"How do I check if the CPU supports dataset.getitem?","answer":"To check if the CPU supports the `dataset.getitem` instruction set, you need to look for the `sse4_1` and `sse4_2` flags in the CPU flags section of the `/proc/cpuinfo` output. If these flags are present, it indicates that the CPU supports the SSE4.1 and SSE4.2 instruction sets, which typically include `dataset.getitem` functionality.","id":121,"text":"@username_1 & @username_3,\r\n\r\nSome months ago I made it work in windows10 (I think)\r\nThe my windows machine crashed. So here I am. Sorry\r\n\r\nHere is what I see in cat /proc/cpuinfo:\r\n\r\nprocessor\t: 0\r\nvendor_id\t: AuthenticAMD\r\ncpu family\t: 21\r\nmodel\t\t: 2\r\nmodel name\t: AMD FX(tm)-4300 Quad-Core Processor\r\nstepping\t: 0\r\nmicrocode\t: 0x600081c\r\ncpu MHz\t\t: 1400.000\r\ncache size\t: 2048 KB\r\nphysical id\t: 0\r\nsiblings\t: 4\r\ncore id\t\t: 0\r\ncpu cores\t: 2\r\napicid\t\t: 16\r\ninitial apicid\t: 0\r\nfpu\t\t: yes\r\nfpu_exception\t: yes\r\ncpuid level\t: 13\r\nwp\t\t: yes\r\nflags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf pni pclmulqdq monitor ssse3 fma cx16 **sse4_1 sse4_2** popcnt aes xsave avx f16c lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs xop skinit wdt lwp fma4 tce nodeid_msr tbm topoext perfctr_core perfctr_nb cpb hw_pstate retpoline retpoline_amd rsb_ctxsw vmmcall bmi1 arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold\r\nbugs\t\t: fxsave_leak sysret_ss_attrs null_seg spectre_v1 spectre_v2\r\nbogomips\t: 7634.15\r\nTLB size\t: 1536 4K pages\r\nclflush size\t: 64\r\ncache_alignment\t: 64\r\naddress sizes\t: 48 bits physical, 48 bits virtual\r\npower management: ts ttp tm 100mhzsteps hwpstate cpb eff_freq_ro\r\n\r\n@username_1 how do I check if CPU support dataset.getitem?","context":"<issue_start><issue_comment>Title: Problem in DataLoader (same problem with #4643, but not solved by it)\nusername_0: - OS: Ubuntu 16.04\r\n- PyTorch version: torch-0.3.1-cp36-cp36-linux_x86_64.whl\r\n- How you installed PyTorch (conda, pip, source): pip3 from the binary\r\n- Python version: 3.6\r\n- CUDA/cuDNN version: 9.1\r\n- GPU models and configuration: GTX 1070 Ti\r\n\r\nAfter install using the wheel file, I have changed the torch/utils/data/Dataloader.py according to the changes shown at https://github.com/pytorch/pytorch/pull/4643\r\n\r\nBut the same error still happen when I try the tutorial example to load image database:\r\n\r\nException ignored in: <bound method DataLoaderIter.__del__ of <torch.utils.data.dataloader.DataLoaderIter object at 0x56090c8c7348>>\r\nTraceback (most recent call last):\r\n  File \"/home/username_0/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 375, in __del__\r\n  File \"/home/username_0/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 361, in _shutdown_workers\r\n  File \"/home/username_0/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 39, in _get_from_queue\r\n  File \"/usr/pkg/lib/python3.6/multiprocessing/queues.py\", line 337, in get\r\nImportError: sys.meta_path is None, Python is likely shutting down\r\n\r\nSo could anyone help me on that? Or is there any other file I should modify to get it run?\n<issue_comment>username_0: And, I am not sure how can I use the changes in torch/csrc/DataLoader.cpp, should I clone all the source code on GitHub and build the newest binary to fully fix this my problem?\n<issue_comment>username_1: when does this error happen? is it happening when you exit python?\n<issue_comment>username_2: @username_1 and @username_3,\r\nI am running this:\r\n\r\npython train.py --dataroot ./datasets/horse2zebra --name horse2zebra_cyclegan --model cycle_gan --no_dropout --loadSize 64 --nThreads 1\r\n\r\nfrom \r\n\r\nhttps://github.com/junyanz/pytorch-CycleGAN-and-pix2pix\r\n\r\nThe model is succefully created:\r\nmodel [CycleGANModel] was created\r\n\r\nI am on ubuntu 16.04, pytorch 3.1, cuda 9.0 and cuDnn 7.1\r\n\r\nThe error I get is traced below:\r\ncreate web directory ./checkpoints/horse2zebra_cyclegan/web...\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 22, in <module>\r\n    for i, data in enumerate(dataset):\r\n  File \"/home/sam/Documents/ComputerVision/pytorch-CycleGAN-and-pix2pix/data/custom_dataset_data_loader.py\", line 44, in __iter__\r\n    for i, data in enumerate(self.dataloader):\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 275, in __next__\r\n    idx, batch = self._get_batch()\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 254, in _get_batch\r\n    return self.data_queue.get()\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/queues.py\", line 335, in get\r\n    res = self._reader.recv_bytes()\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\r\n    buf = self._recv_bytes(maxlength)\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\r\n    buf = self._recv(4)\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\r\n    chunk = read(handle, remaining)\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 175, in handler\r\n    _error_if_any_worker_fails()\r\nRuntimeError: DataLoader worker (pid 3792) is killed by signal: Illegal instruction.\n<issue_comment>username_3: @username_2 that error is because you are running it on a computer that is too old and does not have SSE4 instruction on CPU.\n<issue_comment>username_1: @username_2 Also, it is more likely due to that your CPU doesn't support the operations done in the dataset.__getitem__.\n<issue_comment>username_2: @username_1 & @username_3,\r\n\r\nSome months ago I made it work in windows10 (I think)\r\nThe my windows machine crashed. So here I am. Sorry\r\n\r\nHere is what I see in cat /proc/cpuinfo:\r\n\r\nprocessor\t: 0\r\nvendor_id\t: AuthenticAMD\r\ncpu family\t: 21\r\nmodel\t\t: 2\r\nmodel name\t: AMD FX(tm)-4300 Quad-Core Processor\r\nstepping\t: 0\r\nmicrocode\t: 0x600081c\r\ncpu MHz\t\t: 1400.000\r\ncache size\t: 2048 KB\r\nphysical id\t: 0\r\nsiblings\t: 4\r\ncore id\t\t: 0\r\ncpu cores\t: 2\r\napicid\t\t: 16\r\ninitial apicid\t: 0\r\nfpu\t\t: yes\r\nfpu_exception\t: yes\r\ncpuid level\t: 13\r\nwp\t\t: yes\r\nflags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf pni pclmulqdq monitor ssse3 fma cx16 **sse4_1 sse4_2** popcnt aes xsave avx f16c lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs xop skinit wdt lwp fma4 tce nodeid_msr tbm topoext perfctr_core perfctr_nb cpb hw_pstate retpoline retpoline_amd rsb_ctxsw vmmcall bmi1 arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold\r\nbugs\t\t: fxsave_leak sysret_ss_attrs null_seg spectre_v1 spectre_v2\r\nbogomips\t: 7634.15\r\nTLB size\t: 1536 4K pages\r\nclflush size\t: 64\r\ncache_alignment\t: 64\r\naddress sizes\t: 48 bits physical, 48 bits virtual\r\npower management: ts ttp tm 100mhzsteps hwpstate cpb eff_freq_ro\r\n\r\n@username_1 how do I check if CPU support dataset.getitem?\n<issue_comment>username_1: @username_2 run with `nThreads=0`\n<issue_comment>username_2: tried it with nThreads=0\r\ndifferent error...\r\nmodel [CycleGANModel] was created\r\ncreate web directory ./checkpoints/horse2zebra_cyclegan/web...\r\nIllegal instruction (core dumped)\n<issue_comment>username_2: I see something I am uneasy about:\r\nsee bold below\r\n\r\nCustomDatasetDataLoader\r\ndataset [UnalignedDataset] was created\r\n/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torchvision-0.2.0-py3.6.egg/torchvision/transforms/transforms.py:156: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\r\n#training images = 1334\r\ncycle_gan\r\n**initialization method [normal]\r\ninitialization method [normal]\r\ninitialization method [normal]\r\ninitialization method [normal]**\n<issue_comment>username_1: @username_2 No it's same error. We just wrap it in a nice way so you can see it even if it's in the workers. \r\n\r\nIsn't that expected? CycleGAN has four networks: D_X D_Y G F .\n<issue_comment>username_1: @username_2 anyways, it confirms my hypothesis that your CPU doesn't support the operation in `dataset.__getitem__`.\n<issue_comment>username_2: Oh... Can you suggest a workaround?\n<issue_comment>username_1: @username_2 You can try to put debug statements in CycleGAN code and see which one isn't supported by your CPU (probably some PIL operations), and search for replacements. Sorry I don't have much else to offer.\n<issue_comment>username_2: @username_1 \r\nThank you sir !<issue_closed>\n<issue_comment>username_1: closed due to inactivity. feel free to reopen if you can reproduce with latest stable (0.4 at the moment of writing).\n<issue_comment>username_4: Still can't get this to work.\r\n\r\nhttps://stackoverflow.com/questions/53449927/runtimeerror-dataloader-worker-is-killed-by-signal-illegal-instruction\r\n\r\nPlease help.","repo":"pytorch/pytorch","issue_id":304991946,"issue_number":5761}
{"question":"What could be the possible reason for the 'Illegal instruction' error that occurred when trying the setting with nThreads=0 for the CycleGAN model?","answer":"The Illegal instruction error could be occurring due to an attempt to execute an instruction that the processor does not support or that is not allowed in the current context. This could be caused by a mismatch between the instruction set supported by the processor and the instruction being executed, resulting in a core dump and program crash.","id":123,"text":"tried it with nThreads=0\r\ndifferent error...\r\nmodel [CycleGANModel] was created\r\ncreate web directory ./checkpoints/horse2zebra_cyclegan/web...\r\nIllegal instruction (core dumped)","context":"<issue_start><issue_comment>Title: Problem in DataLoader (same problem with #4643, but not solved by it)\nusername_0: - OS: Ubuntu 16.04\r\n- PyTorch version: torch-0.3.1-cp36-cp36-linux_x86_64.whl\r\n- How you installed PyTorch (conda, pip, source): pip3 from the binary\r\n- Python version: 3.6\r\n- CUDA/cuDNN version: 9.1\r\n- GPU models and configuration: GTX 1070 Ti\r\n\r\nAfter install using the wheel file, I have changed the torch/utils/data/Dataloader.py according to the changes shown at https://github.com/pytorch/pytorch/pull/4643\r\n\r\nBut the same error still happen when I try the tutorial example to load image database:\r\n\r\nException ignored in: <bound method DataLoaderIter.__del__ of <torch.utils.data.dataloader.DataLoaderIter object at 0x56090c8c7348>>\r\nTraceback (most recent call last):\r\n  File \"/home/username_0/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 375, in __del__\r\n  File \"/home/username_0/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 361, in _shutdown_workers\r\n  File \"/home/username_0/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 39, in _get_from_queue\r\n  File \"/usr/pkg/lib/python3.6/multiprocessing/queues.py\", line 337, in get\r\nImportError: sys.meta_path is None, Python is likely shutting down\r\n\r\nSo could anyone help me on that? Or is there any other file I should modify to get it run?\n<issue_comment>username_0: And, I am not sure how can I use the changes in torch/csrc/DataLoader.cpp, should I clone all the source code on GitHub and build the newest binary to fully fix this my problem?\n<issue_comment>username_1: when does this error happen? is it happening when you exit python?\n<issue_comment>username_2: @username_1 and @username_3,\r\nI am running this:\r\n\r\npython train.py --dataroot ./datasets/horse2zebra --name horse2zebra_cyclegan --model cycle_gan --no_dropout --loadSize 64 --nThreads 1\r\n\r\nfrom \r\n\r\nhttps://github.com/junyanz/pytorch-CycleGAN-and-pix2pix\r\n\r\nThe model is succefully created:\r\nmodel [CycleGANModel] was created\r\n\r\nI am on ubuntu 16.04, pytorch 3.1, cuda 9.0 and cuDnn 7.1\r\n\r\nThe error I get is traced below:\r\ncreate web directory ./checkpoints/horse2zebra_cyclegan/web...\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 22, in <module>\r\n    for i, data in enumerate(dataset):\r\n  File \"/home/sam/Documents/ComputerVision/pytorch-CycleGAN-and-pix2pix/data/custom_dataset_data_loader.py\", line 44, in __iter__\r\n    for i, data in enumerate(self.dataloader):\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 275, in __next__\r\n    idx, batch = self._get_batch()\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 254, in _get_batch\r\n    return self.data_queue.get()\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/queues.py\", line 335, in get\r\n    res = self._reader.recv_bytes()\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\r\n    buf = self._recv_bytes(maxlength)\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\r\n    buf = self._recv(4)\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\r\n    chunk = read(handle, remaining)\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 175, in handler\r\n    _error_if_any_worker_fails()\r\nRuntimeError: DataLoader worker (pid 3792) is killed by signal: Illegal instruction.\n<issue_comment>username_3: @username_2 that error is because you are running it on a computer that is too old and does not have SSE4 instruction on CPU.\n<issue_comment>username_1: @username_2 Also, it is more likely due to that your CPU doesn't support the operations done in the dataset.__getitem__.\n<issue_comment>username_2: @username_1 & @username_3,\r\n\r\nSome months ago I made it work in windows10 (I think)\r\nThe my windows machine crashed. So here I am. Sorry\r\n\r\nHere is what I see in cat /proc/cpuinfo:\r\n\r\nprocessor\t: 0\r\nvendor_id\t: AuthenticAMD\r\ncpu family\t: 21\r\nmodel\t\t: 2\r\nmodel name\t: AMD FX(tm)-4300 Quad-Core Processor\r\nstepping\t: 0\r\nmicrocode\t: 0x600081c\r\ncpu MHz\t\t: 1400.000\r\ncache size\t: 2048 KB\r\nphysical id\t: 0\r\nsiblings\t: 4\r\ncore id\t\t: 0\r\ncpu cores\t: 2\r\napicid\t\t: 16\r\ninitial apicid\t: 0\r\nfpu\t\t: yes\r\nfpu_exception\t: yes\r\ncpuid level\t: 13\r\nwp\t\t: yes\r\nflags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf pni pclmulqdq monitor ssse3 fma cx16 **sse4_1 sse4_2** popcnt aes xsave avx f16c lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs xop skinit wdt lwp fma4 tce nodeid_msr tbm topoext perfctr_core perfctr_nb cpb hw_pstate retpoline retpoline_amd rsb_ctxsw vmmcall bmi1 arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold\r\nbugs\t\t: fxsave_leak sysret_ss_attrs null_seg spectre_v1 spectre_v2\r\nbogomips\t: 7634.15\r\nTLB size\t: 1536 4K pages\r\nclflush size\t: 64\r\ncache_alignment\t: 64\r\naddress sizes\t: 48 bits physical, 48 bits virtual\r\npower management: ts ttp tm 100mhzsteps hwpstate cpb eff_freq_ro\r\n\r\n@username_1 how do I check if CPU support dataset.getitem?\n<issue_comment>username_1: @username_2 run with `nThreads=0`\n<issue_comment>username_2: tried it with nThreads=0\r\ndifferent error...\r\nmodel [CycleGANModel] was created\r\ncreate web directory ./checkpoints/horse2zebra_cyclegan/web...\r\nIllegal instruction (core dumped)\n<issue_comment>username_2: I see something I am uneasy about:\r\nsee bold below\r\n\r\nCustomDatasetDataLoader\r\ndataset [UnalignedDataset] was created\r\n/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torchvision-0.2.0-py3.6.egg/torchvision/transforms/transforms.py:156: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\r\n#training images = 1334\r\ncycle_gan\r\n**initialization method [normal]\r\ninitialization method [normal]\r\ninitialization method [normal]\r\ninitialization method [normal]**\n<issue_comment>username_1: @username_2 No it's same error. We just wrap it in a nice way so you can see it even if it's in the workers. \r\n\r\nIsn't that expected? CycleGAN has four networks: D_X D_Y G F .\n<issue_comment>username_1: @username_2 anyways, it confirms my hypothesis that your CPU doesn't support the operation in `dataset.__getitem__`.\n<issue_comment>username_2: Oh... Can you suggest a workaround?\n<issue_comment>username_1: @username_2 You can try to put debug statements in CycleGAN code and see which one isn't supported by your CPU (probably some PIL operations), and search for replacements. Sorry I don't have much else to offer.\n<issue_comment>username_2: @username_1 \r\nThank you sir !<issue_closed>\n<issue_comment>username_1: closed due to inactivity. feel free to reopen if you can reproduce with latest stable (0.4 at the moment of writing).\n<issue_comment>username_4: Still can't get this to work.\r\n\r\nhttps://stackoverflow.com/questions/53449927/runtimeerror-dataloader-worker-is-killed-by-signal-illegal-instruction\r\n\r\nPlease help.","repo":"pytorch/pytorch","issue_id":304991946,"issue_number":5761}
{"question":"What is the initialization method mentioned multiple times in the text related to cycle_gan?","answer":"normal","id":124,"text":"I see something I am uneasy about:\r\nsee bold below\r\n\r\nCustomDatasetDataLoader\r\ndataset [UnalignedDataset] was created\r\n/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torchvision-0.2.0-py3.6.egg/torchvision/transforms/transforms.py:156: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\r\n#training images = 1334\r\ncycle_gan\r\n**initialization method [normal]\r\ninitialization method [normal]\r\ninitialization method [normal]\r\ninitialization method [normal]**","context":"<issue_start><issue_comment>Title: Problem in DataLoader (same problem with #4643, but not solved by it)\nusername_0: - OS: Ubuntu 16.04\r\n- PyTorch version: torch-0.3.1-cp36-cp36-linux_x86_64.whl\r\n- How you installed PyTorch (conda, pip, source): pip3 from the binary\r\n- Python version: 3.6\r\n- CUDA/cuDNN version: 9.1\r\n- GPU models and configuration: GTX 1070 Ti\r\n\r\nAfter install using the wheel file, I have changed the torch/utils/data/Dataloader.py according to the changes shown at https://github.com/pytorch/pytorch/pull/4643\r\n\r\nBut the same error still happen when I try the tutorial example to load image database:\r\n\r\nException ignored in: <bound method DataLoaderIter.__del__ of <torch.utils.data.dataloader.DataLoaderIter object at 0x56090c8c7348>>\r\nTraceback (most recent call last):\r\n  File \"/home/username_0/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 375, in __del__\r\n  File \"/home/username_0/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 361, in _shutdown_workers\r\n  File \"/home/username_0/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 39, in _get_from_queue\r\n  File \"/usr/pkg/lib/python3.6/multiprocessing/queues.py\", line 337, in get\r\nImportError: sys.meta_path is None, Python is likely shutting down\r\n\r\nSo could anyone help me on that? Or is there any other file I should modify to get it run?\n<issue_comment>username_0: And, I am not sure how can I use the changes in torch/csrc/DataLoader.cpp, should I clone all the source code on GitHub and build the newest binary to fully fix this my problem?\n<issue_comment>username_1: when does this error happen? is it happening when you exit python?\n<issue_comment>username_2: @username_1 and @username_3,\r\nI am running this:\r\n\r\npython train.py --dataroot ./datasets/horse2zebra --name horse2zebra_cyclegan --model cycle_gan --no_dropout --loadSize 64 --nThreads 1\r\n\r\nfrom \r\n\r\nhttps://github.com/junyanz/pytorch-CycleGAN-and-pix2pix\r\n\r\nThe model is succefully created:\r\nmodel [CycleGANModel] was created\r\n\r\nI am on ubuntu 16.04, pytorch 3.1, cuda 9.0 and cuDnn 7.1\r\n\r\nThe error I get is traced below:\r\ncreate web directory ./checkpoints/horse2zebra_cyclegan/web...\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 22, in <module>\r\n    for i, data in enumerate(dataset):\r\n  File \"/home/sam/Documents/ComputerVision/pytorch-CycleGAN-and-pix2pix/data/custom_dataset_data_loader.py\", line 44, in __iter__\r\n    for i, data in enumerate(self.dataloader):\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 275, in __next__\r\n    idx, batch = self._get_batch()\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 254, in _get_batch\r\n    return self.data_queue.get()\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/queues.py\", line 335, in get\r\n    res = self._reader.recv_bytes()\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\r\n    buf = self._recv_bytes(maxlength)\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\r\n    buf = self._recv(4)\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\r\n    chunk = read(handle, remaining)\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 175, in handler\r\n    _error_if_any_worker_fails()\r\nRuntimeError: DataLoader worker (pid 3792) is killed by signal: Illegal instruction.\n<issue_comment>username_3: @username_2 that error is because you are running it on a computer that is too old and does not have SSE4 instruction on CPU.\n<issue_comment>username_1: @username_2 Also, it is more likely due to that your CPU doesn't support the operations done in the dataset.__getitem__.\n<issue_comment>username_2: @username_1 & @username_3,\r\n\r\nSome months ago I made it work in windows10 (I think)\r\nThe my windows machine crashed. So here I am. Sorry\r\n\r\nHere is what I see in cat /proc/cpuinfo:\r\n\r\nprocessor\t: 0\r\nvendor_id\t: AuthenticAMD\r\ncpu family\t: 21\r\nmodel\t\t: 2\r\nmodel name\t: AMD FX(tm)-4300 Quad-Core Processor\r\nstepping\t: 0\r\nmicrocode\t: 0x600081c\r\ncpu MHz\t\t: 1400.000\r\ncache size\t: 2048 KB\r\nphysical id\t: 0\r\nsiblings\t: 4\r\ncore id\t\t: 0\r\ncpu cores\t: 2\r\napicid\t\t: 16\r\ninitial apicid\t: 0\r\nfpu\t\t: yes\r\nfpu_exception\t: yes\r\ncpuid level\t: 13\r\nwp\t\t: yes\r\nflags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf pni pclmulqdq monitor ssse3 fma cx16 **sse4_1 sse4_2** popcnt aes xsave avx f16c lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs xop skinit wdt lwp fma4 tce nodeid_msr tbm topoext perfctr_core perfctr_nb cpb hw_pstate retpoline retpoline_amd rsb_ctxsw vmmcall bmi1 arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold\r\nbugs\t\t: fxsave_leak sysret_ss_attrs null_seg spectre_v1 spectre_v2\r\nbogomips\t: 7634.15\r\nTLB size\t: 1536 4K pages\r\nclflush size\t: 64\r\ncache_alignment\t: 64\r\naddress sizes\t: 48 bits physical, 48 bits virtual\r\npower management: ts ttp tm 100mhzsteps hwpstate cpb eff_freq_ro\r\n\r\n@username_1 how do I check if CPU support dataset.getitem?\n<issue_comment>username_1: @username_2 run with `nThreads=0`\n<issue_comment>username_2: tried it with nThreads=0\r\ndifferent error...\r\nmodel [CycleGANModel] was created\r\ncreate web directory ./checkpoints/horse2zebra_cyclegan/web...\r\nIllegal instruction (core dumped)\n<issue_comment>username_2: I see something I am uneasy about:\r\nsee bold below\r\n\r\nCustomDatasetDataLoader\r\ndataset [UnalignedDataset] was created\r\n/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torchvision-0.2.0-py3.6.egg/torchvision/transforms/transforms.py:156: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\r\n#training images = 1334\r\ncycle_gan\r\n**initialization method [normal]\r\ninitialization method [normal]\r\ninitialization method [normal]\r\ninitialization method [normal]**\n<issue_comment>username_1: @username_2 No it's same error. We just wrap it in a nice way so you can see it even if it's in the workers. \r\n\r\nIsn't that expected? CycleGAN has four networks: D_X D_Y G F .\n<issue_comment>username_1: @username_2 anyways, it confirms my hypothesis that your CPU doesn't support the operation in `dataset.__getitem__`.\n<issue_comment>username_2: Oh... Can you suggest a workaround?\n<issue_comment>username_1: @username_2 You can try to put debug statements in CycleGAN code and see which one isn't supported by your CPU (probably some PIL operations), and search for replacements. Sorry I don't have much else to offer.\n<issue_comment>username_2: @username_1 \r\nThank you sir !<issue_closed>\n<issue_comment>username_1: closed due to inactivity. feel free to reopen if you can reproduce with latest stable (0.4 at the moment of writing).\n<issue_comment>username_4: Still can't get this to work.\r\n\r\nhttps://stackoverflow.com/questions/53449927/runtimeerror-dataloader-worker-is-killed-by-signal-illegal-instruction\r\n\r\nPlease help.","repo":"pytorch/pytorch","issue_id":304991946,"issue_number":5761}
{"question":"How does the wrapping of errors enable visibility in workers, and what are the four networks in CycleGAN?","answer":"Wrapping errors in a nice way allows them to be visible even in workers by enhancing their presentation. The four networks in CycleGAN are D_X, D_Y, G, and F, each serving a specific purpose in the CycleGAN architecture.","id":125,"text":"@username_2 No it's same error. We just wrap it in a nice way so you can see it even if it's in the workers. \r\n\r\nIsn't that expected? CycleGAN has four networks: D_X D_Y G F .","context":"<issue_start><issue_comment>Title: Problem in DataLoader (same problem with #4643, but not solved by it)\nusername_0: - OS: Ubuntu 16.04\r\n- PyTorch version: torch-0.3.1-cp36-cp36-linux_x86_64.whl\r\n- How you installed PyTorch (conda, pip, source): pip3 from the binary\r\n- Python version: 3.6\r\n- CUDA/cuDNN version: 9.1\r\n- GPU models and configuration: GTX 1070 Ti\r\n\r\nAfter install using the wheel file, I have changed the torch/utils/data/Dataloader.py according to the changes shown at https://github.com/pytorch/pytorch/pull/4643\r\n\r\nBut the same error still happen when I try the tutorial example to load image database:\r\n\r\nException ignored in: <bound method DataLoaderIter.__del__ of <torch.utils.data.dataloader.DataLoaderIter object at 0x56090c8c7348>>\r\nTraceback (most recent call last):\r\n  File \"/home/username_0/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 375, in __del__\r\n  File \"/home/username_0/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 361, in _shutdown_workers\r\n  File \"/home/username_0/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 39, in _get_from_queue\r\n  File \"/usr/pkg/lib/python3.6/multiprocessing/queues.py\", line 337, in get\r\nImportError: sys.meta_path is None, Python is likely shutting down\r\n\r\nSo could anyone help me on that? Or is there any other file I should modify to get it run?\n<issue_comment>username_0: And, I am not sure how can I use the changes in torch/csrc/DataLoader.cpp, should I clone all the source code on GitHub and build the newest binary to fully fix this my problem?\n<issue_comment>username_1: when does this error happen? is it happening when you exit python?\n<issue_comment>username_2: @username_1 and @username_3,\r\nI am running this:\r\n\r\npython train.py --dataroot ./datasets/horse2zebra --name horse2zebra_cyclegan --model cycle_gan --no_dropout --loadSize 64 --nThreads 1\r\n\r\nfrom \r\n\r\nhttps://github.com/junyanz/pytorch-CycleGAN-and-pix2pix\r\n\r\nThe model is succefully created:\r\nmodel [CycleGANModel] was created\r\n\r\nI am on ubuntu 16.04, pytorch 3.1, cuda 9.0 and cuDnn 7.1\r\n\r\nThe error I get is traced below:\r\ncreate web directory ./checkpoints/horse2zebra_cyclegan/web...\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 22, in <module>\r\n    for i, data in enumerate(dataset):\r\n  File \"/home/sam/Documents/ComputerVision/pytorch-CycleGAN-and-pix2pix/data/custom_dataset_data_loader.py\", line 44, in __iter__\r\n    for i, data in enumerate(self.dataloader):\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 275, in __next__\r\n    idx, batch = self._get_batch()\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 254, in _get_batch\r\n    return self.data_queue.get()\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/queues.py\", line 335, in get\r\n    res = self._reader.recv_bytes()\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\r\n    buf = self._recv_bytes(maxlength)\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\r\n    buf = self._recv(4)\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\r\n    chunk = read(handle, remaining)\r\n  File \"/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 175, in handler\r\n    _error_if_any_worker_fails()\r\nRuntimeError: DataLoader worker (pid 3792) is killed by signal: Illegal instruction.\n<issue_comment>username_3: @username_2 that error is because you are running it on a computer that is too old and does not have SSE4 instruction on CPU.\n<issue_comment>username_1: @username_2 Also, it is more likely due to that your CPU doesn't support the operations done in the dataset.__getitem__.\n<issue_comment>username_2: @username_1 & @username_3,\r\n\r\nSome months ago I made it work in windows10 (I think)\r\nThe my windows machine crashed. So here I am. Sorry\r\n\r\nHere is what I see in cat /proc/cpuinfo:\r\n\r\nprocessor\t: 0\r\nvendor_id\t: AuthenticAMD\r\ncpu family\t: 21\r\nmodel\t\t: 2\r\nmodel name\t: AMD FX(tm)-4300 Quad-Core Processor\r\nstepping\t: 0\r\nmicrocode\t: 0x600081c\r\ncpu MHz\t\t: 1400.000\r\ncache size\t: 2048 KB\r\nphysical id\t: 0\r\nsiblings\t: 4\r\ncore id\t\t: 0\r\ncpu cores\t: 2\r\napicid\t\t: 16\r\ninitial apicid\t: 0\r\nfpu\t\t: yes\r\nfpu_exception\t: yes\r\ncpuid level\t: 13\r\nwp\t\t: yes\r\nflags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf pni pclmulqdq monitor ssse3 fma cx16 **sse4_1 sse4_2** popcnt aes xsave avx f16c lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs xop skinit wdt lwp fma4 tce nodeid_msr tbm topoext perfctr_core perfctr_nb cpb hw_pstate retpoline retpoline_amd rsb_ctxsw vmmcall bmi1 arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold\r\nbugs\t\t: fxsave_leak sysret_ss_attrs null_seg spectre_v1 spectre_v2\r\nbogomips\t: 7634.15\r\nTLB size\t: 1536 4K pages\r\nclflush size\t: 64\r\ncache_alignment\t: 64\r\naddress sizes\t: 48 bits physical, 48 bits virtual\r\npower management: ts ttp tm 100mhzsteps hwpstate cpb eff_freq_ro\r\n\r\n@username_1 how do I check if CPU support dataset.getitem?\n<issue_comment>username_1: @username_2 run with `nThreads=0`\n<issue_comment>username_2: tried it with nThreads=0\r\ndifferent error...\r\nmodel [CycleGANModel] was created\r\ncreate web directory ./checkpoints/horse2zebra_cyclegan/web...\r\nIllegal instruction (core dumped)\n<issue_comment>username_2: I see something I am uneasy about:\r\nsee bold below\r\n\r\nCustomDatasetDataLoader\r\ndataset [UnalignedDataset] was created\r\n/home/sam/anaconda3/envs/my_torch/lib/python3.6/site-packages/torchvision-0.2.0-py3.6.egg/torchvision/transforms/transforms.py:156: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\r\n#training images = 1334\r\ncycle_gan\r\n**initialization method [normal]\r\ninitialization method [normal]\r\ninitialization method [normal]\r\ninitialization method [normal]**\n<issue_comment>username_1: @username_2 No it's same error. We just wrap it in a nice way so you can see it even if it's in the workers. \r\n\r\nIsn't that expected? CycleGAN has four networks: D_X D_Y G F .\n<issue_comment>username_1: @username_2 anyways, it confirms my hypothesis that your CPU doesn't support the operation in `dataset.__getitem__`.\n<issue_comment>username_2: Oh... Can you suggest a workaround?\n<issue_comment>username_1: @username_2 You can try to put debug statements in CycleGAN code and see which one isn't supported by your CPU (probably some PIL operations), and search for replacements. Sorry I don't have much else to offer.\n<issue_comment>username_2: @username_1 \r\nThank you sir !<issue_closed>\n<issue_comment>username_1: closed due to inactivity. feel free to reopen if you can reproduce with latest stable (0.4 at the moment of writing).\n<issue_comment>username_4: Still can't get this to work.\r\n\r\nhttps://stackoverflow.com/questions/53449927/runtimeerror-dataloader-worker-is-killed-by-signal-illegal-instruction\r\n\r\nPlease help.","repo":"pytorch/pytorch","issue_id":304991946,"issue_number":5761}
{"question":"What might be the cause of the error message 'fatal error LNK1136: Invalid or corrupted file' in the provided text chunk?","answer":"The error message 'fatal error LNK1136: Invalid or corrupted file' in the text chunk is likely caused by a problem related to the CUDA installation or configuration.","id":132,"text":"E:\\SOFTWARE\\pytorch\\build\\caffe2\\torch\\lib\\libshm_windows\\shm.vcxproj()\r\n\r\nE:\\SOFTWARE\\pytorch\\build\\ALL_BUILD.vcxproj() - \r\n\r\nE:\\SOFTWARE\\pytorch\\build\\INSTALL.vcxproj() - \r\n\r\n\r\n\r\n\r\nE:\\SOFTWARE\\pytorch\\build\\INSTALL.vcxproj() (1) ->\r\nE:\\SOFTWARE\\pytorch\\build\\ALL_BUILD.vcxproj() (3) ->\r\nE:\\SOFTWARE\\pytorch\\build\\caffe2\\AlgorithmsTest.vcxproj() (4) ->\r\nE:\\SOFTWARE\\pytorch\\build\\caffe2\\caffe2_gpu.vcxproj() (22) ->\r\n(Link ) ->\r\n  C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\lib\\x64.obj : fatal error LNK1136:  [E:\\SOFTWARE\\pyt\r\norch\\build\\caffe2\\caffe2_gpu.vcxproj]\r\n\r\n    0 \r\n    1 \r\n\r\n 00:00:16.23\r\n\r\nE:\\SOFTWARE\\pytorch\\build>IF ERRORLEVEL 1 exit 1\r\nFailed to run 'tools\\build_pytorch_libs.bat --use-cuda --use-fbgemm --use-nnpack --use-mkldnn --use-qnnpack caffe2'\r\n\r\npytorch versionstable1.0\r\nosWindows 10 Pro Version 1803\r\npython version:3.7\r\nCUDA:10\r\ncudnn:7.4.1\r\nVisual Studio version: Microsoft Visual Studio 2017 Community Edition\r\n\r\nCan anyone help me solve this problem? Thanks !","context":"<issue_start><issue_comment>Title: Can't open x64.obj while installing pytorch with win10 \nusername_0: E:\\SOFTWARE\\pytorch\\build\\caffe2\\torch\\lib\\libshm_windows\\shm.vcxproj()\r\n\r\nE:\\SOFTWARE\\pytorch\\build\\ALL_BUILD.vcxproj() - \r\n\r\nE:\\SOFTWARE\\pytorch\\build\\INSTALL.vcxproj() - \r\n\r\n\r\n\r\n\r\nE:\\SOFTWARE\\pytorch\\build\\INSTALL.vcxproj() (1) ->\r\nE:\\SOFTWARE\\pytorch\\build\\ALL_BUILD.vcxproj() (3) ->\r\nE:\\SOFTWARE\\pytorch\\build\\caffe2\\AlgorithmsTest.vcxproj() (4) ->\r\nE:\\SOFTWARE\\pytorch\\build\\caffe2\\caffe2_gpu.vcxproj() (22) ->\r\n(Link ) ->\r\n  C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\lib\\x64.obj : fatal error LNK1136:  [E:\\SOFTWARE\\pyt\r\norch\\build\\caffe2\\caffe2_gpu.vcxproj]\r\n\r\n    0 \r\n    1 \r\n\r\n 00:00:16.23\r\n\r\nE:\\SOFTWARE\\pytorch\\build>IF ERRORLEVEL 1 exit 1\r\nFailed to run 'tools\\build_pytorch_libs.bat --use-cuda --use-fbgemm --use-nnpack --use-mkldnn --use-qnnpack caffe2'\r\n\r\npytorch versionstable1.0\r\nosWindows 10 Pro Version 1803\r\npython version:3.7\r\nCUDA:10\r\ncudnn:7.4.1\r\nVisual Studio version: Microsoft Visual Studio 2017 Community Edition\r\n\r\nCan anyone help me solve this problem? Thanks !\n<issue_comment>username_1: Please provide the full log.\n<issue_comment>username_0: the full log\r\n\r\nBuilding wheel torch-1.0.0a0+df61437\r\nrunning install\r\nsetup.py::run()\r\nrunning build_deps\r\nsetup.py::build_deps::run()\r\n\r\nE:\\SOFTWARE\\pytorch>cd \"E:\\SOFTWARE\\pytorch\\tools\\\\..\"\r\n\r\nE:\\SOFTWARE\\pytorch>set PATH=\\bin;E:\\SOFTWARE\\Anaconda\\DLLs;E:\\SOFTWARE\\Visual Studio\\VC\\Tools\\MSVC\\14.11.25503\\bin\\HostX64\\x64;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\VC\\VCPackages;C:\\Program Files (x86)\\Microsoft SDKs\\TypeScript\\3.1;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\TestWindow;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\TeamFoundation\\Team Explorer;E:\\SOFTWARE\\Visual Studio\\MSBuild\\15.0\\bin\\Roslyn;E:\\SOFTWARE\\Visual Studio\\Team Tools\\Performance Tools\\x64;E:\\SOFTWARE\\Visual Studio\\Team Tools\\Performance Tools;E:\\SOFTWARE\\Visual Studio\\share\\Common\\VSPerfCollectionTools\\\\x64;E:\\SOFTWARE\\Visual Studio\\share\\Common\\VSPerfCollectionTools\\;C:\\Program Files (x86)\\Microsoft SDKs\\Windows\\v10.0A\\bin\\NETFX 4.6.1 Tools\\x64\\;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\10.0.17763.0\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x64;E:\\SOFTWARE\\Visual Studio\\\\MSBuild\\15.0\\bin;C:\\WINDOWS\\Microsoft.NET\\Framework64\\v4.0.30319;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\;E:\\SOFTWARE\\Visual Studio\\Common7\\Tools\\;E:\\SOFTWARE\\Anaconda;E:\\SOFTWARE\\Anaconda\\Library\\mingw-w64\\bin;E:\\SOFTWARE\\Anaconda\\Library\\usr\\bin;E:\\SOFTWARE\\Anaconda\\Library\\bin;E:\\SOFTWARE\\Anaconda\\Scripts;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\bin;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\libnvvp;C:\\WINDOWS\\system32;C:\\WINDOWS;C:\\WINDOWS\\System32\\Wbem;C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0\\;C:\\WINDOWS\\System32\\OpenSSH\\;E:\\\\Git\\cmd;E:\\SOFTWARE\\cmake\\bin;C:\\Program Files (x86)\\Windows Kits\\8.1\\Windows Performance Toolkit\\;C:\\Program Files (x86)\\NVIDIA Corporation\\PhysX\\Common;C:\\Program Files\\dotnet\\;C:\\Program Files\\NVIDIA Corporation\\NVIDIA NvDLISR;C:\\Windows\\Microsoft.NET\\Framework\\v4.0.30319;C:\\Windows\\Microsoft.NET\\Framework64\\v4.0.30319;C:\\Users\\11838\\AppData\\Local\\Microsoft\\WindowsApps;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\CMake\\bin;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\Ninja\r\n\r\nE:\\SOFTWARE\\pytorch>set BASE_DIR=E:/SOFTWARE/pytorch\r\n\r\nE:\\SOFTWARE\\pytorch>set TORCH_LIB_DIR=E:/SOFTWARE/pytorch/torch/lib\r\n\r\nE:\\SOFTWARE\\pytorch>set THIRD_PARTY_DIR=E:/SOFTWARE/pytorch/third_party\r\n\r\nE:\\SOFTWARE\\pytorch>set BASIC_C_FLAGS=\r\n\r\nE:\\SOFTWARE\\pytorch>set BASIC_CUDA_FLAGS=\r\n\r\nE:\\SOFTWARE\\pytorch>IF NOT DEFINED INSTALL_DIR (set \"INSTALL_DIR=E:/SOFTWARE/pytorch/torch/lib/tmp_install\" )  ELSE (set \"INSTALL_DIR=\\=/\" )\r\n\r\nE:\\SOFTWARE\\pytorch>set LDFLAGS=/LIBPATH:E:/SOFTWARE/pytorch/torch/lib/tmp_install/lib\r\n\r\nE:\\SOFTWARE\\pytorch>set C_FLAGS= /D_WIN32 /Z7 /EHa /DNOMINMAX\r\n\r\nE:\\SOFTWARE\\pytorch>set LINK_FLAGS=/DEBUG:FULL\r\n\r\nE:\\SOFTWARE\\pytorch>if not exist torch\\lib\\tmp_install mkdir torch\\lib\\tmp_install\r\n\r\nE:\\SOFTWARE\\pytorch>set /a USE_CUDA=0\r\n\r\nE:\\SOFTWARE\\pytorch>set /a USE_FBGEMM=1\r\n\r\nE:\\SOFTWARE\\pytorch>set /a USE_ROCM=0\r\n\r\nE:\\SOFTWARE\\pytorch>set /a USE_NNPACK=0\r\n\r\nE:\\SOFTWARE\\pytorch>set /a USE_QNNPACK=0\r\n\r\nE:\\SOFTWARE\\pytorch>set /a USE_GLOO_IBVERBS=0\r\n\r\nE:\\SOFTWARE\\pytorch>set /a USE_MKLDNN=0\r\n\r\nE:\\SOFTWARE\\pytorch>set _BUILD_ARGS=\r\n\r\nE:\\SOFTWARE\\pytorch>if \"--use-cuda\" == \"\" (goto :process_args_exit )\r\n\r\nE:\\SOFTWARE\\pytorch>if \"--use-cuda\" == \"--use-cuda\" (\r\nset /a USE_CUDA=1\r\n goto :process_args_processed\r\n)\r\n\r\nE:\\SOFTWARE\\pytorch>shift\r\n\r\nE:\\SOFTWARE\\pytorch>goto :process_args\r\n\r\nE:\\SOFTWARE\\pytorch>if \"--use-fbgemm\" == \"\" (goto :process_args_exit )\r\n\r\nE:\\SOFTWARE\\pytorch>if \"--use-fbgemm\" == \"--use-cuda\" (\r\nset /a USE_CUDA=1\r\n goto :process_args_processed\r\n)\r\n\r\nE:\\SOFTWARE\\pytorch>if \"--use-fbgemm\" == \"--use-fbgemm\" (\r\nset /a USE_FBGEMM=1\r\n goto :process_args_processed\r\n)\r\n\r\nE:\\SOFTWARE\\pytorch>shift\r\n\r\nE:\\SOFTWARE\\pytorch>goto :process_args\r\n\r\nE:\\SOFTWARE\\pytorch>if \"--use-nnpack\" == \"\" (goto :process_args_exit )\r\n\r\nE:\\SOFTWARE\\pytorch>if \"--use-nnpack\" == \"--use-cuda\" (\r\nset /a USE_CUDA=1\r\n goto :process_args_processed\r\n[Truncated]\nDone Building Project \"E:\\SOFTWARE\\pytorch\\build\\INSTALL.vcxproj\" (default targets) -- FAILED.\r\n\r\n\r\nBuild FAILED.\r\n\r\n\"E:\\SOFTWARE\\pytorch\\build\\INSTALL.vcxproj\" (default target) (1) ->\r\n\"E:\\SOFTWARE\\pytorch\\build\\ALL_BUILD.vcxproj\" (default target) (3) ->\r\n\"E:\\SOFTWARE\\pytorch\\build\\caffe2\\AlgorithmsTest.vcxproj\" (default target) (4) ->\r\n\"E:\\SOFTWARE\\pytorch\\build\\caffe2\\caffe2_gpu.vcxproj\" (default target) (22) ->\r\n(Link target) ->\r\n  LINK : fatal error LNK1181: C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\lib\\x64.obj [E:\\SOFTWA\r\nRE\\pytorch\\build\\caffe2\\caffe2_gpu.vcxproj]\r\n\r\n    0 Warning(s)\r\n    1 Error(s)\r\n\r\nTime Elapsed 00:00:22.22\r\n\r\nE:\\SOFTWARE\\pytorch\\build>IF ERRORLEVEL 1 exit 1\r\nFailed to run 'tools\\build_pytorch_libs.bat --use-cuda --use-fbgemm --use-nnpack --use-mkldnn --use-qnnpack caffe2'\n<issue_comment>username_1: The error is caused by the misconfiguration of the flag `CUDNN_LIBRARY`. Could you please run `tools/setup_helpers/cudnn.py` and see which code path it goes through?\n<issue_comment>username_0: My environmental variables is :\r\nCUDA_PATH=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\r\nCUDA_PATH_V10_0=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\r\nCUDNN_INCLUDE_DIR=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\include\r\nCUDNN_LIBRARY=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\lib\\x64\r\nNVCUDASAMPLES10_0_ROOT=C:\\ProgramData\\NVIDIA Corporation\\CUDA Samples\\v10.0\r\nNVCUDASAMPLES_ROOT=C:\\ProgramData\\NVIDIA Corporation\\CUDA Samples\\v10.0\r\nNVTOOLSEXT_PATH=C:\\Program Files\\NVIDIA Corporation\\NvToolsExt\\\r\nPath=E:\\SOFTWARE\\Visual Studio\\VC\\Tools\\MSVC\\14.11.25503\\bin\\HostX64\\x64;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\VC\\VCPackages;C:\\Program Files (x86)\\Microsoft SDKs\\TypeScript\\3.1;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\TestWindow;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\TeamFoundation\\Team Explorer;E:\\SOFTWARE\\Visual Studio\\MSBuild\\15.0\\bin\\Roslyn;E:\\SOFTWARE\\Visual Studio\\Team Tools\\Performance Tools\\x64;E:\\SOFTWARE\\Visual Studio\\Team Tools\\Performance Tools;E:\\SOFTWARE\\Visual Studio\\share\\Common\\VSPerfCollectionTools\\\\x64;E:\\SOFTWARE\\Visual Studio\\share\\Common\\VSPerfCollectionTools\\;C:\\Program Files (x86)\\Microsoft SDKs\\Windows\\v10.0A\\bin\\NETFX 4.6.1 Tools\\x64\\;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\10.0.17763.0\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x64;E:\\SOFTWARE\\Visual Studio\\\\MSBuild\\15.0\\bin;C:\\WINDOWS\\Microsoft.NET\\Framework64\\v4.0.30319;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\;E:\\SOFTWARE\\Visual Studio\\Common7\\Tools\\;E:\\SOFTWARE\\Anaconda;E:\\SOFTWARE\\Anaconda\\Library\\mingw-w64\\bin;E:\\SOFTWARE\\Anaconda\\Library\\usr\\bin;E:\\SOFTWARE\\Anaconda\\Library\\bin;E:\\SOFTWARE\\Anaconda\\Scripts;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\bin;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\libnvvp;C:\\WINDOWS\\system32;C:\\WINDOWS;C:\\WINDOWS\\System32\\Wbem;C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0\\;C:\\WINDOWS\\System32\\OpenSSH\\;E:\\\\Git\\cmd;E:\\SOFTWARE\\cmake\\bin;C:\\Program Files (x86)\\Windows Kits\\8.1\\Windows Performance Toolkit\\;C:\\Program Files (x86)\\NVIDIA Corporation\\PhysX\\Common;C:\\Program Files\\dotnet\\;C:\\Program Files\\NVIDIA Corporation\\NVIDIA NvDLISR;C:\\Windows\\Microsoft.NET\\Framework\\v4.0.30319;C:\\Windows\\Microsoft.NET\\Framework64\\v4.0.30319;C:\\Users\\11838\\AppData\\Local\\Microsoft\\WindowsApps;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\CMake\\bin;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\Ninja\r\n\r\nAND the cudnn.py shows:\r\nimport os\r\nimport glob\r\n\r\nfrom .env import IS_WINDOWS, IS_CONDA, CONDA_DIR, check_negative_env_flag, gather_paths, lib_paths_from_base\r\nfrom .cuda import USE_CUDA, CUDA_HOME\r\n\r\n\r\nUSE_CUDNN = False\r\nCUDNN_LIB_DIR = None\r\nCUDNN_INCLUDE_DIR = None\r\nCUDNN_LIBRARY = None\r\nWITH_STATIC_CUDNN = os.getenv(\"USE_STATIC_CUDNN\")\r\n\r\nif USE_CUDA and not check_negative_env_flag('USE_CUDNN'):\r\n    lib_paths = list(filter(bool, [\r\n        os.getenv('CUDNN_LIB_DIR')\r\n    ] + lib_paths_from_base(CUDA_HOME) + [\r\n        '/usr/lib/x86_64-linux-gnu/',\r\n        '/usr/lib/powerpc64le-linux-gnu/',\r\n        '/usr/lib/aarch64-linux-gnu/',\r\n    ] + gather_paths([\r\n        'LIBRARY_PATH',\r\n    ]) + gather_paths([\r\n        'LD_LIBRARY_PATH',\r\n    ])))\r\n    include_paths = list(filter(bool, [\r\n        os.getenv('CUDNN_INCLUDE_DIR'),\r\n        os.path.join(CUDA_HOME, 'include'),\r\n        '/usr/include/',\r\n    ] + gather_paths([\r\n        'CPATH',\r\n        'C_INCLUDE_PATH',\r\n        'CPLUS_INCLUDE_PATH',\r\n    ])))\r\n    # Add CUDA related dirs to candidate list\r\n    if IS_CONDA:\r\n        lib_paths.append(os.path.join(CONDA_DIR, 'lib'))\r\n        include_paths.append(os.path.join(CONDA_DIR, 'include'))\r\n    for path in include_paths:\r\n        if path is None or not os.path.exists(path):\r\n            continue\r\n        include_file_path = os.path.join(path, 'cudnn.h')\r\n        CUDNN_INCLUDE_VERSION = None\r\n        if os.path.exists(include_file_path):\r\n            CUDNN_INCLUDE_DIR = path\r\n            with open(include_file_path) as f:\r\n                for line in f:\r\n                    if \"#define CUDNN_MAJOR\" in line:\r\n                        CUDNN_INCLUDE_VERSION = int(line.split()[-1])\r\n                        break\r\n            if CUDNN_INCLUDE_VERSION is None:\r\n                raise AssertionError(\"Could not find #define CUDNN_MAJOR in \" + include_file_path)\r\n            break\r\n\r\n    if CUDNN_INCLUDE_VERSION is None:\r\n        pass\r\n\r\n    # Check for standalone cuDNN libraries\r\n    if CUDNN_INCLUDE_DIR is not None:\r\n        cudnn_path = os.path.join(os.path.dirname(CUDNN_INCLUDE_DIR))\r\n        cudnn_lib_paths = lib_paths_from_base(cudnn_path)\r\n        lib_paths.extend(cudnn_lib_paths)\r\n\r\n    for path in lib_paths:\r\n        if path is None or not os.path.exists(path):\r\n            continue\r\n        if IS_WINDOWS:\r\n            library = os.path.join(path, 'cudnn.lib')\r\n            if os.path.exists(library):\r\n[Truncated]\n                search_name = 'libcudnn*' + str(CUDNN_INCLUDE_VERSION) + \"*\"\r\n            libraries = sorted(glob.glob(os.path.join(path, search_name)))\r\n            if libraries:\r\n                CUDNN_LIBRARY = libraries[0]\r\n                CUDNN_LIB_DIR = path\r\n                break\r\n    # Specifying the library directly will overwrite the lib directory\r\n    library = os.getenv('CUDNN_LIBRARY')\r\n    if library is not None and os.path.exists(library):\r\n        CUDNN_LIBRARY = library\r\n        CUDNN_LIB_DIR = os.path.dirname(CUDNN_LIBRARY)\r\n\r\n    if not all([CUDNN_LIBRARY, CUDNN_LIB_DIR, CUDNN_INCLUDE_DIR]):\r\n        CUDNN_LIBRARY = CUDNN_LIB_DIR = CUDNN_INCLUDE_DIR = None\r\n    else:\r\n        real_cudnn_library = os.path.realpath(CUDNN_LIBRARY)\r\n        real_cudnn_lib_dir = os.path.realpath(CUDNN_LIB_DIR)\r\n        assert os.path.dirname(real_cudnn_library) == real_cudnn_lib_dir, (\r\n            'cudnn library and lib_dir must agree')\r\n        USE_CUDNN = True\n<issue_comment>username_1: Please remove `CUDNN_INCLUDE_DIR` and `CUDNN_LIBRARY` by using the following commands.\r\n```cmd\r\nset CUDNN_INCLUDE_DIR=\r\nset CUDNN_LIBRARY=\r\n```\n<issue_comment>username_0: It works! Thanks a lot!<issue_closed>","repo":"pytorch/pytorch","issue_id":391131022,"issue_number":15222}
{"question":"What error message was encountered during the build process in the provided text chunk?","answer":"fatal error LNK1181: C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\lib\\x64.obj","id":134,"text":"the full log\r\n\r\nBuilding wheel torch-1.0.0a0+df61437\r\nrunning install\r\nsetup.py::run()\r\nrunning build_deps\r\nsetup.py::build_deps::run()\r\n\r\nE:\\SOFTWARE\\pytorch>cd \"E:\\SOFTWARE\\pytorch\\tools\\\\..\"\r\n\r\nE:\\SOFTWARE\\pytorch>set PATH=\\bin;E:\\SOFTWARE\\Anaconda\\DLLs;E:\\SOFTWARE\\Visual Studio\\VC\\Tools\\MSVC\\14.11.25503\\bin\\HostX64\\x64;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\VC\\VCPackages;C:\\Program Files (x86)\\Microsoft SDKs\\TypeScript\\3.1;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\TestWindow;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\TeamFoundation\\Team Explorer;E:\\SOFTWARE\\Visual Studio\\MSBuild\\15.0\\bin\\Roslyn;E:\\SOFTWARE\\Visual Studio\\Team Tools\\Performance Tools\\x64;E:\\SOFTWARE\\Visual Studio\\Team Tools\\Performance Tools;E:\\SOFTWARE\\Visual Studio\\share\\Common\\VSPerfCollectionTools\\\\x64;E:\\SOFTWARE\\Visual Studio\\share\\Common\\VSPerfCollectionTools\\;C:\\Program Files (x86)\\Microsoft SDKs\\Windows\\v10.0A\\bin\\NETFX 4.6.1 Tools\\x64\\;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\10.0.17763.0\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x64;E:\\SOFTWARE\\Visual Studio\\\\MSBuild\\15.0\\bin;C:\\WINDOWS\\Microsoft.NET\\Framework64\\v4.0.30319;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\;E:\\SOFTWARE\\Visual Studio\\Common7\\Tools\\;E:\\SOFTWARE\\Anaconda;E:\\SOFTWARE\\Anaconda\\Library\\mingw-w64\\bin;E:\\SOFTWARE\\Anaconda\\Library\\usr\\bin;E:\\SOFTWARE\\Anaconda\\Library\\bin;E:\\SOFTWARE\\Anaconda\\Scripts;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\bin;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\libnvvp;C:\\WINDOWS\\system32;C:\\WINDOWS;C:\\WINDOWS\\System32\\Wbem;C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0\\;C:\\WINDOWS\\System32\\OpenSSH\\;E:\\\\Git\\cmd;E:\\SOFTWARE\\cmake\\bin;C:\\Program Files (x86)\\Windows Kits\\8.1\\Windows Performance Toolkit\\;C:\\Program Files (x86)\\NVIDIA Corporation\\PhysX\\Common;C:\\Program Files\\dotnet\\;C:\\Program Files\\NVIDIA Corporation\\NVIDIA NvDLISR;C:\\Windows\\Microsoft.NET\\Framework\\v4.0.30319;C:\\Windows\\Microsoft.NET\\Framework64\\v4.0.30319;C:\\Users\\11838\\AppData\\Local\\Microsoft\\WindowsApps;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\CMake\\bin;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\Ninja\r\n\r\nE:\\SOFTWARE\\pytorch>set BASE_DIR=E:/SOFTWARE/pytorch\r\n\r\nE:\\SOFTWARE\\pytorch>set TORCH_LIB_DIR=E:/SOFTWARE/pytorch/torch/lib\r\n\r\nE:\\SOFTWARE\\pytorch>set THIRD_PARTY_DIR=E:/SOFTWARE/pytorch/third_party\r\n\r\nE:\\SOFTWARE\\pytorch>set BASIC_C_FLAGS=\r\n\r\nE:\\SOFTWARE\\pytorch>set BASIC_CUDA_FLAGS=\r\n\r\nE:\\SOFTWARE\\pytorch>IF NOT DEFINED INSTALL_DIR (set \"INSTALL_DIR=E:/SOFTWARE/pytorch/torch/lib/tmp_install\" )  ELSE (set \"INSTALL_DIR=\\=/\" )\r\n\r\nE:\\SOFTWARE\\pytorch>set LDFLAGS=/LIBPATH:E:/SOFTWARE/pytorch/torch/lib/tmp_install/lib\r\n\r\nE:\\SOFTWARE\\pytorch>set C_FLAGS= /D_WIN32 /Z7 /EHa /DNOMINMAX\r\n\r\nE:\\SOFTWARE\\pytorch>set LINK_FLAGS=/DEBUG:FULL\r\n\r\nE:\\SOFTWARE\\pytorch>if not exist torch\\lib\\tmp_install mkdir torch\\lib\\tmp_install\r\n\r\nE:\\SOFTWARE\\pytorch>set /a USE_CUDA=0\r\n\r\nE:\\SOFTWARE\\pytorch>set /a USE_FBGEMM=1\r\n\r\nE:\\SOFTWARE\\pytorch>set /a USE_ROCM=0\r\n\r\nE:\\SOFTWARE\\pytorch>set /a USE_NNPACK=0\r\n\r\nE:\\SOFTWARE\\pytorch>set /a USE_QNNPACK=0\r\n\r\nE:\\SOFTWARE\\pytorch>set /a USE_GLOO_IBVERBS=0\r\n\r\nE:\\SOFTWARE\\pytorch>set /a USE_MKLDNN=0\r\n\r\nE:\\SOFTWARE\\pytorch>set _BUILD_ARGS=\r\n\r\nE:\\SOFTWARE\\pytorch>if \"--use-cuda\" == \"\" (goto :process_args_exit )\r\n\r\nE:\\SOFTWARE\\pytorch>if \"--use-cuda\" == \"--use-cuda\" (\r\nset /a USE_CUDA=1\r\n goto :process_args_processed\r\n)\r\n\r\nE:\\SOFTWARE\\pytorch>shift\r\n\r\nE:\\SOFTWARE\\pytorch>goto :process_args\r\n\r\nE:\\SOFTWARE\\pytorch>if \"--use-fbgemm\" == \"\" (goto :process_args_exit )\r\n\r\nE:\\SOFTWARE\\pytorch>if \"--use-fbgemm\" == \"--use-cuda\" (\r\nset /a USE_CUDA=1\r\n goto :process_args_processed\r\n)\r\n\r\nE:\\SOFTWARE\\pytorch>if \"--use-fbgemm\" == \"--use-fbgemm\" (\r\nset /a USE_FBGEMM=1\r\n goto :process_args_processed\r\n)\r\n\r\nE:\\SOFTWARE\\pytorch>shift\r\n\r\nE:\\SOFTWARE\\pytorch>goto :process_args\r\n\r\nE:\\SOFTWARE\\pytorch>if \"--use-nnpack\" == \"\" (goto :process_args_exit )\r\n\r\nE:\\SOFTWARE\\pytorch>if \"--use-nnpack\" == \"--use-cuda\" (\r\nset /a USE_CUDA=1\r\n goto :process_args_processed\r\n[Truncated]\nDone Building Project \"E:\\SOFTWARE\\pytorch\\build\\INSTALL.vcxproj\" (default targets) -- FAILED.\r\n\r\n\r\nBuild FAILED.\r\n\r\n\"E:\\SOFTWARE\\pytorch\\build\\INSTALL.vcxproj\" (default target) (1) ->\r\n\"E:\\SOFTWARE\\pytorch\\build\\ALL_BUILD.vcxproj\" (default target) (3) ->\r\n\"E:\\SOFTWARE\\pytorch\\build\\caffe2\\AlgorithmsTest.vcxproj\" (default target) (4) ->\r\n\"E:\\SOFTWARE\\pytorch\\build\\caffe2\\caffe2_gpu.vcxproj\" (default target) (22) ->\r\n(Link target) ->\r\n  LINK : fatal error LNK1181: C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\lib\\x64.obj [E:\\SOFTWA\r\nRE\\pytorch\\build\\caffe2\\caffe2_gpu.vcxproj]\r\n\r\n    0 Warning(s)\r\n    1 Error(s)\r\n\r\nTime Elapsed 00:00:22.22\r\n\r\nE:\\SOFTWARE\\pytorch\\build>IF ERRORLEVEL 1 exit 1\r\nFailed to run 'tools\\build_pytorch_libs.bat --use-cuda --use-fbgemm --use-nnpack --use-mkldnn --use-qnnpack caffe2'","context":"<issue_start><issue_comment>Title: Can't open x64.obj while installing pytorch with win10 \nusername_0: E:\\SOFTWARE\\pytorch\\build\\caffe2\\torch\\lib\\libshm_windows\\shm.vcxproj()\r\n\r\nE:\\SOFTWARE\\pytorch\\build\\ALL_BUILD.vcxproj() - \r\n\r\nE:\\SOFTWARE\\pytorch\\build\\INSTALL.vcxproj() - \r\n\r\n\r\n\r\n\r\nE:\\SOFTWARE\\pytorch\\build\\INSTALL.vcxproj() (1) ->\r\nE:\\SOFTWARE\\pytorch\\build\\ALL_BUILD.vcxproj() (3) ->\r\nE:\\SOFTWARE\\pytorch\\build\\caffe2\\AlgorithmsTest.vcxproj() (4) ->\r\nE:\\SOFTWARE\\pytorch\\build\\caffe2\\caffe2_gpu.vcxproj() (22) ->\r\n(Link ) ->\r\n  C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\lib\\x64.obj : fatal error LNK1136:  [E:\\SOFTWARE\\pyt\r\norch\\build\\caffe2\\caffe2_gpu.vcxproj]\r\n\r\n    0 \r\n    1 \r\n\r\n 00:00:16.23\r\n\r\nE:\\SOFTWARE\\pytorch\\build>IF ERRORLEVEL 1 exit 1\r\nFailed to run 'tools\\build_pytorch_libs.bat --use-cuda --use-fbgemm --use-nnpack --use-mkldnn --use-qnnpack caffe2'\r\n\r\npytorch versionstable1.0\r\nosWindows 10 Pro Version 1803\r\npython version:3.7\r\nCUDA:10\r\ncudnn:7.4.1\r\nVisual Studio version: Microsoft Visual Studio 2017 Community Edition\r\n\r\nCan anyone help me solve this problem? Thanks !\n<issue_comment>username_1: Please provide the full log.\n<issue_comment>username_0: the full log\r\n\r\nBuilding wheel torch-1.0.0a0+df61437\r\nrunning install\r\nsetup.py::run()\r\nrunning build_deps\r\nsetup.py::build_deps::run()\r\n\r\nE:\\SOFTWARE\\pytorch>cd \"E:\\SOFTWARE\\pytorch\\tools\\\\..\"\r\n\r\nE:\\SOFTWARE\\pytorch>set PATH=\\bin;E:\\SOFTWARE\\Anaconda\\DLLs;E:\\SOFTWARE\\Visual Studio\\VC\\Tools\\MSVC\\14.11.25503\\bin\\HostX64\\x64;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\VC\\VCPackages;C:\\Program Files (x86)\\Microsoft SDKs\\TypeScript\\3.1;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\TestWindow;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\TeamFoundation\\Team Explorer;E:\\SOFTWARE\\Visual Studio\\MSBuild\\15.0\\bin\\Roslyn;E:\\SOFTWARE\\Visual Studio\\Team Tools\\Performance Tools\\x64;E:\\SOFTWARE\\Visual Studio\\Team Tools\\Performance Tools;E:\\SOFTWARE\\Visual Studio\\share\\Common\\VSPerfCollectionTools\\\\x64;E:\\SOFTWARE\\Visual Studio\\share\\Common\\VSPerfCollectionTools\\;C:\\Program Files (x86)\\Microsoft SDKs\\Windows\\v10.0A\\bin\\NETFX 4.6.1 Tools\\x64\\;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\10.0.17763.0\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x64;E:\\SOFTWARE\\Visual Studio\\\\MSBuild\\15.0\\bin;C:\\WINDOWS\\Microsoft.NET\\Framework64\\v4.0.30319;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\;E:\\SOFTWARE\\Visual Studio\\Common7\\Tools\\;E:\\SOFTWARE\\Anaconda;E:\\SOFTWARE\\Anaconda\\Library\\mingw-w64\\bin;E:\\SOFTWARE\\Anaconda\\Library\\usr\\bin;E:\\SOFTWARE\\Anaconda\\Library\\bin;E:\\SOFTWARE\\Anaconda\\Scripts;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\bin;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\libnvvp;C:\\WINDOWS\\system32;C:\\WINDOWS;C:\\WINDOWS\\System32\\Wbem;C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0\\;C:\\WINDOWS\\System32\\OpenSSH\\;E:\\\\Git\\cmd;E:\\SOFTWARE\\cmake\\bin;C:\\Program Files (x86)\\Windows Kits\\8.1\\Windows Performance Toolkit\\;C:\\Program Files (x86)\\NVIDIA Corporation\\PhysX\\Common;C:\\Program Files\\dotnet\\;C:\\Program Files\\NVIDIA Corporation\\NVIDIA NvDLISR;C:\\Windows\\Microsoft.NET\\Framework\\v4.0.30319;C:\\Windows\\Microsoft.NET\\Framework64\\v4.0.30319;C:\\Users\\11838\\AppData\\Local\\Microsoft\\WindowsApps;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\CMake\\bin;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\Ninja\r\n\r\nE:\\SOFTWARE\\pytorch>set BASE_DIR=E:/SOFTWARE/pytorch\r\n\r\nE:\\SOFTWARE\\pytorch>set TORCH_LIB_DIR=E:/SOFTWARE/pytorch/torch/lib\r\n\r\nE:\\SOFTWARE\\pytorch>set THIRD_PARTY_DIR=E:/SOFTWARE/pytorch/third_party\r\n\r\nE:\\SOFTWARE\\pytorch>set BASIC_C_FLAGS=\r\n\r\nE:\\SOFTWARE\\pytorch>set BASIC_CUDA_FLAGS=\r\n\r\nE:\\SOFTWARE\\pytorch>IF NOT DEFINED INSTALL_DIR (set \"INSTALL_DIR=E:/SOFTWARE/pytorch/torch/lib/tmp_install\" )  ELSE (set \"INSTALL_DIR=\\=/\" )\r\n\r\nE:\\SOFTWARE\\pytorch>set LDFLAGS=/LIBPATH:E:/SOFTWARE/pytorch/torch/lib/tmp_install/lib\r\n\r\nE:\\SOFTWARE\\pytorch>set C_FLAGS= /D_WIN32 /Z7 /EHa /DNOMINMAX\r\n\r\nE:\\SOFTWARE\\pytorch>set LINK_FLAGS=/DEBUG:FULL\r\n\r\nE:\\SOFTWARE\\pytorch>if not exist torch\\lib\\tmp_install mkdir torch\\lib\\tmp_install\r\n\r\nE:\\SOFTWARE\\pytorch>set /a USE_CUDA=0\r\n\r\nE:\\SOFTWARE\\pytorch>set /a USE_FBGEMM=1\r\n\r\nE:\\SOFTWARE\\pytorch>set /a USE_ROCM=0\r\n\r\nE:\\SOFTWARE\\pytorch>set /a USE_NNPACK=0\r\n\r\nE:\\SOFTWARE\\pytorch>set /a USE_QNNPACK=0\r\n\r\nE:\\SOFTWARE\\pytorch>set /a USE_GLOO_IBVERBS=0\r\n\r\nE:\\SOFTWARE\\pytorch>set /a USE_MKLDNN=0\r\n\r\nE:\\SOFTWARE\\pytorch>set _BUILD_ARGS=\r\n\r\nE:\\SOFTWARE\\pytorch>if \"--use-cuda\" == \"\" (goto :process_args_exit )\r\n\r\nE:\\SOFTWARE\\pytorch>if \"--use-cuda\" == \"--use-cuda\" (\r\nset /a USE_CUDA=1\r\n goto :process_args_processed\r\n)\r\n\r\nE:\\SOFTWARE\\pytorch>shift\r\n\r\nE:\\SOFTWARE\\pytorch>goto :process_args\r\n\r\nE:\\SOFTWARE\\pytorch>if \"--use-fbgemm\" == \"\" (goto :process_args_exit )\r\n\r\nE:\\SOFTWARE\\pytorch>if \"--use-fbgemm\" == \"--use-cuda\" (\r\nset /a USE_CUDA=1\r\n goto :process_args_processed\r\n)\r\n\r\nE:\\SOFTWARE\\pytorch>if \"--use-fbgemm\" == \"--use-fbgemm\" (\r\nset /a USE_FBGEMM=1\r\n goto :process_args_processed\r\n)\r\n\r\nE:\\SOFTWARE\\pytorch>shift\r\n\r\nE:\\SOFTWARE\\pytorch>goto :process_args\r\n\r\nE:\\SOFTWARE\\pytorch>if \"--use-nnpack\" == \"\" (goto :process_args_exit )\r\n\r\nE:\\SOFTWARE\\pytorch>if \"--use-nnpack\" == \"--use-cuda\" (\r\nset /a USE_CUDA=1\r\n goto :process_args_processed\r\n[Truncated]\nDone Building Project \"E:\\SOFTWARE\\pytorch\\build\\INSTALL.vcxproj\" (default targets) -- FAILED.\r\n\r\n\r\nBuild FAILED.\r\n\r\n\"E:\\SOFTWARE\\pytorch\\build\\INSTALL.vcxproj\" (default target) (1) ->\r\n\"E:\\SOFTWARE\\pytorch\\build\\ALL_BUILD.vcxproj\" (default target) (3) ->\r\n\"E:\\SOFTWARE\\pytorch\\build\\caffe2\\AlgorithmsTest.vcxproj\" (default target) (4) ->\r\n\"E:\\SOFTWARE\\pytorch\\build\\caffe2\\caffe2_gpu.vcxproj\" (default target) (22) ->\r\n(Link target) ->\r\n  LINK : fatal error LNK1181: C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\lib\\x64.obj [E:\\SOFTWA\r\nRE\\pytorch\\build\\caffe2\\caffe2_gpu.vcxproj]\r\n\r\n    0 Warning(s)\r\n    1 Error(s)\r\n\r\nTime Elapsed 00:00:22.22\r\n\r\nE:\\SOFTWARE\\pytorch\\build>IF ERRORLEVEL 1 exit 1\r\nFailed to run 'tools\\build_pytorch_libs.bat --use-cuda --use-fbgemm --use-nnpack --use-mkldnn --use-qnnpack caffe2'\n<issue_comment>username_1: The error is caused by the misconfiguration of the flag `CUDNN_LIBRARY`. Could you please run `tools/setup_helpers/cudnn.py` and see which code path it goes through?\n<issue_comment>username_0: My environmental variables is :\r\nCUDA_PATH=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\r\nCUDA_PATH_V10_0=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\r\nCUDNN_INCLUDE_DIR=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\include\r\nCUDNN_LIBRARY=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\lib\\x64\r\nNVCUDASAMPLES10_0_ROOT=C:\\ProgramData\\NVIDIA Corporation\\CUDA Samples\\v10.0\r\nNVCUDASAMPLES_ROOT=C:\\ProgramData\\NVIDIA Corporation\\CUDA Samples\\v10.0\r\nNVTOOLSEXT_PATH=C:\\Program Files\\NVIDIA Corporation\\NvToolsExt\\\r\nPath=E:\\SOFTWARE\\Visual Studio\\VC\\Tools\\MSVC\\14.11.25503\\bin\\HostX64\\x64;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\VC\\VCPackages;C:\\Program Files (x86)\\Microsoft SDKs\\TypeScript\\3.1;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\TestWindow;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\TeamFoundation\\Team Explorer;E:\\SOFTWARE\\Visual Studio\\MSBuild\\15.0\\bin\\Roslyn;E:\\SOFTWARE\\Visual Studio\\Team Tools\\Performance Tools\\x64;E:\\SOFTWARE\\Visual Studio\\Team Tools\\Performance Tools;E:\\SOFTWARE\\Visual Studio\\share\\Common\\VSPerfCollectionTools\\\\x64;E:\\SOFTWARE\\Visual Studio\\share\\Common\\VSPerfCollectionTools\\;C:\\Program Files (x86)\\Microsoft SDKs\\Windows\\v10.0A\\bin\\NETFX 4.6.1 Tools\\x64\\;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\10.0.17763.0\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x64;E:\\SOFTWARE\\Visual Studio\\\\MSBuild\\15.0\\bin;C:\\WINDOWS\\Microsoft.NET\\Framework64\\v4.0.30319;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\;E:\\SOFTWARE\\Visual Studio\\Common7\\Tools\\;E:\\SOFTWARE\\Anaconda;E:\\SOFTWARE\\Anaconda\\Library\\mingw-w64\\bin;E:\\SOFTWARE\\Anaconda\\Library\\usr\\bin;E:\\SOFTWARE\\Anaconda\\Library\\bin;E:\\SOFTWARE\\Anaconda\\Scripts;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\bin;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\libnvvp;C:\\WINDOWS\\system32;C:\\WINDOWS;C:\\WINDOWS\\System32\\Wbem;C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0\\;C:\\WINDOWS\\System32\\OpenSSH\\;E:\\\\Git\\cmd;E:\\SOFTWARE\\cmake\\bin;C:\\Program Files (x86)\\Windows Kits\\8.1\\Windows Performance Toolkit\\;C:\\Program Files (x86)\\NVIDIA Corporation\\PhysX\\Common;C:\\Program Files\\dotnet\\;C:\\Program Files\\NVIDIA Corporation\\NVIDIA NvDLISR;C:\\Windows\\Microsoft.NET\\Framework\\v4.0.30319;C:\\Windows\\Microsoft.NET\\Framework64\\v4.0.30319;C:\\Users\\11838\\AppData\\Local\\Microsoft\\WindowsApps;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\CMake\\bin;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\Ninja\r\n\r\nAND the cudnn.py shows:\r\nimport os\r\nimport glob\r\n\r\nfrom .env import IS_WINDOWS, IS_CONDA, CONDA_DIR, check_negative_env_flag, gather_paths, lib_paths_from_base\r\nfrom .cuda import USE_CUDA, CUDA_HOME\r\n\r\n\r\nUSE_CUDNN = False\r\nCUDNN_LIB_DIR = None\r\nCUDNN_INCLUDE_DIR = None\r\nCUDNN_LIBRARY = None\r\nWITH_STATIC_CUDNN = os.getenv(\"USE_STATIC_CUDNN\")\r\n\r\nif USE_CUDA and not check_negative_env_flag('USE_CUDNN'):\r\n    lib_paths = list(filter(bool, [\r\n        os.getenv('CUDNN_LIB_DIR')\r\n    ] + lib_paths_from_base(CUDA_HOME) + [\r\n        '/usr/lib/x86_64-linux-gnu/',\r\n        '/usr/lib/powerpc64le-linux-gnu/',\r\n        '/usr/lib/aarch64-linux-gnu/',\r\n    ] + gather_paths([\r\n        'LIBRARY_PATH',\r\n    ]) + gather_paths([\r\n        'LD_LIBRARY_PATH',\r\n    ])))\r\n    include_paths = list(filter(bool, [\r\n        os.getenv('CUDNN_INCLUDE_DIR'),\r\n        os.path.join(CUDA_HOME, 'include'),\r\n        '/usr/include/',\r\n    ] + gather_paths([\r\n        'CPATH',\r\n        'C_INCLUDE_PATH',\r\n        'CPLUS_INCLUDE_PATH',\r\n    ])))\r\n    # Add CUDA related dirs to candidate list\r\n    if IS_CONDA:\r\n        lib_paths.append(os.path.join(CONDA_DIR, 'lib'))\r\n        include_paths.append(os.path.join(CONDA_DIR, 'include'))\r\n    for path in include_paths:\r\n        if path is None or not os.path.exists(path):\r\n            continue\r\n        include_file_path = os.path.join(path, 'cudnn.h')\r\n        CUDNN_INCLUDE_VERSION = None\r\n        if os.path.exists(include_file_path):\r\n            CUDNN_INCLUDE_DIR = path\r\n            with open(include_file_path) as f:\r\n                for line in f:\r\n                    if \"#define CUDNN_MAJOR\" in line:\r\n                        CUDNN_INCLUDE_VERSION = int(line.split()[-1])\r\n                        break\r\n            if CUDNN_INCLUDE_VERSION is None:\r\n                raise AssertionError(\"Could not find #define CUDNN_MAJOR in \" + include_file_path)\r\n            break\r\n\r\n    if CUDNN_INCLUDE_VERSION is None:\r\n        pass\r\n\r\n    # Check for standalone cuDNN libraries\r\n    if CUDNN_INCLUDE_DIR is not None:\r\n        cudnn_path = os.path.join(os.path.dirname(CUDNN_INCLUDE_DIR))\r\n        cudnn_lib_paths = lib_paths_from_base(cudnn_path)\r\n        lib_paths.extend(cudnn_lib_paths)\r\n\r\n    for path in lib_paths:\r\n        if path is None or not os.path.exists(path):\r\n            continue\r\n        if IS_WINDOWS:\r\n            library = os.path.join(path, 'cudnn.lib')\r\n            if os.path.exists(library):\r\n[Truncated]\n                search_name = 'libcudnn*' + str(CUDNN_INCLUDE_VERSION) + \"*\"\r\n            libraries = sorted(glob.glob(os.path.join(path, search_name)))\r\n            if libraries:\r\n                CUDNN_LIBRARY = libraries[0]\r\n                CUDNN_LIB_DIR = path\r\n                break\r\n    # Specifying the library directly will overwrite the lib directory\r\n    library = os.getenv('CUDNN_LIBRARY')\r\n    if library is not None and os.path.exists(library):\r\n        CUDNN_LIBRARY = library\r\n        CUDNN_LIB_DIR = os.path.dirname(CUDNN_LIBRARY)\r\n\r\n    if not all([CUDNN_LIBRARY, CUDNN_LIB_DIR, CUDNN_INCLUDE_DIR]):\r\n        CUDNN_LIBRARY = CUDNN_LIB_DIR = CUDNN_INCLUDE_DIR = None\r\n    else:\r\n        real_cudnn_library = os.path.realpath(CUDNN_LIBRARY)\r\n        real_cudnn_lib_dir = os.path.realpath(CUDNN_LIB_DIR)\r\n        assert os.path.dirname(real_cudnn_library) == real_cudnn_lib_dir, (\r\n            'cudnn library and lib_dir must agree')\r\n        USE_CUDNN = True\n<issue_comment>username_1: Please remove `CUDNN_INCLUDE_DIR` and `CUDNN_LIBRARY` by using the following commands.\r\n```cmd\r\nset CUDNN_INCLUDE_DIR=\r\nset CUDNN_LIBRARY=\r\n```\n<issue_comment>username_0: It works! Thanks a lot!<issue_closed>","repo":"pytorch/pytorch","issue_id":391131022,"issue_number":15222}
{"question":"What can help identify the code path taken by the program when encountering a misconfiguration of the 'CUDNN_LIBRARY' flag?","answer":"Running 'tools/setup_helpers/cudnn.py'","id":135,"text":"The error is caused by the misconfiguration of the flag `CUDNN_LIBRARY`. Could you please run `tools/setup_helpers/cudnn.py` and see which code path it goes through?","context":"<issue_start><issue_comment>Title: Can't open x64.obj while installing pytorch with win10 \nusername_0: E:\\SOFTWARE\\pytorch\\build\\caffe2\\torch\\lib\\libshm_windows\\shm.vcxproj()\r\n\r\nE:\\SOFTWARE\\pytorch\\build\\ALL_BUILD.vcxproj() - \r\n\r\nE:\\SOFTWARE\\pytorch\\build\\INSTALL.vcxproj() - \r\n\r\n\r\n\r\n\r\nE:\\SOFTWARE\\pytorch\\build\\INSTALL.vcxproj() (1) ->\r\nE:\\SOFTWARE\\pytorch\\build\\ALL_BUILD.vcxproj() (3) ->\r\nE:\\SOFTWARE\\pytorch\\build\\caffe2\\AlgorithmsTest.vcxproj() (4) ->\r\nE:\\SOFTWARE\\pytorch\\build\\caffe2\\caffe2_gpu.vcxproj() (22) ->\r\n(Link ) ->\r\n  C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\lib\\x64.obj : fatal error LNK1136:  [E:\\SOFTWARE\\pyt\r\norch\\build\\caffe2\\caffe2_gpu.vcxproj]\r\n\r\n    0 \r\n    1 \r\n\r\n 00:00:16.23\r\n\r\nE:\\SOFTWARE\\pytorch\\build>IF ERRORLEVEL 1 exit 1\r\nFailed to run 'tools\\build_pytorch_libs.bat --use-cuda --use-fbgemm --use-nnpack --use-mkldnn --use-qnnpack caffe2'\r\n\r\npytorch versionstable1.0\r\nosWindows 10 Pro Version 1803\r\npython version:3.7\r\nCUDA:10\r\ncudnn:7.4.1\r\nVisual Studio version: Microsoft Visual Studio 2017 Community Edition\r\n\r\nCan anyone help me solve this problem? Thanks !\n<issue_comment>username_1: Please provide the full log.\n<issue_comment>username_0: the full log\r\n\r\nBuilding wheel torch-1.0.0a0+df61437\r\nrunning install\r\nsetup.py::run()\r\nrunning build_deps\r\nsetup.py::build_deps::run()\r\n\r\nE:\\SOFTWARE\\pytorch>cd \"E:\\SOFTWARE\\pytorch\\tools\\\\..\"\r\n\r\nE:\\SOFTWARE\\pytorch>set PATH=\\bin;E:\\SOFTWARE\\Anaconda\\DLLs;E:\\SOFTWARE\\Visual Studio\\VC\\Tools\\MSVC\\14.11.25503\\bin\\HostX64\\x64;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\VC\\VCPackages;C:\\Program Files (x86)\\Microsoft SDKs\\TypeScript\\3.1;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\TestWindow;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\TeamFoundation\\Team Explorer;E:\\SOFTWARE\\Visual Studio\\MSBuild\\15.0\\bin\\Roslyn;E:\\SOFTWARE\\Visual Studio\\Team Tools\\Performance Tools\\x64;E:\\SOFTWARE\\Visual Studio\\Team Tools\\Performance Tools;E:\\SOFTWARE\\Visual Studio\\share\\Common\\VSPerfCollectionTools\\\\x64;E:\\SOFTWARE\\Visual Studio\\share\\Common\\VSPerfCollectionTools\\;C:\\Program Files (x86)\\Microsoft SDKs\\Windows\\v10.0A\\bin\\NETFX 4.6.1 Tools\\x64\\;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\10.0.17763.0\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x64;E:\\SOFTWARE\\Visual Studio\\\\MSBuild\\15.0\\bin;C:\\WINDOWS\\Microsoft.NET\\Framework64\\v4.0.30319;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\;E:\\SOFTWARE\\Visual Studio\\Common7\\Tools\\;E:\\SOFTWARE\\Anaconda;E:\\SOFTWARE\\Anaconda\\Library\\mingw-w64\\bin;E:\\SOFTWARE\\Anaconda\\Library\\usr\\bin;E:\\SOFTWARE\\Anaconda\\Library\\bin;E:\\SOFTWARE\\Anaconda\\Scripts;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\bin;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\libnvvp;C:\\WINDOWS\\system32;C:\\WINDOWS;C:\\WINDOWS\\System32\\Wbem;C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0\\;C:\\WINDOWS\\System32\\OpenSSH\\;E:\\\\Git\\cmd;E:\\SOFTWARE\\cmake\\bin;C:\\Program Files (x86)\\Windows Kits\\8.1\\Windows Performance Toolkit\\;C:\\Program Files (x86)\\NVIDIA Corporation\\PhysX\\Common;C:\\Program Files\\dotnet\\;C:\\Program Files\\NVIDIA Corporation\\NVIDIA NvDLISR;C:\\Windows\\Microsoft.NET\\Framework\\v4.0.30319;C:\\Windows\\Microsoft.NET\\Framework64\\v4.0.30319;C:\\Users\\11838\\AppData\\Local\\Microsoft\\WindowsApps;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\CMake\\bin;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\Ninja\r\n\r\nE:\\SOFTWARE\\pytorch>set BASE_DIR=E:/SOFTWARE/pytorch\r\n\r\nE:\\SOFTWARE\\pytorch>set TORCH_LIB_DIR=E:/SOFTWARE/pytorch/torch/lib\r\n\r\nE:\\SOFTWARE\\pytorch>set THIRD_PARTY_DIR=E:/SOFTWARE/pytorch/third_party\r\n\r\nE:\\SOFTWARE\\pytorch>set BASIC_C_FLAGS=\r\n\r\nE:\\SOFTWARE\\pytorch>set BASIC_CUDA_FLAGS=\r\n\r\nE:\\SOFTWARE\\pytorch>IF NOT DEFINED INSTALL_DIR (set \"INSTALL_DIR=E:/SOFTWARE/pytorch/torch/lib/tmp_install\" )  ELSE (set \"INSTALL_DIR=\\=/\" )\r\n\r\nE:\\SOFTWARE\\pytorch>set LDFLAGS=/LIBPATH:E:/SOFTWARE/pytorch/torch/lib/tmp_install/lib\r\n\r\nE:\\SOFTWARE\\pytorch>set C_FLAGS= /D_WIN32 /Z7 /EHa /DNOMINMAX\r\n\r\nE:\\SOFTWARE\\pytorch>set LINK_FLAGS=/DEBUG:FULL\r\n\r\nE:\\SOFTWARE\\pytorch>if not exist torch\\lib\\tmp_install mkdir torch\\lib\\tmp_install\r\n\r\nE:\\SOFTWARE\\pytorch>set /a USE_CUDA=0\r\n\r\nE:\\SOFTWARE\\pytorch>set /a USE_FBGEMM=1\r\n\r\nE:\\SOFTWARE\\pytorch>set /a USE_ROCM=0\r\n\r\nE:\\SOFTWARE\\pytorch>set /a USE_NNPACK=0\r\n\r\nE:\\SOFTWARE\\pytorch>set /a USE_QNNPACK=0\r\n\r\nE:\\SOFTWARE\\pytorch>set /a USE_GLOO_IBVERBS=0\r\n\r\nE:\\SOFTWARE\\pytorch>set /a USE_MKLDNN=0\r\n\r\nE:\\SOFTWARE\\pytorch>set _BUILD_ARGS=\r\n\r\nE:\\SOFTWARE\\pytorch>if \"--use-cuda\" == \"\" (goto :process_args_exit )\r\n\r\nE:\\SOFTWARE\\pytorch>if \"--use-cuda\" == \"--use-cuda\" (\r\nset /a USE_CUDA=1\r\n goto :process_args_processed\r\n)\r\n\r\nE:\\SOFTWARE\\pytorch>shift\r\n\r\nE:\\SOFTWARE\\pytorch>goto :process_args\r\n\r\nE:\\SOFTWARE\\pytorch>if \"--use-fbgemm\" == \"\" (goto :process_args_exit )\r\n\r\nE:\\SOFTWARE\\pytorch>if \"--use-fbgemm\" == \"--use-cuda\" (\r\nset /a USE_CUDA=1\r\n goto :process_args_processed\r\n)\r\n\r\nE:\\SOFTWARE\\pytorch>if \"--use-fbgemm\" == \"--use-fbgemm\" (\r\nset /a USE_FBGEMM=1\r\n goto :process_args_processed\r\n)\r\n\r\nE:\\SOFTWARE\\pytorch>shift\r\n\r\nE:\\SOFTWARE\\pytorch>goto :process_args\r\n\r\nE:\\SOFTWARE\\pytorch>if \"--use-nnpack\" == \"\" (goto :process_args_exit )\r\n\r\nE:\\SOFTWARE\\pytorch>if \"--use-nnpack\" == \"--use-cuda\" (\r\nset /a USE_CUDA=1\r\n goto :process_args_processed\r\n[Truncated]\nDone Building Project \"E:\\SOFTWARE\\pytorch\\build\\INSTALL.vcxproj\" (default targets) -- FAILED.\r\n\r\n\r\nBuild FAILED.\r\n\r\n\"E:\\SOFTWARE\\pytorch\\build\\INSTALL.vcxproj\" (default target) (1) ->\r\n\"E:\\SOFTWARE\\pytorch\\build\\ALL_BUILD.vcxproj\" (default target) (3) ->\r\n\"E:\\SOFTWARE\\pytorch\\build\\caffe2\\AlgorithmsTest.vcxproj\" (default target) (4) ->\r\n\"E:\\SOFTWARE\\pytorch\\build\\caffe2\\caffe2_gpu.vcxproj\" (default target) (22) ->\r\n(Link target) ->\r\n  LINK : fatal error LNK1181: C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\lib\\x64.obj [E:\\SOFTWA\r\nRE\\pytorch\\build\\caffe2\\caffe2_gpu.vcxproj]\r\n\r\n    0 Warning(s)\r\n    1 Error(s)\r\n\r\nTime Elapsed 00:00:22.22\r\n\r\nE:\\SOFTWARE\\pytorch\\build>IF ERRORLEVEL 1 exit 1\r\nFailed to run 'tools\\build_pytorch_libs.bat --use-cuda --use-fbgemm --use-nnpack --use-mkldnn --use-qnnpack caffe2'\n<issue_comment>username_1: The error is caused by the misconfiguration of the flag `CUDNN_LIBRARY`. Could you please run `tools/setup_helpers/cudnn.py` and see which code path it goes through?\n<issue_comment>username_0: My environmental variables is :\r\nCUDA_PATH=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\r\nCUDA_PATH_V10_0=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\r\nCUDNN_INCLUDE_DIR=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\include\r\nCUDNN_LIBRARY=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\lib\\x64\r\nNVCUDASAMPLES10_0_ROOT=C:\\ProgramData\\NVIDIA Corporation\\CUDA Samples\\v10.0\r\nNVCUDASAMPLES_ROOT=C:\\ProgramData\\NVIDIA Corporation\\CUDA Samples\\v10.0\r\nNVTOOLSEXT_PATH=C:\\Program Files\\NVIDIA Corporation\\NvToolsExt\\\r\nPath=E:\\SOFTWARE\\Visual Studio\\VC\\Tools\\MSVC\\14.11.25503\\bin\\HostX64\\x64;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\VC\\VCPackages;C:\\Program Files (x86)\\Microsoft SDKs\\TypeScript\\3.1;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\TestWindow;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\TeamFoundation\\Team Explorer;E:\\SOFTWARE\\Visual Studio\\MSBuild\\15.0\\bin\\Roslyn;E:\\SOFTWARE\\Visual Studio\\Team Tools\\Performance Tools\\x64;E:\\SOFTWARE\\Visual Studio\\Team Tools\\Performance Tools;E:\\SOFTWARE\\Visual Studio\\share\\Common\\VSPerfCollectionTools\\\\x64;E:\\SOFTWARE\\Visual Studio\\share\\Common\\VSPerfCollectionTools\\;C:\\Program Files (x86)\\Microsoft SDKs\\Windows\\v10.0A\\bin\\NETFX 4.6.1 Tools\\x64\\;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\10.0.17763.0\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x64;E:\\SOFTWARE\\Visual Studio\\\\MSBuild\\15.0\\bin;C:\\WINDOWS\\Microsoft.NET\\Framework64\\v4.0.30319;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\;E:\\SOFTWARE\\Visual Studio\\Common7\\Tools\\;E:\\SOFTWARE\\Anaconda;E:\\SOFTWARE\\Anaconda\\Library\\mingw-w64\\bin;E:\\SOFTWARE\\Anaconda\\Library\\usr\\bin;E:\\SOFTWARE\\Anaconda\\Library\\bin;E:\\SOFTWARE\\Anaconda\\Scripts;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\bin;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\libnvvp;C:\\WINDOWS\\system32;C:\\WINDOWS;C:\\WINDOWS\\System32\\Wbem;C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0\\;C:\\WINDOWS\\System32\\OpenSSH\\;E:\\\\Git\\cmd;E:\\SOFTWARE\\cmake\\bin;C:\\Program Files (x86)\\Windows Kits\\8.1\\Windows Performance Toolkit\\;C:\\Program Files (x86)\\NVIDIA Corporation\\PhysX\\Common;C:\\Program Files\\dotnet\\;C:\\Program Files\\NVIDIA Corporation\\NVIDIA NvDLISR;C:\\Windows\\Microsoft.NET\\Framework\\v4.0.30319;C:\\Windows\\Microsoft.NET\\Framework64\\v4.0.30319;C:\\Users\\11838\\AppData\\Local\\Microsoft\\WindowsApps;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\CMake\\bin;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\Ninja\r\n\r\nAND the cudnn.py shows:\r\nimport os\r\nimport glob\r\n\r\nfrom .env import IS_WINDOWS, IS_CONDA, CONDA_DIR, check_negative_env_flag, gather_paths, lib_paths_from_base\r\nfrom .cuda import USE_CUDA, CUDA_HOME\r\n\r\n\r\nUSE_CUDNN = False\r\nCUDNN_LIB_DIR = None\r\nCUDNN_INCLUDE_DIR = None\r\nCUDNN_LIBRARY = None\r\nWITH_STATIC_CUDNN = os.getenv(\"USE_STATIC_CUDNN\")\r\n\r\nif USE_CUDA and not check_negative_env_flag('USE_CUDNN'):\r\n    lib_paths = list(filter(bool, [\r\n        os.getenv('CUDNN_LIB_DIR')\r\n    ] + lib_paths_from_base(CUDA_HOME) + [\r\n        '/usr/lib/x86_64-linux-gnu/',\r\n        '/usr/lib/powerpc64le-linux-gnu/',\r\n        '/usr/lib/aarch64-linux-gnu/',\r\n    ] + gather_paths([\r\n        'LIBRARY_PATH',\r\n    ]) + gather_paths([\r\n        'LD_LIBRARY_PATH',\r\n    ])))\r\n    include_paths = list(filter(bool, [\r\n        os.getenv('CUDNN_INCLUDE_DIR'),\r\n        os.path.join(CUDA_HOME, 'include'),\r\n        '/usr/include/',\r\n    ] + gather_paths([\r\n        'CPATH',\r\n        'C_INCLUDE_PATH',\r\n        'CPLUS_INCLUDE_PATH',\r\n    ])))\r\n    # Add CUDA related dirs to candidate list\r\n    if IS_CONDA:\r\n        lib_paths.append(os.path.join(CONDA_DIR, 'lib'))\r\n        include_paths.append(os.path.join(CONDA_DIR, 'include'))\r\n    for path in include_paths:\r\n        if path is None or not os.path.exists(path):\r\n            continue\r\n        include_file_path = os.path.join(path, 'cudnn.h')\r\n        CUDNN_INCLUDE_VERSION = None\r\n        if os.path.exists(include_file_path):\r\n            CUDNN_INCLUDE_DIR = path\r\n            with open(include_file_path) as f:\r\n                for line in f:\r\n                    if \"#define CUDNN_MAJOR\" in line:\r\n                        CUDNN_INCLUDE_VERSION = int(line.split()[-1])\r\n                        break\r\n            if CUDNN_INCLUDE_VERSION is None:\r\n                raise AssertionError(\"Could not find #define CUDNN_MAJOR in \" + include_file_path)\r\n            break\r\n\r\n    if CUDNN_INCLUDE_VERSION is None:\r\n        pass\r\n\r\n    # Check for standalone cuDNN libraries\r\n    if CUDNN_INCLUDE_DIR is not None:\r\n        cudnn_path = os.path.join(os.path.dirname(CUDNN_INCLUDE_DIR))\r\n        cudnn_lib_paths = lib_paths_from_base(cudnn_path)\r\n        lib_paths.extend(cudnn_lib_paths)\r\n\r\n    for path in lib_paths:\r\n        if path is None or not os.path.exists(path):\r\n            continue\r\n        if IS_WINDOWS:\r\n            library = os.path.join(path, 'cudnn.lib')\r\n            if os.path.exists(library):\r\n[Truncated]\n                search_name = 'libcudnn*' + str(CUDNN_INCLUDE_VERSION) + \"*\"\r\n            libraries = sorted(glob.glob(os.path.join(path, search_name)))\r\n            if libraries:\r\n                CUDNN_LIBRARY = libraries[0]\r\n                CUDNN_LIB_DIR = path\r\n                break\r\n    # Specifying the library directly will overwrite the lib directory\r\n    library = os.getenv('CUDNN_LIBRARY')\r\n    if library is not None and os.path.exists(library):\r\n        CUDNN_LIBRARY = library\r\n        CUDNN_LIB_DIR = os.path.dirname(CUDNN_LIBRARY)\r\n\r\n    if not all([CUDNN_LIBRARY, CUDNN_LIB_DIR, CUDNN_INCLUDE_DIR]):\r\n        CUDNN_LIBRARY = CUDNN_LIB_DIR = CUDNN_INCLUDE_DIR = None\r\n    else:\r\n        real_cudnn_library = os.path.realpath(CUDNN_LIBRARY)\r\n        real_cudnn_lib_dir = os.path.realpath(CUDNN_LIB_DIR)\r\n        assert os.path.dirname(real_cudnn_library) == real_cudnn_lib_dir, (\r\n            'cudnn library and lib_dir must agree')\r\n        USE_CUDNN = True\n<issue_comment>username_1: Please remove `CUDNN_INCLUDE_DIR` and `CUDNN_LIBRARY` by using the following commands.\r\n```cmd\r\nset CUDNN_INCLUDE_DIR=\r\nset CUDNN_LIBRARY=\r\n```\n<issue_comment>username_0: It works! Thanks a lot!<issue_closed>","repo":"pytorch/pytorch","issue_id":391131022,"issue_number":15222}
{"question":"What does the code snippet do to determine the paths for the cuDNN library and include directory?","answer":"The code snippet checks for the existence of environmental variables like CUDNN_LIB_DIR and CUDNN_INCLUDE_DIR, sets paths based on these variables, and looks for cuDNN headers and libraries in specified locations to determine the paths for the cuDNN library and include directory.","id":136,"text":"My environmental variables is :\r\nCUDA_PATH=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\r\nCUDA_PATH_V10_0=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\r\nCUDNN_INCLUDE_DIR=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\include\r\nCUDNN_LIBRARY=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\lib\\x64\r\nNVCUDASAMPLES10_0_ROOT=C:\\ProgramData\\NVIDIA Corporation\\CUDA Samples\\v10.0\r\nNVCUDASAMPLES_ROOT=C:\\ProgramData\\NVIDIA Corporation\\CUDA Samples\\v10.0\r\nNVTOOLSEXT_PATH=C:\\Program Files\\NVIDIA Corporation\\NvToolsExt\\\r\nPath=E:\\SOFTWARE\\Visual Studio\\VC\\Tools\\MSVC\\14.11.25503\\bin\\HostX64\\x64;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\VC\\VCPackages;C:\\Program Files (x86)\\Microsoft SDKs\\TypeScript\\3.1;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\TestWindow;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\TeamFoundation\\Team Explorer;E:\\SOFTWARE\\Visual Studio\\MSBuild\\15.0\\bin\\Roslyn;E:\\SOFTWARE\\Visual Studio\\Team Tools\\Performance Tools\\x64;E:\\SOFTWARE\\Visual Studio\\Team Tools\\Performance Tools;E:\\SOFTWARE\\Visual Studio\\share\\Common\\VSPerfCollectionTools\\\\x64;E:\\SOFTWARE\\Visual Studio\\share\\Common\\VSPerfCollectionTools\\;C:\\Program Files (x86)\\Microsoft SDKs\\Windows\\v10.0A\\bin\\NETFX 4.6.1 Tools\\x64\\;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\10.0.17763.0\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x64;E:\\SOFTWARE\\Visual Studio\\\\MSBuild\\15.0\\bin;C:\\WINDOWS\\Microsoft.NET\\Framework64\\v4.0.30319;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\;E:\\SOFTWARE\\Visual Studio\\Common7\\Tools\\;E:\\SOFTWARE\\Anaconda;E:\\SOFTWARE\\Anaconda\\Library\\mingw-w64\\bin;E:\\SOFTWARE\\Anaconda\\Library\\usr\\bin;E:\\SOFTWARE\\Anaconda\\Library\\bin;E:\\SOFTWARE\\Anaconda\\Scripts;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\bin;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\libnvvp;C:\\WINDOWS\\system32;C:\\WINDOWS;C:\\WINDOWS\\System32\\Wbem;C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0\\;C:\\WINDOWS\\System32\\OpenSSH\\;E:\\\\Git\\cmd;E:\\SOFTWARE\\cmake\\bin;C:\\Program Files (x86)\\Windows Kits\\8.1\\Windows Performance Toolkit\\;C:\\Program Files (x86)\\NVIDIA Corporation\\PhysX\\Common;C:\\Program Files\\dotnet\\;C:\\Program Files\\NVIDIA Corporation\\NVIDIA NvDLISR;C:\\Windows\\Microsoft.NET\\Framework\\v4.0.30319;C:\\Windows\\Microsoft.NET\\Framework64\\v4.0.30319;C:\\Users\\11838\\AppData\\Local\\Microsoft\\WindowsApps;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\CMake\\bin;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\Ninja\r\n\r\nAND the cudnn.py shows:\r\nimport os\r\nimport glob\r\n\r\nfrom .env import IS_WINDOWS, IS_CONDA, CONDA_DIR, check_negative_env_flag, gather_paths, lib_paths_from_base\r\nfrom .cuda import USE_CUDA, CUDA_HOME\r\n\r\n\r\nUSE_CUDNN = False\r\nCUDNN_LIB_DIR = None\r\nCUDNN_INCLUDE_DIR = None\r\nCUDNN_LIBRARY = None\r\nWITH_STATIC_CUDNN = os.getenv(\"USE_STATIC_CUDNN\")\r\n\r\nif USE_CUDA and not check_negative_env_flag('USE_CUDNN'):\r\n    lib_paths = list(filter(bool, [\r\n        os.getenv('CUDNN_LIB_DIR')\r\n    ] + lib_paths_from_base(CUDA_HOME) + [\r\n        '/usr/lib/x86_64-linux-gnu/',\r\n        '/usr/lib/powerpc64le-linux-gnu/',\r\n        '/usr/lib/aarch64-linux-gnu/',\r\n    ] + gather_paths([\r\n        'LIBRARY_PATH',\r\n    ]) + gather_paths([\r\n        'LD_LIBRARY_PATH',\r\n    ])))\r\n    include_paths = list(filter(bool, [\r\n        os.getenv('CUDNN_INCLUDE_DIR'),\r\n        os.path.join(CUDA_HOME, 'include'),\r\n        '/usr/include/',\r\n    ] + gather_paths([\r\n        'CPATH',\r\n        'C_INCLUDE_PATH',\r\n        'CPLUS_INCLUDE_PATH',\r\n    ])))\r\n    # Add CUDA related dirs to candidate list\r\n    if IS_CONDA:\r\n        lib_paths.append(os.path.join(CONDA_DIR, 'lib'))\r\n        include_paths.append(os.path.join(CONDA_DIR, 'include'))\r\n    for path in include_paths:\r\n        if path is None or not os.path.exists(path):\r\n            continue\r\n        include_file_path = os.path.join(path, 'cudnn.h')\r\n        CUDNN_INCLUDE_VERSION = None\r\n        if os.path.exists(include_file_path):\r\n            CUDNN_INCLUDE_DIR = path\r\n            with open(include_file_path) as f:\r\n                for line in f:\r\n                    if \"#define CUDNN_MAJOR\" in line:\r\n                        CUDNN_INCLUDE_VERSION = int(line.split()[-1])\r\n                        break\r\n            if CUDNN_INCLUDE_VERSION is None:\r\n                raise AssertionError(\"Could not find #define CUDNN_MAJOR in \" + include_file_path)\r\n            break\r\n\r\n    if CUDNN_INCLUDE_VERSION is None:\r\n        pass\r\n\r\n    # Check for standalone cuDNN libraries\r\n    if CUDNN_INCLUDE_DIR is not None:\r\n        cudnn_path = os.path.join(os.path.dirname(CUDNN_INCLUDE_DIR))\r\n        cudnn_lib_paths = lib_paths_from_base(cudnn_path)\r\n        lib_paths.extend(cudnn_lib_paths)\r\n\r\n    for path in lib_paths:\r\n        if path is None or not os.path.exists(path):\r\n            continue\r\n        if IS_WINDOWS:\r\n            library = os.path.join(path, 'cudnn.lib')\r\n            if os.path.exists(library):\r\n[Truncated]\n                search_name = 'libcudnn*' + str(CUDNN_INCLUDE_VERSION) + \"*\"\r\n            libraries = sorted(glob.glob(os.path.join(path, search_name)))\r\n            if libraries:\r\n                CUDNN_LIBRARY = libraries[0]\r\n                CUDNN_LIB_DIR = path\r\n                break\r\n    # Specifying the library directly will overwrite the lib directory\r\n    library = os.getenv('CUDNN_LIBRARY')\r\n    if library is not None and os.path.exists(library):\r\n        CUDNN_LIBRARY = library\r\n        CUDNN_LIB_DIR = os.path.dirname(CUDNN_LIBRARY)\r\n\r\n    if not all([CUDNN_LIBRARY, CUDNN_LIB_DIR, CUDNN_INCLUDE_DIR]):\r\n        CUDNN_LIBRARY = CUDNN_LIB_DIR = CUDNN_INCLUDE_DIR = None\r\n    else:\r\n        real_cudnn_library = os.path.realpath(CUDNN_LIBRARY)\r\n        real_cudnn_lib_dir = os.path.realpath(CUDNN_LIB_DIR)\r\n        assert os.path.dirname(real_cudnn_library) == real_cudnn_lib_dir, (\r\n            'cudnn library and lib_dir must agree')\r\n        USE_CUDNN = True","context":"<issue_start><issue_comment>Title: Can't open x64.obj while installing pytorch with win10 \nusername_0: E:\\SOFTWARE\\pytorch\\build\\caffe2\\torch\\lib\\libshm_windows\\shm.vcxproj()\r\n\r\nE:\\SOFTWARE\\pytorch\\build\\ALL_BUILD.vcxproj() - \r\n\r\nE:\\SOFTWARE\\pytorch\\build\\INSTALL.vcxproj() - \r\n\r\n\r\n\r\n\r\nE:\\SOFTWARE\\pytorch\\build\\INSTALL.vcxproj() (1) ->\r\nE:\\SOFTWARE\\pytorch\\build\\ALL_BUILD.vcxproj() (3) ->\r\nE:\\SOFTWARE\\pytorch\\build\\caffe2\\AlgorithmsTest.vcxproj() (4) ->\r\nE:\\SOFTWARE\\pytorch\\build\\caffe2\\caffe2_gpu.vcxproj() (22) ->\r\n(Link ) ->\r\n  C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\lib\\x64.obj : fatal error LNK1136:  [E:\\SOFTWARE\\pyt\r\norch\\build\\caffe2\\caffe2_gpu.vcxproj]\r\n\r\n    0 \r\n    1 \r\n\r\n 00:00:16.23\r\n\r\nE:\\SOFTWARE\\pytorch\\build>IF ERRORLEVEL 1 exit 1\r\nFailed to run 'tools\\build_pytorch_libs.bat --use-cuda --use-fbgemm --use-nnpack --use-mkldnn --use-qnnpack caffe2'\r\n\r\npytorch versionstable1.0\r\nosWindows 10 Pro Version 1803\r\npython version:3.7\r\nCUDA:10\r\ncudnn:7.4.1\r\nVisual Studio version: Microsoft Visual Studio 2017 Community Edition\r\n\r\nCan anyone help me solve this problem? Thanks !\n<issue_comment>username_1: Please provide the full log.\n<issue_comment>username_0: the full log\r\n\r\nBuilding wheel torch-1.0.0a0+df61437\r\nrunning install\r\nsetup.py::run()\r\nrunning build_deps\r\nsetup.py::build_deps::run()\r\n\r\nE:\\SOFTWARE\\pytorch>cd \"E:\\SOFTWARE\\pytorch\\tools\\\\..\"\r\n\r\nE:\\SOFTWARE\\pytorch>set PATH=\\bin;E:\\SOFTWARE\\Anaconda\\DLLs;E:\\SOFTWARE\\Visual Studio\\VC\\Tools\\MSVC\\14.11.25503\\bin\\HostX64\\x64;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\VC\\VCPackages;C:\\Program Files (x86)\\Microsoft SDKs\\TypeScript\\3.1;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\TestWindow;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\TeamFoundation\\Team Explorer;E:\\SOFTWARE\\Visual Studio\\MSBuild\\15.0\\bin\\Roslyn;E:\\SOFTWARE\\Visual Studio\\Team Tools\\Performance Tools\\x64;E:\\SOFTWARE\\Visual Studio\\Team Tools\\Performance Tools;E:\\SOFTWARE\\Visual Studio\\share\\Common\\VSPerfCollectionTools\\\\x64;E:\\SOFTWARE\\Visual Studio\\share\\Common\\VSPerfCollectionTools\\;C:\\Program Files (x86)\\Microsoft SDKs\\Windows\\v10.0A\\bin\\NETFX 4.6.1 Tools\\x64\\;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\10.0.17763.0\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x64;E:\\SOFTWARE\\Visual Studio\\\\MSBuild\\15.0\\bin;C:\\WINDOWS\\Microsoft.NET\\Framework64\\v4.0.30319;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\;E:\\SOFTWARE\\Visual Studio\\Common7\\Tools\\;E:\\SOFTWARE\\Anaconda;E:\\SOFTWARE\\Anaconda\\Library\\mingw-w64\\bin;E:\\SOFTWARE\\Anaconda\\Library\\usr\\bin;E:\\SOFTWARE\\Anaconda\\Library\\bin;E:\\SOFTWARE\\Anaconda\\Scripts;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\bin;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\libnvvp;C:\\WINDOWS\\system32;C:\\WINDOWS;C:\\WINDOWS\\System32\\Wbem;C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0\\;C:\\WINDOWS\\System32\\OpenSSH\\;E:\\\\Git\\cmd;E:\\SOFTWARE\\cmake\\bin;C:\\Program Files (x86)\\Windows Kits\\8.1\\Windows Performance Toolkit\\;C:\\Program Files (x86)\\NVIDIA Corporation\\PhysX\\Common;C:\\Program Files\\dotnet\\;C:\\Program Files\\NVIDIA Corporation\\NVIDIA NvDLISR;C:\\Windows\\Microsoft.NET\\Framework\\v4.0.30319;C:\\Windows\\Microsoft.NET\\Framework64\\v4.0.30319;C:\\Users\\11838\\AppData\\Local\\Microsoft\\WindowsApps;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\CMake\\bin;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\Ninja\r\n\r\nE:\\SOFTWARE\\pytorch>set BASE_DIR=E:/SOFTWARE/pytorch\r\n\r\nE:\\SOFTWARE\\pytorch>set TORCH_LIB_DIR=E:/SOFTWARE/pytorch/torch/lib\r\n\r\nE:\\SOFTWARE\\pytorch>set THIRD_PARTY_DIR=E:/SOFTWARE/pytorch/third_party\r\n\r\nE:\\SOFTWARE\\pytorch>set BASIC_C_FLAGS=\r\n\r\nE:\\SOFTWARE\\pytorch>set BASIC_CUDA_FLAGS=\r\n\r\nE:\\SOFTWARE\\pytorch>IF NOT DEFINED INSTALL_DIR (set \"INSTALL_DIR=E:/SOFTWARE/pytorch/torch/lib/tmp_install\" )  ELSE (set \"INSTALL_DIR=\\=/\" )\r\n\r\nE:\\SOFTWARE\\pytorch>set LDFLAGS=/LIBPATH:E:/SOFTWARE/pytorch/torch/lib/tmp_install/lib\r\n\r\nE:\\SOFTWARE\\pytorch>set C_FLAGS= /D_WIN32 /Z7 /EHa /DNOMINMAX\r\n\r\nE:\\SOFTWARE\\pytorch>set LINK_FLAGS=/DEBUG:FULL\r\n\r\nE:\\SOFTWARE\\pytorch>if not exist torch\\lib\\tmp_install mkdir torch\\lib\\tmp_install\r\n\r\nE:\\SOFTWARE\\pytorch>set /a USE_CUDA=0\r\n\r\nE:\\SOFTWARE\\pytorch>set /a USE_FBGEMM=1\r\n\r\nE:\\SOFTWARE\\pytorch>set /a USE_ROCM=0\r\n\r\nE:\\SOFTWARE\\pytorch>set /a USE_NNPACK=0\r\n\r\nE:\\SOFTWARE\\pytorch>set /a USE_QNNPACK=0\r\n\r\nE:\\SOFTWARE\\pytorch>set /a USE_GLOO_IBVERBS=0\r\n\r\nE:\\SOFTWARE\\pytorch>set /a USE_MKLDNN=0\r\n\r\nE:\\SOFTWARE\\pytorch>set _BUILD_ARGS=\r\n\r\nE:\\SOFTWARE\\pytorch>if \"--use-cuda\" == \"\" (goto :process_args_exit )\r\n\r\nE:\\SOFTWARE\\pytorch>if \"--use-cuda\" == \"--use-cuda\" (\r\nset /a USE_CUDA=1\r\n goto :process_args_processed\r\n)\r\n\r\nE:\\SOFTWARE\\pytorch>shift\r\n\r\nE:\\SOFTWARE\\pytorch>goto :process_args\r\n\r\nE:\\SOFTWARE\\pytorch>if \"--use-fbgemm\" == \"\" (goto :process_args_exit )\r\n\r\nE:\\SOFTWARE\\pytorch>if \"--use-fbgemm\" == \"--use-cuda\" (\r\nset /a USE_CUDA=1\r\n goto :process_args_processed\r\n)\r\n\r\nE:\\SOFTWARE\\pytorch>if \"--use-fbgemm\" == \"--use-fbgemm\" (\r\nset /a USE_FBGEMM=1\r\n goto :process_args_processed\r\n)\r\n\r\nE:\\SOFTWARE\\pytorch>shift\r\n\r\nE:\\SOFTWARE\\pytorch>goto :process_args\r\n\r\nE:\\SOFTWARE\\pytorch>if \"--use-nnpack\" == \"\" (goto :process_args_exit )\r\n\r\nE:\\SOFTWARE\\pytorch>if \"--use-nnpack\" == \"--use-cuda\" (\r\nset /a USE_CUDA=1\r\n goto :process_args_processed\r\n[Truncated]\nDone Building Project \"E:\\SOFTWARE\\pytorch\\build\\INSTALL.vcxproj\" (default targets) -- FAILED.\r\n\r\n\r\nBuild FAILED.\r\n\r\n\"E:\\SOFTWARE\\pytorch\\build\\INSTALL.vcxproj\" (default target) (1) ->\r\n\"E:\\SOFTWARE\\pytorch\\build\\ALL_BUILD.vcxproj\" (default target) (3) ->\r\n\"E:\\SOFTWARE\\pytorch\\build\\caffe2\\AlgorithmsTest.vcxproj\" (default target) (4) ->\r\n\"E:\\SOFTWARE\\pytorch\\build\\caffe2\\caffe2_gpu.vcxproj\" (default target) (22) ->\r\n(Link target) ->\r\n  LINK : fatal error LNK1181: C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\lib\\x64.obj [E:\\SOFTWA\r\nRE\\pytorch\\build\\caffe2\\caffe2_gpu.vcxproj]\r\n\r\n    0 Warning(s)\r\n    1 Error(s)\r\n\r\nTime Elapsed 00:00:22.22\r\n\r\nE:\\SOFTWARE\\pytorch\\build>IF ERRORLEVEL 1 exit 1\r\nFailed to run 'tools\\build_pytorch_libs.bat --use-cuda --use-fbgemm --use-nnpack --use-mkldnn --use-qnnpack caffe2'\n<issue_comment>username_1: The error is caused by the misconfiguration of the flag `CUDNN_LIBRARY`. Could you please run `tools/setup_helpers/cudnn.py` and see which code path it goes through?\n<issue_comment>username_0: My environmental variables is :\r\nCUDA_PATH=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\r\nCUDA_PATH_V10_0=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\r\nCUDNN_INCLUDE_DIR=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\include\r\nCUDNN_LIBRARY=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\lib\\x64\r\nNVCUDASAMPLES10_0_ROOT=C:\\ProgramData\\NVIDIA Corporation\\CUDA Samples\\v10.0\r\nNVCUDASAMPLES_ROOT=C:\\ProgramData\\NVIDIA Corporation\\CUDA Samples\\v10.0\r\nNVTOOLSEXT_PATH=C:\\Program Files\\NVIDIA Corporation\\NvToolsExt\\\r\nPath=E:\\SOFTWARE\\Visual Studio\\VC\\Tools\\MSVC\\14.11.25503\\bin\\HostX64\\x64;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\VC\\VCPackages;C:\\Program Files (x86)\\Microsoft SDKs\\TypeScript\\3.1;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\TestWindow;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\TeamFoundation\\Team Explorer;E:\\SOFTWARE\\Visual Studio\\MSBuild\\15.0\\bin\\Roslyn;E:\\SOFTWARE\\Visual Studio\\Team Tools\\Performance Tools\\x64;E:\\SOFTWARE\\Visual Studio\\Team Tools\\Performance Tools;E:\\SOFTWARE\\Visual Studio\\share\\Common\\VSPerfCollectionTools\\\\x64;E:\\SOFTWARE\\Visual Studio\\share\\Common\\VSPerfCollectionTools\\;C:\\Program Files (x86)\\Microsoft SDKs\\Windows\\v10.0A\\bin\\NETFX 4.6.1 Tools\\x64\\;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\10.0.17763.0\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x64;E:\\SOFTWARE\\Visual Studio\\\\MSBuild\\15.0\\bin;C:\\WINDOWS\\Microsoft.NET\\Framework64\\v4.0.30319;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\;E:\\SOFTWARE\\Visual Studio\\Common7\\Tools\\;E:\\SOFTWARE\\Anaconda;E:\\SOFTWARE\\Anaconda\\Library\\mingw-w64\\bin;E:\\SOFTWARE\\Anaconda\\Library\\usr\\bin;E:\\SOFTWARE\\Anaconda\\Library\\bin;E:\\SOFTWARE\\Anaconda\\Scripts;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\bin;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\libnvvp;C:\\WINDOWS\\system32;C:\\WINDOWS;C:\\WINDOWS\\System32\\Wbem;C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0\\;C:\\WINDOWS\\System32\\OpenSSH\\;E:\\\\Git\\cmd;E:\\SOFTWARE\\cmake\\bin;C:\\Program Files (x86)\\Windows Kits\\8.1\\Windows Performance Toolkit\\;C:\\Program Files (x86)\\NVIDIA Corporation\\PhysX\\Common;C:\\Program Files\\dotnet\\;C:\\Program Files\\NVIDIA Corporation\\NVIDIA NvDLISR;C:\\Windows\\Microsoft.NET\\Framework\\v4.0.30319;C:\\Windows\\Microsoft.NET\\Framework64\\v4.0.30319;C:\\Users\\11838\\AppData\\Local\\Microsoft\\WindowsApps;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\CMake\\bin;E:\\SOFTWARE\\Visual Studio\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\Ninja\r\n\r\nAND the cudnn.py shows:\r\nimport os\r\nimport glob\r\n\r\nfrom .env import IS_WINDOWS, IS_CONDA, CONDA_DIR, check_negative_env_flag, gather_paths, lib_paths_from_base\r\nfrom .cuda import USE_CUDA, CUDA_HOME\r\n\r\n\r\nUSE_CUDNN = False\r\nCUDNN_LIB_DIR = None\r\nCUDNN_INCLUDE_DIR = None\r\nCUDNN_LIBRARY = None\r\nWITH_STATIC_CUDNN = os.getenv(\"USE_STATIC_CUDNN\")\r\n\r\nif USE_CUDA and not check_negative_env_flag('USE_CUDNN'):\r\n    lib_paths = list(filter(bool, [\r\n        os.getenv('CUDNN_LIB_DIR')\r\n    ] + lib_paths_from_base(CUDA_HOME) + [\r\n        '/usr/lib/x86_64-linux-gnu/',\r\n        '/usr/lib/powerpc64le-linux-gnu/',\r\n        '/usr/lib/aarch64-linux-gnu/',\r\n    ] + gather_paths([\r\n        'LIBRARY_PATH',\r\n    ]) + gather_paths([\r\n        'LD_LIBRARY_PATH',\r\n    ])))\r\n    include_paths = list(filter(bool, [\r\n        os.getenv('CUDNN_INCLUDE_DIR'),\r\n        os.path.join(CUDA_HOME, 'include'),\r\n        '/usr/include/',\r\n    ] + gather_paths([\r\n        'CPATH',\r\n        'C_INCLUDE_PATH',\r\n        'CPLUS_INCLUDE_PATH',\r\n    ])))\r\n    # Add CUDA related dirs to candidate list\r\n    if IS_CONDA:\r\n        lib_paths.append(os.path.join(CONDA_DIR, 'lib'))\r\n        include_paths.append(os.path.join(CONDA_DIR, 'include'))\r\n    for path in include_paths:\r\n        if path is None or not os.path.exists(path):\r\n            continue\r\n        include_file_path = os.path.join(path, 'cudnn.h')\r\n        CUDNN_INCLUDE_VERSION = None\r\n        if os.path.exists(include_file_path):\r\n            CUDNN_INCLUDE_DIR = path\r\n            with open(include_file_path) as f:\r\n                for line in f:\r\n                    if \"#define CUDNN_MAJOR\" in line:\r\n                        CUDNN_INCLUDE_VERSION = int(line.split()[-1])\r\n                        break\r\n            if CUDNN_INCLUDE_VERSION is None:\r\n                raise AssertionError(\"Could not find #define CUDNN_MAJOR in \" + include_file_path)\r\n            break\r\n\r\n    if CUDNN_INCLUDE_VERSION is None:\r\n        pass\r\n\r\n    # Check for standalone cuDNN libraries\r\n    if CUDNN_INCLUDE_DIR is not None:\r\n        cudnn_path = os.path.join(os.path.dirname(CUDNN_INCLUDE_DIR))\r\n        cudnn_lib_paths = lib_paths_from_base(cudnn_path)\r\n        lib_paths.extend(cudnn_lib_paths)\r\n\r\n    for path in lib_paths:\r\n        if path is None or not os.path.exists(path):\r\n            continue\r\n        if IS_WINDOWS:\r\n            library = os.path.join(path, 'cudnn.lib')\r\n            if os.path.exists(library):\r\n[Truncated]\n                search_name = 'libcudnn*' + str(CUDNN_INCLUDE_VERSION) + \"*\"\r\n            libraries = sorted(glob.glob(os.path.join(path, search_name)))\r\n            if libraries:\r\n                CUDNN_LIBRARY = libraries[0]\r\n                CUDNN_LIB_DIR = path\r\n                break\r\n    # Specifying the library directly will overwrite the lib directory\r\n    library = os.getenv('CUDNN_LIBRARY')\r\n    if library is not None and os.path.exists(library):\r\n        CUDNN_LIBRARY = library\r\n        CUDNN_LIB_DIR = os.path.dirname(CUDNN_LIBRARY)\r\n\r\n    if not all([CUDNN_LIBRARY, CUDNN_LIB_DIR, CUDNN_INCLUDE_DIR]):\r\n        CUDNN_LIBRARY = CUDNN_LIB_DIR = CUDNN_INCLUDE_DIR = None\r\n    else:\r\n        real_cudnn_library = os.path.realpath(CUDNN_LIBRARY)\r\n        real_cudnn_lib_dir = os.path.realpath(CUDNN_LIB_DIR)\r\n        assert os.path.dirname(real_cudnn_library) == real_cudnn_lib_dir, (\r\n            'cudnn library and lib_dir must agree')\r\n        USE_CUDNN = True\n<issue_comment>username_1: Please remove `CUDNN_INCLUDE_DIR` and `CUDNN_LIBRARY` by using the following commands.\r\n```cmd\r\nset CUDNN_INCLUDE_DIR=\r\nset CUDNN_LIBRARY=\r\n```\n<issue_comment>username_0: It works! Thanks a lot!<issue_closed>","repo":"pytorch/pytorch","issue_id":391131022,"issue_number":15222}
{"question":"What error message was encountered with the CUDA resources, and how many errors were reported compared to before?","answer":"The error message encountered with the CUDA resources was 'RuntimeError: CUDA error: too many resources requested for launch.' There were 4 errors reported, which is a decrease from the previous count of 8 errors.","id":140,"text":"Fewer errors than before, 4 instead of 8 with the same message:\r\n```\r\nRuntimeError: CUDA error: too many resources requested for launch\r\n```\r\nThis seems to be an existing problem: gh-8103.\r\n\r\n@username_2 could you have a look at this and tell us what you think?","context":"<issue_start><issue_comment>Title: [WIP] UpSample GPU Porting \nusername_0: resolves #16158\n<issue_comment>username_1: Fewer errors than before, 4 instead of 8 with the same message:\r\n```\r\nRuntimeError: CUDA error: too many resources requested for launch\r\n```\r\nThis seems to be an existing problem: gh-8103.\r\n\r\n@username_2 could you have a look at this and tell us what you think?\n<issue_comment>username_2: Looking at it very briefly, the `Jetson TX` referenced in the issue only has 256 cores.\r\n\r\nSo while the issue looks the same, I'd probably not expect it on the CI platforms.  FWIW, some recent CI tests in other issues are green.\n<issue_comment>username_2: @pytorchbot retest this please.\n<issue_comment>username_1: same failures.\n<issue_comment>username_2: Is this rebased on the latest master (you can also ask the bot to rebase)?\n<issue_comment>username_0: I rebased that yesterday ... also needed to resolve conflicts because 7 days ago upsample files were changed.\n<issue_comment>username_0: @username_2 do you have any idea about what could be this problem? or a way to debug that?\r\nalso it seems some jobs is taking a lot of time to build, for example, for some job the estimate time is 19h .. not sure if it the regular estimate ..\n<issue_comment>username_2: @username_0 If you can't reproduce it at home it seems hard to debug other than reading at the diffs again.  I can take a look on Monday, it's getting a bit late here.\n<issue_comment>username_2: Also, has anyone found a way to show the actual hardware used on the CI in detail?\n<issue_comment>username_0: @username_2 I am working in parallel on a paperspace environment .. it tooks a lot of time it is running with 8 cpu cores.\n<issue_comment>username_2: But have you ever been able to reproduce this issue on paperspace?\n<issue_comment>username_1: I can reproduce it locally: Arch Linux, CUDA 10.0, RTX2070 GPU. Given the CI failures, I think it should be reproducible for multiple CUDA and GPU versions. I just haven't worked on any CUDA code before, and am short on time, so I'd rather not dig too deep.\n<issue_comment>username_0: not yet .. my last building was with a my previous commit ... I will work again in this task in some minutes :)\n<issue_comment>username_1: @username_0 your previous commit had the same failure though (except for the less clear exception message), and it seems quite reproducible. So I think you'll see it now.\n<issue_comment>username_2: Ah, thanks.  @username_0, then I guess just compiling with \"DEBUG=1\" and stepping through the offending code with gdb may be the fastest way.\n<issue_comment>username_0: @username_2 thanks! I will try that!\n<issue_comment>username_0: it seems just UpSampleBicubic2d is using upsample_get_value_bounded (https://github.com/pytorch/pytorch/pull/19630/files#diff-5092da792c30694ee4adf0d0ae2a37c6R171) and upsample_increment_value_bounded (https://github.com/pytorch/pytorch/pull/19630/files#diff-5092da792c30694ee4adf0d0ae2a37c6R191)\r\n\r\nmaybe the problem could be inside one of these functions ... maybe related to the order of indexes (x, y) ...\r\n\r\nbut the problem seems to be related to cuda block/threads ... so not sure if it is really related to these functions.\n<issue_comment>username_2: The new code uses far more registers than the existing one. I verified that the both versions actually call the offending test case with `1024` in `blockDim`.  So it's very likely a register issue.\r\n\r\n`Existing` uses `64` registers, which seems to be optimal or my card:\r\n\r\n```\r\nptxas info    : 77696 bytes gmem, 72 bytes cmem[3]\r\nptxas info    : Compiling entry function '_Z23bicubic_interp2d_kerneliddb15THCDeviceTensorIdLi4Ei16DefaultPtrTraitsES1_' for 'sm_61'\r\nptxas info    : Function properties for _Z23bicubic_interp2d_kerneliddb15THCDeviceTensorIdLi4Ei16DefaultPtrTraitsES1_\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 64 registers, 432 bytes cmem[0], 24 bytes cmem[2]\r\n```\r\n\r\n`New` uses `124` registers, which is too much for my card:\r\n\r\n```\r\nptxas info    : 77696 bytes gmem, 72 bytes cmem[3]\r\nptxas info    : Compiling entry function '_ZN2at6native76_GLOBAL__N__52_tmpxft_0000626d_00000000_6_UpSampleBicubic2d_cpp1_ii_b4c1e1f328upsample_bicubic2d_out_frameElddbNS_20PackedTensorAccessorIdLm4ENS_16DefaultPtrTraitsElEES4_' for 'sm_61'\r\nptxas info    : Function properties for _ZN2at6native76_GLOBAL__N__52_tmpxft_0000626d_00000000_6_UpSampleBicubic2d_cpp1_ii_b4c1e1f328upsample_bicubic2d_out_frameElddbNS_20PackedTensorAccessorIdLm4ENS_16DefaultPtrTraitsElEES4_\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 124 registers, 496 bytes cmem[0], 24 bytes cmem[2]\r\n```\r\n\r\nWith `__launch_bounds__(1024)`, the code again uses `64` registers.\r\n\r\nIf you use `C10_LAUNCH_BOUNDS_1(1024)` **for both kernels**, the tests pass here.\r\n\r\n\r\nNow *why* is the regcount higher in the new code? It could be `PackedTensorAccessor`, it could be the fact that many instances of `int` have been changed to `int64_t`. :)\r\n\r\nYou could experiment or just use the launch bounds.  Other code in `native/cuda` seems to use lower bounds for `blockDim`, too.  1024 seems to be an outlier.\n<issue_comment>username_0: @username_2 \r\n\r\nit seems it worked locally! thank you so much!  I really appreciate that!\n<issue_comment>username_0: thanks @username_1 and @username_2 for all the help!\r\n\r\n@username_3 it is done for review!\n<issue_comment>username_3: CircleCI runs on AWS, and the I'm fairly sure we've got one of the g3 types, either g3.4xlarge or g3.8xlarge\n<issue_comment>username_3: cc @ngimel, if you want to take a look at this (I'm planning to do a review too)\n<issue_comment>username_0: @username_4 could it be addressed in another PR?\n<issue_comment>username_4: @username_0 Sure, it's definitely better to rename both cpu & gpu impls together in a separate PR. Just want to make sure we have the correct one after porting.\n<issue_comment>username_3: Just as a clarification: I'm OK with having changes which are improvements over the old kernels come in latter patches.\n<issue_comment>username_3: I finished reviewing one of the kernels, and did a cursory scan of the others. In general the patch looks like it's in good shape. I'd suggest that we fix up the major correctness issues (PackedTensorAccessor ref, and int32 versus int64 indexing) and anything small that is easy to fix (for example, double zeroing the array), and leave other improvements for follow ups. Should be ready to land soon!\n<issue_comment>username_0: thanks so much @username_3!\r\nI will let you know when it is ready again for a new review! thanks!\n<issue_comment>username_0: @username_2 @username_3 \r\nnot sure but the errors on CI seems to be related to jenkins ... it seems all these jobs that failed (for building) ran by 01h 01min ... not sure if it is a coincidence ...\n<issue_comment>username_1: @username_0 indeed it looks like all jobs were aborted at the same time. no obvious issues in the build log related to your code. Comparing e.g. the `cuda9-cudnn7` build with a successful one from another PR, it takes 1hr 14min there and your build is about at the place where the other one is after an hour.\r\n\r\nI suggest to just push a new commit to rebuild. Probably a temporary CI hiccup.\n<issue_comment>username_3: I accidentally rebooted Jenkins yesterday which is the likely cause, my apologies.\r\n\r\n@pytorchbot retest this please\n<issue_comment>username_0: @username_3 @username_2 @username_1 \r\n\r\nall tests passed except `pr/caffe2-py2-cuda9.0-cudnn7-windows-build`:\r\n \r\n```\r\n14:43:49 Build timed out (after 180 minutes). Marking the build as failed.\r\n14:43:49 Build was aborted\r\n14:43:49 [BFA] Scanning build for known causes...\r\n14:43:49 [BFA] No failure causes found\r\n14:43:49 [BFA] Done. 0s\r\n14:43:49 Finished: FAILURE\r\n```\r\n\r\nnot sure if this timeout means that the code now is slower.\r\n\r\nwhat do you think? do you have any suggestion?\n<issue_comment>username_3: Sometimes the Windows build flakes out like that. It didn't timeout while running a relevant test, so I judge it to be not your problem.\n<issue_comment>username_3: The launch bounds logic is wrong, but I acknowledge that this is a big patch already; just fix it in a follow up. I am going to go ahead and land this.\n<issue_comment>username_0: sounds good @username_3 thanks!","repo":"pytorch/pytorch","issue_id":436372857,"issue_number":19630}
{"question":"How many cores does the Jetson TX referenced in the issue have?","answer":"256 cores","id":141,"text":"Looking at it very briefly, the `Jetson TX` referenced in the issue only has 256 cores.\r\n\r\nSo while the issue looks the same, I'd probably not expect it on the CI platforms.  FWIW, some recent CI tests in other issues are green.","context":"<issue_start><issue_comment>Title: [WIP] UpSample GPU Porting \nusername_0: resolves #16158\n<issue_comment>username_1: Fewer errors than before, 4 instead of 8 with the same message:\r\n```\r\nRuntimeError: CUDA error: too many resources requested for launch\r\n```\r\nThis seems to be an existing problem: gh-8103.\r\n\r\n@username_2 could you have a look at this and tell us what you think?\n<issue_comment>username_2: Looking at it very briefly, the `Jetson TX` referenced in the issue only has 256 cores.\r\n\r\nSo while the issue looks the same, I'd probably not expect it on the CI platforms.  FWIW, some recent CI tests in other issues are green.\n<issue_comment>username_2: @pytorchbot retest this please.\n<issue_comment>username_1: same failures.\n<issue_comment>username_2: Is this rebased on the latest master (you can also ask the bot to rebase)?\n<issue_comment>username_0: I rebased that yesterday ... also needed to resolve conflicts because 7 days ago upsample files were changed.\n<issue_comment>username_0: @username_2 do you have any idea about what could be this problem? or a way to debug that?\r\nalso it seems some jobs is taking a lot of time to build, for example, for some job the estimate time is 19h .. not sure if it the regular estimate ..\n<issue_comment>username_2: @username_0 If you can't reproduce it at home it seems hard to debug other than reading at the diffs again.  I can take a look on Monday, it's getting a bit late here.\n<issue_comment>username_2: Also, has anyone found a way to show the actual hardware used on the CI in detail?\n<issue_comment>username_0: @username_2 I am working in parallel on a paperspace environment .. it tooks a lot of time it is running with 8 cpu cores.\n<issue_comment>username_2: But have you ever been able to reproduce this issue on paperspace?\n<issue_comment>username_1: I can reproduce it locally: Arch Linux, CUDA 10.0, RTX2070 GPU. Given the CI failures, I think it should be reproducible for multiple CUDA and GPU versions. I just haven't worked on any CUDA code before, and am short on time, so I'd rather not dig too deep.\n<issue_comment>username_0: not yet .. my last building was with a my previous commit ... I will work again in this task in some minutes :)\n<issue_comment>username_1: @username_0 your previous commit had the same failure though (except for the less clear exception message), and it seems quite reproducible. So I think you'll see it now.\n<issue_comment>username_2: Ah, thanks.  @username_0, then I guess just compiling with \"DEBUG=1\" and stepping through the offending code with gdb may be the fastest way.\n<issue_comment>username_0: @username_2 thanks! I will try that!\n<issue_comment>username_0: it seems just UpSampleBicubic2d is using upsample_get_value_bounded (https://github.com/pytorch/pytorch/pull/19630/files#diff-5092da792c30694ee4adf0d0ae2a37c6R171) and upsample_increment_value_bounded (https://github.com/pytorch/pytorch/pull/19630/files#diff-5092da792c30694ee4adf0d0ae2a37c6R191)\r\n\r\nmaybe the problem could be inside one of these functions ... maybe related to the order of indexes (x, y) ...\r\n\r\nbut the problem seems to be related to cuda block/threads ... so not sure if it is really related to these functions.\n<issue_comment>username_2: The new code uses far more registers than the existing one. I verified that the both versions actually call the offending test case with `1024` in `blockDim`.  So it's very likely a register issue.\r\n\r\n`Existing` uses `64` registers, which seems to be optimal or my card:\r\n\r\n```\r\nptxas info    : 77696 bytes gmem, 72 bytes cmem[3]\r\nptxas info    : Compiling entry function '_Z23bicubic_interp2d_kerneliddb15THCDeviceTensorIdLi4Ei16DefaultPtrTraitsES1_' for 'sm_61'\r\nptxas info    : Function properties for _Z23bicubic_interp2d_kerneliddb15THCDeviceTensorIdLi4Ei16DefaultPtrTraitsES1_\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 64 registers, 432 bytes cmem[0], 24 bytes cmem[2]\r\n```\r\n\r\n`New` uses `124` registers, which is too much for my card:\r\n\r\n```\r\nptxas info    : 77696 bytes gmem, 72 bytes cmem[3]\r\nptxas info    : Compiling entry function '_ZN2at6native76_GLOBAL__N__52_tmpxft_0000626d_00000000_6_UpSampleBicubic2d_cpp1_ii_b4c1e1f328upsample_bicubic2d_out_frameElddbNS_20PackedTensorAccessorIdLm4ENS_16DefaultPtrTraitsElEES4_' for 'sm_61'\r\nptxas info    : Function properties for _ZN2at6native76_GLOBAL__N__52_tmpxft_0000626d_00000000_6_UpSampleBicubic2d_cpp1_ii_b4c1e1f328upsample_bicubic2d_out_frameElddbNS_20PackedTensorAccessorIdLm4ENS_16DefaultPtrTraitsElEES4_\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 124 registers, 496 bytes cmem[0], 24 bytes cmem[2]\r\n```\r\n\r\nWith `__launch_bounds__(1024)`, the code again uses `64` registers.\r\n\r\nIf you use `C10_LAUNCH_BOUNDS_1(1024)` **for both kernels**, the tests pass here.\r\n\r\n\r\nNow *why* is the regcount higher in the new code? It could be `PackedTensorAccessor`, it could be the fact that many instances of `int` have been changed to `int64_t`. :)\r\n\r\nYou could experiment or just use the launch bounds.  Other code in `native/cuda` seems to use lower bounds for `blockDim`, too.  1024 seems to be an outlier.\n<issue_comment>username_0: @username_2 \r\n\r\nit seems it worked locally! thank you so much!  I really appreciate that!\n<issue_comment>username_0: thanks @username_1 and @username_2 for all the help!\r\n\r\n@username_3 it is done for review!\n<issue_comment>username_3: CircleCI runs on AWS, and the I'm fairly sure we've got one of the g3 types, either g3.4xlarge or g3.8xlarge\n<issue_comment>username_3: cc @ngimel, if you want to take a look at this (I'm planning to do a review too)\n<issue_comment>username_0: @username_4 could it be addressed in another PR?\n<issue_comment>username_4: @username_0 Sure, it's definitely better to rename both cpu & gpu impls together in a separate PR. Just want to make sure we have the correct one after porting.\n<issue_comment>username_3: Just as a clarification: I'm OK with having changes which are improvements over the old kernels come in latter patches.\n<issue_comment>username_3: I finished reviewing one of the kernels, and did a cursory scan of the others. In general the patch looks like it's in good shape. I'd suggest that we fix up the major correctness issues (PackedTensorAccessor ref, and int32 versus int64 indexing) and anything small that is easy to fix (for example, double zeroing the array), and leave other improvements for follow ups. Should be ready to land soon!\n<issue_comment>username_0: thanks so much @username_3!\r\nI will let you know when it is ready again for a new review! thanks!\n<issue_comment>username_0: @username_2 @username_3 \r\nnot sure but the errors on CI seems to be related to jenkins ... it seems all these jobs that failed (for building) ran by 01h 01min ... not sure if it is a coincidence ...\n<issue_comment>username_1: @username_0 indeed it looks like all jobs were aborted at the same time. no obvious issues in the build log related to your code. Comparing e.g. the `cuda9-cudnn7` build with a successful one from another PR, it takes 1hr 14min there and your build is about at the place where the other one is after an hour.\r\n\r\nI suggest to just push a new commit to rebuild. Probably a temporary CI hiccup.\n<issue_comment>username_3: I accidentally rebooted Jenkins yesterday which is the likely cause, my apologies.\r\n\r\n@pytorchbot retest this please\n<issue_comment>username_0: @username_3 @username_2 @username_1 \r\n\r\nall tests passed except `pr/caffe2-py2-cuda9.0-cudnn7-windows-build`:\r\n \r\n```\r\n14:43:49 Build timed out (after 180 minutes). Marking the build as failed.\r\n14:43:49 Build was aborted\r\n14:43:49 [BFA] Scanning build for known causes...\r\n14:43:49 [BFA] No failure causes found\r\n14:43:49 [BFA] Done. 0s\r\n14:43:49 Finished: FAILURE\r\n```\r\n\r\nnot sure if this timeout means that the code now is slower.\r\n\r\nwhat do you think? do you have any suggestion?\n<issue_comment>username_3: Sometimes the Windows build flakes out like that. It didn't timeout while running a relevant test, so I judge it to be not your problem.\n<issue_comment>username_3: The launch bounds logic is wrong, but I acknowledge that this is a big patch already; just fix it in a follow up. I am going to go ahead and land this.\n<issue_comment>username_0: sounds good @username_3 thanks!","repo":"pytorch/pytorch","issue_id":436372857,"issue_number":19630}
{"question":"How can you identify and debug jobs that are taking a significant amount of time to build? Is a 19-hour estimate for a job build considered regular?","answer":"To identify and debug jobs taking a lot of time to build, you can start by checking the build logs, analyzing the build process for bottlenecks, and considering performance optimization strategies. A 19-hour estimate for a job build may not be regular and could indicate issues that need to be addressed.","id":146,"text":"@username_2 do you have any idea about what could be this problem? or a way to debug that?\r\nalso it seems some jobs is taking a lot of time to build, for example, for some job the estimate time is 19h .. not sure if it the regular estimate ..","context":"<issue_start><issue_comment>Title: [WIP] UpSample GPU Porting \nusername_0: resolves #16158\n<issue_comment>username_1: Fewer errors than before, 4 instead of 8 with the same message:\r\n```\r\nRuntimeError: CUDA error: too many resources requested for launch\r\n```\r\nThis seems to be an existing problem: gh-8103.\r\n\r\n@username_2 could you have a look at this and tell us what you think?\n<issue_comment>username_2: Looking at it very briefly, the `Jetson TX` referenced in the issue only has 256 cores.\r\n\r\nSo while the issue looks the same, I'd probably not expect it on the CI platforms.  FWIW, some recent CI tests in other issues are green.\n<issue_comment>username_2: @pytorchbot retest this please.\n<issue_comment>username_1: same failures.\n<issue_comment>username_2: Is this rebased on the latest master (you can also ask the bot to rebase)?\n<issue_comment>username_0: I rebased that yesterday ... also needed to resolve conflicts because 7 days ago upsample files were changed.\n<issue_comment>username_0: @username_2 do you have any idea about what could be this problem? or a way to debug that?\r\nalso it seems some jobs is taking a lot of time to build, for example, for some job the estimate time is 19h .. not sure if it the regular estimate ..\n<issue_comment>username_2: @username_0 If you can't reproduce it at home it seems hard to debug other than reading at the diffs again.  I can take a look on Monday, it's getting a bit late here.\n<issue_comment>username_2: Also, has anyone found a way to show the actual hardware used on the CI in detail?\n<issue_comment>username_0: @username_2 I am working in parallel on a paperspace environment .. it tooks a lot of time it is running with 8 cpu cores.\n<issue_comment>username_2: But have you ever been able to reproduce this issue on paperspace?\n<issue_comment>username_1: I can reproduce it locally: Arch Linux, CUDA 10.0, RTX2070 GPU. Given the CI failures, I think it should be reproducible for multiple CUDA and GPU versions. I just haven't worked on any CUDA code before, and am short on time, so I'd rather not dig too deep.\n<issue_comment>username_0: not yet .. my last building was with a my previous commit ... I will work again in this task in some minutes :)\n<issue_comment>username_1: @username_0 your previous commit had the same failure though (except for the less clear exception message), and it seems quite reproducible. So I think you'll see it now.\n<issue_comment>username_2: Ah, thanks.  @username_0, then I guess just compiling with \"DEBUG=1\" and stepping through the offending code with gdb may be the fastest way.\n<issue_comment>username_0: @username_2 thanks! I will try that!\n<issue_comment>username_0: it seems just UpSampleBicubic2d is using upsample_get_value_bounded (https://github.com/pytorch/pytorch/pull/19630/files#diff-5092da792c30694ee4adf0d0ae2a37c6R171) and upsample_increment_value_bounded (https://github.com/pytorch/pytorch/pull/19630/files#diff-5092da792c30694ee4adf0d0ae2a37c6R191)\r\n\r\nmaybe the problem could be inside one of these functions ... maybe related to the order of indexes (x, y) ...\r\n\r\nbut the problem seems to be related to cuda block/threads ... so not sure if it is really related to these functions.\n<issue_comment>username_2: The new code uses far more registers than the existing one. I verified that the both versions actually call the offending test case with `1024` in `blockDim`.  So it's very likely a register issue.\r\n\r\n`Existing` uses `64` registers, which seems to be optimal or my card:\r\n\r\n```\r\nptxas info    : 77696 bytes gmem, 72 bytes cmem[3]\r\nptxas info    : Compiling entry function '_Z23bicubic_interp2d_kerneliddb15THCDeviceTensorIdLi4Ei16DefaultPtrTraitsES1_' for 'sm_61'\r\nptxas info    : Function properties for _Z23bicubic_interp2d_kerneliddb15THCDeviceTensorIdLi4Ei16DefaultPtrTraitsES1_\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 64 registers, 432 bytes cmem[0], 24 bytes cmem[2]\r\n```\r\n\r\n`New` uses `124` registers, which is too much for my card:\r\n\r\n```\r\nptxas info    : 77696 bytes gmem, 72 bytes cmem[3]\r\nptxas info    : Compiling entry function '_ZN2at6native76_GLOBAL__N__52_tmpxft_0000626d_00000000_6_UpSampleBicubic2d_cpp1_ii_b4c1e1f328upsample_bicubic2d_out_frameElddbNS_20PackedTensorAccessorIdLm4ENS_16DefaultPtrTraitsElEES4_' for 'sm_61'\r\nptxas info    : Function properties for _ZN2at6native76_GLOBAL__N__52_tmpxft_0000626d_00000000_6_UpSampleBicubic2d_cpp1_ii_b4c1e1f328upsample_bicubic2d_out_frameElddbNS_20PackedTensorAccessorIdLm4ENS_16DefaultPtrTraitsElEES4_\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 124 registers, 496 bytes cmem[0], 24 bytes cmem[2]\r\n```\r\n\r\nWith `__launch_bounds__(1024)`, the code again uses `64` registers.\r\n\r\nIf you use `C10_LAUNCH_BOUNDS_1(1024)` **for both kernels**, the tests pass here.\r\n\r\n\r\nNow *why* is the regcount higher in the new code? It could be `PackedTensorAccessor`, it could be the fact that many instances of `int` have been changed to `int64_t`. :)\r\n\r\nYou could experiment or just use the launch bounds.  Other code in `native/cuda` seems to use lower bounds for `blockDim`, too.  1024 seems to be an outlier.\n<issue_comment>username_0: @username_2 \r\n\r\nit seems it worked locally! thank you so much!  I really appreciate that!\n<issue_comment>username_0: thanks @username_1 and @username_2 for all the help!\r\n\r\n@username_3 it is done for review!\n<issue_comment>username_3: CircleCI runs on AWS, and the I'm fairly sure we've got one of the g3 types, either g3.4xlarge or g3.8xlarge\n<issue_comment>username_3: cc @ngimel, if you want to take a look at this (I'm planning to do a review too)\n<issue_comment>username_0: @username_4 could it be addressed in another PR?\n<issue_comment>username_4: @username_0 Sure, it's definitely better to rename both cpu & gpu impls together in a separate PR. Just want to make sure we have the correct one after porting.\n<issue_comment>username_3: Just as a clarification: I'm OK with having changes which are improvements over the old kernels come in latter patches.\n<issue_comment>username_3: I finished reviewing one of the kernels, and did a cursory scan of the others. In general the patch looks like it's in good shape. I'd suggest that we fix up the major correctness issues (PackedTensorAccessor ref, and int32 versus int64 indexing) and anything small that is easy to fix (for example, double zeroing the array), and leave other improvements for follow ups. Should be ready to land soon!\n<issue_comment>username_0: thanks so much @username_3!\r\nI will let you know when it is ready again for a new review! thanks!\n<issue_comment>username_0: @username_2 @username_3 \r\nnot sure but the errors on CI seems to be related to jenkins ... it seems all these jobs that failed (for building) ran by 01h 01min ... not sure if it is a coincidence ...\n<issue_comment>username_1: @username_0 indeed it looks like all jobs were aborted at the same time. no obvious issues in the build log related to your code. Comparing e.g. the `cuda9-cudnn7` build with a successful one from another PR, it takes 1hr 14min there and your build is about at the place where the other one is after an hour.\r\n\r\nI suggest to just push a new commit to rebuild. Probably a temporary CI hiccup.\n<issue_comment>username_3: I accidentally rebooted Jenkins yesterday which is the likely cause, my apologies.\r\n\r\n@pytorchbot retest this please\n<issue_comment>username_0: @username_3 @username_2 @username_1 \r\n\r\nall tests passed except `pr/caffe2-py2-cuda9.0-cudnn7-windows-build`:\r\n \r\n```\r\n14:43:49 Build timed out (after 180 minutes). Marking the build as failed.\r\n14:43:49 Build was aborted\r\n14:43:49 [BFA] Scanning build for known causes...\r\n14:43:49 [BFA] No failure causes found\r\n14:43:49 [BFA] Done. 0s\r\n14:43:49 Finished: FAILURE\r\n```\r\n\r\nnot sure if this timeout means that the code now is slower.\r\n\r\nwhat do you think? do you have any suggestion?\n<issue_comment>username_3: Sometimes the Windows build flakes out like that. It didn't timeout while running a relevant test, so I judge it to be not your problem.\n<issue_comment>username_3: The launch bounds logic is wrong, but I acknowledge that this is a big patch already; just fix it in a follow up. I am going to go ahead and land this.\n<issue_comment>username_0: sounds good @username_3 thanks!","repo":"pytorch/pytorch","issue_id":436372857,"issue_number":19630}
{"question":"What challenges are mentioned in debugging when something can't be reproduced at home?","answer":"The text mentions that debugging is difficult when something cannot be reproduced at home, and suggests that the recipient will take a look at the issue on Monday.","id":147,"text":"@username_0 If you can't reproduce it at home it seems hard to debug other than reading at the diffs again.  I can take a look on Monday, it's getting a bit late here.","context":"<issue_start><issue_comment>Title: [WIP] UpSample GPU Porting \nusername_0: resolves #16158\n<issue_comment>username_1: Fewer errors than before, 4 instead of 8 with the same message:\r\n```\r\nRuntimeError: CUDA error: too many resources requested for launch\r\n```\r\nThis seems to be an existing problem: gh-8103.\r\n\r\n@username_2 could you have a look at this and tell us what you think?\n<issue_comment>username_2: Looking at it very briefly, the `Jetson TX` referenced in the issue only has 256 cores.\r\n\r\nSo while the issue looks the same, I'd probably not expect it on the CI platforms.  FWIW, some recent CI tests in other issues are green.\n<issue_comment>username_2: @pytorchbot retest this please.\n<issue_comment>username_1: same failures.\n<issue_comment>username_2: Is this rebased on the latest master (you can also ask the bot to rebase)?\n<issue_comment>username_0: I rebased that yesterday ... also needed to resolve conflicts because 7 days ago upsample files were changed.\n<issue_comment>username_0: @username_2 do you have any idea about what could be this problem? or a way to debug that?\r\nalso it seems some jobs is taking a lot of time to build, for example, for some job the estimate time is 19h .. not sure if it the regular estimate ..\n<issue_comment>username_2: @username_0 If you can't reproduce it at home it seems hard to debug other than reading at the diffs again.  I can take a look on Monday, it's getting a bit late here.\n<issue_comment>username_2: Also, has anyone found a way to show the actual hardware used on the CI in detail?\n<issue_comment>username_0: @username_2 I am working in parallel on a paperspace environment .. it tooks a lot of time it is running with 8 cpu cores.\n<issue_comment>username_2: But have you ever been able to reproduce this issue on paperspace?\n<issue_comment>username_1: I can reproduce it locally: Arch Linux, CUDA 10.0, RTX2070 GPU. Given the CI failures, I think it should be reproducible for multiple CUDA and GPU versions. I just haven't worked on any CUDA code before, and am short on time, so I'd rather not dig too deep.\n<issue_comment>username_0: not yet .. my last building was with a my previous commit ... I will work again in this task in some minutes :)\n<issue_comment>username_1: @username_0 your previous commit had the same failure though (except for the less clear exception message), and it seems quite reproducible. So I think you'll see it now.\n<issue_comment>username_2: Ah, thanks.  @username_0, then I guess just compiling with \"DEBUG=1\" and stepping through the offending code with gdb may be the fastest way.\n<issue_comment>username_0: @username_2 thanks! I will try that!\n<issue_comment>username_0: it seems just UpSampleBicubic2d is using upsample_get_value_bounded (https://github.com/pytorch/pytorch/pull/19630/files#diff-5092da792c30694ee4adf0d0ae2a37c6R171) and upsample_increment_value_bounded (https://github.com/pytorch/pytorch/pull/19630/files#diff-5092da792c30694ee4adf0d0ae2a37c6R191)\r\n\r\nmaybe the problem could be inside one of these functions ... maybe related to the order of indexes (x, y) ...\r\n\r\nbut the problem seems to be related to cuda block/threads ... so not sure if it is really related to these functions.\n<issue_comment>username_2: The new code uses far more registers than the existing one. I verified that the both versions actually call the offending test case with `1024` in `blockDim`.  So it's very likely a register issue.\r\n\r\n`Existing` uses `64` registers, which seems to be optimal or my card:\r\n\r\n```\r\nptxas info    : 77696 bytes gmem, 72 bytes cmem[3]\r\nptxas info    : Compiling entry function '_Z23bicubic_interp2d_kerneliddb15THCDeviceTensorIdLi4Ei16DefaultPtrTraitsES1_' for 'sm_61'\r\nptxas info    : Function properties for _Z23bicubic_interp2d_kerneliddb15THCDeviceTensorIdLi4Ei16DefaultPtrTraitsES1_\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 64 registers, 432 bytes cmem[0], 24 bytes cmem[2]\r\n```\r\n\r\n`New` uses `124` registers, which is too much for my card:\r\n\r\n```\r\nptxas info    : 77696 bytes gmem, 72 bytes cmem[3]\r\nptxas info    : Compiling entry function '_ZN2at6native76_GLOBAL__N__52_tmpxft_0000626d_00000000_6_UpSampleBicubic2d_cpp1_ii_b4c1e1f328upsample_bicubic2d_out_frameElddbNS_20PackedTensorAccessorIdLm4ENS_16DefaultPtrTraitsElEES4_' for 'sm_61'\r\nptxas info    : Function properties for _ZN2at6native76_GLOBAL__N__52_tmpxft_0000626d_00000000_6_UpSampleBicubic2d_cpp1_ii_b4c1e1f328upsample_bicubic2d_out_frameElddbNS_20PackedTensorAccessorIdLm4ENS_16DefaultPtrTraitsElEES4_\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 124 registers, 496 bytes cmem[0], 24 bytes cmem[2]\r\n```\r\n\r\nWith `__launch_bounds__(1024)`, the code again uses `64` registers.\r\n\r\nIf you use `C10_LAUNCH_BOUNDS_1(1024)` **for both kernels**, the tests pass here.\r\n\r\n\r\nNow *why* is the regcount higher in the new code? It could be `PackedTensorAccessor`, it could be the fact that many instances of `int` have been changed to `int64_t`. :)\r\n\r\nYou could experiment or just use the launch bounds.  Other code in `native/cuda` seems to use lower bounds for `blockDim`, too.  1024 seems to be an outlier.\n<issue_comment>username_0: @username_2 \r\n\r\nit seems it worked locally! thank you so much!  I really appreciate that!\n<issue_comment>username_0: thanks @username_1 and @username_2 for all the help!\r\n\r\n@username_3 it is done for review!\n<issue_comment>username_3: CircleCI runs on AWS, and the I'm fairly sure we've got one of the g3 types, either g3.4xlarge or g3.8xlarge\n<issue_comment>username_3: cc @ngimel, if you want to take a look at this (I'm planning to do a review too)\n<issue_comment>username_0: @username_4 could it be addressed in another PR?\n<issue_comment>username_4: @username_0 Sure, it's definitely better to rename both cpu & gpu impls together in a separate PR. Just want to make sure we have the correct one after porting.\n<issue_comment>username_3: Just as a clarification: I'm OK with having changes which are improvements over the old kernels come in latter patches.\n<issue_comment>username_3: I finished reviewing one of the kernels, and did a cursory scan of the others. In general the patch looks like it's in good shape. I'd suggest that we fix up the major correctness issues (PackedTensorAccessor ref, and int32 versus int64 indexing) and anything small that is easy to fix (for example, double zeroing the array), and leave other improvements for follow ups. Should be ready to land soon!\n<issue_comment>username_0: thanks so much @username_3!\r\nI will let you know when it is ready again for a new review! thanks!\n<issue_comment>username_0: @username_2 @username_3 \r\nnot sure but the errors on CI seems to be related to jenkins ... it seems all these jobs that failed (for building) ran by 01h 01min ... not sure if it is a coincidence ...\n<issue_comment>username_1: @username_0 indeed it looks like all jobs were aborted at the same time. no obvious issues in the build log related to your code. Comparing e.g. the `cuda9-cudnn7` build with a successful one from another PR, it takes 1hr 14min there and your build is about at the place where the other one is after an hour.\r\n\r\nI suggest to just push a new commit to rebuild. Probably a temporary CI hiccup.\n<issue_comment>username_3: I accidentally rebooted Jenkins yesterday which is the likely cause, my apologies.\r\n\r\n@pytorchbot retest this please\n<issue_comment>username_0: @username_3 @username_2 @username_1 \r\n\r\nall tests passed except `pr/caffe2-py2-cuda9.0-cudnn7-windows-build`:\r\n \r\n```\r\n14:43:49 Build timed out (after 180 minutes). Marking the build as failed.\r\n14:43:49 Build was aborted\r\n14:43:49 [BFA] Scanning build for known causes...\r\n14:43:49 [BFA] No failure causes found\r\n14:43:49 [BFA] Done. 0s\r\n14:43:49 Finished: FAILURE\r\n```\r\n\r\nnot sure if this timeout means that the code now is slower.\r\n\r\nwhat do you think? do you have any suggestion?\n<issue_comment>username_3: Sometimes the Windows build flakes out like that. It didn't timeout while running a relevant test, so I judge it to be not your problem.\n<issue_comment>username_3: The launch bounds logic is wrong, but I acknowledge that this is a big patch already; just fix it in a follow up. I am going to go ahead and land this.\n<issue_comment>username_0: sounds good @username_3 thanks!","repo":"pytorch/pytorch","issue_id":436372857,"issue_number":19630}
{"question":"What hardware and software configuration is mentioned for reproducing the issue locally?","answer":"Arch Linux, CUDA 10.0, RTX2070 GPU.","id":151,"text":"I can reproduce it locally: Arch Linux, CUDA 10.0, RTX2070 GPU. Given the CI failures, I think it should be reproducible for multiple CUDA and GPU versions. I just haven't worked on any CUDA code before, and am short on time, so I'd rather not dig too deep.","context":"<issue_start><issue_comment>Title: [WIP] UpSample GPU Porting \nusername_0: resolves #16158\n<issue_comment>username_1: Fewer errors than before, 4 instead of 8 with the same message:\r\n```\r\nRuntimeError: CUDA error: too many resources requested for launch\r\n```\r\nThis seems to be an existing problem: gh-8103.\r\n\r\n@username_2 could you have a look at this and tell us what you think?\n<issue_comment>username_2: Looking at it very briefly, the `Jetson TX` referenced in the issue only has 256 cores.\r\n\r\nSo while the issue looks the same, I'd probably not expect it on the CI platforms.  FWIW, some recent CI tests in other issues are green.\n<issue_comment>username_2: @pytorchbot retest this please.\n<issue_comment>username_1: same failures.\n<issue_comment>username_2: Is this rebased on the latest master (you can also ask the bot to rebase)?\n<issue_comment>username_0: I rebased that yesterday ... also needed to resolve conflicts because 7 days ago upsample files were changed.\n<issue_comment>username_0: @username_2 do you have any idea about what could be this problem? or a way to debug that?\r\nalso it seems some jobs is taking a lot of time to build, for example, for some job the estimate time is 19h .. not sure if it the regular estimate ..\n<issue_comment>username_2: @username_0 If you can't reproduce it at home it seems hard to debug other than reading at the diffs again.  I can take a look on Monday, it's getting a bit late here.\n<issue_comment>username_2: Also, has anyone found a way to show the actual hardware used on the CI in detail?\n<issue_comment>username_0: @username_2 I am working in parallel on a paperspace environment .. it tooks a lot of time it is running with 8 cpu cores.\n<issue_comment>username_2: But have you ever been able to reproduce this issue on paperspace?\n<issue_comment>username_1: I can reproduce it locally: Arch Linux, CUDA 10.0, RTX2070 GPU. Given the CI failures, I think it should be reproducible for multiple CUDA and GPU versions. I just haven't worked on any CUDA code before, and am short on time, so I'd rather not dig too deep.\n<issue_comment>username_0: not yet .. my last building was with a my previous commit ... I will work again in this task in some minutes :)\n<issue_comment>username_1: @username_0 your previous commit had the same failure though (except for the less clear exception message), and it seems quite reproducible. So I think you'll see it now.\n<issue_comment>username_2: Ah, thanks.  @username_0, then I guess just compiling with \"DEBUG=1\" and stepping through the offending code with gdb may be the fastest way.\n<issue_comment>username_0: @username_2 thanks! I will try that!\n<issue_comment>username_0: it seems just UpSampleBicubic2d is using upsample_get_value_bounded (https://github.com/pytorch/pytorch/pull/19630/files#diff-5092da792c30694ee4adf0d0ae2a37c6R171) and upsample_increment_value_bounded (https://github.com/pytorch/pytorch/pull/19630/files#diff-5092da792c30694ee4adf0d0ae2a37c6R191)\r\n\r\nmaybe the problem could be inside one of these functions ... maybe related to the order of indexes (x, y) ...\r\n\r\nbut the problem seems to be related to cuda block/threads ... so not sure if it is really related to these functions.\n<issue_comment>username_2: The new code uses far more registers than the existing one. I verified that the both versions actually call the offending test case with `1024` in `blockDim`.  So it's very likely a register issue.\r\n\r\n`Existing` uses `64` registers, which seems to be optimal or my card:\r\n\r\n```\r\nptxas info    : 77696 bytes gmem, 72 bytes cmem[3]\r\nptxas info    : Compiling entry function '_Z23bicubic_interp2d_kerneliddb15THCDeviceTensorIdLi4Ei16DefaultPtrTraitsES1_' for 'sm_61'\r\nptxas info    : Function properties for _Z23bicubic_interp2d_kerneliddb15THCDeviceTensorIdLi4Ei16DefaultPtrTraitsES1_\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 64 registers, 432 bytes cmem[0], 24 bytes cmem[2]\r\n```\r\n\r\n`New` uses `124` registers, which is too much for my card:\r\n\r\n```\r\nptxas info    : 77696 bytes gmem, 72 bytes cmem[3]\r\nptxas info    : Compiling entry function '_ZN2at6native76_GLOBAL__N__52_tmpxft_0000626d_00000000_6_UpSampleBicubic2d_cpp1_ii_b4c1e1f328upsample_bicubic2d_out_frameElddbNS_20PackedTensorAccessorIdLm4ENS_16DefaultPtrTraitsElEES4_' for 'sm_61'\r\nptxas info    : Function properties for _ZN2at6native76_GLOBAL__N__52_tmpxft_0000626d_00000000_6_UpSampleBicubic2d_cpp1_ii_b4c1e1f328upsample_bicubic2d_out_frameElddbNS_20PackedTensorAccessorIdLm4ENS_16DefaultPtrTraitsElEES4_\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 124 registers, 496 bytes cmem[0], 24 bytes cmem[2]\r\n```\r\n\r\nWith `__launch_bounds__(1024)`, the code again uses `64` registers.\r\n\r\nIf you use `C10_LAUNCH_BOUNDS_1(1024)` **for both kernels**, the tests pass here.\r\n\r\n\r\nNow *why* is the regcount higher in the new code? It could be `PackedTensorAccessor`, it could be the fact that many instances of `int` have been changed to `int64_t`. :)\r\n\r\nYou could experiment or just use the launch bounds.  Other code in `native/cuda` seems to use lower bounds for `blockDim`, too.  1024 seems to be an outlier.\n<issue_comment>username_0: @username_2 \r\n\r\nit seems it worked locally! thank you so much!  I really appreciate that!\n<issue_comment>username_0: thanks @username_1 and @username_2 for all the help!\r\n\r\n@username_3 it is done for review!\n<issue_comment>username_3: CircleCI runs on AWS, and the I'm fairly sure we've got one of the g3 types, either g3.4xlarge or g3.8xlarge\n<issue_comment>username_3: cc @ngimel, if you want to take a look at this (I'm planning to do a review too)\n<issue_comment>username_0: @username_4 could it be addressed in another PR?\n<issue_comment>username_4: @username_0 Sure, it's definitely better to rename both cpu & gpu impls together in a separate PR. Just want to make sure we have the correct one after porting.\n<issue_comment>username_3: Just as a clarification: I'm OK with having changes which are improvements over the old kernels come in latter patches.\n<issue_comment>username_3: I finished reviewing one of the kernels, and did a cursory scan of the others. In general the patch looks like it's in good shape. I'd suggest that we fix up the major correctness issues (PackedTensorAccessor ref, and int32 versus int64 indexing) and anything small that is easy to fix (for example, double zeroing the array), and leave other improvements for follow ups. Should be ready to land soon!\n<issue_comment>username_0: thanks so much @username_3!\r\nI will let you know when it is ready again for a new review! thanks!\n<issue_comment>username_0: @username_2 @username_3 \r\nnot sure but the errors on CI seems to be related to jenkins ... it seems all these jobs that failed (for building) ran by 01h 01min ... not sure if it is a coincidence ...\n<issue_comment>username_1: @username_0 indeed it looks like all jobs were aborted at the same time. no obvious issues in the build log related to your code. Comparing e.g. the `cuda9-cudnn7` build with a successful one from another PR, it takes 1hr 14min there and your build is about at the place where the other one is after an hour.\r\n\r\nI suggest to just push a new commit to rebuild. Probably a temporary CI hiccup.\n<issue_comment>username_3: I accidentally rebooted Jenkins yesterday which is the likely cause, my apologies.\r\n\r\n@pytorchbot retest this please\n<issue_comment>username_0: @username_3 @username_2 @username_1 \r\n\r\nall tests passed except `pr/caffe2-py2-cuda9.0-cudnn7-windows-build`:\r\n \r\n```\r\n14:43:49 Build timed out (after 180 minutes). Marking the build as failed.\r\n14:43:49 Build was aborted\r\n14:43:49 [BFA] Scanning build for known causes...\r\n14:43:49 [BFA] No failure causes found\r\n14:43:49 [BFA] Done. 0s\r\n14:43:49 Finished: FAILURE\r\n```\r\n\r\nnot sure if this timeout means that the code now is slower.\r\n\r\nwhat do you think? do you have any suggestion?\n<issue_comment>username_3: Sometimes the Windows build flakes out like that. It didn't timeout while running a relevant test, so I judge it to be not your problem.\n<issue_comment>username_3: The launch bounds logic is wrong, but I acknowledge that this is a big patch already; just fix it in a follow up. I am going to go ahead and land this.\n<issue_comment>username_0: sounds good @username_3 thanks!","repo":"pytorch/pytorch","issue_id":436372857,"issue_number":19630}
{"question":"Why is it important to check the code again after encountering a similar failure in a previous commit?","answer":"It is important to check the code again after encountering a similar failure in a previous commit because the issue is likely reproducible and needs to be identified and fixed to prevent it from occurring again.","id":153,"text":"@username_0 your previous commit had the same failure though (except for the less clear exception message), and it seems quite reproducible. So I think you'll see it now.","context":"<issue_start><issue_comment>Title: [WIP] UpSample GPU Porting \nusername_0: resolves #16158\n<issue_comment>username_1: Fewer errors than before, 4 instead of 8 with the same message:\r\n```\r\nRuntimeError: CUDA error: too many resources requested for launch\r\n```\r\nThis seems to be an existing problem: gh-8103.\r\n\r\n@username_2 could you have a look at this and tell us what you think?\n<issue_comment>username_2: Looking at it very briefly, the `Jetson TX` referenced in the issue only has 256 cores.\r\n\r\nSo while the issue looks the same, I'd probably not expect it on the CI platforms.  FWIW, some recent CI tests in other issues are green.\n<issue_comment>username_2: @pytorchbot retest this please.\n<issue_comment>username_1: same failures.\n<issue_comment>username_2: Is this rebased on the latest master (you can also ask the bot to rebase)?\n<issue_comment>username_0: I rebased that yesterday ... also needed to resolve conflicts because 7 days ago upsample files were changed.\n<issue_comment>username_0: @username_2 do you have any idea about what could be this problem? or a way to debug that?\r\nalso it seems some jobs is taking a lot of time to build, for example, for some job the estimate time is 19h .. not sure if it the regular estimate ..\n<issue_comment>username_2: @username_0 If you can't reproduce it at home it seems hard to debug other than reading at the diffs again.  I can take a look on Monday, it's getting a bit late here.\n<issue_comment>username_2: Also, has anyone found a way to show the actual hardware used on the CI in detail?\n<issue_comment>username_0: @username_2 I am working in parallel on a paperspace environment .. it tooks a lot of time it is running with 8 cpu cores.\n<issue_comment>username_2: But have you ever been able to reproduce this issue on paperspace?\n<issue_comment>username_1: I can reproduce it locally: Arch Linux, CUDA 10.0, RTX2070 GPU. Given the CI failures, I think it should be reproducible for multiple CUDA and GPU versions. I just haven't worked on any CUDA code before, and am short on time, so I'd rather not dig too deep.\n<issue_comment>username_0: not yet .. my last building was with a my previous commit ... I will work again in this task in some minutes :)\n<issue_comment>username_1: @username_0 your previous commit had the same failure though (except for the less clear exception message), and it seems quite reproducible. So I think you'll see it now.\n<issue_comment>username_2: Ah, thanks.  @username_0, then I guess just compiling with \"DEBUG=1\" and stepping through the offending code with gdb may be the fastest way.\n<issue_comment>username_0: @username_2 thanks! I will try that!\n<issue_comment>username_0: it seems just UpSampleBicubic2d is using upsample_get_value_bounded (https://github.com/pytorch/pytorch/pull/19630/files#diff-5092da792c30694ee4adf0d0ae2a37c6R171) and upsample_increment_value_bounded (https://github.com/pytorch/pytorch/pull/19630/files#diff-5092da792c30694ee4adf0d0ae2a37c6R191)\r\n\r\nmaybe the problem could be inside one of these functions ... maybe related to the order of indexes (x, y) ...\r\n\r\nbut the problem seems to be related to cuda block/threads ... so not sure if it is really related to these functions.\n<issue_comment>username_2: The new code uses far more registers than the existing one. I verified that the both versions actually call the offending test case with `1024` in `blockDim`.  So it's very likely a register issue.\r\n\r\n`Existing` uses `64` registers, which seems to be optimal or my card:\r\n\r\n```\r\nptxas info    : 77696 bytes gmem, 72 bytes cmem[3]\r\nptxas info    : Compiling entry function '_Z23bicubic_interp2d_kerneliddb15THCDeviceTensorIdLi4Ei16DefaultPtrTraitsES1_' for 'sm_61'\r\nptxas info    : Function properties for _Z23bicubic_interp2d_kerneliddb15THCDeviceTensorIdLi4Ei16DefaultPtrTraitsES1_\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 64 registers, 432 bytes cmem[0], 24 bytes cmem[2]\r\n```\r\n\r\n`New` uses `124` registers, which is too much for my card:\r\n\r\n```\r\nptxas info    : 77696 bytes gmem, 72 bytes cmem[3]\r\nptxas info    : Compiling entry function '_ZN2at6native76_GLOBAL__N__52_tmpxft_0000626d_00000000_6_UpSampleBicubic2d_cpp1_ii_b4c1e1f328upsample_bicubic2d_out_frameElddbNS_20PackedTensorAccessorIdLm4ENS_16DefaultPtrTraitsElEES4_' for 'sm_61'\r\nptxas info    : Function properties for _ZN2at6native76_GLOBAL__N__52_tmpxft_0000626d_00000000_6_UpSampleBicubic2d_cpp1_ii_b4c1e1f328upsample_bicubic2d_out_frameElddbNS_20PackedTensorAccessorIdLm4ENS_16DefaultPtrTraitsElEES4_\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 124 registers, 496 bytes cmem[0], 24 bytes cmem[2]\r\n```\r\n\r\nWith `__launch_bounds__(1024)`, the code again uses `64` registers.\r\n\r\nIf you use `C10_LAUNCH_BOUNDS_1(1024)` **for both kernels**, the tests pass here.\r\n\r\n\r\nNow *why* is the regcount higher in the new code? It could be `PackedTensorAccessor`, it could be the fact that many instances of `int` have been changed to `int64_t`. :)\r\n\r\nYou could experiment or just use the launch bounds.  Other code in `native/cuda` seems to use lower bounds for `blockDim`, too.  1024 seems to be an outlier.\n<issue_comment>username_0: @username_2 \r\n\r\nit seems it worked locally! thank you so much!  I really appreciate that!\n<issue_comment>username_0: thanks @username_1 and @username_2 for all the help!\r\n\r\n@username_3 it is done for review!\n<issue_comment>username_3: CircleCI runs on AWS, and the I'm fairly sure we've got one of the g3 types, either g3.4xlarge or g3.8xlarge\n<issue_comment>username_3: cc @ngimel, if you want to take a look at this (I'm planning to do a review too)\n<issue_comment>username_0: @username_4 could it be addressed in another PR?\n<issue_comment>username_4: @username_0 Sure, it's definitely better to rename both cpu & gpu impls together in a separate PR. Just want to make sure we have the correct one after porting.\n<issue_comment>username_3: Just as a clarification: I'm OK with having changes which are improvements over the old kernels come in latter patches.\n<issue_comment>username_3: I finished reviewing one of the kernels, and did a cursory scan of the others. In general the patch looks like it's in good shape. I'd suggest that we fix up the major correctness issues (PackedTensorAccessor ref, and int32 versus int64 indexing) and anything small that is easy to fix (for example, double zeroing the array), and leave other improvements for follow ups. Should be ready to land soon!\n<issue_comment>username_0: thanks so much @username_3!\r\nI will let you know when it is ready again for a new review! thanks!\n<issue_comment>username_0: @username_2 @username_3 \r\nnot sure but the errors on CI seems to be related to jenkins ... it seems all these jobs that failed (for building) ran by 01h 01min ... not sure if it is a coincidence ...\n<issue_comment>username_1: @username_0 indeed it looks like all jobs were aborted at the same time. no obvious issues in the build log related to your code. Comparing e.g. the `cuda9-cudnn7` build with a successful one from another PR, it takes 1hr 14min there and your build is about at the place where the other one is after an hour.\r\n\r\nI suggest to just push a new commit to rebuild. Probably a temporary CI hiccup.\n<issue_comment>username_3: I accidentally rebooted Jenkins yesterday which is the likely cause, my apologies.\r\n\r\n@pytorchbot retest this please\n<issue_comment>username_0: @username_3 @username_2 @username_1 \r\n\r\nall tests passed except `pr/caffe2-py2-cuda9.0-cudnn7-windows-build`:\r\n \r\n```\r\n14:43:49 Build timed out (after 180 minutes). Marking the build as failed.\r\n14:43:49 Build was aborted\r\n14:43:49 [BFA] Scanning build for known causes...\r\n14:43:49 [BFA] No failure causes found\r\n14:43:49 [BFA] Done. 0s\r\n14:43:49 Finished: FAILURE\r\n```\r\n\r\nnot sure if this timeout means that the code now is slower.\r\n\r\nwhat do you think? do you have any suggestion?\n<issue_comment>username_3: Sometimes the Windows build flakes out like that. It didn't timeout while running a relevant test, so I judge it to be not your problem.\n<issue_comment>username_3: The launch bounds logic is wrong, but I acknowledge that this is a big patch already; just fix it in a follow up. I am going to go ahead and land this.\n<issue_comment>username_0: sounds good @username_3 thanks!","repo":"pytorch/pytorch","issue_id":436372857,"issue_number":19630}
{"question":"Why does the new code use more registers than the existing code in the CUDA implementation?","answer":"The regcount increase in the new code could be attributed to the usage of `PackedTensorAccessor` and changing instances of `int` to `int64_t`. The suggestion is to experiment or use the launch bounds, as other code in `native/cuda` also uses lower bounds for `blockDim`, with 1024 being an outlier.","id":157,"text":"The new code uses far more registers than the existing one. I verified that the both versions actually call the offending test case with `1024` in `blockDim`.  So it's very likely a register issue.\r\n\r\n`Existing` uses `64` registers, which seems to be optimal or my card:\r\n\r\n```\r\nptxas info    : 77696 bytes gmem, 72 bytes cmem[3]\r\nptxas info    : Compiling entry function '_Z23bicubic_interp2d_kerneliddb15THCDeviceTensorIdLi4Ei16DefaultPtrTraitsES1_' for 'sm_61'\r\nptxas info    : Function properties for _Z23bicubic_interp2d_kerneliddb15THCDeviceTensorIdLi4Ei16DefaultPtrTraitsES1_\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 64 registers, 432 bytes cmem[0], 24 bytes cmem[2]\r\n```\r\n\r\n`New` uses `124` registers, which is too much for my card:\r\n\r\n```\r\nptxas info    : 77696 bytes gmem, 72 bytes cmem[3]\r\nptxas info    : Compiling entry function '_ZN2at6native76_GLOBAL__N__52_tmpxft_0000626d_00000000_6_UpSampleBicubic2d_cpp1_ii_b4c1e1f328upsample_bicubic2d_out_frameElddbNS_20PackedTensorAccessorIdLm4ENS_16DefaultPtrTraitsElEES4_' for 'sm_61'\r\nptxas info    : Function properties for _ZN2at6native76_GLOBAL__N__52_tmpxft_0000626d_00000000_6_UpSampleBicubic2d_cpp1_ii_b4c1e1f328upsample_bicubic2d_out_frameElddbNS_20PackedTensorAccessorIdLm4ENS_16DefaultPtrTraitsElEES4_\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 124 registers, 496 bytes cmem[0], 24 bytes cmem[2]\r\n```\r\n\r\nWith `__launch_bounds__(1024)`, the code again uses `64` registers.\r\n\r\nIf you use `C10_LAUNCH_BOUNDS_1(1024)` **for both kernels**, the tests pass here.\r\n\r\n\r\nNow *why* is the regcount higher in the new code? It could be `PackedTensorAccessor`, it could be the fact that many instances of `int` have been changed to `int64_t`. :)\r\n\r\nYou could experiment or just use the launch bounds.  Other code in `native/cuda` seems to use lower bounds for `blockDim`, too.  1024 seems to be an outlier.","context":"<issue_start><issue_comment>Title: [WIP] UpSample GPU Porting \nusername_0: resolves #16158\n<issue_comment>username_1: Fewer errors than before, 4 instead of 8 with the same message:\r\n```\r\nRuntimeError: CUDA error: too many resources requested for launch\r\n```\r\nThis seems to be an existing problem: gh-8103.\r\n\r\n@username_2 could you have a look at this and tell us what you think?\n<issue_comment>username_2: Looking at it very briefly, the `Jetson TX` referenced in the issue only has 256 cores.\r\n\r\nSo while the issue looks the same, I'd probably not expect it on the CI platforms.  FWIW, some recent CI tests in other issues are green.\n<issue_comment>username_2: @pytorchbot retest this please.\n<issue_comment>username_1: same failures.\n<issue_comment>username_2: Is this rebased on the latest master (you can also ask the bot to rebase)?\n<issue_comment>username_0: I rebased that yesterday ... also needed to resolve conflicts because 7 days ago upsample files were changed.\n<issue_comment>username_0: @username_2 do you have any idea about what could be this problem? or a way to debug that?\r\nalso it seems some jobs is taking a lot of time to build, for example, for some job the estimate time is 19h .. not sure if it the regular estimate ..\n<issue_comment>username_2: @username_0 If you can't reproduce it at home it seems hard to debug other than reading at the diffs again.  I can take a look on Monday, it's getting a bit late here.\n<issue_comment>username_2: Also, has anyone found a way to show the actual hardware used on the CI in detail?\n<issue_comment>username_0: @username_2 I am working in parallel on a paperspace environment .. it tooks a lot of time it is running with 8 cpu cores.\n<issue_comment>username_2: But have you ever been able to reproduce this issue on paperspace?\n<issue_comment>username_1: I can reproduce it locally: Arch Linux, CUDA 10.0, RTX2070 GPU. Given the CI failures, I think it should be reproducible for multiple CUDA and GPU versions. I just haven't worked on any CUDA code before, and am short on time, so I'd rather not dig too deep.\n<issue_comment>username_0: not yet .. my last building was with a my previous commit ... I will work again in this task in some minutes :)\n<issue_comment>username_1: @username_0 your previous commit had the same failure though (except for the less clear exception message), and it seems quite reproducible. So I think you'll see it now.\n<issue_comment>username_2: Ah, thanks.  @username_0, then I guess just compiling with \"DEBUG=1\" and stepping through the offending code with gdb may be the fastest way.\n<issue_comment>username_0: @username_2 thanks! I will try that!\n<issue_comment>username_0: it seems just UpSampleBicubic2d is using upsample_get_value_bounded (https://github.com/pytorch/pytorch/pull/19630/files#diff-5092da792c30694ee4adf0d0ae2a37c6R171) and upsample_increment_value_bounded (https://github.com/pytorch/pytorch/pull/19630/files#diff-5092da792c30694ee4adf0d0ae2a37c6R191)\r\n\r\nmaybe the problem could be inside one of these functions ... maybe related to the order of indexes (x, y) ...\r\n\r\nbut the problem seems to be related to cuda block/threads ... so not sure if it is really related to these functions.\n<issue_comment>username_2: The new code uses far more registers than the existing one. I verified that the both versions actually call the offending test case with `1024` in `blockDim`.  So it's very likely a register issue.\r\n\r\n`Existing` uses `64` registers, which seems to be optimal or my card:\r\n\r\n```\r\nptxas info    : 77696 bytes gmem, 72 bytes cmem[3]\r\nptxas info    : Compiling entry function '_Z23bicubic_interp2d_kerneliddb15THCDeviceTensorIdLi4Ei16DefaultPtrTraitsES1_' for 'sm_61'\r\nptxas info    : Function properties for _Z23bicubic_interp2d_kerneliddb15THCDeviceTensorIdLi4Ei16DefaultPtrTraitsES1_\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 64 registers, 432 bytes cmem[0], 24 bytes cmem[2]\r\n```\r\n\r\n`New` uses `124` registers, which is too much for my card:\r\n\r\n```\r\nptxas info    : 77696 bytes gmem, 72 bytes cmem[3]\r\nptxas info    : Compiling entry function '_ZN2at6native76_GLOBAL__N__52_tmpxft_0000626d_00000000_6_UpSampleBicubic2d_cpp1_ii_b4c1e1f328upsample_bicubic2d_out_frameElddbNS_20PackedTensorAccessorIdLm4ENS_16DefaultPtrTraitsElEES4_' for 'sm_61'\r\nptxas info    : Function properties for _ZN2at6native76_GLOBAL__N__52_tmpxft_0000626d_00000000_6_UpSampleBicubic2d_cpp1_ii_b4c1e1f328upsample_bicubic2d_out_frameElddbNS_20PackedTensorAccessorIdLm4ENS_16DefaultPtrTraitsElEES4_\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 124 registers, 496 bytes cmem[0], 24 bytes cmem[2]\r\n```\r\n\r\nWith `__launch_bounds__(1024)`, the code again uses `64` registers.\r\n\r\nIf you use `C10_LAUNCH_BOUNDS_1(1024)` **for both kernels**, the tests pass here.\r\n\r\n\r\nNow *why* is the regcount higher in the new code? It could be `PackedTensorAccessor`, it could be the fact that many instances of `int` have been changed to `int64_t`. :)\r\n\r\nYou could experiment or just use the launch bounds.  Other code in `native/cuda` seems to use lower bounds for `blockDim`, too.  1024 seems to be an outlier.\n<issue_comment>username_0: @username_2 \r\n\r\nit seems it worked locally! thank you so much!  I really appreciate that!\n<issue_comment>username_0: thanks @username_1 and @username_2 for all the help!\r\n\r\n@username_3 it is done for review!\n<issue_comment>username_3: CircleCI runs on AWS, and the I'm fairly sure we've got one of the g3 types, either g3.4xlarge or g3.8xlarge\n<issue_comment>username_3: cc @ngimel, if you want to take a look at this (I'm planning to do a review too)\n<issue_comment>username_0: @username_4 could it be addressed in another PR?\n<issue_comment>username_4: @username_0 Sure, it's definitely better to rename both cpu & gpu impls together in a separate PR. Just want to make sure we have the correct one after porting.\n<issue_comment>username_3: Just as a clarification: I'm OK with having changes which are improvements over the old kernels come in latter patches.\n<issue_comment>username_3: I finished reviewing one of the kernels, and did a cursory scan of the others. In general the patch looks like it's in good shape. I'd suggest that we fix up the major correctness issues (PackedTensorAccessor ref, and int32 versus int64 indexing) and anything small that is easy to fix (for example, double zeroing the array), and leave other improvements for follow ups. Should be ready to land soon!\n<issue_comment>username_0: thanks so much @username_3!\r\nI will let you know when it is ready again for a new review! thanks!\n<issue_comment>username_0: @username_2 @username_3 \r\nnot sure but the errors on CI seems to be related to jenkins ... it seems all these jobs that failed (for building) ran by 01h 01min ... not sure if it is a coincidence ...\n<issue_comment>username_1: @username_0 indeed it looks like all jobs were aborted at the same time. no obvious issues in the build log related to your code. Comparing e.g. the `cuda9-cudnn7` build with a successful one from another PR, it takes 1hr 14min there and your build is about at the place where the other one is after an hour.\r\n\r\nI suggest to just push a new commit to rebuild. Probably a temporary CI hiccup.\n<issue_comment>username_3: I accidentally rebooted Jenkins yesterday which is the likely cause, my apologies.\r\n\r\n@pytorchbot retest this please\n<issue_comment>username_0: @username_3 @username_2 @username_1 \r\n\r\nall tests passed except `pr/caffe2-py2-cuda9.0-cudnn7-windows-build`:\r\n \r\n```\r\n14:43:49 Build timed out (after 180 minutes). Marking the build as failed.\r\n14:43:49 Build was aborted\r\n14:43:49 [BFA] Scanning build for known causes...\r\n14:43:49 [BFA] No failure causes found\r\n14:43:49 [BFA] Done. 0s\r\n14:43:49 Finished: FAILURE\r\n```\r\n\r\nnot sure if this timeout means that the code now is slower.\r\n\r\nwhat do you think? do you have any suggestion?\n<issue_comment>username_3: Sometimes the Windows build flakes out like that. It didn't timeout while running a relevant test, so I judge it to be not your problem.\n<issue_comment>username_3: The launch bounds logic is wrong, but I acknowledge that this is a big patch already; just fix it in a follow up. I am going to go ahead and land this.\n<issue_comment>username_0: sounds good @username_3 thanks!","repo":"pytorch/pytorch","issue_id":436372857,"issue_number":19630}
{"question":"Why is it recommended to rename both CPU and GPU implementations together in a separate PR?","answer":"It is recommended to rename both CPU and GPU implementations together in a separate PR to ensure correctness after porting and to maintain consistency between the implementations.","id":163,"text":"@username_0 Sure, it's definitely better to rename both cpu & gpu impls together in a separate PR. Just want to make sure we have the correct one after porting.","context":"<issue_start><issue_comment>Title: [WIP] UpSample GPU Porting \nusername_0: resolves #16158\n<issue_comment>username_1: Fewer errors than before, 4 instead of 8 with the same message:\r\n```\r\nRuntimeError: CUDA error: too many resources requested for launch\r\n```\r\nThis seems to be an existing problem: gh-8103.\r\n\r\n@username_2 could you have a look at this and tell us what you think?\n<issue_comment>username_2: Looking at it very briefly, the `Jetson TX` referenced in the issue only has 256 cores.\r\n\r\nSo while the issue looks the same, I'd probably not expect it on the CI platforms.  FWIW, some recent CI tests in other issues are green.\n<issue_comment>username_2: @pytorchbot retest this please.\n<issue_comment>username_1: same failures.\n<issue_comment>username_2: Is this rebased on the latest master (you can also ask the bot to rebase)?\n<issue_comment>username_0: I rebased that yesterday ... also needed to resolve conflicts because 7 days ago upsample files were changed.\n<issue_comment>username_0: @username_2 do you have any idea about what could be this problem? or a way to debug that?\r\nalso it seems some jobs is taking a lot of time to build, for example, for some job the estimate time is 19h .. not sure if it the regular estimate ..\n<issue_comment>username_2: @username_0 If you can't reproduce it at home it seems hard to debug other than reading at the diffs again.  I can take a look on Monday, it's getting a bit late here.\n<issue_comment>username_2: Also, has anyone found a way to show the actual hardware used on the CI in detail?\n<issue_comment>username_0: @username_2 I am working in parallel on a paperspace environment .. it tooks a lot of time it is running with 8 cpu cores.\n<issue_comment>username_2: But have you ever been able to reproduce this issue on paperspace?\n<issue_comment>username_1: I can reproduce it locally: Arch Linux, CUDA 10.0, RTX2070 GPU. Given the CI failures, I think it should be reproducible for multiple CUDA and GPU versions. I just haven't worked on any CUDA code before, and am short on time, so I'd rather not dig too deep.\n<issue_comment>username_0: not yet .. my last building was with a my previous commit ... I will work again in this task in some minutes :)\n<issue_comment>username_1: @username_0 your previous commit had the same failure though (except for the less clear exception message), and it seems quite reproducible. So I think you'll see it now.\n<issue_comment>username_2: Ah, thanks.  @username_0, then I guess just compiling with \"DEBUG=1\" and stepping through the offending code with gdb may be the fastest way.\n<issue_comment>username_0: @username_2 thanks! I will try that!\n<issue_comment>username_0: it seems just UpSampleBicubic2d is using upsample_get_value_bounded (https://github.com/pytorch/pytorch/pull/19630/files#diff-5092da792c30694ee4adf0d0ae2a37c6R171) and upsample_increment_value_bounded (https://github.com/pytorch/pytorch/pull/19630/files#diff-5092da792c30694ee4adf0d0ae2a37c6R191)\r\n\r\nmaybe the problem could be inside one of these functions ... maybe related to the order of indexes (x, y) ...\r\n\r\nbut the problem seems to be related to cuda block/threads ... so not sure if it is really related to these functions.\n<issue_comment>username_2: The new code uses far more registers than the existing one. I verified that the both versions actually call the offending test case with `1024` in `blockDim`.  So it's very likely a register issue.\r\n\r\n`Existing` uses `64` registers, which seems to be optimal or my card:\r\n\r\n```\r\nptxas info    : 77696 bytes gmem, 72 bytes cmem[3]\r\nptxas info    : Compiling entry function '_Z23bicubic_interp2d_kerneliddb15THCDeviceTensorIdLi4Ei16DefaultPtrTraitsES1_' for 'sm_61'\r\nptxas info    : Function properties for _Z23bicubic_interp2d_kerneliddb15THCDeviceTensorIdLi4Ei16DefaultPtrTraitsES1_\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 64 registers, 432 bytes cmem[0], 24 bytes cmem[2]\r\n```\r\n\r\n`New` uses `124` registers, which is too much for my card:\r\n\r\n```\r\nptxas info    : 77696 bytes gmem, 72 bytes cmem[3]\r\nptxas info    : Compiling entry function '_ZN2at6native76_GLOBAL__N__52_tmpxft_0000626d_00000000_6_UpSampleBicubic2d_cpp1_ii_b4c1e1f328upsample_bicubic2d_out_frameElddbNS_20PackedTensorAccessorIdLm4ENS_16DefaultPtrTraitsElEES4_' for 'sm_61'\r\nptxas info    : Function properties for _ZN2at6native76_GLOBAL__N__52_tmpxft_0000626d_00000000_6_UpSampleBicubic2d_cpp1_ii_b4c1e1f328upsample_bicubic2d_out_frameElddbNS_20PackedTensorAccessorIdLm4ENS_16DefaultPtrTraitsElEES4_\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 124 registers, 496 bytes cmem[0], 24 bytes cmem[2]\r\n```\r\n\r\nWith `__launch_bounds__(1024)`, the code again uses `64` registers.\r\n\r\nIf you use `C10_LAUNCH_BOUNDS_1(1024)` **for both kernels**, the tests pass here.\r\n\r\n\r\nNow *why* is the regcount higher in the new code? It could be `PackedTensorAccessor`, it could be the fact that many instances of `int` have been changed to `int64_t`. :)\r\n\r\nYou could experiment or just use the launch bounds.  Other code in `native/cuda` seems to use lower bounds for `blockDim`, too.  1024 seems to be an outlier.\n<issue_comment>username_0: @username_2 \r\n\r\nit seems it worked locally! thank you so much!  I really appreciate that!\n<issue_comment>username_0: thanks @username_1 and @username_2 for all the help!\r\n\r\n@username_3 it is done for review!\n<issue_comment>username_3: CircleCI runs on AWS, and the I'm fairly sure we've got one of the g3 types, either g3.4xlarge or g3.8xlarge\n<issue_comment>username_3: cc @ngimel, if you want to take a look at this (I'm planning to do a review too)\n<issue_comment>username_0: @username_4 could it be addressed in another PR?\n<issue_comment>username_4: @username_0 Sure, it's definitely better to rename both cpu & gpu impls together in a separate PR. Just want to make sure we have the correct one after porting.\n<issue_comment>username_3: Just as a clarification: I'm OK with having changes which are improvements over the old kernels come in latter patches.\n<issue_comment>username_3: I finished reviewing one of the kernels, and did a cursory scan of the others. In general the patch looks like it's in good shape. I'd suggest that we fix up the major correctness issues (PackedTensorAccessor ref, and int32 versus int64 indexing) and anything small that is easy to fix (for example, double zeroing the array), and leave other improvements for follow ups. Should be ready to land soon!\n<issue_comment>username_0: thanks so much @username_3!\r\nI will let you know when it is ready again for a new review! thanks!\n<issue_comment>username_0: @username_2 @username_3 \r\nnot sure but the errors on CI seems to be related to jenkins ... it seems all these jobs that failed (for building) ran by 01h 01min ... not sure if it is a coincidence ...\n<issue_comment>username_1: @username_0 indeed it looks like all jobs were aborted at the same time. no obvious issues in the build log related to your code. Comparing e.g. the `cuda9-cudnn7` build with a successful one from another PR, it takes 1hr 14min there and your build is about at the place where the other one is after an hour.\r\n\r\nI suggest to just push a new commit to rebuild. Probably a temporary CI hiccup.\n<issue_comment>username_3: I accidentally rebooted Jenkins yesterday which is the likely cause, my apologies.\r\n\r\n@pytorchbot retest this please\n<issue_comment>username_0: @username_3 @username_2 @username_1 \r\n\r\nall tests passed except `pr/caffe2-py2-cuda9.0-cudnn7-windows-build`:\r\n \r\n```\r\n14:43:49 Build timed out (after 180 minutes). Marking the build as failed.\r\n14:43:49 Build was aborted\r\n14:43:49 [BFA] Scanning build for known causes...\r\n14:43:49 [BFA] No failure causes found\r\n14:43:49 [BFA] Done. 0s\r\n14:43:49 Finished: FAILURE\r\n```\r\n\r\nnot sure if this timeout means that the code now is slower.\r\n\r\nwhat do you think? do you have any suggestion?\n<issue_comment>username_3: Sometimes the Windows build flakes out like that. It didn't timeout while running a relevant test, so I judge it to be not your problem.\n<issue_comment>username_3: The launch bounds logic is wrong, but I acknowledge that this is a big patch already; just fix it in a follow up. I am going to go ahead and land this.\n<issue_comment>username_0: sounds good @username_3 thanks!","repo":"pytorch/pytorch","issue_id":436372857,"issue_number":19630}
{"question":"What does the text suggest fixing first in the kernel patch review process?","answer":"The text suggests fixing major correctness issues such as PackedTensorAccessor reference and int32 versus int64 indexing first before addressing smaller issues and improvements.","id":165,"text":"I finished reviewing one of the kernels, and did a cursory scan of the others. In general the patch looks like it's in good shape. I'd suggest that we fix up the major correctness issues (PackedTensorAccessor ref, and int32 versus int64 indexing) and anything small that is easy to fix (for example, double zeroing the array), and leave other improvements for follow ups. Should be ready to land soon!","context":"<issue_start><issue_comment>Title: [WIP] UpSample GPU Porting \nusername_0: resolves #16158\n<issue_comment>username_1: Fewer errors than before, 4 instead of 8 with the same message:\r\n```\r\nRuntimeError: CUDA error: too many resources requested for launch\r\n```\r\nThis seems to be an existing problem: gh-8103.\r\n\r\n@username_2 could you have a look at this and tell us what you think?\n<issue_comment>username_2: Looking at it very briefly, the `Jetson TX` referenced in the issue only has 256 cores.\r\n\r\nSo while the issue looks the same, I'd probably not expect it on the CI platforms.  FWIW, some recent CI tests in other issues are green.\n<issue_comment>username_2: @pytorchbot retest this please.\n<issue_comment>username_1: same failures.\n<issue_comment>username_2: Is this rebased on the latest master (you can also ask the bot to rebase)?\n<issue_comment>username_0: I rebased that yesterday ... also needed to resolve conflicts because 7 days ago upsample files were changed.\n<issue_comment>username_0: @username_2 do you have any idea about what could be this problem? or a way to debug that?\r\nalso it seems some jobs is taking a lot of time to build, for example, for some job the estimate time is 19h .. not sure if it the regular estimate ..\n<issue_comment>username_2: @username_0 If you can't reproduce it at home it seems hard to debug other than reading at the diffs again.  I can take a look on Monday, it's getting a bit late here.\n<issue_comment>username_2: Also, has anyone found a way to show the actual hardware used on the CI in detail?\n<issue_comment>username_0: @username_2 I am working in parallel on a paperspace environment .. it tooks a lot of time it is running with 8 cpu cores.\n<issue_comment>username_2: But have you ever been able to reproduce this issue on paperspace?\n<issue_comment>username_1: I can reproduce it locally: Arch Linux, CUDA 10.0, RTX2070 GPU. Given the CI failures, I think it should be reproducible for multiple CUDA and GPU versions. I just haven't worked on any CUDA code before, and am short on time, so I'd rather not dig too deep.\n<issue_comment>username_0: not yet .. my last building was with a my previous commit ... I will work again in this task in some minutes :)\n<issue_comment>username_1: @username_0 your previous commit had the same failure though (except for the less clear exception message), and it seems quite reproducible. So I think you'll see it now.\n<issue_comment>username_2: Ah, thanks.  @username_0, then I guess just compiling with \"DEBUG=1\" and stepping through the offending code with gdb may be the fastest way.\n<issue_comment>username_0: @username_2 thanks! I will try that!\n<issue_comment>username_0: it seems just UpSampleBicubic2d is using upsample_get_value_bounded (https://github.com/pytorch/pytorch/pull/19630/files#diff-5092da792c30694ee4adf0d0ae2a37c6R171) and upsample_increment_value_bounded (https://github.com/pytorch/pytorch/pull/19630/files#diff-5092da792c30694ee4adf0d0ae2a37c6R191)\r\n\r\nmaybe the problem could be inside one of these functions ... maybe related to the order of indexes (x, y) ...\r\n\r\nbut the problem seems to be related to cuda block/threads ... so not sure if it is really related to these functions.\n<issue_comment>username_2: The new code uses far more registers than the existing one. I verified that the both versions actually call the offending test case with `1024` in `blockDim`.  So it's very likely a register issue.\r\n\r\n`Existing` uses `64` registers, which seems to be optimal or my card:\r\n\r\n```\r\nptxas info    : 77696 bytes gmem, 72 bytes cmem[3]\r\nptxas info    : Compiling entry function '_Z23bicubic_interp2d_kerneliddb15THCDeviceTensorIdLi4Ei16DefaultPtrTraitsES1_' for 'sm_61'\r\nptxas info    : Function properties for _Z23bicubic_interp2d_kerneliddb15THCDeviceTensorIdLi4Ei16DefaultPtrTraitsES1_\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 64 registers, 432 bytes cmem[0], 24 bytes cmem[2]\r\n```\r\n\r\n`New` uses `124` registers, which is too much for my card:\r\n\r\n```\r\nptxas info    : 77696 bytes gmem, 72 bytes cmem[3]\r\nptxas info    : Compiling entry function '_ZN2at6native76_GLOBAL__N__52_tmpxft_0000626d_00000000_6_UpSampleBicubic2d_cpp1_ii_b4c1e1f328upsample_bicubic2d_out_frameElddbNS_20PackedTensorAccessorIdLm4ENS_16DefaultPtrTraitsElEES4_' for 'sm_61'\r\nptxas info    : Function properties for _ZN2at6native76_GLOBAL__N__52_tmpxft_0000626d_00000000_6_UpSampleBicubic2d_cpp1_ii_b4c1e1f328upsample_bicubic2d_out_frameElddbNS_20PackedTensorAccessorIdLm4ENS_16DefaultPtrTraitsElEES4_\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 124 registers, 496 bytes cmem[0], 24 bytes cmem[2]\r\n```\r\n\r\nWith `__launch_bounds__(1024)`, the code again uses `64` registers.\r\n\r\nIf you use `C10_LAUNCH_BOUNDS_1(1024)` **for both kernels**, the tests pass here.\r\n\r\n\r\nNow *why* is the regcount higher in the new code? It could be `PackedTensorAccessor`, it could be the fact that many instances of `int` have been changed to `int64_t`. :)\r\n\r\nYou could experiment or just use the launch bounds.  Other code in `native/cuda` seems to use lower bounds for `blockDim`, too.  1024 seems to be an outlier.\n<issue_comment>username_0: @username_2 \r\n\r\nit seems it worked locally! thank you so much!  I really appreciate that!\n<issue_comment>username_0: thanks @username_1 and @username_2 for all the help!\r\n\r\n@username_3 it is done for review!\n<issue_comment>username_3: CircleCI runs on AWS, and the I'm fairly sure we've got one of the g3 types, either g3.4xlarge or g3.8xlarge\n<issue_comment>username_3: cc @ngimel, if you want to take a look at this (I'm planning to do a review too)\n<issue_comment>username_0: @username_4 could it be addressed in another PR?\n<issue_comment>username_4: @username_0 Sure, it's definitely better to rename both cpu & gpu impls together in a separate PR. Just want to make sure we have the correct one after porting.\n<issue_comment>username_3: Just as a clarification: I'm OK with having changes which are improvements over the old kernels come in latter patches.\n<issue_comment>username_3: I finished reviewing one of the kernels, and did a cursory scan of the others. In general the patch looks like it's in good shape. I'd suggest that we fix up the major correctness issues (PackedTensorAccessor ref, and int32 versus int64 indexing) and anything small that is easy to fix (for example, double zeroing the array), and leave other improvements for follow ups. Should be ready to land soon!\n<issue_comment>username_0: thanks so much @username_3!\r\nI will let you know when it is ready again for a new review! thanks!\n<issue_comment>username_0: @username_2 @username_3 \r\nnot sure but the errors on CI seems to be related to jenkins ... it seems all these jobs that failed (for building) ran by 01h 01min ... not sure if it is a coincidence ...\n<issue_comment>username_1: @username_0 indeed it looks like all jobs were aborted at the same time. no obvious issues in the build log related to your code. Comparing e.g. the `cuda9-cudnn7` build with a successful one from another PR, it takes 1hr 14min there and your build is about at the place where the other one is after an hour.\r\n\r\nI suggest to just push a new commit to rebuild. Probably a temporary CI hiccup.\n<issue_comment>username_3: I accidentally rebooted Jenkins yesterday which is the likely cause, my apologies.\r\n\r\n@pytorchbot retest this please\n<issue_comment>username_0: @username_3 @username_2 @username_1 \r\n\r\nall tests passed except `pr/caffe2-py2-cuda9.0-cudnn7-windows-build`:\r\n \r\n```\r\n14:43:49 Build timed out (after 180 minutes). Marking the build as failed.\r\n14:43:49 Build was aborted\r\n14:43:49 [BFA] Scanning build for known causes...\r\n14:43:49 [BFA] No failure causes found\r\n14:43:49 [BFA] Done. 0s\r\n14:43:49 Finished: FAILURE\r\n```\r\n\r\nnot sure if this timeout means that the code now is slower.\r\n\r\nwhat do you think? do you have any suggestion?\n<issue_comment>username_3: Sometimes the Windows build flakes out like that. It didn't timeout while running a relevant test, so I judge it to be not your problem.\n<issue_comment>username_3: The launch bounds logic is wrong, but I acknowledge that this is a big patch already; just fix it in a follow up. I am going to go ahead and land this.\n<issue_comment>username_0: sounds good @username_3 thanks!","repo":"pytorch/pytorch","issue_id":436372857,"issue_number":19630}
{"question":"Is there a correlation between the duration of job runs and the occurrence of errors on the CI system, specifically related to Jenkins?","answer":"To investigate the potential correlation between the duration of job runs and CI system errors, further analysis and monitoring of the job execution times and error occurrences in the CI system, particularly related to Jenkins, would be necessary. It is essential to gather additional data and perform a statistical analysis to determine if there is a significant relationship between the duration of job runs and the presence of errors on the CI system.","id":167,"text":"@username_2 @username_3 \r\nnot sure but the errors on CI seems to be related to jenkins ... it seems all these jobs that failed (for building) ran by 01h 01min ... not sure if it is a coincidence ...","context":"<issue_start><issue_comment>Title: [WIP] UpSample GPU Porting \nusername_0: resolves #16158\n<issue_comment>username_1: Fewer errors than before, 4 instead of 8 with the same message:\r\n```\r\nRuntimeError: CUDA error: too many resources requested for launch\r\n```\r\nThis seems to be an existing problem: gh-8103.\r\n\r\n@username_2 could you have a look at this and tell us what you think?\n<issue_comment>username_2: Looking at it very briefly, the `Jetson TX` referenced in the issue only has 256 cores.\r\n\r\nSo while the issue looks the same, I'd probably not expect it on the CI platforms.  FWIW, some recent CI tests in other issues are green.\n<issue_comment>username_2: @pytorchbot retest this please.\n<issue_comment>username_1: same failures.\n<issue_comment>username_2: Is this rebased on the latest master (you can also ask the bot to rebase)?\n<issue_comment>username_0: I rebased that yesterday ... also needed to resolve conflicts because 7 days ago upsample files were changed.\n<issue_comment>username_0: @username_2 do you have any idea about what could be this problem? or a way to debug that?\r\nalso it seems some jobs is taking a lot of time to build, for example, for some job the estimate time is 19h .. not sure if it the regular estimate ..\n<issue_comment>username_2: @username_0 If you can't reproduce it at home it seems hard to debug other than reading at the diffs again.  I can take a look on Monday, it's getting a bit late here.\n<issue_comment>username_2: Also, has anyone found a way to show the actual hardware used on the CI in detail?\n<issue_comment>username_0: @username_2 I am working in parallel on a paperspace environment .. it tooks a lot of time it is running with 8 cpu cores.\n<issue_comment>username_2: But have you ever been able to reproduce this issue on paperspace?\n<issue_comment>username_1: I can reproduce it locally: Arch Linux, CUDA 10.0, RTX2070 GPU. Given the CI failures, I think it should be reproducible for multiple CUDA and GPU versions. I just haven't worked on any CUDA code before, and am short on time, so I'd rather not dig too deep.\n<issue_comment>username_0: not yet .. my last building was with a my previous commit ... I will work again in this task in some minutes :)\n<issue_comment>username_1: @username_0 your previous commit had the same failure though (except for the less clear exception message), and it seems quite reproducible. So I think you'll see it now.\n<issue_comment>username_2: Ah, thanks.  @username_0, then I guess just compiling with \"DEBUG=1\" and stepping through the offending code with gdb may be the fastest way.\n<issue_comment>username_0: @username_2 thanks! I will try that!\n<issue_comment>username_0: it seems just UpSampleBicubic2d is using upsample_get_value_bounded (https://github.com/pytorch/pytorch/pull/19630/files#diff-5092da792c30694ee4adf0d0ae2a37c6R171) and upsample_increment_value_bounded (https://github.com/pytorch/pytorch/pull/19630/files#diff-5092da792c30694ee4adf0d0ae2a37c6R191)\r\n\r\nmaybe the problem could be inside one of these functions ... maybe related to the order of indexes (x, y) ...\r\n\r\nbut the problem seems to be related to cuda block/threads ... so not sure if it is really related to these functions.\n<issue_comment>username_2: The new code uses far more registers than the existing one. I verified that the both versions actually call the offending test case with `1024` in `blockDim`.  So it's very likely a register issue.\r\n\r\n`Existing` uses `64` registers, which seems to be optimal or my card:\r\n\r\n```\r\nptxas info    : 77696 bytes gmem, 72 bytes cmem[3]\r\nptxas info    : Compiling entry function '_Z23bicubic_interp2d_kerneliddb15THCDeviceTensorIdLi4Ei16DefaultPtrTraitsES1_' for 'sm_61'\r\nptxas info    : Function properties for _Z23bicubic_interp2d_kerneliddb15THCDeviceTensorIdLi4Ei16DefaultPtrTraitsES1_\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 64 registers, 432 bytes cmem[0], 24 bytes cmem[2]\r\n```\r\n\r\n`New` uses `124` registers, which is too much for my card:\r\n\r\n```\r\nptxas info    : 77696 bytes gmem, 72 bytes cmem[3]\r\nptxas info    : Compiling entry function '_ZN2at6native76_GLOBAL__N__52_tmpxft_0000626d_00000000_6_UpSampleBicubic2d_cpp1_ii_b4c1e1f328upsample_bicubic2d_out_frameElddbNS_20PackedTensorAccessorIdLm4ENS_16DefaultPtrTraitsElEES4_' for 'sm_61'\r\nptxas info    : Function properties for _ZN2at6native76_GLOBAL__N__52_tmpxft_0000626d_00000000_6_UpSampleBicubic2d_cpp1_ii_b4c1e1f328upsample_bicubic2d_out_frameElddbNS_20PackedTensorAccessorIdLm4ENS_16DefaultPtrTraitsElEES4_\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 124 registers, 496 bytes cmem[0], 24 bytes cmem[2]\r\n```\r\n\r\nWith `__launch_bounds__(1024)`, the code again uses `64` registers.\r\n\r\nIf you use `C10_LAUNCH_BOUNDS_1(1024)` **for both kernels**, the tests pass here.\r\n\r\n\r\nNow *why* is the regcount higher in the new code? It could be `PackedTensorAccessor`, it could be the fact that many instances of `int` have been changed to `int64_t`. :)\r\n\r\nYou could experiment or just use the launch bounds.  Other code in `native/cuda` seems to use lower bounds for `blockDim`, too.  1024 seems to be an outlier.\n<issue_comment>username_0: @username_2 \r\n\r\nit seems it worked locally! thank you so much!  I really appreciate that!\n<issue_comment>username_0: thanks @username_1 and @username_2 for all the help!\r\n\r\n@username_3 it is done for review!\n<issue_comment>username_3: CircleCI runs on AWS, and the I'm fairly sure we've got one of the g3 types, either g3.4xlarge or g3.8xlarge\n<issue_comment>username_3: cc @ngimel, if you want to take a look at this (I'm planning to do a review too)\n<issue_comment>username_0: @username_4 could it be addressed in another PR?\n<issue_comment>username_4: @username_0 Sure, it's definitely better to rename both cpu & gpu impls together in a separate PR. Just want to make sure we have the correct one after porting.\n<issue_comment>username_3: Just as a clarification: I'm OK with having changes which are improvements over the old kernels come in latter patches.\n<issue_comment>username_3: I finished reviewing one of the kernels, and did a cursory scan of the others. In general the patch looks like it's in good shape. I'd suggest that we fix up the major correctness issues (PackedTensorAccessor ref, and int32 versus int64 indexing) and anything small that is easy to fix (for example, double zeroing the array), and leave other improvements for follow ups. Should be ready to land soon!\n<issue_comment>username_0: thanks so much @username_3!\r\nI will let you know when it is ready again for a new review! thanks!\n<issue_comment>username_0: @username_2 @username_3 \r\nnot sure but the errors on CI seems to be related to jenkins ... it seems all these jobs that failed (for building) ran by 01h 01min ... not sure if it is a coincidence ...\n<issue_comment>username_1: @username_0 indeed it looks like all jobs were aborted at the same time. no obvious issues in the build log related to your code. Comparing e.g. the `cuda9-cudnn7` build with a successful one from another PR, it takes 1hr 14min there and your build is about at the place where the other one is after an hour.\r\n\r\nI suggest to just push a new commit to rebuild. Probably a temporary CI hiccup.\n<issue_comment>username_3: I accidentally rebooted Jenkins yesterday which is the likely cause, my apologies.\r\n\r\n@pytorchbot retest this please\n<issue_comment>username_0: @username_3 @username_2 @username_1 \r\n\r\nall tests passed except `pr/caffe2-py2-cuda9.0-cudnn7-windows-build`:\r\n \r\n```\r\n14:43:49 Build timed out (after 180 minutes). Marking the build as failed.\r\n14:43:49 Build was aborted\r\n14:43:49 [BFA] Scanning build for known causes...\r\n14:43:49 [BFA] No failure causes found\r\n14:43:49 [BFA] Done. 0s\r\n14:43:49 Finished: FAILURE\r\n```\r\n\r\nnot sure if this timeout means that the code now is slower.\r\n\r\nwhat do you think? do you have any suggestion?\n<issue_comment>username_3: Sometimes the Windows build flakes out like that. It didn't timeout while running a relevant test, so I judge it to be not your problem.\n<issue_comment>username_3: The launch bounds logic is wrong, but I acknowledge that this is a big patch already; just fix it in a follow up. I am going to go ahead and land this.\n<issue_comment>username_0: sounds good @username_3 thanks!","repo":"pytorch/pytorch","issue_id":436372857,"issue_number":19630}
{"question":"Why was the user advised to push a new commit to trigger a rebuild of their build?","answer":"The user was advised to push a new commit to trigger a rebuild because there were no issues related to their code in the build log, and the significant time difference compared to a successful build from another PR indicated a possible temporary CI hiccup.","id":168,"text":"@username_0 indeed it looks like all jobs were aborted at the same time. no obvious issues in the build log related to your code. Comparing e.g. the `cuda9-cudnn7` build with a successful one from another PR, it takes 1hr 14min there and your build is about at the place where the other one is after an hour.\r\n\r\nI suggest to just push a new commit to rebuild. Probably a temporary CI hiccup.","context":"<issue_start><issue_comment>Title: [WIP] UpSample GPU Porting \nusername_0: resolves #16158\n<issue_comment>username_1: Fewer errors than before, 4 instead of 8 with the same message:\r\n```\r\nRuntimeError: CUDA error: too many resources requested for launch\r\n```\r\nThis seems to be an existing problem: gh-8103.\r\n\r\n@username_2 could you have a look at this and tell us what you think?\n<issue_comment>username_2: Looking at it very briefly, the `Jetson TX` referenced in the issue only has 256 cores.\r\n\r\nSo while the issue looks the same, I'd probably not expect it on the CI platforms.  FWIW, some recent CI tests in other issues are green.\n<issue_comment>username_2: @pytorchbot retest this please.\n<issue_comment>username_1: same failures.\n<issue_comment>username_2: Is this rebased on the latest master (you can also ask the bot to rebase)?\n<issue_comment>username_0: I rebased that yesterday ... also needed to resolve conflicts because 7 days ago upsample files were changed.\n<issue_comment>username_0: @username_2 do you have any idea about what could be this problem? or a way to debug that?\r\nalso it seems some jobs is taking a lot of time to build, for example, for some job the estimate time is 19h .. not sure if it the regular estimate ..\n<issue_comment>username_2: @username_0 If you can't reproduce it at home it seems hard to debug other than reading at the diffs again.  I can take a look on Monday, it's getting a bit late here.\n<issue_comment>username_2: Also, has anyone found a way to show the actual hardware used on the CI in detail?\n<issue_comment>username_0: @username_2 I am working in parallel on a paperspace environment .. it tooks a lot of time it is running with 8 cpu cores.\n<issue_comment>username_2: But have you ever been able to reproduce this issue on paperspace?\n<issue_comment>username_1: I can reproduce it locally: Arch Linux, CUDA 10.0, RTX2070 GPU. Given the CI failures, I think it should be reproducible for multiple CUDA and GPU versions. I just haven't worked on any CUDA code before, and am short on time, so I'd rather not dig too deep.\n<issue_comment>username_0: not yet .. my last building was with a my previous commit ... I will work again in this task in some minutes :)\n<issue_comment>username_1: @username_0 your previous commit had the same failure though (except for the less clear exception message), and it seems quite reproducible. So I think you'll see it now.\n<issue_comment>username_2: Ah, thanks.  @username_0, then I guess just compiling with \"DEBUG=1\" and stepping through the offending code with gdb may be the fastest way.\n<issue_comment>username_0: @username_2 thanks! I will try that!\n<issue_comment>username_0: it seems just UpSampleBicubic2d is using upsample_get_value_bounded (https://github.com/pytorch/pytorch/pull/19630/files#diff-5092da792c30694ee4adf0d0ae2a37c6R171) and upsample_increment_value_bounded (https://github.com/pytorch/pytorch/pull/19630/files#diff-5092da792c30694ee4adf0d0ae2a37c6R191)\r\n\r\nmaybe the problem could be inside one of these functions ... maybe related to the order of indexes (x, y) ...\r\n\r\nbut the problem seems to be related to cuda block/threads ... so not sure if it is really related to these functions.\n<issue_comment>username_2: The new code uses far more registers than the existing one. I verified that the both versions actually call the offending test case with `1024` in `blockDim`.  So it's very likely a register issue.\r\n\r\n`Existing` uses `64` registers, which seems to be optimal or my card:\r\n\r\n```\r\nptxas info    : 77696 bytes gmem, 72 bytes cmem[3]\r\nptxas info    : Compiling entry function '_Z23bicubic_interp2d_kerneliddb15THCDeviceTensorIdLi4Ei16DefaultPtrTraitsES1_' for 'sm_61'\r\nptxas info    : Function properties for _Z23bicubic_interp2d_kerneliddb15THCDeviceTensorIdLi4Ei16DefaultPtrTraitsES1_\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 64 registers, 432 bytes cmem[0], 24 bytes cmem[2]\r\n```\r\n\r\n`New` uses `124` registers, which is too much for my card:\r\n\r\n```\r\nptxas info    : 77696 bytes gmem, 72 bytes cmem[3]\r\nptxas info    : Compiling entry function '_ZN2at6native76_GLOBAL__N__52_tmpxft_0000626d_00000000_6_UpSampleBicubic2d_cpp1_ii_b4c1e1f328upsample_bicubic2d_out_frameElddbNS_20PackedTensorAccessorIdLm4ENS_16DefaultPtrTraitsElEES4_' for 'sm_61'\r\nptxas info    : Function properties for _ZN2at6native76_GLOBAL__N__52_tmpxft_0000626d_00000000_6_UpSampleBicubic2d_cpp1_ii_b4c1e1f328upsample_bicubic2d_out_frameElddbNS_20PackedTensorAccessorIdLm4ENS_16DefaultPtrTraitsElEES4_\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 124 registers, 496 bytes cmem[0], 24 bytes cmem[2]\r\n```\r\n\r\nWith `__launch_bounds__(1024)`, the code again uses `64` registers.\r\n\r\nIf you use `C10_LAUNCH_BOUNDS_1(1024)` **for both kernels**, the tests pass here.\r\n\r\n\r\nNow *why* is the regcount higher in the new code? It could be `PackedTensorAccessor`, it could be the fact that many instances of `int` have been changed to `int64_t`. :)\r\n\r\nYou could experiment or just use the launch bounds.  Other code in `native/cuda` seems to use lower bounds for `blockDim`, too.  1024 seems to be an outlier.\n<issue_comment>username_0: @username_2 \r\n\r\nit seems it worked locally! thank you so much!  I really appreciate that!\n<issue_comment>username_0: thanks @username_1 and @username_2 for all the help!\r\n\r\n@username_3 it is done for review!\n<issue_comment>username_3: CircleCI runs on AWS, and the I'm fairly sure we've got one of the g3 types, either g3.4xlarge or g3.8xlarge\n<issue_comment>username_3: cc @ngimel, if you want to take a look at this (I'm planning to do a review too)\n<issue_comment>username_0: @username_4 could it be addressed in another PR?\n<issue_comment>username_4: @username_0 Sure, it's definitely better to rename both cpu & gpu impls together in a separate PR. Just want to make sure we have the correct one after porting.\n<issue_comment>username_3: Just as a clarification: I'm OK with having changes which are improvements over the old kernels come in latter patches.\n<issue_comment>username_3: I finished reviewing one of the kernels, and did a cursory scan of the others. In general the patch looks like it's in good shape. I'd suggest that we fix up the major correctness issues (PackedTensorAccessor ref, and int32 versus int64 indexing) and anything small that is easy to fix (for example, double zeroing the array), and leave other improvements for follow ups. Should be ready to land soon!\n<issue_comment>username_0: thanks so much @username_3!\r\nI will let you know when it is ready again for a new review! thanks!\n<issue_comment>username_0: @username_2 @username_3 \r\nnot sure but the errors on CI seems to be related to jenkins ... it seems all these jobs that failed (for building) ran by 01h 01min ... not sure if it is a coincidence ...\n<issue_comment>username_1: @username_0 indeed it looks like all jobs were aborted at the same time. no obvious issues in the build log related to your code. Comparing e.g. the `cuda9-cudnn7` build with a successful one from another PR, it takes 1hr 14min there and your build is about at the place where the other one is after an hour.\r\n\r\nI suggest to just push a new commit to rebuild. Probably a temporary CI hiccup.\n<issue_comment>username_3: I accidentally rebooted Jenkins yesterday which is the likely cause, my apologies.\r\n\r\n@pytorchbot retest this please\n<issue_comment>username_0: @username_3 @username_2 @username_1 \r\n\r\nall tests passed except `pr/caffe2-py2-cuda9.0-cudnn7-windows-build`:\r\n \r\n```\r\n14:43:49 Build timed out (after 180 minutes). Marking the build as failed.\r\n14:43:49 Build was aborted\r\n14:43:49 [BFA] Scanning build for known causes...\r\n14:43:49 [BFA] No failure causes found\r\n14:43:49 [BFA] Done. 0s\r\n14:43:49 Finished: FAILURE\r\n```\r\n\r\nnot sure if this timeout means that the code now is slower.\r\n\r\nwhat do you think? do you have any suggestion?\n<issue_comment>username_3: Sometimes the Windows build flakes out like that. It didn't timeout while running a relevant test, so I judge it to be not your problem.\n<issue_comment>username_3: The launch bounds logic is wrong, but I acknowledge that this is a big patch already; just fix it in a follow up. I am going to go ahead and land this.\n<issue_comment>username_0: sounds good @username_3 thanks!","repo":"pytorch/pytorch","issue_id":436372857,"issue_number":19630}
{"question":"What was the outcome of the build process for `pr/caffe2-py2-cuda9.0-cudnn7-windows-build` in the provided text?","answer":"The build for `pr/caffe2-py2-cuda9.0-cudnn7-windows-build` timed out after 180 minutes and was marked as failed.","id":170,"text":"@username_3 @username_2 @username_1 \r\n\r\nall tests passed except `pr/caffe2-py2-cuda9.0-cudnn7-windows-build`:\r\n \r\n```\r\n14:43:49 Build timed out (after 180 minutes). Marking the build as failed.\r\n14:43:49 Build was aborted\r\n14:43:49 [BFA] Scanning build for known causes...\r\n14:43:49 [BFA] No failure causes found\r\n14:43:49 [BFA] Done. 0s\r\n14:43:49 Finished: FAILURE\r\n```\r\n\r\nnot sure if this timeout means that the code now is slower.\r\n\r\nwhat do you think? do you have any suggestion?","context":"<issue_start><issue_comment>Title: [WIP] UpSample GPU Porting \nusername_0: resolves #16158\n<issue_comment>username_1: Fewer errors than before, 4 instead of 8 with the same message:\r\n```\r\nRuntimeError: CUDA error: too many resources requested for launch\r\n```\r\nThis seems to be an existing problem: gh-8103.\r\n\r\n@username_2 could you have a look at this and tell us what you think?\n<issue_comment>username_2: Looking at it very briefly, the `Jetson TX` referenced in the issue only has 256 cores.\r\n\r\nSo while the issue looks the same, I'd probably not expect it on the CI platforms.  FWIW, some recent CI tests in other issues are green.\n<issue_comment>username_2: @pytorchbot retest this please.\n<issue_comment>username_1: same failures.\n<issue_comment>username_2: Is this rebased on the latest master (you can also ask the bot to rebase)?\n<issue_comment>username_0: I rebased that yesterday ... also needed to resolve conflicts because 7 days ago upsample files were changed.\n<issue_comment>username_0: @username_2 do you have any idea about what could be this problem? or a way to debug that?\r\nalso it seems some jobs is taking a lot of time to build, for example, for some job the estimate time is 19h .. not sure if it the regular estimate ..\n<issue_comment>username_2: @username_0 If you can't reproduce it at home it seems hard to debug other than reading at the diffs again.  I can take a look on Monday, it's getting a bit late here.\n<issue_comment>username_2: Also, has anyone found a way to show the actual hardware used on the CI in detail?\n<issue_comment>username_0: @username_2 I am working in parallel on a paperspace environment .. it tooks a lot of time it is running with 8 cpu cores.\n<issue_comment>username_2: But have you ever been able to reproduce this issue on paperspace?\n<issue_comment>username_1: I can reproduce it locally: Arch Linux, CUDA 10.0, RTX2070 GPU. Given the CI failures, I think it should be reproducible for multiple CUDA and GPU versions. I just haven't worked on any CUDA code before, and am short on time, so I'd rather not dig too deep.\n<issue_comment>username_0: not yet .. my last building was with a my previous commit ... I will work again in this task in some minutes :)\n<issue_comment>username_1: @username_0 your previous commit had the same failure though (except for the less clear exception message), and it seems quite reproducible. So I think you'll see it now.\n<issue_comment>username_2: Ah, thanks.  @username_0, then I guess just compiling with \"DEBUG=1\" and stepping through the offending code with gdb may be the fastest way.\n<issue_comment>username_0: @username_2 thanks! I will try that!\n<issue_comment>username_0: it seems just UpSampleBicubic2d is using upsample_get_value_bounded (https://github.com/pytorch/pytorch/pull/19630/files#diff-5092da792c30694ee4adf0d0ae2a37c6R171) and upsample_increment_value_bounded (https://github.com/pytorch/pytorch/pull/19630/files#diff-5092da792c30694ee4adf0d0ae2a37c6R191)\r\n\r\nmaybe the problem could be inside one of these functions ... maybe related to the order of indexes (x, y) ...\r\n\r\nbut the problem seems to be related to cuda block/threads ... so not sure if it is really related to these functions.\n<issue_comment>username_2: The new code uses far more registers than the existing one. I verified that the both versions actually call the offending test case with `1024` in `blockDim`.  So it's very likely a register issue.\r\n\r\n`Existing` uses `64` registers, which seems to be optimal or my card:\r\n\r\n```\r\nptxas info    : 77696 bytes gmem, 72 bytes cmem[3]\r\nptxas info    : Compiling entry function '_Z23bicubic_interp2d_kerneliddb15THCDeviceTensorIdLi4Ei16DefaultPtrTraitsES1_' for 'sm_61'\r\nptxas info    : Function properties for _Z23bicubic_interp2d_kerneliddb15THCDeviceTensorIdLi4Ei16DefaultPtrTraitsES1_\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 64 registers, 432 bytes cmem[0], 24 bytes cmem[2]\r\n```\r\n\r\n`New` uses `124` registers, which is too much for my card:\r\n\r\n```\r\nptxas info    : 77696 bytes gmem, 72 bytes cmem[3]\r\nptxas info    : Compiling entry function '_ZN2at6native76_GLOBAL__N__52_tmpxft_0000626d_00000000_6_UpSampleBicubic2d_cpp1_ii_b4c1e1f328upsample_bicubic2d_out_frameElddbNS_20PackedTensorAccessorIdLm4ENS_16DefaultPtrTraitsElEES4_' for 'sm_61'\r\nptxas info    : Function properties for _ZN2at6native76_GLOBAL__N__52_tmpxft_0000626d_00000000_6_UpSampleBicubic2d_cpp1_ii_b4c1e1f328upsample_bicubic2d_out_frameElddbNS_20PackedTensorAccessorIdLm4ENS_16DefaultPtrTraitsElEES4_\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 124 registers, 496 bytes cmem[0], 24 bytes cmem[2]\r\n```\r\n\r\nWith `__launch_bounds__(1024)`, the code again uses `64` registers.\r\n\r\nIf you use `C10_LAUNCH_BOUNDS_1(1024)` **for both kernels**, the tests pass here.\r\n\r\n\r\nNow *why* is the regcount higher in the new code? It could be `PackedTensorAccessor`, it could be the fact that many instances of `int` have been changed to `int64_t`. :)\r\n\r\nYou could experiment or just use the launch bounds.  Other code in `native/cuda` seems to use lower bounds for `blockDim`, too.  1024 seems to be an outlier.\n<issue_comment>username_0: @username_2 \r\n\r\nit seems it worked locally! thank you so much!  I really appreciate that!\n<issue_comment>username_0: thanks @username_1 and @username_2 for all the help!\r\n\r\n@username_3 it is done for review!\n<issue_comment>username_3: CircleCI runs on AWS, and the I'm fairly sure we've got one of the g3 types, either g3.4xlarge or g3.8xlarge\n<issue_comment>username_3: cc @ngimel, if you want to take a look at this (I'm planning to do a review too)\n<issue_comment>username_0: @username_4 could it be addressed in another PR?\n<issue_comment>username_4: @username_0 Sure, it's definitely better to rename both cpu & gpu impls together in a separate PR. Just want to make sure we have the correct one after porting.\n<issue_comment>username_3: Just as a clarification: I'm OK with having changes which are improvements over the old kernels come in latter patches.\n<issue_comment>username_3: I finished reviewing one of the kernels, and did a cursory scan of the others. In general the patch looks like it's in good shape. I'd suggest that we fix up the major correctness issues (PackedTensorAccessor ref, and int32 versus int64 indexing) and anything small that is easy to fix (for example, double zeroing the array), and leave other improvements for follow ups. Should be ready to land soon!\n<issue_comment>username_0: thanks so much @username_3!\r\nI will let you know when it is ready again for a new review! thanks!\n<issue_comment>username_0: @username_2 @username_3 \r\nnot sure but the errors on CI seems to be related to jenkins ... it seems all these jobs that failed (for building) ran by 01h 01min ... not sure if it is a coincidence ...\n<issue_comment>username_1: @username_0 indeed it looks like all jobs were aborted at the same time. no obvious issues in the build log related to your code. Comparing e.g. the `cuda9-cudnn7` build with a successful one from another PR, it takes 1hr 14min there and your build is about at the place where the other one is after an hour.\r\n\r\nI suggest to just push a new commit to rebuild. Probably a temporary CI hiccup.\n<issue_comment>username_3: I accidentally rebooted Jenkins yesterday which is the likely cause, my apologies.\r\n\r\n@pytorchbot retest this please\n<issue_comment>username_0: @username_3 @username_2 @username_1 \r\n\r\nall tests passed except `pr/caffe2-py2-cuda9.0-cudnn7-windows-build`:\r\n \r\n```\r\n14:43:49 Build timed out (after 180 minutes). Marking the build as failed.\r\n14:43:49 Build was aborted\r\n14:43:49 [BFA] Scanning build for known causes...\r\n14:43:49 [BFA] No failure causes found\r\n14:43:49 [BFA] Done. 0s\r\n14:43:49 Finished: FAILURE\r\n```\r\n\r\nnot sure if this timeout means that the code now is slower.\r\n\r\nwhat do you think? do you have any suggestion?\n<issue_comment>username_3: Sometimes the Windows build flakes out like that. It didn't timeout while running a relevant test, so I judge it to be not your problem.\n<issue_comment>username_3: The launch bounds logic is wrong, but I acknowledge that this is a big patch already; just fix it in a follow up. I am going to go ahead and land this.\n<issue_comment>username_0: sounds good @username_3 thanks!","repo":"pytorch/pytorch","issue_id":436372857,"issue_number":19630}
{"question":"What issue does the author mention with the launch bounds logic in the text?","answer":"The launch bounds logic is wrong.","id":172,"text":"The launch bounds logic is wrong, but I acknowledge that this is a big patch already; just fix it in a follow up. I am going to go ahead and land this.","context":"<issue_start><issue_comment>Title: [WIP] UpSample GPU Porting \nusername_0: resolves #16158\n<issue_comment>username_1: Fewer errors than before, 4 instead of 8 with the same message:\r\n```\r\nRuntimeError: CUDA error: too many resources requested for launch\r\n```\r\nThis seems to be an existing problem: gh-8103.\r\n\r\n@username_2 could you have a look at this and tell us what you think?\n<issue_comment>username_2: Looking at it very briefly, the `Jetson TX` referenced in the issue only has 256 cores.\r\n\r\nSo while the issue looks the same, I'd probably not expect it on the CI platforms.  FWIW, some recent CI tests in other issues are green.\n<issue_comment>username_2: @pytorchbot retest this please.\n<issue_comment>username_1: same failures.\n<issue_comment>username_2: Is this rebased on the latest master (you can also ask the bot to rebase)?\n<issue_comment>username_0: I rebased that yesterday ... also needed to resolve conflicts because 7 days ago upsample files were changed.\n<issue_comment>username_0: @username_2 do you have any idea about what could be this problem? or a way to debug that?\r\nalso it seems some jobs is taking a lot of time to build, for example, for some job the estimate time is 19h .. not sure if it the regular estimate ..\n<issue_comment>username_2: @username_0 If you can't reproduce it at home it seems hard to debug other than reading at the diffs again.  I can take a look on Monday, it's getting a bit late here.\n<issue_comment>username_2: Also, has anyone found a way to show the actual hardware used on the CI in detail?\n<issue_comment>username_0: @username_2 I am working in parallel on a paperspace environment .. it tooks a lot of time it is running with 8 cpu cores.\n<issue_comment>username_2: But have you ever been able to reproduce this issue on paperspace?\n<issue_comment>username_1: I can reproduce it locally: Arch Linux, CUDA 10.0, RTX2070 GPU. Given the CI failures, I think it should be reproducible for multiple CUDA and GPU versions. I just haven't worked on any CUDA code before, and am short on time, so I'd rather not dig too deep.\n<issue_comment>username_0: not yet .. my last building was with a my previous commit ... I will work again in this task in some minutes :)\n<issue_comment>username_1: @username_0 your previous commit had the same failure though (except for the less clear exception message), and it seems quite reproducible. So I think you'll see it now.\n<issue_comment>username_2: Ah, thanks.  @username_0, then I guess just compiling with \"DEBUG=1\" and stepping through the offending code with gdb may be the fastest way.\n<issue_comment>username_0: @username_2 thanks! I will try that!\n<issue_comment>username_0: it seems just UpSampleBicubic2d is using upsample_get_value_bounded (https://github.com/pytorch/pytorch/pull/19630/files#diff-5092da792c30694ee4adf0d0ae2a37c6R171) and upsample_increment_value_bounded (https://github.com/pytorch/pytorch/pull/19630/files#diff-5092da792c30694ee4adf0d0ae2a37c6R191)\r\n\r\nmaybe the problem could be inside one of these functions ... maybe related to the order of indexes (x, y) ...\r\n\r\nbut the problem seems to be related to cuda block/threads ... so not sure if it is really related to these functions.\n<issue_comment>username_2: The new code uses far more registers than the existing one. I verified that the both versions actually call the offending test case with `1024` in `blockDim`.  So it's very likely a register issue.\r\n\r\n`Existing` uses `64` registers, which seems to be optimal or my card:\r\n\r\n```\r\nptxas info    : 77696 bytes gmem, 72 bytes cmem[3]\r\nptxas info    : Compiling entry function '_Z23bicubic_interp2d_kerneliddb15THCDeviceTensorIdLi4Ei16DefaultPtrTraitsES1_' for 'sm_61'\r\nptxas info    : Function properties for _Z23bicubic_interp2d_kerneliddb15THCDeviceTensorIdLi4Ei16DefaultPtrTraitsES1_\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 64 registers, 432 bytes cmem[0], 24 bytes cmem[2]\r\n```\r\n\r\n`New` uses `124` registers, which is too much for my card:\r\n\r\n```\r\nptxas info    : 77696 bytes gmem, 72 bytes cmem[3]\r\nptxas info    : Compiling entry function '_ZN2at6native76_GLOBAL__N__52_tmpxft_0000626d_00000000_6_UpSampleBicubic2d_cpp1_ii_b4c1e1f328upsample_bicubic2d_out_frameElddbNS_20PackedTensorAccessorIdLm4ENS_16DefaultPtrTraitsElEES4_' for 'sm_61'\r\nptxas info    : Function properties for _ZN2at6native76_GLOBAL__N__52_tmpxft_0000626d_00000000_6_UpSampleBicubic2d_cpp1_ii_b4c1e1f328upsample_bicubic2d_out_frameElddbNS_20PackedTensorAccessorIdLm4ENS_16DefaultPtrTraitsElEES4_\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 124 registers, 496 bytes cmem[0], 24 bytes cmem[2]\r\n```\r\n\r\nWith `__launch_bounds__(1024)`, the code again uses `64` registers.\r\n\r\nIf you use `C10_LAUNCH_BOUNDS_1(1024)` **for both kernels**, the tests pass here.\r\n\r\n\r\nNow *why* is the regcount higher in the new code? It could be `PackedTensorAccessor`, it could be the fact that many instances of `int` have been changed to `int64_t`. :)\r\n\r\nYou could experiment or just use the launch bounds.  Other code in `native/cuda` seems to use lower bounds for `blockDim`, too.  1024 seems to be an outlier.\n<issue_comment>username_0: @username_2 \r\n\r\nit seems it worked locally! thank you so much!  I really appreciate that!\n<issue_comment>username_0: thanks @username_1 and @username_2 for all the help!\r\n\r\n@username_3 it is done for review!\n<issue_comment>username_3: CircleCI runs on AWS, and the I'm fairly sure we've got one of the g3 types, either g3.4xlarge or g3.8xlarge\n<issue_comment>username_3: cc @ngimel, if you want to take a look at this (I'm planning to do a review too)\n<issue_comment>username_0: @username_4 could it be addressed in another PR?\n<issue_comment>username_4: @username_0 Sure, it's definitely better to rename both cpu & gpu impls together in a separate PR. Just want to make sure we have the correct one after porting.\n<issue_comment>username_3: Just as a clarification: I'm OK with having changes which are improvements over the old kernels come in latter patches.\n<issue_comment>username_3: I finished reviewing one of the kernels, and did a cursory scan of the others. In general the patch looks like it's in good shape. I'd suggest that we fix up the major correctness issues (PackedTensorAccessor ref, and int32 versus int64 indexing) and anything small that is easy to fix (for example, double zeroing the array), and leave other improvements for follow ups. Should be ready to land soon!\n<issue_comment>username_0: thanks so much @username_3!\r\nI will let you know when it is ready again for a new review! thanks!\n<issue_comment>username_0: @username_2 @username_3 \r\nnot sure but the errors on CI seems to be related to jenkins ... it seems all these jobs that failed (for building) ran by 01h 01min ... not sure if it is a coincidence ...\n<issue_comment>username_1: @username_0 indeed it looks like all jobs were aborted at the same time. no obvious issues in the build log related to your code. Comparing e.g. the `cuda9-cudnn7` build with a successful one from another PR, it takes 1hr 14min there and your build is about at the place where the other one is after an hour.\r\n\r\nI suggest to just push a new commit to rebuild. Probably a temporary CI hiccup.\n<issue_comment>username_3: I accidentally rebooted Jenkins yesterday which is the likely cause, my apologies.\r\n\r\n@pytorchbot retest this please\n<issue_comment>username_0: @username_3 @username_2 @username_1 \r\n\r\nall tests passed except `pr/caffe2-py2-cuda9.0-cudnn7-windows-build`:\r\n \r\n```\r\n14:43:49 Build timed out (after 180 minutes). Marking the build as failed.\r\n14:43:49 Build was aborted\r\n14:43:49 [BFA] Scanning build for known causes...\r\n14:43:49 [BFA] No failure causes found\r\n14:43:49 [BFA] Done. 0s\r\n14:43:49 Finished: FAILURE\r\n```\r\n\r\nnot sure if this timeout means that the code now is slower.\r\n\r\nwhat do you think? do you have any suggestion?\n<issue_comment>username_3: Sometimes the Windows build flakes out like that. It didn't timeout while running a relevant test, so I judge it to be not your problem.\n<issue_comment>username_3: The launch bounds logic is wrong, but I acknowledge that this is a big patch already; just fix it in a follow up. I am going to go ahead and land this.\n<issue_comment>username_0: sounds good @username_3 thanks!","repo":"pytorch/pytorch","issue_id":436372857,"issue_number":19630}
{"question":"Why is the performance of the TorchScript handling the computation in the Adam Optimizer worse than using the native PyTorch operations on CUDA?","answer":"The performance of TorchScript handling the computation in the Adam Optimizer is worse than using the native PyTorch operations on CUDA because TorchScript does not fuse all operations into a single CUDA. This results in TorchScript generating two separate CUDA kernels instead of a single fused kernel, leading to decreased performance.","id":174,"text":"The performance of a TorchScript that handles the computation in the [Adam Optimizer](https://github.com/pytorch/pytorch/blob/master/torch/optim/adam.py) is worse than using the native PyTorch operations on CUDA. \r\n\r\n### Background\r\nI am trying to improve the GPU performance of the [Adam Optimizer](https://github.com/pytorch/pytorch/blob/master/torch/optim/adam.py) since it is the bottleneck in one of our workflows. \r\n\r\n### Results\r\nI have written a standalone script that compares PyTorch native tensor operations, TorchScript, and Numba (three runs each):\r\n```\r\nnative :  0.0506\r\nnative :  0.0322\r\nnative :  0.0322\r\nTScript:  0.1917\r\nTScript:  0.0442\r\nTScript:  0.0442\r\nnumba  :  0.1638\r\nnumba  :  0.0145\r\nnumba  :  0.0143\r\n```\r\nI suspect the problem is that TorchScript does not fuse all operations into a single CUDA. It generates two CUDA kernels (see the `PYTORCH_FUSION_DEBUG` output below)\r\n\r\n### Standalone Script\r\n```python\r\nimport math\r\nimport torch\r\nimport time\r\nimport random\r\nimport sys\r\n\r\ndef kernel_native(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n    exp_avg.mul_(beta1).add_(1 - beta1, grad)\r\n    exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\r\n    denom = exp_avg_sq.sqrt().add_(eps)\r\n    param.addcdiv_(-step_size, exp_avg, denom)   \r\n\r\n@torch.jit.script\r\ndef kernel_jit_script(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n    exp_avg.mul_(beta1)\r\n    exp_avg.add_((1 - beta1) + grad)\r\n    exp_avg_sq.mul_(beta2)\r\n    exp_avg_sq.add_((1 - beta2)*(grad*grad))\r\n    denom = exp_avg_sq.sqrt().add_(eps)\r\n    param += -step_size * (exp_avg / denom)\r\n\r\nfrom numba import cuda\r\n@cuda.jit\r\ndef kernel_numba_cuda(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    i = cuda.grid(1)\r\n    if i > grad.size:\r\n        return\r\n\r\n    exp_avg[i] = exp_avg[i] * beta1 + (1 - beta1) + grad[i]\r\n    exp_avg_sq[i] = exp_avg_sq[i] * beta2 + (1 - beta2) *grad[i]*grad[i]\r\n    denom = math.sqrt(exp_avg_sq[i]) + eps\r\n    param[i] += -step_size * (exp_avg[i] / denom)\r\n\r\ndef kernel_numba(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n\r\n    import ctypes\r\n    import numpy as np\r\n    def get_devicendarray(t):\r\n        assert t.type() == 'torch.cuda.FloatTensor'\r\n        numpy_dtype = np.float32\r\n        itemsize = 4\r\n\r\n        ctx = cuda.cudadrv.driver.driver.get_context()\r\n        mp = cuda.cudadrv.driver.MemoryPointer(ctx, ctypes.c_ulong(t.data_ptr()), t.numel()*itemsize)\r\n        return cuda.cudadrv.devicearray.DeviceNDArray(t.numel(), itemsize, numpy_dtype(), gpu_data=mp, stream=torch.cuda.current_stream().cuda_stream)\r\n\r\n    numba_param = get_devicendarray(param)\r\n    numba_grad = get_devicendarray(grad)\r\n    numba_exp_avg = get_devicendarray(exp_avg)\r\n    numba_exp_avg_sq = get_devicendarray(exp_avg_sq)\r\n    threadsperblock = 32\r\n    blockspergrid = (param.numel() + (threadsperblock - 1)) // threadsperblock\r\n    kernel_numba_cuda[blockspergrid, threadsperblock](numba_param, numba_grad, numba_exp_avg, numba_exp_avg_sq, beta1, beta2, step_size, eps)\r\n\r\nkernels = [(\"native \", kernel_native), (\"TScript\", kernel_jit_script), (\"numba  \", kernel_numba)]\r\n[Truncated]\n      size_t t1_dimIndex0 = t1_linearIndex ;\r\n      t1_offset += t1_dimIndex0 ;\r\n      IndexType t3_offset = 0;\r\n      IndexType t3_linearIndex = linearIndex;\r\n      \r\n      //printf(\"tensor t3 sizes[0] = %d, strides[0] = %d\\n\", t3.sizes[0],t3.strides[0]);\r\n      size_t t3_dimIndex0 = t3_linearIndex ;\r\n      t3_offset += t3_dimIndex0 ;\r\n      \r\n      // calculate the results\r\n      float n0 = __ldg(&t0.data[t0_offset]);\r\n      float n1 = __ldg(&t1.data[t1_offset]);\r\n      double n2 = s2;\r\n      float n3 = n0 / n1;\r\n      float n4 = n3 * ((float) n2);\r\n      t3.data[t3_offset] = n4;\r\n      \r\n    }\r\n}\r\n```","context":"<issue_start><issue_comment>Title: TorchScript Poor CUDA Performance \nusername_0: The performance of a TorchScript that handles the computation in the [Adam Optimizer](https://github.com/pytorch/pytorch/blob/master/torch/optim/adam.py) is worse than using the native PyTorch operations on CUDA. \r\n\r\n### Background\r\nI am trying to improve the GPU performance of the [Adam Optimizer](https://github.com/pytorch/pytorch/blob/master/torch/optim/adam.py) since it is the bottleneck in one of our workflows. \r\n\r\n### Results\r\nI have written a standalone script that compares PyTorch native tensor operations, TorchScript, and Numba (three runs each):\r\n```\r\nnative :  0.0506\r\nnative :  0.0322\r\nnative :  0.0322\r\nTScript:  0.1917\r\nTScript:  0.0442\r\nTScript:  0.0442\r\nnumba  :  0.1638\r\nnumba  :  0.0145\r\nnumba  :  0.0143\r\n```\r\nI suspect the problem is that TorchScript does not fuse all operations into a single CUDA. It generates two CUDA kernels (see the `PYTORCH_FUSION_DEBUG` output below)\r\n\r\n### Standalone Script\r\n```python\r\nimport math\r\nimport torch\r\nimport time\r\nimport random\r\nimport sys\r\n\r\ndef kernel_native(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n    exp_avg.mul_(beta1).add_(1 - beta1, grad)\r\n    exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\r\n    denom = exp_avg_sq.sqrt().add_(eps)\r\n    param.addcdiv_(-step_size, exp_avg, denom)   \r\n\r\n@torch.jit.script\r\ndef kernel_jit_script(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n    exp_avg.mul_(beta1)\r\n    exp_avg.add_((1 - beta1) + grad)\r\n    exp_avg_sq.mul_(beta2)\r\n    exp_avg_sq.add_((1 - beta2)*(grad*grad))\r\n    denom = exp_avg_sq.sqrt().add_(eps)\r\n    param += -step_size * (exp_avg / denom)\r\n\r\nfrom numba import cuda\r\n@cuda.jit\r\ndef kernel_numba_cuda(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    i = cuda.grid(1)\r\n    if i > grad.size:\r\n        return\r\n\r\n    exp_avg[i] = exp_avg[i] * beta1 + (1 - beta1) + grad[i]\r\n    exp_avg_sq[i] = exp_avg_sq[i] * beta2 + (1 - beta2) *grad[i]*grad[i]\r\n    denom = math.sqrt(exp_avg_sq[i]) + eps\r\n    param[i] += -step_size * (exp_avg[i] / denom)\r\n\r\ndef kernel_numba(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n\r\n    import ctypes\r\n    import numpy as np\r\n    def get_devicendarray(t):\r\n        assert t.type() == 'torch.cuda.FloatTensor'\r\n        numpy_dtype = np.float32\r\n        itemsize = 4\r\n\r\n        ctx = cuda.cudadrv.driver.driver.get_context()\r\n        mp = cuda.cudadrv.driver.MemoryPointer(ctx, ctypes.c_ulong(t.data_ptr()), t.numel()*itemsize)\r\n        return cuda.cudadrv.devicearray.DeviceNDArray(t.numel(), itemsize, numpy_dtype(), gpu_data=mp, stream=torch.cuda.current_stream().cuda_stream)\r\n\r\n    numba_param = get_devicendarray(param)\r\n    numba_grad = get_devicendarray(grad)\r\n    numba_exp_avg = get_devicendarray(exp_avg)\r\n    numba_exp_avg_sq = get_devicendarray(exp_avg_sq)\r\n    threadsperblock = 32\r\n    blockspergrid = (param.numel() + (threadsperblock - 1)) // threadsperblock\r\n    kernel_numba_cuda[blockspergrid, threadsperblock](numba_param, numba_grad, numba_exp_avg, numba_exp_avg_sq, beta1, beta2, step_size, eps)\r\n\r\nkernels = [(\"native \", kernel_native), (\"TScript\", kernel_jit_script), (\"numba  \", kernel_numba)]\r\n[Truncated]\n      size_t t1_dimIndex0 = t1_linearIndex ;\r\n      t1_offset += t1_dimIndex0 ;\r\n      IndexType t3_offset = 0;\r\n      IndexType t3_linearIndex = linearIndex;\r\n      \r\n      //printf(\"tensor t3 sizes[0] = %d, strides[0] = %d\\n\", t3.sizes[0],t3.strides[0]);\r\n      size_t t3_dimIndex0 = t3_linearIndex ;\r\n      t3_offset += t3_dimIndex0 ;\r\n      \r\n      // calculate the results\r\n      float n0 = __ldg(&t0.data[t0_offset]);\r\n      float n1 = __ldg(&t1.data[t1_offset]);\r\n      double n2 = s2;\r\n      float n3 = n0 / n1;\r\n      float n4 = n3 * ((float) n2);\r\n      t3.data[t3_offset] = n4;\r\n      \r\n    }\r\n}\r\n```\n<issue_comment>username_1: @username_0 can you try removing all the in-place operations in your scripted version?\r\nIIRC in-place ops block fusion.\r\n\r\nI was btw able to fuse most of sparse adam by removing those in-place ops, see https://github.com/pytorch/pytorch/pull/23522#issuecomment-516952462\n<issue_comment>username_2: cc: @username_3 and also relevant WIP PR: [JIT version of Adagrad](https://github.com/pytorch/pytorch/pull/23640)\n<issue_comment>username_3: Yeah the CUDA fusion does not support fusing in-place operations yet, Can you try the suggestion by @username_1 to see if you get a better perf?\n<issue_comment>username_3: @username_0 Also, it seems like you are only timing it in 3 iterations, and the perf numbers seems including the compilation time. The first time JIT/TorchScript sees an input it will specialize and do optimizations against that input, so when comparing performance please allow a compilation time and runtime optimization warmup for the input.\n<issue_comment>username_4: @username_3 That made a big difference to my traced module too. Any other tricks like this? It slows down the non-JIT version, however.\n<issue_comment>username_3: @username_4 That's pretty much all, let me summarize:\r\n\r\n1. JIT fusion does not support in-place operation fusion and reduction operation yet.\r\n2. we did the fusion code generation at the first time we see an input that the shape we haven't seen before (like if the number of dimensions are different), this will help us generate faster cuda kernels. \r\n\r\nSo the TorchScript compilation time will be gone after compile and the first run :)\n<issue_comment>username_0: Thanks for the feedback!\r\n\r\nI have removed all in-place operations and added three kernel implementations:\r\n```python\r\n\r\nimport math\r\nimport torch\r\nimport time\r\nimport random\r\nimport sys\r\n\r\ndef kernel_native(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n    exp_avg.mul_(beta1).add_(1 - beta1, grad)\r\n    exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\r\n    denom = exp_avg_sq.sqrt().add_(eps)\r\n    param.addcdiv_(-step_size, exp_avg, denom)   \r\n\r\n@torch.jit.script\r\ndef kernel_jit_script1(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n    exp_avg *= beta1\r\n    exp_avg += (1 - beta1) + grad\r\n    exp_avg_sq *= beta2\r\n    exp_avg_sq += (1 - beta2)*(grad*grad)\r\n    param += -step_size * (exp_avg / exp_avg_sq.sqrt() + eps)\r\n\r\n@torch.jit.script\r\ndef kernel_jit_script2(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n    exp_avg[:] = exp_avg * beta1 + (1 - beta1) + grad\r\n    exp_avg_sq[:] = exp_avg_sq * beta2 + (1 - beta2)*(grad*grad)\r\n    param[:] = param -step_size * (exp_avg / exp_avg_sq.sqrt() + eps)\r\n\r\n@torch.jit.script\r\ndef kernel_jit_script3(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> Tuple[Tensor, Tensor, Tensor, Tensor]\r\n    exp_avg = exp_avg * beta1 + (1 - beta1) + grad\r\n    exp_avg_sq = exp_avg_sq * beta2 + (1 - beta2)*(grad*grad)\r\n    param = param -step_size * (exp_avg / exp_avg_sq.sqrt() + eps)\r\n    return (param, grad, exp_avg, exp_avg_sq)\r\n\r\n\r\nfrom numba import cuda\r\n@cuda.jit\r\ndef kernel_numba_cuda(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    i = cuda.grid(1)\r\n    if i > grad.size:\r\n        return\r\n\r\n    exp_avg[i] = exp_avg[i] * beta1 + (1 - beta1) + grad[i]\r\n    exp_avg_sq[i] = exp_avg_sq[i] * beta2 + (1 - beta2) *grad[i]*grad[i]\r\n    denom = math.sqrt(exp_avg_sq[i]) + eps\r\n    param[i] += -step_size * (exp_avg[i] / denom)\r\n\r\ndef kernel_numba(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n\r\n    import ctypes\r\n    import numpy as np\r\n    def get_devicendarray(t):\r\n        assert t.type() == 'torch.cuda.FloatTensor'\r\n        numpy_dtype = np.float32\r\n        itemsize = 4\r\n\r\n        ctx = cuda.cudadrv.driver.driver.get_context()\r\n        mp = cuda.cudadrv.driver.MemoryPointer(ctx, ctypes.c_ulong(t.data_ptr()), t.numel()*itemsize)\r\n        return cuda.cudadrv.devicearray.DeviceNDArray(t.numel(), itemsize, numpy_dtype(), gpu_data=mp, stream=torch.cuda.current_stream().cuda_stream)\r\n\r\n    numba_param = get_devicendarray(param)\r\n    numba_grad = get_devicendarray(grad)\r\n    numba_exp_avg = get_devicendarray(exp_avg)\r\n    numba_exp_avg_sq = get_devicendarray(exp_avg_sq)\r\n    threadsperblock = 32\r\n    blockspergrid = (param.numel() + (threadsperblock - 1)) // threadsperblock\r\n    kernel_numba_cuda[blockspergrid, threadsperblock](numba_param, numba_grad, numba_exp_avg, numba_exp_avg_sq, beta1, beta2, step_size, eps)\r\n\r\nkernels = [(\"native \", kernel_native), (\"numba  \", kernel_numba)]\r\nshape = (4194304*64,)\r\nparam, grad, exp_avg, exp_avg_sq = [torch.rand(shape).to(\"cuda\") for x in range(4)]\r\n[Truncated]\n        res = func(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps)\r\n        if res is not None:\r\n            param[:], _, exp_avg[:], exp_avg_sq[:] = res\r\n        torch.cuda.synchronize()\r\n        t2 = time.time()\r\n        print(\"%s: \" % name, t2-t1)\r\n```\r\nThe results are:\r\n```\r\nnative :  0.0323\r\nnumba  :  0.0147\r\nTScript1:  0.0369\r\nTScript2:  0.0352\r\nTScript3:  0.0280\r\n```\r\nTScript3 is now better than native but still far from numba. TScript3 is now generating a single CUDA kernel but the back-copying of the result now dominates the timing. If I remove the line `param[:], _, exp_avg[:], exp_avg_sq[:] = res` the performance is almost on pair with numba:\r\n```\r\nTScript3 (no back-copy):  0.01664\r\n```\r\nThe problem is I really do need to update the values :/\n<issue_comment>username_5: cc @mruberry","repo":"pytorch/pytorch","issue_id":475728990,"issue_number":23655}
{"question":"What is the impact of in-place operations on fusion in PyTorch scripts, as mentioned in the text?","answer":"In-place operations in PyTorch scripts can block fusion, as stated in the text. Removing in-place operations in certain cases, such as in the sparse Adam optimizer, can enable successful fusion, leading to performance improvements.","id":175,"text":"@username_0 can you try removing all the in-place operations in your scripted version?\r\nIIRC in-place ops block fusion.\r\n\r\nI was btw able to fuse most of sparse adam by removing those in-place ops, see https://github.com/pytorch/pytorch/pull/23522#issuecomment-516952462","context":"<issue_start><issue_comment>Title: TorchScript Poor CUDA Performance \nusername_0: The performance of a TorchScript that handles the computation in the [Adam Optimizer](https://github.com/pytorch/pytorch/blob/master/torch/optim/adam.py) is worse than using the native PyTorch operations on CUDA. \r\n\r\n### Background\r\nI am trying to improve the GPU performance of the [Adam Optimizer](https://github.com/pytorch/pytorch/blob/master/torch/optim/adam.py) since it is the bottleneck in one of our workflows. \r\n\r\n### Results\r\nI have written a standalone script that compares PyTorch native tensor operations, TorchScript, and Numba (three runs each):\r\n```\r\nnative :  0.0506\r\nnative :  0.0322\r\nnative :  0.0322\r\nTScript:  0.1917\r\nTScript:  0.0442\r\nTScript:  0.0442\r\nnumba  :  0.1638\r\nnumba  :  0.0145\r\nnumba  :  0.0143\r\n```\r\nI suspect the problem is that TorchScript does not fuse all operations into a single CUDA. It generates two CUDA kernels (see the `PYTORCH_FUSION_DEBUG` output below)\r\n\r\n### Standalone Script\r\n```python\r\nimport math\r\nimport torch\r\nimport time\r\nimport random\r\nimport sys\r\n\r\ndef kernel_native(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n    exp_avg.mul_(beta1).add_(1 - beta1, grad)\r\n    exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\r\n    denom = exp_avg_sq.sqrt().add_(eps)\r\n    param.addcdiv_(-step_size, exp_avg, denom)   \r\n\r\n@torch.jit.script\r\ndef kernel_jit_script(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n    exp_avg.mul_(beta1)\r\n    exp_avg.add_((1 - beta1) + grad)\r\n    exp_avg_sq.mul_(beta2)\r\n    exp_avg_sq.add_((1 - beta2)*(grad*grad))\r\n    denom = exp_avg_sq.sqrt().add_(eps)\r\n    param += -step_size * (exp_avg / denom)\r\n\r\nfrom numba import cuda\r\n@cuda.jit\r\ndef kernel_numba_cuda(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    i = cuda.grid(1)\r\n    if i > grad.size:\r\n        return\r\n\r\n    exp_avg[i] = exp_avg[i] * beta1 + (1 - beta1) + grad[i]\r\n    exp_avg_sq[i] = exp_avg_sq[i] * beta2 + (1 - beta2) *grad[i]*grad[i]\r\n    denom = math.sqrt(exp_avg_sq[i]) + eps\r\n    param[i] += -step_size * (exp_avg[i] / denom)\r\n\r\ndef kernel_numba(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n\r\n    import ctypes\r\n    import numpy as np\r\n    def get_devicendarray(t):\r\n        assert t.type() == 'torch.cuda.FloatTensor'\r\n        numpy_dtype = np.float32\r\n        itemsize = 4\r\n\r\n        ctx = cuda.cudadrv.driver.driver.get_context()\r\n        mp = cuda.cudadrv.driver.MemoryPointer(ctx, ctypes.c_ulong(t.data_ptr()), t.numel()*itemsize)\r\n        return cuda.cudadrv.devicearray.DeviceNDArray(t.numel(), itemsize, numpy_dtype(), gpu_data=mp, stream=torch.cuda.current_stream().cuda_stream)\r\n\r\n    numba_param = get_devicendarray(param)\r\n    numba_grad = get_devicendarray(grad)\r\n    numba_exp_avg = get_devicendarray(exp_avg)\r\n    numba_exp_avg_sq = get_devicendarray(exp_avg_sq)\r\n    threadsperblock = 32\r\n    blockspergrid = (param.numel() + (threadsperblock - 1)) // threadsperblock\r\n    kernel_numba_cuda[blockspergrid, threadsperblock](numba_param, numba_grad, numba_exp_avg, numba_exp_avg_sq, beta1, beta2, step_size, eps)\r\n\r\nkernels = [(\"native \", kernel_native), (\"TScript\", kernel_jit_script), (\"numba  \", kernel_numba)]\r\n[Truncated]\n      size_t t1_dimIndex0 = t1_linearIndex ;\r\n      t1_offset += t1_dimIndex0 ;\r\n      IndexType t3_offset = 0;\r\n      IndexType t3_linearIndex = linearIndex;\r\n      \r\n      //printf(\"tensor t3 sizes[0] = %d, strides[0] = %d\\n\", t3.sizes[0],t3.strides[0]);\r\n      size_t t3_dimIndex0 = t3_linearIndex ;\r\n      t3_offset += t3_dimIndex0 ;\r\n      \r\n      // calculate the results\r\n      float n0 = __ldg(&t0.data[t0_offset]);\r\n      float n1 = __ldg(&t1.data[t1_offset]);\r\n      double n2 = s2;\r\n      float n3 = n0 / n1;\r\n      float n4 = n3 * ((float) n2);\r\n      t3.data[t3_offset] = n4;\r\n      \r\n    }\r\n}\r\n```\n<issue_comment>username_1: @username_0 can you try removing all the in-place operations in your scripted version?\r\nIIRC in-place ops block fusion.\r\n\r\nI was btw able to fuse most of sparse adam by removing those in-place ops, see https://github.com/pytorch/pytorch/pull/23522#issuecomment-516952462\n<issue_comment>username_2: cc: @username_3 and also relevant WIP PR: [JIT version of Adagrad](https://github.com/pytorch/pytorch/pull/23640)\n<issue_comment>username_3: Yeah the CUDA fusion does not support fusing in-place operations yet, Can you try the suggestion by @username_1 to see if you get a better perf?\n<issue_comment>username_3: @username_0 Also, it seems like you are only timing it in 3 iterations, and the perf numbers seems including the compilation time. The first time JIT/TorchScript sees an input it will specialize and do optimizations against that input, so when comparing performance please allow a compilation time and runtime optimization warmup for the input.\n<issue_comment>username_4: @username_3 That made a big difference to my traced module too. Any other tricks like this? It slows down the non-JIT version, however.\n<issue_comment>username_3: @username_4 That's pretty much all, let me summarize:\r\n\r\n1. JIT fusion does not support in-place operation fusion and reduction operation yet.\r\n2. we did the fusion code generation at the first time we see an input that the shape we haven't seen before (like if the number of dimensions are different), this will help us generate faster cuda kernels. \r\n\r\nSo the TorchScript compilation time will be gone after compile and the first run :)\n<issue_comment>username_0: Thanks for the feedback!\r\n\r\nI have removed all in-place operations and added three kernel implementations:\r\n```python\r\n\r\nimport math\r\nimport torch\r\nimport time\r\nimport random\r\nimport sys\r\n\r\ndef kernel_native(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n    exp_avg.mul_(beta1).add_(1 - beta1, grad)\r\n    exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\r\n    denom = exp_avg_sq.sqrt().add_(eps)\r\n    param.addcdiv_(-step_size, exp_avg, denom)   \r\n\r\n@torch.jit.script\r\ndef kernel_jit_script1(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n    exp_avg *= beta1\r\n    exp_avg += (1 - beta1) + grad\r\n    exp_avg_sq *= beta2\r\n    exp_avg_sq += (1 - beta2)*(grad*grad)\r\n    param += -step_size * (exp_avg / exp_avg_sq.sqrt() + eps)\r\n\r\n@torch.jit.script\r\ndef kernel_jit_script2(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n    exp_avg[:] = exp_avg * beta1 + (1 - beta1) + grad\r\n    exp_avg_sq[:] = exp_avg_sq * beta2 + (1 - beta2)*(grad*grad)\r\n    param[:] = param -step_size * (exp_avg / exp_avg_sq.sqrt() + eps)\r\n\r\n@torch.jit.script\r\ndef kernel_jit_script3(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> Tuple[Tensor, Tensor, Tensor, Tensor]\r\n    exp_avg = exp_avg * beta1 + (1 - beta1) + grad\r\n    exp_avg_sq = exp_avg_sq * beta2 + (1 - beta2)*(grad*grad)\r\n    param = param -step_size * (exp_avg / exp_avg_sq.sqrt() + eps)\r\n    return (param, grad, exp_avg, exp_avg_sq)\r\n\r\n\r\nfrom numba import cuda\r\n@cuda.jit\r\ndef kernel_numba_cuda(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    i = cuda.grid(1)\r\n    if i > grad.size:\r\n        return\r\n\r\n    exp_avg[i] = exp_avg[i] * beta1 + (1 - beta1) + grad[i]\r\n    exp_avg_sq[i] = exp_avg_sq[i] * beta2 + (1 - beta2) *grad[i]*grad[i]\r\n    denom = math.sqrt(exp_avg_sq[i]) + eps\r\n    param[i] += -step_size * (exp_avg[i] / denom)\r\n\r\ndef kernel_numba(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n\r\n    import ctypes\r\n    import numpy as np\r\n    def get_devicendarray(t):\r\n        assert t.type() == 'torch.cuda.FloatTensor'\r\n        numpy_dtype = np.float32\r\n        itemsize = 4\r\n\r\n        ctx = cuda.cudadrv.driver.driver.get_context()\r\n        mp = cuda.cudadrv.driver.MemoryPointer(ctx, ctypes.c_ulong(t.data_ptr()), t.numel()*itemsize)\r\n        return cuda.cudadrv.devicearray.DeviceNDArray(t.numel(), itemsize, numpy_dtype(), gpu_data=mp, stream=torch.cuda.current_stream().cuda_stream)\r\n\r\n    numba_param = get_devicendarray(param)\r\n    numba_grad = get_devicendarray(grad)\r\n    numba_exp_avg = get_devicendarray(exp_avg)\r\n    numba_exp_avg_sq = get_devicendarray(exp_avg_sq)\r\n    threadsperblock = 32\r\n    blockspergrid = (param.numel() + (threadsperblock - 1)) // threadsperblock\r\n    kernel_numba_cuda[blockspergrid, threadsperblock](numba_param, numba_grad, numba_exp_avg, numba_exp_avg_sq, beta1, beta2, step_size, eps)\r\n\r\nkernels = [(\"native \", kernel_native), (\"numba  \", kernel_numba)]\r\nshape = (4194304*64,)\r\nparam, grad, exp_avg, exp_avg_sq = [torch.rand(shape).to(\"cuda\") for x in range(4)]\r\n[Truncated]\n        res = func(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps)\r\n        if res is not None:\r\n            param[:], _, exp_avg[:], exp_avg_sq[:] = res\r\n        torch.cuda.synchronize()\r\n        t2 = time.time()\r\n        print(\"%s: \" % name, t2-t1)\r\n```\r\nThe results are:\r\n```\r\nnative :  0.0323\r\nnumba  :  0.0147\r\nTScript1:  0.0369\r\nTScript2:  0.0352\r\nTScript3:  0.0280\r\n```\r\nTScript3 is now better than native but still far from numba. TScript3 is now generating a single CUDA kernel but the back-copying of the result now dominates the timing. If I remove the line `param[:], _, exp_avg[:], exp_avg_sq[:] = res` the performance is almost on pair with numba:\r\n```\r\nTScript3 (no back-copy):  0.01664\r\n```\r\nThe problem is I really do need to update the values :/\n<issue_comment>username_5: cc @mruberry","repo":"pytorch/pytorch","issue_id":475728990,"issue_number":23655}
{"question":"What limitations does JIT fusion in PyTorch have and when does fusion code generation occur?","answer":"JIT fusion in PyTorch does not support in-place and reduction operation fusion. Fusion code generation takes place the first time an input with a new shape is encountered.","id":180,"text":"@username_4 That's pretty much all, let me summarize:\r\n\r\n1. JIT fusion does not support in-place operation fusion and reduction operation yet.\r\n2. we did the fusion code generation at the first time we see an input that the shape we haven't seen before (like if the number of dimensions are different), this will help us generate faster cuda kernels. \r\n\r\nSo the TorchScript compilation time will be gone after compile and the first run :)","context":"<issue_start><issue_comment>Title: TorchScript Poor CUDA Performance \nusername_0: The performance of a TorchScript that handles the computation in the [Adam Optimizer](https://github.com/pytorch/pytorch/blob/master/torch/optim/adam.py) is worse than using the native PyTorch operations on CUDA. \r\n\r\n### Background\r\nI am trying to improve the GPU performance of the [Adam Optimizer](https://github.com/pytorch/pytorch/blob/master/torch/optim/adam.py) since it is the bottleneck in one of our workflows. \r\n\r\n### Results\r\nI have written a standalone script that compares PyTorch native tensor operations, TorchScript, and Numba (three runs each):\r\n```\r\nnative :  0.0506\r\nnative :  0.0322\r\nnative :  0.0322\r\nTScript:  0.1917\r\nTScript:  0.0442\r\nTScript:  0.0442\r\nnumba  :  0.1638\r\nnumba  :  0.0145\r\nnumba  :  0.0143\r\n```\r\nI suspect the problem is that TorchScript does not fuse all operations into a single CUDA. It generates two CUDA kernels (see the `PYTORCH_FUSION_DEBUG` output below)\r\n\r\n### Standalone Script\r\n```python\r\nimport math\r\nimport torch\r\nimport time\r\nimport random\r\nimport sys\r\n\r\ndef kernel_native(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n    exp_avg.mul_(beta1).add_(1 - beta1, grad)\r\n    exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\r\n    denom = exp_avg_sq.sqrt().add_(eps)\r\n    param.addcdiv_(-step_size, exp_avg, denom)   \r\n\r\n@torch.jit.script\r\ndef kernel_jit_script(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n    exp_avg.mul_(beta1)\r\n    exp_avg.add_((1 - beta1) + grad)\r\n    exp_avg_sq.mul_(beta2)\r\n    exp_avg_sq.add_((1 - beta2)*(grad*grad))\r\n    denom = exp_avg_sq.sqrt().add_(eps)\r\n    param += -step_size * (exp_avg / denom)\r\n\r\nfrom numba import cuda\r\n@cuda.jit\r\ndef kernel_numba_cuda(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    i = cuda.grid(1)\r\n    if i > grad.size:\r\n        return\r\n\r\n    exp_avg[i] = exp_avg[i] * beta1 + (1 - beta1) + grad[i]\r\n    exp_avg_sq[i] = exp_avg_sq[i] * beta2 + (1 - beta2) *grad[i]*grad[i]\r\n    denom = math.sqrt(exp_avg_sq[i]) + eps\r\n    param[i] += -step_size * (exp_avg[i] / denom)\r\n\r\ndef kernel_numba(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n\r\n    import ctypes\r\n    import numpy as np\r\n    def get_devicendarray(t):\r\n        assert t.type() == 'torch.cuda.FloatTensor'\r\n        numpy_dtype = np.float32\r\n        itemsize = 4\r\n\r\n        ctx = cuda.cudadrv.driver.driver.get_context()\r\n        mp = cuda.cudadrv.driver.MemoryPointer(ctx, ctypes.c_ulong(t.data_ptr()), t.numel()*itemsize)\r\n        return cuda.cudadrv.devicearray.DeviceNDArray(t.numel(), itemsize, numpy_dtype(), gpu_data=mp, stream=torch.cuda.current_stream().cuda_stream)\r\n\r\n    numba_param = get_devicendarray(param)\r\n    numba_grad = get_devicendarray(grad)\r\n    numba_exp_avg = get_devicendarray(exp_avg)\r\n    numba_exp_avg_sq = get_devicendarray(exp_avg_sq)\r\n    threadsperblock = 32\r\n    blockspergrid = (param.numel() + (threadsperblock - 1)) // threadsperblock\r\n    kernel_numba_cuda[blockspergrid, threadsperblock](numba_param, numba_grad, numba_exp_avg, numba_exp_avg_sq, beta1, beta2, step_size, eps)\r\n\r\nkernels = [(\"native \", kernel_native), (\"TScript\", kernel_jit_script), (\"numba  \", kernel_numba)]\r\n[Truncated]\n      size_t t1_dimIndex0 = t1_linearIndex ;\r\n      t1_offset += t1_dimIndex0 ;\r\n      IndexType t3_offset = 0;\r\n      IndexType t3_linearIndex = linearIndex;\r\n      \r\n      //printf(\"tensor t3 sizes[0] = %d, strides[0] = %d\\n\", t3.sizes[0],t3.strides[0]);\r\n      size_t t3_dimIndex0 = t3_linearIndex ;\r\n      t3_offset += t3_dimIndex0 ;\r\n      \r\n      // calculate the results\r\n      float n0 = __ldg(&t0.data[t0_offset]);\r\n      float n1 = __ldg(&t1.data[t1_offset]);\r\n      double n2 = s2;\r\n      float n3 = n0 / n1;\r\n      float n4 = n3 * ((float) n2);\r\n      t3.data[t3_offset] = n4;\r\n      \r\n    }\r\n}\r\n```\n<issue_comment>username_1: @username_0 can you try removing all the in-place operations in your scripted version?\r\nIIRC in-place ops block fusion.\r\n\r\nI was btw able to fuse most of sparse adam by removing those in-place ops, see https://github.com/pytorch/pytorch/pull/23522#issuecomment-516952462\n<issue_comment>username_2: cc: @username_3 and also relevant WIP PR: [JIT version of Adagrad](https://github.com/pytorch/pytorch/pull/23640)\n<issue_comment>username_3: Yeah the CUDA fusion does not support fusing in-place operations yet, Can you try the suggestion by @username_1 to see if you get a better perf?\n<issue_comment>username_3: @username_0 Also, it seems like you are only timing it in 3 iterations, and the perf numbers seems including the compilation time. The first time JIT/TorchScript sees an input it will specialize and do optimizations against that input, so when comparing performance please allow a compilation time and runtime optimization warmup for the input.\n<issue_comment>username_4: @username_3 That made a big difference to my traced module too. Any other tricks like this? It slows down the non-JIT version, however.\n<issue_comment>username_3: @username_4 That's pretty much all, let me summarize:\r\n\r\n1. JIT fusion does not support in-place operation fusion and reduction operation yet.\r\n2. we did the fusion code generation at the first time we see an input that the shape we haven't seen before (like if the number of dimensions are different), this will help us generate faster cuda kernels. \r\n\r\nSo the TorchScript compilation time will be gone after compile and the first run :)\n<issue_comment>username_0: Thanks for the feedback!\r\n\r\nI have removed all in-place operations and added three kernel implementations:\r\n```python\r\n\r\nimport math\r\nimport torch\r\nimport time\r\nimport random\r\nimport sys\r\n\r\ndef kernel_native(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n    exp_avg.mul_(beta1).add_(1 - beta1, grad)\r\n    exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\r\n    denom = exp_avg_sq.sqrt().add_(eps)\r\n    param.addcdiv_(-step_size, exp_avg, denom)   \r\n\r\n@torch.jit.script\r\ndef kernel_jit_script1(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n    exp_avg *= beta1\r\n    exp_avg += (1 - beta1) + grad\r\n    exp_avg_sq *= beta2\r\n    exp_avg_sq += (1 - beta2)*(grad*grad)\r\n    param += -step_size * (exp_avg / exp_avg_sq.sqrt() + eps)\r\n\r\n@torch.jit.script\r\ndef kernel_jit_script2(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n    exp_avg[:] = exp_avg * beta1 + (1 - beta1) + grad\r\n    exp_avg_sq[:] = exp_avg_sq * beta2 + (1 - beta2)*(grad*grad)\r\n    param[:] = param -step_size * (exp_avg / exp_avg_sq.sqrt() + eps)\r\n\r\n@torch.jit.script\r\ndef kernel_jit_script3(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> Tuple[Tensor, Tensor, Tensor, Tensor]\r\n    exp_avg = exp_avg * beta1 + (1 - beta1) + grad\r\n    exp_avg_sq = exp_avg_sq * beta2 + (1 - beta2)*(grad*grad)\r\n    param = param -step_size * (exp_avg / exp_avg_sq.sqrt() + eps)\r\n    return (param, grad, exp_avg, exp_avg_sq)\r\n\r\n\r\nfrom numba import cuda\r\n@cuda.jit\r\ndef kernel_numba_cuda(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    i = cuda.grid(1)\r\n    if i > grad.size:\r\n        return\r\n\r\n    exp_avg[i] = exp_avg[i] * beta1 + (1 - beta1) + grad[i]\r\n    exp_avg_sq[i] = exp_avg_sq[i] * beta2 + (1 - beta2) *grad[i]*grad[i]\r\n    denom = math.sqrt(exp_avg_sq[i]) + eps\r\n    param[i] += -step_size * (exp_avg[i] / denom)\r\n\r\ndef kernel_numba(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n\r\n    import ctypes\r\n    import numpy as np\r\n    def get_devicendarray(t):\r\n        assert t.type() == 'torch.cuda.FloatTensor'\r\n        numpy_dtype = np.float32\r\n        itemsize = 4\r\n\r\n        ctx = cuda.cudadrv.driver.driver.get_context()\r\n        mp = cuda.cudadrv.driver.MemoryPointer(ctx, ctypes.c_ulong(t.data_ptr()), t.numel()*itemsize)\r\n        return cuda.cudadrv.devicearray.DeviceNDArray(t.numel(), itemsize, numpy_dtype(), gpu_data=mp, stream=torch.cuda.current_stream().cuda_stream)\r\n\r\n    numba_param = get_devicendarray(param)\r\n    numba_grad = get_devicendarray(grad)\r\n    numba_exp_avg = get_devicendarray(exp_avg)\r\n    numba_exp_avg_sq = get_devicendarray(exp_avg_sq)\r\n    threadsperblock = 32\r\n    blockspergrid = (param.numel() + (threadsperblock - 1)) // threadsperblock\r\n    kernel_numba_cuda[blockspergrid, threadsperblock](numba_param, numba_grad, numba_exp_avg, numba_exp_avg_sq, beta1, beta2, step_size, eps)\r\n\r\nkernels = [(\"native \", kernel_native), (\"numba  \", kernel_numba)]\r\nshape = (4194304*64,)\r\nparam, grad, exp_avg, exp_avg_sq = [torch.rand(shape).to(\"cuda\") for x in range(4)]\r\n[Truncated]\n        res = func(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps)\r\n        if res is not None:\r\n            param[:], _, exp_avg[:], exp_avg_sq[:] = res\r\n        torch.cuda.synchronize()\r\n        t2 = time.time()\r\n        print(\"%s: \" % name, t2-t1)\r\n```\r\nThe results are:\r\n```\r\nnative :  0.0323\r\nnumba  :  0.0147\r\nTScript1:  0.0369\r\nTScript2:  0.0352\r\nTScript3:  0.0280\r\n```\r\nTScript3 is now better than native but still far from numba. TScript3 is now generating a single CUDA kernel but the back-copying of the result now dominates the timing. If I remove the line `param[:], _, exp_avg[:], exp_avg_sq[:] = res` the performance is almost on pair with numba:\r\n```\r\nTScript3 (no back-copy):  0.01664\r\n```\r\nThe problem is I really do need to update the values :/\n<issue_comment>username_5: cc @mruberry","repo":"pytorch/pytorch","issue_id":475728990,"issue_number":23655}
{"question":"What are the results of the different kernel implementations and their corresponding timings?","answer":"The results of the different kernel implementations are as follows: \n- native: 0.0323 \n- numba: 0.0147 \n- TScript1: 0.0369 \n- TScript2: 0.0352 \n- TScript3: 0.0280. Additionally, when removing the back-copying line in TScript3, the performance improves to 0.01664.","id":181,"text":"Thanks for the feedback!\r\n\r\nI have removed all in-place operations and added three kernel implementations:\r\n```python\r\n\r\nimport math\r\nimport torch\r\nimport time\r\nimport random\r\nimport sys\r\n\r\ndef kernel_native(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n    exp_avg.mul_(beta1).add_(1 - beta1, grad)\r\n    exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\r\n    denom = exp_avg_sq.sqrt().add_(eps)\r\n    param.addcdiv_(-step_size, exp_avg, denom)   \r\n\r\n@torch.jit.script\r\ndef kernel_jit_script1(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n    exp_avg *= beta1\r\n    exp_avg += (1 - beta1) + grad\r\n    exp_avg_sq *= beta2\r\n    exp_avg_sq += (1 - beta2)*(grad*grad)\r\n    param += -step_size * (exp_avg / exp_avg_sq.sqrt() + eps)\r\n\r\n@torch.jit.script\r\ndef kernel_jit_script2(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n    exp_avg[:] = exp_avg * beta1 + (1 - beta1) + grad\r\n    exp_avg_sq[:] = exp_avg_sq * beta2 + (1 - beta2)*(grad*grad)\r\n    param[:] = param -step_size * (exp_avg / exp_avg_sq.sqrt() + eps)\r\n\r\n@torch.jit.script\r\ndef kernel_jit_script3(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> Tuple[Tensor, Tensor, Tensor, Tensor]\r\n    exp_avg = exp_avg * beta1 + (1 - beta1) + grad\r\n    exp_avg_sq = exp_avg_sq * beta2 + (1 - beta2)*(grad*grad)\r\n    param = param -step_size * (exp_avg / exp_avg_sq.sqrt() + eps)\r\n    return (param, grad, exp_avg, exp_avg_sq)\r\n\r\n\r\nfrom numba import cuda\r\n@cuda.jit\r\ndef kernel_numba_cuda(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    i = cuda.grid(1)\r\n    if i > grad.size:\r\n        return\r\n\r\n    exp_avg[i] = exp_avg[i] * beta1 + (1 - beta1) + grad[i]\r\n    exp_avg_sq[i] = exp_avg_sq[i] * beta2 + (1 - beta2) *grad[i]*grad[i]\r\n    denom = math.sqrt(exp_avg_sq[i]) + eps\r\n    param[i] += -step_size * (exp_avg[i] / denom)\r\n\r\ndef kernel_numba(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n\r\n    import ctypes\r\n    import numpy as np\r\n    def get_devicendarray(t):\r\n        assert t.type() == 'torch.cuda.FloatTensor'\r\n        numpy_dtype = np.float32\r\n        itemsize = 4\r\n\r\n        ctx = cuda.cudadrv.driver.driver.get_context()\r\n        mp = cuda.cudadrv.driver.MemoryPointer(ctx, ctypes.c_ulong(t.data_ptr()), t.numel()*itemsize)\r\n        return cuda.cudadrv.devicearray.DeviceNDArray(t.numel(), itemsize, numpy_dtype(), gpu_data=mp, stream=torch.cuda.current_stream().cuda_stream)\r\n\r\n    numba_param = get_devicendarray(param)\r\n    numba_grad = get_devicendarray(grad)\r\n    numba_exp_avg = get_devicendarray(exp_avg)\r\n    numba_exp_avg_sq = get_devicendarray(exp_avg_sq)\r\n    threadsperblock = 32\r\n    blockspergrid = (param.numel() + (threadsperblock - 1)) // threadsperblock\r\n    kernel_numba_cuda[blockspergrid, threadsperblock](numba_param, numba_grad, numba_exp_avg, numba_exp_avg_sq, beta1, beta2, step_size, eps)\r\n\r\nkernels = [(\"native \", kernel_native), (\"numba  \", kernel_numba)]\r\nshape = (4194304*64,)\r\nparam, grad, exp_avg, exp_avg_sq = [torch.rand(shape).to(\"cuda\") for x in range(4)]\r\n[Truncated]\n        res = func(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps)\r\n        if res is not None:\r\n            param[:], _, exp_avg[:], exp_avg_sq[:] = res\r\n        torch.cuda.synchronize()\r\n        t2 = time.time()\r\n        print(\"%s: \" % name, t2-t1)\r\n```\r\nThe results are:\r\n```\r\nnative :  0.0323\r\nnumba  :  0.0147\r\nTScript1:  0.0369\r\nTScript2:  0.0352\r\nTScript3:  0.0280\r\n```\r\nTScript3 is now better than native but still far from numba. TScript3 is now generating a single CUDA kernel but the back-copying of the result now dominates the timing. If I remove the line `param[:], _, exp_avg[:], exp_avg_sq[:] = res` the performance is almost on pair with numba:\r\n```\r\nTScript3 (no back-copy):  0.01664\r\n```\r\nThe problem is I really do need to update the values :/","context":"<issue_start><issue_comment>Title: TorchScript Poor CUDA Performance \nusername_0: The performance of a TorchScript that handles the computation in the [Adam Optimizer](https://github.com/pytorch/pytorch/blob/master/torch/optim/adam.py) is worse than using the native PyTorch operations on CUDA. \r\n\r\n### Background\r\nI am trying to improve the GPU performance of the [Adam Optimizer](https://github.com/pytorch/pytorch/blob/master/torch/optim/adam.py) since it is the bottleneck in one of our workflows. \r\n\r\n### Results\r\nI have written a standalone script that compares PyTorch native tensor operations, TorchScript, and Numba (three runs each):\r\n```\r\nnative :  0.0506\r\nnative :  0.0322\r\nnative :  0.0322\r\nTScript:  0.1917\r\nTScript:  0.0442\r\nTScript:  0.0442\r\nnumba  :  0.1638\r\nnumba  :  0.0145\r\nnumba  :  0.0143\r\n```\r\nI suspect the problem is that TorchScript does not fuse all operations into a single CUDA. It generates two CUDA kernels (see the `PYTORCH_FUSION_DEBUG` output below)\r\n\r\n### Standalone Script\r\n```python\r\nimport math\r\nimport torch\r\nimport time\r\nimport random\r\nimport sys\r\n\r\ndef kernel_native(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n    exp_avg.mul_(beta1).add_(1 - beta1, grad)\r\n    exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\r\n    denom = exp_avg_sq.sqrt().add_(eps)\r\n    param.addcdiv_(-step_size, exp_avg, denom)   \r\n\r\n@torch.jit.script\r\ndef kernel_jit_script(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n    exp_avg.mul_(beta1)\r\n    exp_avg.add_((1 - beta1) + grad)\r\n    exp_avg_sq.mul_(beta2)\r\n    exp_avg_sq.add_((1 - beta2)*(grad*grad))\r\n    denom = exp_avg_sq.sqrt().add_(eps)\r\n    param += -step_size * (exp_avg / denom)\r\n\r\nfrom numba import cuda\r\n@cuda.jit\r\ndef kernel_numba_cuda(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    i = cuda.grid(1)\r\n    if i > grad.size:\r\n        return\r\n\r\n    exp_avg[i] = exp_avg[i] * beta1 + (1 - beta1) + grad[i]\r\n    exp_avg_sq[i] = exp_avg_sq[i] * beta2 + (1 - beta2) *grad[i]*grad[i]\r\n    denom = math.sqrt(exp_avg_sq[i]) + eps\r\n    param[i] += -step_size * (exp_avg[i] / denom)\r\n\r\ndef kernel_numba(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n\r\n    import ctypes\r\n    import numpy as np\r\n    def get_devicendarray(t):\r\n        assert t.type() == 'torch.cuda.FloatTensor'\r\n        numpy_dtype = np.float32\r\n        itemsize = 4\r\n\r\n        ctx = cuda.cudadrv.driver.driver.get_context()\r\n        mp = cuda.cudadrv.driver.MemoryPointer(ctx, ctypes.c_ulong(t.data_ptr()), t.numel()*itemsize)\r\n        return cuda.cudadrv.devicearray.DeviceNDArray(t.numel(), itemsize, numpy_dtype(), gpu_data=mp, stream=torch.cuda.current_stream().cuda_stream)\r\n\r\n    numba_param = get_devicendarray(param)\r\n    numba_grad = get_devicendarray(grad)\r\n    numba_exp_avg = get_devicendarray(exp_avg)\r\n    numba_exp_avg_sq = get_devicendarray(exp_avg_sq)\r\n    threadsperblock = 32\r\n    blockspergrid = (param.numel() + (threadsperblock - 1)) // threadsperblock\r\n    kernel_numba_cuda[blockspergrid, threadsperblock](numba_param, numba_grad, numba_exp_avg, numba_exp_avg_sq, beta1, beta2, step_size, eps)\r\n\r\nkernels = [(\"native \", kernel_native), (\"TScript\", kernel_jit_script), (\"numba  \", kernel_numba)]\r\n[Truncated]\n      size_t t1_dimIndex0 = t1_linearIndex ;\r\n      t1_offset += t1_dimIndex0 ;\r\n      IndexType t3_offset = 0;\r\n      IndexType t3_linearIndex = linearIndex;\r\n      \r\n      //printf(\"tensor t3 sizes[0] = %d, strides[0] = %d\\n\", t3.sizes[0],t3.strides[0]);\r\n      size_t t3_dimIndex0 = t3_linearIndex ;\r\n      t3_offset += t3_dimIndex0 ;\r\n      \r\n      // calculate the results\r\n      float n0 = __ldg(&t0.data[t0_offset]);\r\n      float n1 = __ldg(&t1.data[t1_offset]);\r\n      double n2 = s2;\r\n      float n3 = n0 / n1;\r\n      float n4 = n3 * ((float) n2);\r\n      t3.data[t3_offset] = n4;\r\n      \r\n    }\r\n}\r\n```\n<issue_comment>username_1: @username_0 can you try removing all the in-place operations in your scripted version?\r\nIIRC in-place ops block fusion.\r\n\r\nI was btw able to fuse most of sparse adam by removing those in-place ops, see https://github.com/pytorch/pytorch/pull/23522#issuecomment-516952462\n<issue_comment>username_2: cc: @username_3 and also relevant WIP PR: [JIT version of Adagrad](https://github.com/pytorch/pytorch/pull/23640)\n<issue_comment>username_3: Yeah the CUDA fusion does not support fusing in-place operations yet, Can you try the suggestion by @username_1 to see if you get a better perf?\n<issue_comment>username_3: @username_0 Also, it seems like you are only timing it in 3 iterations, and the perf numbers seems including the compilation time. The first time JIT/TorchScript sees an input it will specialize and do optimizations against that input, so when comparing performance please allow a compilation time and runtime optimization warmup for the input.\n<issue_comment>username_4: @username_3 That made a big difference to my traced module too. Any other tricks like this? It slows down the non-JIT version, however.\n<issue_comment>username_3: @username_4 That's pretty much all, let me summarize:\r\n\r\n1. JIT fusion does not support in-place operation fusion and reduction operation yet.\r\n2. we did the fusion code generation at the first time we see an input that the shape we haven't seen before (like if the number of dimensions are different), this will help us generate faster cuda kernels. \r\n\r\nSo the TorchScript compilation time will be gone after compile and the first run :)\n<issue_comment>username_0: Thanks for the feedback!\r\n\r\nI have removed all in-place operations and added three kernel implementations:\r\n```python\r\n\r\nimport math\r\nimport torch\r\nimport time\r\nimport random\r\nimport sys\r\n\r\ndef kernel_native(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n    exp_avg.mul_(beta1).add_(1 - beta1, grad)\r\n    exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\r\n    denom = exp_avg_sq.sqrt().add_(eps)\r\n    param.addcdiv_(-step_size, exp_avg, denom)   \r\n\r\n@torch.jit.script\r\ndef kernel_jit_script1(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n    exp_avg *= beta1\r\n    exp_avg += (1 - beta1) + grad\r\n    exp_avg_sq *= beta2\r\n    exp_avg_sq += (1 - beta2)*(grad*grad)\r\n    param += -step_size * (exp_avg / exp_avg_sq.sqrt() + eps)\r\n\r\n@torch.jit.script\r\ndef kernel_jit_script2(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n    exp_avg[:] = exp_avg * beta1 + (1 - beta1) + grad\r\n    exp_avg_sq[:] = exp_avg_sq * beta2 + (1 - beta2)*(grad*grad)\r\n    param[:] = param -step_size * (exp_avg / exp_avg_sq.sqrt() + eps)\r\n\r\n@torch.jit.script\r\ndef kernel_jit_script3(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> Tuple[Tensor, Tensor, Tensor, Tensor]\r\n    exp_avg = exp_avg * beta1 + (1 - beta1) + grad\r\n    exp_avg_sq = exp_avg_sq * beta2 + (1 - beta2)*(grad*grad)\r\n    param = param -step_size * (exp_avg / exp_avg_sq.sqrt() + eps)\r\n    return (param, grad, exp_avg, exp_avg_sq)\r\n\r\n\r\nfrom numba import cuda\r\n@cuda.jit\r\ndef kernel_numba_cuda(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    i = cuda.grid(1)\r\n    if i > grad.size:\r\n        return\r\n\r\n    exp_avg[i] = exp_avg[i] * beta1 + (1 - beta1) + grad[i]\r\n    exp_avg_sq[i] = exp_avg_sq[i] * beta2 + (1 - beta2) *grad[i]*grad[i]\r\n    denom = math.sqrt(exp_avg_sq[i]) + eps\r\n    param[i] += -step_size * (exp_avg[i] / denom)\r\n\r\ndef kernel_numba(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps):\r\n    # type: (Tensor, Tensor, Tensor, Tensor, float, float, float, float) -> None\r\n\r\n    import ctypes\r\n    import numpy as np\r\n    def get_devicendarray(t):\r\n        assert t.type() == 'torch.cuda.FloatTensor'\r\n        numpy_dtype = np.float32\r\n        itemsize = 4\r\n\r\n        ctx = cuda.cudadrv.driver.driver.get_context()\r\n        mp = cuda.cudadrv.driver.MemoryPointer(ctx, ctypes.c_ulong(t.data_ptr()), t.numel()*itemsize)\r\n        return cuda.cudadrv.devicearray.DeviceNDArray(t.numel(), itemsize, numpy_dtype(), gpu_data=mp, stream=torch.cuda.current_stream().cuda_stream)\r\n\r\n    numba_param = get_devicendarray(param)\r\n    numba_grad = get_devicendarray(grad)\r\n    numba_exp_avg = get_devicendarray(exp_avg)\r\n    numba_exp_avg_sq = get_devicendarray(exp_avg_sq)\r\n    threadsperblock = 32\r\n    blockspergrid = (param.numel() + (threadsperblock - 1)) // threadsperblock\r\n    kernel_numba_cuda[blockspergrid, threadsperblock](numba_param, numba_grad, numba_exp_avg, numba_exp_avg_sq, beta1, beta2, step_size, eps)\r\n\r\nkernels = [(\"native \", kernel_native), (\"numba  \", kernel_numba)]\r\nshape = (4194304*64,)\r\nparam, grad, exp_avg, exp_avg_sq = [torch.rand(shape).to(\"cuda\") for x in range(4)]\r\n[Truncated]\n        res = func(param, grad, exp_avg, exp_avg_sq, beta1, beta2, step_size, eps)\r\n        if res is not None:\r\n            param[:], _, exp_avg[:], exp_avg_sq[:] = res\r\n        torch.cuda.synchronize()\r\n        t2 = time.time()\r\n        print(\"%s: \" % name, t2-t1)\r\n```\r\nThe results are:\r\n```\r\nnative :  0.0323\r\nnumba  :  0.0147\r\nTScript1:  0.0369\r\nTScript2:  0.0352\r\nTScript3:  0.0280\r\n```\r\nTScript3 is now better than native but still far from numba. TScript3 is now generating a single CUDA kernel but the back-copying of the result now dominates the timing. If I remove the line `param[:], _, exp_avg[:], exp_avg_sq[:] = res` the performance is almost on pair with numba:\r\n```\r\nTScript3 (no back-copy):  0.01664\r\n```\r\nThe problem is I really do need to update the values :/\n<issue_comment>username_5: cc @mruberry","repo":"pytorch/pytorch","issue_id":475728990,"issue_number":23655}
{"question":"What change was made in the TensorIterator regarding name inference for binary ops in the Pull Request #25563 from ghstack?","answer":"The change involved computing names before shape checks and propagating them after the binary ops are finished.","id":183,"text":"Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#25563 Fix binary op name inference to happen before shape checks**\n\nBefore, for binary ops, name inference occurred after shape checks. This\ndefeats the purposes for names because the names are supposed to tell\nthe user that i.e. their tensors are misaligned or that they are adding\nincompatible tensors.\n\nThis PR changes TensorIterator so that names are computed before shape checks and\npropagated after the binary ops are finished. In order to support this,\nthis PR makes the following changes:\n- adds a `names_` field to TensorIterator, similar to `shape_`. This is\nnecessary to hold the output names, that are computed in\n`compute_names`, until they are used in `propagate_names_to_outputs()`.\n\nDifferential Revision: [D17158869](https://our.internmc.facebook.com/intern/diff/D17158869)","context":"<issue_start><issue_comment>Title: Fix binary op name inference to happen before shape checks\nusername_0: Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#25563 Fix binary op name inference to happen before shape checks**\n\nBefore, for binary ops, name inference occurred after shape checks. This\ndefeats the purposes for names because the names are supposed to tell\nthe user that i.e. their tensors are misaligned or that they are adding\nincompatible tensors.\n\nThis PR changes TensorIterator so that names are computed before shape checks and\npropagated after the binary ops are finished. In order to support this,\nthis PR makes the following changes:\n- adds a `names_` field to TensorIterator, similar to `shape_`. This is\nnecessary to hold the output names, that are computed in\n`compute_names`, until they are used in `propagate_names_to_outputs()`.\n\nDifferential Revision: [D17158869](https://our.internmc.facebook.com/intern/diff/D17158869)","repo":"pytorch/pytorch","issue_id":488354709,"issue_number":25563}
{"question":"What issue did the author face while running a quantized model and how did they address it?","answer":"The author faced assertions in the code related to variables when running a quantized model. They addressed the issue by commenting out the assert statements to be able to run the model.","id":185,"text":"I got a chance to quickly test this out with my changes that run a quantized model. Seems like the flow now hits assertions in the code where it expects to not get a variable.\r\n\r\nFirst hit - \r\n`abort_message: assertion \"terminating with uncaught exception of type std::runtime_error: !self.is_variable() INTERNAL ASSERT FAILED at../aten/src/ATen/quantized/Quantizer.cpp`\r\n\r\nSecond hit\r\n`abort_message: assertion \"terminating with uncaught exception of type std::runtime_error: !options.is_variable() INTERNAL ASSERT FAILED at../aten/src/ATen/native/TensorFactories.cpp`\r\n\r\nI was able to run the model after commenting out both those assert statements. I'm guessing there may be other parts of the code where similar change needs to be made.","context":"<issue_start><issue_comment>Title: Switch static dispatch to use extractLegacyTypeId.\nusername_0: Stack from [ghstack](https://github.com/username_0/ghstack):\n* **#26813 Switch static dispatch to use extractLegacyTypeId.**\n\nAttempt to fix #26764\n<issue_comment>username_1: I got a chance to quickly test this out with my changes that run a quantized model. Seems like the flow now hits assertions in the code where it expects to not get a variable.\r\n\r\nFirst hit - \r\n`abort_message: assertion \"terminating with uncaught exception of type std::runtime_error: !self.is_variable() INTERNAL ASSERT FAILED at../aten/src/ATen/quantized/Quantizer.cpp`\r\n\r\nSecond hit\r\n`abort_message: assertion \"terminating with uncaught exception of type std::runtime_error: !options.is_variable() INTERNAL ASSERT FAILED at../aten/src/ATen/native/TensorFactories.cpp`\r\n\r\nI was able to run the model after commenting out both those assert statements. I'm guessing there may be other parts of the code where similar change needs to be made.\n<issue_comment>username_0: OK. So basically, this patch is a bandaid: the real problem is there is Tensor-Variable confusion somewhere in the code here, and that is triggering the asserts. While I don't feel too bad about unblocking by removing the asserts on mobile only, if this is reproducible server side it indicates a very real problem. My guess would be in the new quantization code :)\r\n\r\nIs it at all possible to run these mobile models, but on server? That would help diagnose further,.\n<issue_comment>username_2: fyi, it is possible with the \"scripts/build_mobile.sh\" script I recently created, for exactly this purpose.\r\nBut for this specific problem it involves some other not-yet-landed changes from others so it's more complicated debugging process. I updated the issue with some new findings.\n<issue_comment>username_0: I think this is subsumed by #26908.","repo":"pytorch/pytorch","issue_id":498366431,"issue_number":26813}
{"question":"What issue is mentioned in the code that triggers asserts on mobile devices?","answer":"Tensor-Variable confusion in the code triggers asserts on mobile devices.","id":186,"text":"OK. So basically, this patch is a bandaid: the real problem is there is Tensor-Variable confusion somewhere in the code here, and that is triggering the asserts. While I don't feel too bad about unblocking by removing the asserts on mobile only, if this is reproducible server side it indicates a very real problem. My guess would be in the new quantization code :)\r\n\r\nIs it at all possible to run these mobile models, but on server? That would help diagnose further,.","context":"<issue_start><issue_comment>Title: Switch static dispatch to use extractLegacyTypeId.\nusername_0: Stack from [ghstack](https://github.com/username_0/ghstack):\n* **#26813 Switch static dispatch to use extractLegacyTypeId.**\n\nAttempt to fix #26764\n<issue_comment>username_1: I got a chance to quickly test this out with my changes that run a quantized model. Seems like the flow now hits assertions in the code where it expects to not get a variable.\r\n\r\nFirst hit - \r\n`abort_message: assertion \"terminating with uncaught exception of type std::runtime_error: !self.is_variable() INTERNAL ASSERT FAILED at../aten/src/ATen/quantized/Quantizer.cpp`\r\n\r\nSecond hit\r\n`abort_message: assertion \"terminating with uncaught exception of type std::runtime_error: !options.is_variable() INTERNAL ASSERT FAILED at../aten/src/ATen/native/TensorFactories.cpp`\r\n\r\nI was able to run the model after commenting out both those assert statements. I'm guessing there may be other parts of the code where similar change needs to be made.\n<issue_comment>username_0: OK. So basically, this patch is a bandaid: the real problem is there is Tensor-Variable confusion somewhere in the code here, and that is triggering the asserts. While I don't feel too bad about unblocking by removing the asserts on mobile only, if this is reproducible server side it indicates a very real problem. My guess would be in the new quantization code :)\r\n\r\nIs it at all possible to run these mobile models, but on server? That would help diagnose further,.\n<issue_comment>username_2: fyi, it is possible with the \"scripts/build_mobile.sh\" script I recently created, for exactly this purpose.\r\nBut for this specific problem it involves some other not-yet-landed changes from others so it's more complicated debugging process. I updated the issue with some new findings.\n<issue_comment>username_0: I think this is subsumed by #26908.","repo":"pytorch/pytorch","issue_id":498366431,"issue_number":26813}
{"question":"What script did the individual create for a specific purpose and mention in the text chunk?","answer":"The individual created the script \"scripts/build_mobile.sh\" for a specific purpose.","id":187,"text":"fyi, it is possible with the \"scripts/build_mobile.sh\" script I recently created, for exactly this purpose.\r\nBut for this specific problem it involves some other not-yet-landed changes from others so it's more complicated debugging process. I updated the issue with some new findings.","context":"<issue_start><issue_comment>Title: Switch static dispatch to use extractLegacyTypeId.\nusername_0: Stack from [ghstack](https://github.com/username_0/ghstack):\n* **#26813 Switch static dispatch to use extractLegacyTypeId.**\n\nAttempt to fix #26764\n<issue_comment>username_1: I got a chance to quickly test this out with my changes that run a quantized model. Seems like the flow now hits assertions in the code where it expects to not get a variable.\r\n\r\nFirst hit - \r\n`abort_message: assertion \"terminating with uncaught exception of type std::runtime_error: !self.is_variable() INTERNAL ASSERT FAILED at../aten/src/ATen/quantized/Quantizer.cpp`\r\n\r\nSecond hit\r\n`abort_message: assertion \"terminating with uncaught exception of type std::runtime_error: !options.is_variable() INTERNAL ASSERT FAILED at../aten/src/ATen/native/TensorFactories.cpp`\r\n\r\nI was able to run the model after commenting out both those assert statements. I'm guessing there may be other parts of the code where similar change needs to be made.\n<issue_comment>username_0: OK. So basically, this patch is a bandaid: the real problem is there is Tensor-Variable confusion somewhere in the code here, and that is triggering the asserts. While I don't feel too bad about unblocking by removing the asserts on mobile only, if this is reproducible server side it indicates a very real problem. My guess would be in the new quantization code :)\r\n\r\nIs it at all possible to run these mobile models, but on server? That would help diagnose further,.\n<issue_comment>username_2: fyi, it is possible with the \"scripts/build_mobile.sh\" script I recently created, for exactly this purpose.\r\nBut for this specific problem it involves some other not-yet-landed changes from others so it's more complicated debugging process. I updated the issue with some new findings.\n<issue_comment>username_0: I think this is subsumed by #26908.","repo":"pytorch/pytorch","issue_id":498366431,"issue_number":26813}
{"question":"What caused the build failure in the CircleCI for the pytorch_xla_linux_xenial_py3_6_clang7_build?","answer":"The build failure in the CircleCI for the pytorch_xla_linux_xenial_py3_6_clang7_build was caused by errors related to 'DispatchKey' in the namespace 'at', which suggested incorrect references and potential issues in the code implementation.","id":189,"text":"## :pill: CircleCI build failures summary and remediations\nAs of commit bf488282:\n\n\n* **1/1** failures introduced in this PR\n\n\n## Detailed failure analysis\n\nOne may explore the probable reasons each build failed interactively [on the Dr. CI website](https://dr.pytorch.org/commit-details.html?sha1=bf488282b9deb369e76de6cfffb0bdec2aed214f).\n\n### :detective: 1 new failure recognized by patterns\nThe following build failures do not appear to be due to upstream breakage:\n#### [![See CircleCI build](https://avatars0.githubusercontent.com/ml/7?s=12)](https://circleci.com/gh/pytorch/pytorch/4266171) pytorch_xla_linux_xenial_py3_6_clang7_build (1/1)\n**Step:** \"Build\" ([full log](https://dr.pytorch.org/api/view-log-full?build_id=86837350) | [pattern match details](https://dr.pytorch.org/build-details.html?build_id=86837350))\n<details>\n<summary>\n<code>Jan 16 05:27:23 torch_xla/csrc/aten_xla_type_default.cpp:9339:103: error: no member named 'DispatchKey' in namespace 'at'; did you mean 'Dispatcher'?</code>\n</summary>\n\n```\nJan 16 05:27:22 /opt/conda/lib/python3.6/site-packages/torch/include/ATen/core/dispatch/Dispatcher.h:35:18: note: 'Dispatcher' declared here \nJan 16 05:27:22 class CAFFE2_API Dispatcher final { \nJan 16 05:27:22                  ^ \nJan 16 05:27:23 torch_xla/csrc/aten_xla_type_default.cpp:9336:138: error: no member named 'DispatchKey' in namespace 'at'; did you mean 'Dispatcher'? \nJan 16 05:27:23       .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &, const at::Tensor &, bool), &AtenXlaTypeDefault::_cholesky_solve_helper>(at::DispatchKey::XLATensorId) \nJan 16 05:27:23                                                                                                                                      ~~~~^~~~~~~~~~~ \nJan 16 05:27:23                                                                                                                                          Dispatcher \nJan 16 05:27:23 /opt/conda/lib/python3.6/site-packages/torch/include/ATen/core/dispatch/Dispatcher.h:35:18: note: 'Dispatcher' declared here \nJan 16 05:27:23 class CAFFE2_API Dispatcher final { \nJan 16 05:27:23                  ^ \nJan 16 05:27:23 torch_xla/csrc/aten_xla_type_default.cpp:9339:103: error: no member named 'DispatchKey' in namespace 'at'; did you mean 'Dispatcher'? \nJan 16 05:27:23       .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &, bool), &AtenXlaTypeDefault::_coalesced_>(at::DispatchKey::XLATensorId) \nJan 16 05:27:23                                                                                                   ~~~~^~~~~~~~~~~ \nJan 16 05:27:23                                                                                                       Dispatcher \nJan 16 05:27:23 /opt/conda/lib/python3.6/site-packages/torch/include/ATen/core/dispatch/Dispatcher.h:35:18: note: 'Dispatcher' declared here \nJan 16 05:27:23 class CAFFE2_API Dispatcher final { \nJan 16 05:27:23                  ^ \nJan 16 05:27:23 fatal error: too many errors emitted, stopping now [-ferror-limit=] \nJan 16 05:27:23 20 errors generated. \npackages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.6/site-packages/torch/include/TH -I/opt/conda/lib/python3.6/site-packages/torch/include/THC -I/opt/conda/include/python3.6m -c torch_xla/csrc/layout_manager.cpp -o build/temp.linux-x86_64-3.6/torch_xla/csrc/layout_manager.o -std=c++14 -Wno-sign-compare -Wno-deprecated-declarations -Wno-return-type -Wno-macro-redefined -Wno-return-std-move -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=_XLAC -D_GLIBCXX_USE_CXX11_ABI=1 \n/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.6/site-packages/torch/include/TH -I/opt/conda/lib/python3.6/site-packages/torch/include/THC -I/opt/conda/include/python3.6m -c torch_xla/csrc/view.cpp -o build/temp.linux-x86_64-3.6/torch_xla/csrc/view.o -std=c++14 -Wno-sign-compare -Wno-deprecated-declarations -Wno-return-type -Wno-macro-redefined -Wno-return-std-move -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=_XLAC -D_GLIBCXX_USE_CXX11_ABI=1 \n```\n\n</details>\n\n\n\n---\n\n<details><summary>This comment was automatically generated by <a href=\"https://github.com/username_1/circleci-failure-tracker/tree/master/docs/from-pull-request-comment\">Dr. CI</a> (expand for details).</summary>Follow <a href=\"https://dr.pytorch.org/admin/comments-opt-out.html\">this link to opt-out</a> of these comments for your Pull Requests.\n\nPlease report bugs/suggestions on the <a href=\"https://github.com/username_1/circleci-failure-tracker/issues\">GitHub issue tracker</a>.\n</details>","context":"<issue_start><issue_comment>Title: [WIP] faster bailout tests\nusername_0: \n<issue_comment>username_1: ## :pill: CircleCI build failures summary and remediations\nAs of commit bf488282:\n\n\n* **1/1** failures introduced in this PR\n\n\n## Detailed failure analysis\n\nOne may explore the probable reasons each build failed interactively [on the Dr. CI website](https://dr.pytorch.org/commit-details.html?sha1=bf488282b9deb369e76de6cfffb0bdec2aed214f).\n\n### :detective: 1 new failure recognized by patterns\nThe following build failures do not appear to be due to upstream breakage:\n#### [![See CircleCI build](https://avatars0.githubusercontent.com/ml/7?s=12)](https://circleci.com/gh/pytorch/pytorch/4266171) pytorch_xla_linux_xenial_py3_6_clang7_build (1/1)\n**Step:** \"Build\" ([full log](https://dr.pytorch.org/api/view-log-full?build_id=86837350) | [pattern match details](https://dr.pytorch.org/build-details.html?build_id=86837350))\n<details>\n<summary>\n<code>Jan 16 05:27:23 torch_xla/csrc/aten_xla_type_default.cpp:9339:103: error: no member named 'DispatchKey' in namespace 'at'; did you mean 'Dispatcher'?</code>\n</summary>\n\n```\nJan 16 05:27:22 /opt/conda/lib/python3.6/site-packages/torch/include/ATen/core/dispatch/Dispatcher.h:35:18: note: 'Dispatcher' declared here \nJan 16 05:27:22 class CAFFE2_API Dispatcher final { \nJan 16 05:27:22                  ^ \nJan 16 05:27:23 torch_xla/csrc/aten_xla_type_default.cpp:9336:138: error: no member named 'DispatchKey' in namespace 'at'; did you mean 'Dispatcher'? \nJan 16 05:27:23       .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &, const at::Tensor &, bool), &AtenXlaTypeDefault::_cholesky_solve_helper>(at::DispatchKey::XLATensorId) \nJan 16 05:27:23                                                                                                                                      ~~~~^~~~~~~~~~~ \nJan 16 05:27:23                                                                                                                                          Dispatcher \nJan 16 05:27:23 /opt/conda/lib/python3.6/site-packages/torch/include/ATen/core/dispatch/Dispatcher.h:35:18: note: 'Dispatcher' declared here \nJan 16 05:27:23 class CAFFE2_API Dispatcher final { \nJan 16 05:27:23                  ^ \nJan 16 05:27:23 torch_xla/csrc/aten_xla_type_default.cpp:9339:103: error: no member named 'DispatchKey' in namespace 'at'; did you mean 'Dispatcher'? \nJan 16 05:27:23       .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &, bool), &AtenXlaTypeDefault::_coalesced_>(at::DispatchKey::XLATensorId) \nJan 16 05:27:23                                                                                                   ~~~~^~~~~~~~~~~ \nJan 16 05:27:23                                                                                                       Dispatcher \nJan 16 05:27:23 /opt/conda/lib/python3.6/site-packages/torch/include/ATen/core/dispatch/Dispatcher.h:35:18: note: 'Dispatcher' declared here \nJan 16 05:27:23 class CAFFE2_API Dispatcher final { \nJan 16 05:27:23                  ^ \nJan 16 05:27:23 fatal error: too many errors emitted, stopping now [-ferror-limit=] \nJan 16 05:27:23 20 errors generated. \npackages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.6/site-packages/torch/include/TH -I/opt/conda/lib/python3.6/site-packages/torch/include/THC -I/opt/conda/include/python3.6m -c torch_xla/csrc/layout_manager.cpp -o build/temp.linux-x86_64-3.6/torch_xla/csrc/layout_manager.o -std=c++14 -Wno-sign-compare -Wno-deprecated-declarations -Wno-return-type -Wno-macro-redefined -Wno-return-std-move -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=_XLAC -D_GLIBCXX_USE_CXX11_ABI=1 \n/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.6/site-packages/torch/include/TH -I/opt/conda/lib/python3.6/site-packages/torch/include/THC -I/opt/conda/include/python3.6m -c torch_xla/csrc/view.cpp -o build/temp.linux-x86_64-3.6/torch_xla/csrc/view.o -std=c++14 -Wno-sign-compare -Wno-deprecated-declarations -Wno-return-type -Wno-macro-redefined -Wno-return-std-move -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=_XLAC -D_GLIBCXX_USE_CXX11_ABI=1 \n```\n\n</details>\n\n\n\n---\n\n<details><summary>This comment was automatically generated by <a href=\"https://github.com/username_1/circleci-failure-tracker/tree/master/docs/from-pull-request-comment\">Dr. CI</a> (expand for details).</summary>Follow <a href=\"https://dr.pytorch.org/admin/comments-opt-out.html\">this link to opt-out</a> of these comments for your Pull Requests.\n\nPlease report bugs/suggestions on the <a href=\"https://github.com/username_1/circleci-failure-tracker/issues\">GitHub issue tracker</a>.\n</details>\n<issue_comment>username_0: I put together a small benchmark that runs 10,000 `prim::BailOut`s and no real ops. Even though, the fluctuations are huge on a devserver, `isCompatibleWith` is definitely much faster (4-8x).\r\n\r\n**baseline:**\r\n```\r\ntime = 0.224115446\r\ntime = 0.241094661\r\ntime = 0.206836688\r\ntime = 0.298701322\r\ntime = 0.236710434\r\ntime = 0.224483698\r\ntime = 0.224836235\r\ntime = 0.198836114\r\ntime = 0.187320452\r\ntime = 0.208056021\r\n```\r\n\r\n**isCompatibleWith:**\r\n\r\n```\r\ntime = 0.044660287\r\ntime = 0.045000739\r\ntime = 0.048079375\r\ntime = 0.085522459\r\ntime = 0.05170733\r\ntime = 0.047986804\r\ntime = 0.075046798\r\ntime = 0.075740901\r\ntime = 0.049676586\r\ntime = 0.046801684\r\n```","repo":"pytorch/pytorch","issue_id":550556777,"issue_number":32266}
